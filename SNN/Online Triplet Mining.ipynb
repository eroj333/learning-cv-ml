{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNN Tensorboard",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eroj333/learning-cv-ml/blob/master/SNN/Online%20Triplet%20Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FbN2h16Sjh3",
        "colab_type": "code",
        "outputId": "d67975e8-9c23-4e0a-9afa-1c8793b1c57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras as k\n",
        "import keras.backend as K\n",
        "import numpy as np \n",
        "from keras.layers import *\n",
        "from keras.models import Sequential, Model\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam, Adadelta\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbeTnb8ZTrXf",
        "colab_type": "code",
        "outputId": "99b4911c-f04d-47b0-b008-4b9e6466f4c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# Load Fashion MNIST dataset\n",
        "(x_train_master, y_train_master), (x_test_master, y_test_master) = k.datasets.fashion_mnist.load_data()\n",
        "x_train_master = x_train_master /  255\n",
        "x_test_master = x_test_master / 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdaGvyvWKL0G",
        "colab_type": "code",
        "outputId": "f99623e4-d41f-4897-d905-d23ad4970ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_train_master.shape, x_test_master.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (10000, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-UwbScVTr7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_master = np.expand_dims(x_train_master, 3)\n",
        "x_test_master = np.expand_dims(x_test_master, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULCigEvnTt33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_master = np.expand_dims(y_train_master, 1)\n",
        "y_test_master = np.expand_dims(y_test_master, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IkIqPueTvwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_per_class = 400\n",
        "n_epochs = 200 + 1\n",
        "batch_size = 20 # less is better\n",
        "embedding_dim = 50  # output of the network\n",
        "embeddings_freq = 100 # frequency to save tensorboard callback data "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OiJClR-Z2yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_classifier_data(dataset, label, sample_per_class=10):\n",
        "  x, y = None, None\n",
        "  for i in range(10):\n",
        "    pos_indices = np.argwhere(label == i)[:,0]\n",
        "\n",
        "    # print(\"pos indices: {}, neg_indices: {}\".format(pos_indices.shape, neg_indices.shape))\n",
        "    choice_anchor = np.random.choice(pos_indices.shape[0], sample_per_class, replace=False)\n",
        "    choice_anchor = pos_indices[choice_anchor]\n",
        "\n",
        "    sub_x_anc = dataset[choice_anchor]\n",
        "    \n",
        "        \n",
        "    if(x is None):\n",
        "      x = sub_x_anc\n",
        "      y = label[choice_anchor]\n",
        "    else:\n",
        "      x = np.vstack((x, (sub_x_anc)))\n",
        "\n",
        "      y = np.hstack((y.flatten(), label[choice_anchor].flatten()))\n",
        "    \n",
        "  return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtwC9sz9Sjye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The hard triplets are computed for a batch. so, we need a generator\n",
        "class BatchGenerator(k.utils.Sequence):\n",
        "\n",
        "    def __init__(self, x_set, y_set, batch_size, embedding_dim):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = True\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        indexes = np.arange(len(self.y))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "            self.x = self.x[indexes]\n",
        "            self.y = self.y[indexes]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.y) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "        return [batch_x, batch_y], np.ones(batch_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cO4QX2pZ8qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y = generate_classifier_data(x_train_master, y_train_master, num_train_per_class)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKFvJLpkxP5F",
        "colab_type": "code",
        "outputId": "c8c179d2-16dc-44d8-f6c3-0b7e6bb02c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_x.shape, train_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 28, 28, 1), (4000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MwK6r44awEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batchGenerator = BatchGenerator(train_x.copy(), train_y.copy(), batch_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YsEjHKO1MKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Define functions to create the triplet loss with online triplet mining.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def _pairwise_distances(embeddings, squared=False):\n",
        "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "    Args:\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # Get the dot product between all embeddings\n",
        "    # shape (batch_size, batch_size)\n",
        "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
        "\n",
        "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "    # shape (batch_size,)\n",
        "    square_norm = tf.diag_part(dot_product)\n",
        "\n",
        "    # Compute the pairwise distance matrix as we have:\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "    # shape (batch_size, batch_size)\n",
        "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
        "\n",
        "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "    distances = tf.maximum(distances, 0.0)\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
        "        distances = distances + mask * 1e-16\n",
        "\n",
        "        distances = tf.sqrt(distances)\n",
        "\n",
        "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i and j are distinct\n",
        "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    indices_not_equal = tf.logical_not(indices_equal)\n",
        "\n",
        "    # Check if labels[i] == labels[j]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "\n",
        "    # Combine the two masks\n",
        "    mask = tf.logical_and(indices_not_equal, labels_equal)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check if labels[i] != labels[k]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "\n",
        "    mask = tf.logical_not(labels_equal)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_triplet_mask(labels):\n",
        "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
        "    A triplet (i, j, k) is valid if:\n",
        "        - i, j, k are distinct\n",
        "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i, j and k are distinct\n",
        "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    indices_not_equal = tf.logical_not(indices_equal)\n",
        "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
        "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
        "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
        "\n",
        "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
        "\n",
        "\n",
        "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
        "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
        "\n",
        "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
        "\n",
        "    # Combine the two masks\n",
        "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
        "\n",
        "    return mask\n",
        "  \n",
        "def batch_hard_triplet_loss(labels, embeddings , margin=1, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # For each anchor, get the hardest positive\n",
        "    # First, we need to get a mask for every valid positive (they should have same label)\n",
        "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
        "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
        "\n",
        "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
        "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
        "    \n",
        "    # shape (batch_size, 1)\n",
        "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
        "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
        "\n",
        "    # For each anchor, get the hardest negative\n",
        "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
        "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
        "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
        "\n",
        "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
        "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
        "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
        "\n",
        "    # shape (batch_size,)\n",
        "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
        "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
        "\n",
        "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "    \n",
        "    return triplet_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkKO6ftcWa1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss_function(labels):\n",
        "  def loss(y_true, y_pred):\n",
        "    return batch_hard_triplet_loss(tf.squeeze(labels), y_pred , margin=10, squared=False)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkV0AF9FT6Qj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(inputs, dist='sqeuclidean', margin='maxplus'):\n",
        "    anchor, positive, negative = inputs\n",
        "    positive_distance = K.square(anchor - positive)\n",
        "    negative_distance = K.square(anchor - negative)\n",
        "    if dist == 'euclidean':\n",
        "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True))\n",
        "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
        "    elif dist == 'sqeuclidean':\n",
        "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
        "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
        "    loss = positive_distance - negative_distance\n",
        "    if margin == 'maxplus':\n",
        "        loss = K.maximum(0.0, 1 + loss)\n",
        "    elif margin == 'softplus':\n",
        "        loss = K.log(1 + K.exp(loss))\n",
        "    return K.mean(loss)\n",
        "\n",
        "def get_embedding_model(input_shape, embedding_dim):\n",
        "    _input = Input(shape=input_shape)\n",
        "    x = Flatten()(_input)\n",
        "    x = Dense(embedding_dim * 4,activation=\"relu\")(x)\n",
        "    x = Dense(embedding_dim * 2, activation='relu')(x)\n",
        "    x = Dense(embedding_dim)(x)\n",
        "    return Model(_input, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Lpcr4Aopaq",
        "colab_type": "code",
        "outputId": "71a41ab1-7ab8-4bd9-aa46-bfa8794913e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "\"\"\"\n",
        "    Model architecture\n",
        "\"\"\"\n",
        "input_shape=x_train_master[0].shape\n",
        "label_shape=(1,)\n",
        "# Define the inputs to the network\n",
        "img_inputs = Input(input_shape, name=\"img_input\")\n",
        "labels = Input(label_shape, name=\"label_input\") # this will be used for calculating loss only\n",
        "\n",
        "x = Flatten()(img_inputs)\n",
        "x = Dense(embedding_dim * 4,activation=\"relu\")(x)\n",
        "x = Dense(embedding_dim * 2, activation='relu')(x)\n",
        "encoded_inputs = Dense(embedding_dim, name=\"enc_op\")(x)\n",
        "\n",
        "# concat = Concatenate(axis=-1)([encoded_inputs, labels])\n",
        "# print(concat.shape)\n",
        "\n",
        "inputs = [img_inputs, labels]\n",
        "outputs = encoded_inputs\n",
        "\n",
        "# Connect the inputs with the outputs\n",
        "siamese_model = Model(inputs=inputs,outputs=outputs)\n",
        "encoding_model = Model(inputs=img_inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbzIQpFzav-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding_model, siamese_model = get_siamese_model(x_train_master[0].shape, (1,),batch_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PXAPGYloHJI",
        "colab_type": "code",
        "outputId": "c153a775-1693-42b7-d5d7-f3ff9e716234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "siamese_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "img_input (InputLayer)       (None, 28, 28, 1)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               157000    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "enc_op (Dense)               (None, 50)                5050      \n",
            "=================================================================\n",
            "Total params: 182,150\n",
            "Trainable params: 182,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8mwTa2_aOYe",
        "colab_type": "code",
        "outputId": "ac235574-f736-4947-9fcb-f974de2aafb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "siamese_model.compile(loss=get_loss_function(labels), optimizer=Adam(0.0001))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-12-80679fd7c36c>:35: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH8njVHipRFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfb_test_callback = k.callbacks.TensorBoard(batch_size=batch_size, log_dir=\"./logs/test\",\n",
        "                                         embeddings_freq=embeddings_freq,  \n",
        "                                         embeddings_layer_names=['enc_op'],  # Embeddings are taken from layers\n",
        "                                         embeddings_metadata='metadata_test.tsv',  # This file will describe the embeddings data (see below)\n",
        "                                         embeddings_data=[x_test_master, y_test_master])  # Data used for the embeddings\n",
        "\n",
        "tfb_train_callback = k.callbacks.TensorBoard(batch_size=batch_size, log_dir=\"./logs/train\",\n",
        "                                         embeddings_freq=embeddings_freq,  \n",
        "                                         embeddings_layer_names=['enc_op'],  # Embeddings are taken from layers\n",
        "                                         embeddings_metadata='metadata_train.tsv',  # This file will describe the embeddings data (see below)\n",
        "                                         embeddings_data=[train_x, train_y])  # Data used for the embeddings\n",
        "callbacks = [tfb_train_callback, tfb_test_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PYfHzeYgbyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('./logs'):\n",
        "    os.makedirs('./logs')\n",
        "\n",
        "# save class labels to disk to color data points in TensorBoard accordingly\n",
        "with open(os.path.join(\"./logs\", 'metadata_train.tsv'), 'w') as f:\n",
        "    np.savetxt(f, train_y)\n",
        "\n",
        "with open(os.path.join(\"./logs\", 'metadata_test.tsv'), 'w') as f:\n",
        "    np.savetxt(f, y_test_master)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WcT_WM4kJXt",
        "colab_type": "code",
        "outputId": "a07836d2-6464-4e61-9d32-3feb72b490ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# history = siamese_model.fit(x=classifier_train_x, y=k.utils.to_categorical(classifier_train_y), shuffle=True, batch_size=1000,\n",
        "#                               validation_split=.1, epochs=n_epochs)\n",
        "\n",
        "history = siamese_model.fit_generator(batchGenerator, epochs=n_epochs, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1159: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Epoch 1/201\n",
            "200/200 [==============================] - 2s 12ms/step - loss: 10.0000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 9.9102\n",
            "Epoch 3/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 9.5397\n",
            "Epoch 4/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 9.0435\n",
            "Epoch 5/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 8.7302\n",
            "Epoch 6/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 8.3474\n",
            "Epoch 7/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 8.1759\n",
            "Epoch 8/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 7.9027\n",
            "Epoch 9/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 7.7136\n",
            "Epoch 10/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 7.4492\n",
            "Epoch 11/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 7.3367\n",
            "Epoch 12/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 7.3127\n",
            "Epoch 13/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 7.0455\n",
            "Epoch 14/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.9667\n",
            "Epoch 15/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.7322\n",
            "Epoch 16/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.6398\n",
            "Epoch 17/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.4027\n",
            "Epoch 18/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.4453\n",
            "Epoch 19/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.2672\n",
            "Epoch 20/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 6.3119\n",
            "Epoch 21/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.9839\n",
            "Epoch 22/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.9559\n",
            "Epoch 23/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.8824\n",
            "Epoch 24/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.7911\n",
            "Epoch 25/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.4906\n",
            "Epoch 26/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.6037\n",
            "Epoch 27/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.5353\n",
            "Epoch 28/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.4704\n",
            "Epoch 29/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.3135\n",
            "Epoch 30/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.3660\n",
            "Epoch 31/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.2094\n",
            "Epoch 32/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.1160\n",
            "Epoch 33/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.1531\n",
            "Epoch 34/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 5.0588\n",
            "Epoch 35/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.8970\n",
            "Epoch 36/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 4.8014\n",
            "Epoch 37/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 4.8920\n",
            "Epoch 38/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 4.9504\n",
            "Epoch 39/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.7201\n",
            "Epoch 40/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 4.6278\n",
            "Epoch 41/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 4.5234\n",
            "Epoch 42/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.4420\n",
            "Epoch 43/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.3829\n",
            "Epoch 44/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.4039\n",
            "Epoch 45/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.2783\n",
            "Epoch 46/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.1852\n",
            "Epoch 47/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.1570\n",
            "Epoch 48/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.1116\n",
            "Epoch 49/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.0784\n",
            "Epoch 50/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.0220\n",
            "Epoch 51/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 4.0199\n",
            "Epoch 52/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.8621\n",
            "Epoch 53/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.8556\n",
            "Epoch 54/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.6489\n",
            "Epoch 55/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.8863\n",
            "Epoch 56/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.7959\n",
            "Epoch 57/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.7889\n",
            "Epoch 58/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.6387\n",
            "Epoch 59/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.5551\n",
            "Epoch 60/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.5060\n",
            "Epoch 61/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.4263\n",
            "Epoch 62/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.4740\n",
            "Epoch 63/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.2527\n",
            "Epoch 64/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.3882\n",
            "Epoch 65/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.3693\n",
            "Epoch 66/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 3.1553\n",
            "Epoch 67/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.1383\n",
            "Epoch 68/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.1554\n",
            "Epoch 69/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 2.9884\n",
            "Epoch 70/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 3.0925\n",
            "Epoch 71/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.9381\n",
            "Epoch 72/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.9395\n",
            "Epoch 73/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.8172\n",
            "Epoch 74/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.8059\n",
            "Epoch 75/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.7341\n",
            "Epoch 76/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.8267\n",
            "Epoch 77/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.7860\n",
            "Epoch 78/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 2.5975\n",
            "Epoch 79/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.6226\n",
            "Epoch 80/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.6569\n",
            "Epoch 81/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.6033\n",
            "Epoch 82/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.3799\n",
            "Epoch 83/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.4797\n",
            "Epoch 84/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.3813\n",
            "Epoch 85/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 2.3459\n",
            "Epoch 86/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.3558\n",
            "Epoch 87/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.2576\n",
            "Epoch 88/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.2136\n",
            "Epoch 89/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 2.1676\n",
            "Epoch 90/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 2.1826\n",
            "Epoch 91/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 2.1238\n",
            "Epoch 92/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 2.0215\n",
            "Epoch 93/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.9786\n",
            "Epoch 94/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.9203\n",
            "Epoch 95/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.9910\n",
            "Epoch 96/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.8582\n",
            "Epoch 97/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.7870\n",
            "Epoch 98/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.7932\n",
            "Epoch 99/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.7660\n",
            "Epoch 100/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.6650\n",
            "Epoch 101/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.7657\n",
            "Epoch 102/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.5489\n",
            "Epoch 103/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.7288\n",
            "Epoch 104/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.5289\n",
            "Epoch 105/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.5194\n",
            "Epoch 106/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.5286\n",
            "Epoch 107/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.4016\n",
            "Epoch 108/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.4241\n",
            "Epoch 109/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.4030\n",
            "Epoch 110/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.4207\n",
            "Epoch 111/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.2547\n",
            "Epoch 112/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 1.2290\n",
            "Epoch 113/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.2787\n",
            "Epoch 114/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.2435\n",
            "Epoch 115/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.2086\n",
            "Epoch 116/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0840\n",
            "Epoch 117/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0777\n",
            "Epoch 118/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.1648\n",
            "Epoch 119/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0000\n",
            "Epoch 120/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0319\n",
            "Epoch 121/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0151\n",
            "Epoch 122/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0849\n",
            "Epoch 123/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0463\n",
            "Epoch 124/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.9286\n",
            "Epoch 125/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.8928\n",
            "Epoch 126/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.9072\n",
            "Epoch 127/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.8470\n",
            "Epoch 128/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.7864\n",
            "Epoch 129/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.8381\n",
            "Epoch 130/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.8023\n",
            "Epoch 131/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.7480\n",
            "Epoch 132/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.8039\n",
            "Epoch 133/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.7974\n",
            "Epoch 134/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.7356\n",
            "Epoch 135/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.6819\n",
            "Epoch 136/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.6520\n",
            "Epoch 137/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.7493\n",
            "Epoch 138/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.5938\n",
            "Epoch 139/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.6462\n",
            "Epoch 140/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.6116\n",
            "Epoch 141/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.7124\n",
            "Epoch 142/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.5054\n",
            "Epoch 143/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.5376\n",
            "Epoch 144/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4938\n",
            "Epoch 145/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.4834\n",
            "Epoch 146/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4521\n",
            "Epoch 147/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4246\n",
            "Epoch 148/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.5051\n",
            "Epoch 149/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.5224\n",
            "Epoch 150/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.5240\n",
            "Epoch 151/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4713\n",
            "Epoch 152/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4044\n",
            "Epoch 153/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3575\n",
            "Epoch 154/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4563\n",
            "Epoch 155/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3537\n",
            "Epoch 156/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.3386\n",
            "Epoch 157/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.3581\n",
            "Epoch 158/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3368\n",
            "Epoch 159/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2888\n",
            "Epoch 160/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4008\n",
            "Epoch 161/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.3412\n",
            "Epoch 162/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3276\n",
            "Epoch 163/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3494\n",
            "Epoch 164/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2595\n",
            "Epoch 165/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.2928\n",
            "Epoch 166/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3039\n",
            "Epoch 167/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.2433\n",
            "Epoch 168/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.2601\n",
            "Epoch 169/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.3345\n",
            "Epoch 170/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.2265\n",
            "Epoch 171/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.2349\n",
            "Epoch 172/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2252\n",
            "Epoch 173/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2597\n",
            "Epoch 174/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2278\n",
            "Epoch 175/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2183\n",
            "Epoch 176/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2643\n",
            "Epoch 177/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2316\n",
            "Epoch 178/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2068\n",
            "Epoch 179/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1818\n",
            "Epoch 180/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1766\n",
            "Epoch 181/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2483\n",
            "Epoch 182/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1727\n",
            "Epoch 183/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1735\n",
            "Epoch 184/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.2431\n",
            "Epoch 185/201\n",
            "200/200 [==============================] - 2s 9ms/step - loss: 0.2323\n",
            "Epoch 186/201\n",
            "200/200 [==============================] - 2s 9ms/step - loss: 0.2264\n",
            "Epoch 187/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1754\n",
            "Epoch 188/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1244\n",
            "Epoch 189/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1231\n",
            "Epoch 190/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1715\n",
            "Epoch 191/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1755\n",
            "Epoch 192/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1121\n",
            "Epoch 193/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1629\n",
            "Epoch 194/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1505\n",
            "Epoch 195/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1566\n",
            "Epoch 196/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1067\n",
            "Epoch 197/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1467\n",
            "Epoch 198/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1610\n",
            "Epoch 199/201\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1929\n",
            "Epoch 200/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1383\n",
            "Epoch 201/201\n",
            "200/200 [==============================] - 1s 7ms/step - loss: 0.1433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzfZn_xnbSSS",
        "colab_type": "code",
        "outputId": "4a9a7a87-4746-4eb0-be22-94b6e78f4634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(history.history[\"loss\"], color=\"blue\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0063c4f4a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqdS6VCd6gcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_embeds = encoding_model.predict(train_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3Vwc7vP8Wbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = np.squeeze(train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA5PL3IC6zJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ZHvAx374zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "def fit_nearest_neighbor(img_encoding, img_class, algorithm='ball_tree'):\n",
        "  classifier = KNeighborsClassifier(n_neighbors=3, algorithm=algorithm)\n",
        "  classifier.fit(img_encoding, img_class)\n",
        "  return classifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGSPnb9i8T_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = fit_nearest_neighbor(train_embeds, target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSaWvVOj8iEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e = encoding_model.predict(x_test_master)\n",
        "op = classifier.predict(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWtj1IUG8ky2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classifier.score(e, y_test_master)\n",
        "# target = y_test_master.flatten()\n",
        "# (np.where(target == op))[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzmTDKWN8mSE",
        "colab_type": "code",
        "outputId": "f212ec66-fa97-4d85-87d8-2821f2a5f628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(np.where(np.squeeze(y_test_master) == op))[0].shape[0] / y_test_master.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8358"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5_0N5xw-ds9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(np.squeeze(y_test_master) , op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yvjfC3uJE9M",
        "colab_type": "code",
        "outputId": "8cd1a081-616f-4e39-88de-db6f51081fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mat.diagonal()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([774, 950, 729, 846, 738, 931, 608, 924, 920, 938])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfUwlecCsXuI",
        "colab_type": "code",
        "outputId": "a9621813-f2d2-4ec5-956c-abf56c35f649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logs  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy1_93musSn4",
        "colab_type": "code",
        "outputId": "259a6ea2-0279-41cd-8a87-d93908e29c09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "# !rm SNN_logs.zip\n",
        "!zip -r SNN_logs.zip ./logs/*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: logs/metadata_test.tsv (deflated 96%)\n",
            "  adding: logs/metadata_train.tsv (deflated 100%)\n",
            "  adding: logs/test/ (stored 0%)\n",
            "  adding: logs/test/keras_embedding.ckpt-0.data-00000-of-00001 (deflated 7%)\n",
            "  adding: logs/test/keras_embedding.ckpt-0.meta (deflated 88%)\n",
            "  adding: logs/test/checkpoint (deflated 68%)\n",
            "  adding: logs/test/projector_config.pbtxt (deflated 16%)\n",
            "  adding: logs/test/keras_embedding.ckpt-200.index (deflated 33%)\n",
            "  adding: logs/test/keras_embedding.ckpt-200.data-00000-of-00001 (deflated 7%)\n",
            "  adding: logs/test/keras_embedding.ckpt-100.data-00000-of-00001 (deflated 7%)\n",
            "  adding: logs/test/keras_embedding.ckpt-200.meta (deflated 88%)\n",
            "  adding: logs/test/keras_embedding.ckpt-100.meta (deflated 88%)\n",
            "  adding: logs/test/keras_embedding.ckpt-100.index (deflated 33%)\n",
            "  adding: logs/test/events.out.tfevents.1570676287.57ba76e31ccf (deflated 88%)\n",
            "  adding: logs/test/keras_embedding.ckpt-0.index (deflated 33%)\n",
            "  adding: logs/train/ (stored 0%)\n",
            "  adding: logs/train/keras_embedding.ckpt-0.data-00000-of-00001 (deflated 7%)\n",
            "  adding: logs/train/keras_embedding.ckpt-0.meta (deflated 88%)\n",
            "  adding: logs/train/checkpoint (deflated 68%)\n",
            "  adding: logs/train/projector_config.pbtxt (deflated 16%)\n",
            "  adding: logs/train/keras_embedding.ckpt-200.index (deflated 34%)\n",
            "  adding: logs/train/keras_embedding.ckpt-200.data-00000-of-00001 (deflated 8%)\n",
            "  adding: logs/train/keras_embedding.ckpt-100.data-00000-of-00001 (deflated 8%)\n",
            "  adding: logs/train/keras_embedding.ckpt-200.meta (deflated 88%)\n",
            "  adding: logs/train/keras_embedding.ckpt-100.meta (deflated 88%)\n",
            "  adding: logs/train/keras_embedding.ckpt-100.index (deflated 34%)\n",
            "  adding: logs/train/events.out.tfevents.1570676286.57ba76e31ccf (deflated 88%)\n",
            "  adding: logs/train/keras_embedding.ckpt-0.index (deflated 34%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZg_vJY4tXB",
        "colab_type": "text"
      },
      "source": [
        "Colab doesn't support projection at the time of writing. So, download the zip file and visualize in the local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCjg7qlw1xbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}