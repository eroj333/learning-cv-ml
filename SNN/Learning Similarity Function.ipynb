{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese NN and One shot learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eroj333/learning-cv-ml/blob/master/SNN/Learning%20Similarity%20Function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jgMhXmvgBok",
        "colab_type": "text"
      },
      "source": [
        "# One shot learning and Siamese NN\n",
        "\n",
        "Commonly, the computer vision today are heavily dependent on Deep Convolution networks. The classification models  require large amount of labelled data and predict a probability over fixed output space. This approach has multiple limitations. Firstly, there is a possibility that a large amount of data cannot be collected. Also, another drawback is that we need to retrain the model everytime the output space changes. This is both cumbersome and expensive.\n",
        "\n",
        "One Shot Learning helps to ameliorate the limitaiton of the traditional learning method. Using this method, the NN can learn with as less as a single example of each class. This is accomplished by learning a similarity function instead of a classification model. With new classes of data, we can take a single example that can serve as a reference to the similarity function and the model can identify new instances of the class easily.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqJVTxBTgAm1",
        "colab_type": "code",
        "outputId": "a0954bf1-1666-4851-b4a1-b529637095cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras as k\n",
        "import numpy as np \n",
        "from keras.layers import *\n",
        "from keras.models import Sequential, Model\n",
        "from keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import Adam\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNSY5pfxJVn-",
        "colab_type": "text"
      },
      "source": [
        "# Siamese Neural Network (SNN)\n",
        "\n",
        "SNN is an architecture that employs one shot learning. This type of architecture consists of multiple models that share identical sub components and weights. \n",
        "\n",
        "Here we will look at two type of implementation of the SNN in the field of Computer Vision. With the first implementation we will learn a similarity function to predict if two images belong to same class. In the second part we will generate the face embeddings with the triplet loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaRdV8fI_l-r",
        "colab_type": "text"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qRNwt6mGU5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "8ece9178-b9c3-4fde-f92e-02ceade7eee9"
      },
      "source": [
        "(x_train_master, y_train_master), (x_test_master, y_test_master) = k.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqmZSktubjMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_master = np.expand_dims(x_train_master, axis=3) / 255\n",
        "x_test_master = np.expand_dims(x_test_master, axis=3) / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_YUA4PeISEq",
        "colab_type": "code",
        "outputId": "ef8617ed-358a-481a-b8d7-995f54a4710b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.imshow( np.squeeze(x_train_master[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1da7f0e438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmYiapzeHxon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_tuples(x, y, n_ways):\n",
        "  y = y.flatten()\n",
        "  x_t1 = []\n",
        "  x_t2 = []\n",
        "  target = []\n",
        "  length = len(y)\n",
        "  \n",
        "  for i in range(length):\n",
        "    cur_x, cur_y = x[i], y[i]\n",
        "    choices = np.random.choice(length, n_ways, replace=False)\n",
        "    choices_x, choices_y = x[choices], y[choices]\n",
        "    for j in range(n_ways):\n",
        "      _x, _y = choices_x[j], choices_y[j]\n",
        "      x_t1.append(cur_x)\n",
        "      x_t2.append(_x)\n",
        "      target.append(1 if cur_y == _y else 0)\n",
        "    \n",
        "  return [np.array(x_t1), np.array(x_t2)], np.array(target)\n",
        "\n",
        "def equalize_class_examples(x, y):\n",
        "  x1, x2 = x\n",
        "  y1 = np.where(y == 1) \n",
        "  \n",
        "  n = y1[0].shape[0]\n",
        "  y0 = np.where(y == 0)[0]\n",
        "  y0 = y0[:n]\n",
        "  \n",
        "  xa = np.vstack((x1[y1], x1[y0]))\n",
        "  xb = np.vstack((x2[y1], x2[y0]))\n",
        "  \n",
        "  yp = np.hstack((y[y1], y[y0]))\n",
        "  return [xa, xb], yp\n",
        "\n",
        "def get_training_data(sample_per_class=5, n_ways=5, batch_size = 32, valid_sample_per_class=2, test_sample_per_class=2):\n",
        "  train_x, train_y, valid_x, valid_y, test_x, test_y = None, None, None, None, None, None\n",
        "  \n",
        "  for i in range(10):\n",
        "    indices = np.argwhere(y_train_master == i)\n",
        "    choice = np.random.choice(indices.shape[0], sample_per_class + valid_sample_per_class, replace=False)\n",
        "    choice = indices[choice]\n",
        "\n",
        "    sub_train_x = x_train_master[choice[:sample_per_class]]\n",
        "    sub_train_y = y_train_master[choice[:sample_per_class]]\n",
        "    \n",
        "    sub_valid_x = x_train_master[choice[sample_per_class:]]\n",
        "    sub_valid_y = y_train_master[choice[sample_per_class:]]\n",
        "    \n",
        "    \n",
        "    \n",
        "    t_indices = np.argwhere(y_test_master == i)\n",
        "    t_choice = np.random.choice(t_indices.shape[0], test_sample_per_class, replace=False)\n",
        "    t_choice = t_indices[t_choice]\n",
        "    sub_test_x = x_test_master[t_choice]\n",
        "    sub_test_y = y_test_master[t_choice]\n",
        "    \n",
        "    sub_train_x, sub_train_y, sub_valid_x, sub_valid_y, sub_test_x, sub_test_y = np.squeeze(sub_train_x, axis=1), (sub_train_y), np.squeeze(sub_valid_x, axis=1), (sub_valid_y), np.squeeze(sub_test_x, axis=1), (sub_test_y)\n",
        "    print(train_x is None)\n",
        "    if(train_x is None):\n",
        "      train_x, train_y, valid_x, valid_y, test_x, test_y = sub_train_x, sub_train_y, sub_valid_x, sub_valid_y, sub_test_x, sub_test_y\n",
        "    else:\n",
        "      train_x = np.vstack((train_x, sub_train_x))\n",
        "      train_y = np.vstack((train_y, sub_train_y))\n",
        "  #     train_x.append(sub_train_x)\n",
        "  #     train_y.append(sub_train_y)\n",
        "\n",
        "      valid_x = np.vstack((valid_x, sub_valid_x))\n",
        "      valid_y = np.vstack((valid_y, sub_valid_y))\n",
        "  #     valid_x.append(sub_valid_x)\n",
        "  #     valid_y.append(sub_valid_y)\n",
        "\n",
        "      test_x = np.vstack((test_x, sub_test_x))\n",
        "      test_y = np.vstack((test_y, sub_test_y))\n",
        "\n",
        "  #     test_x.append(sub_test_x)\n",
        "  #     test_y.append(sub_test_y)\n",
        "  print(train_x.shape, train_y.shape)\n",
        "  train_x, train_y = create_tuples(train_x, train_y, n_ways)\n",
        "  train_x, train_y = equalize_class_examples(train_x, train_y)\n",
        "  valid_x, valid_y = create_tuples(valid_x, valid_y, n_ways)\n",
        "  valid_x, valid_y = equalize_class_examples(valid_x, valid_y)\n",
        "  test_x, test_y = create_tuples(test_x, test_y, n_ways)\n",
        "  test_x, test_y = equalize_class_examples(test_x, test_y)\n",
        "  \n",
        "  return train_x, train_y, valid_x, valid_y, test_x, test_y\n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw7279eYhCaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_data(data, data_class, n):\n",
        "  n = min(len(data[0]), n)\n",
        "  print(\"Same class: \", np.where(data_class ==1)[0].shape[0], \"Different class: \", np.where(data_class==0)[0].shape[0])\n",
        "  fig, ax = plt.subplots(n, 2,figsize=(10,40))\n",
        "  inp1, inp2 = data\n",
        "  for i in range(n):\n",
        "    ax[i, 0].imshow(np.squeeze(inp1[i]), cmap=\"gray\")\n",
        "    ax[i, 1].imshow(np.squeeze(inp2[i]), cmap=\"gray\")\n",
        "    # x_class = str(data_class[i].tolist())\n",
        "    # ax[i, 1].set_title(\"class \" + x_class)\n",
        "\n",
        "    ax[i, 0].set_axis_off()\n",
        "    ax[i, 1].set_axis_off()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4mvt8ZOQRy4",
        "colab_type": "code",
        "outputId": "b05d7424-1316-4681-ca04-e1142b59e7f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "train_x, train_y, valid_x, valid_y, test_x, test_y = get_training_data(sample_per_class=50, n_ways=10, batch_size = 32, valid_sample_per_class=20, test_sample_per_class=50)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "(500, 28, 28, 1) (500, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQpGlJ1RUpO",
        "colab_type": "code",
        "outputId": "9a4e39cb-6fe9-40df-c44c-90919e94aea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "visualize_data(train_x, train_y, 10)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Same class:  472 Different class:  472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAieCAYAAABgRmxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzjwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x2880 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFGmV46WK2o",
        "colab_type": "text"
      },
      "source": [
        "# Learning similarity function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Hj0Ugzd2XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initialize_weights = k.initializers.RandomNormal(mean=0.0, stddev=0.51, seed=50001)\n",
        "initialize_bias = k.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=1221)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U6d94LdJU_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_siamese_conv_unit(input_shape):\n",
        "  # Convolutional Neural Network\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (5,5), activation='relu', input_shape=input_shape,\n",
        "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(64, (3,3), activation='relu',\n",
        "                     kernel_initializer=initialize_weights,\n",
        "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Conv2D(100, (1,1), activation='relu', kernel_initializer=initialize_weights,\n",
        "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024, activation='sigmoid', \n",
        "                   kernel_regularizer=l2(1e-3),\n",
        "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
        "    model.add(Dropout(rate=.05))\n",
        "    \n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def get_classifier_model(input_shape):\n",
        "    \"\"\"\n",
        "        Model architecture\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the tensors for the two input images\n",
        "    left_input = Input(input_shape)\n",
        "    right_input = Input(input_shape)\n",
        "    \n",
        "    # Convolutional Neural Network\n",
        "    model = get_siamese_conv_unit(input_shape)\n",
        "    \n",
        "    # Generate the encodings (feature vectors) for the two images\n",
        "    encoded_l = model(left_input)\n",
        "    encoded_r = model(right_input)\n",
        "    \n",
        "    # Add a customized layer to compute the absolute difference between the encodings\n",
        "    L1_layer = Lambda(lambda tensors:k.backend.abs(tensors[0] - tensors[1]))\n",
        "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "    \n",
        "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
        "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
        "    \n",
        "    # Connect the inputs with the outputs\n",
        "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "    \n",
        "    # return the model\n",
        "    return siamese_net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioocbG1WbYJo",
        "colab_type": "code",
        "outputId": "d48634d6-f2f3-47f0-d2ee-2d3d10951707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "ip_shape = (28,28,1)\n",
        "model = get_classifier_model(ip_shape)\n",
        "\n",
        "optimizer = Adam(lr = 0.000001)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3genvZzhX74",
        "colab_type": "code",
        "outputId": "868343f5-aeab-41b7-9c54-4098dba5c78b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "model.summary()\n",
        "# model.get_layer(\"sequential_1\").summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 1024)         2586852     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1024)         0           sequential_1[1][0]               \n",
            "                                                                 sequential_1[2][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            1025        lambda_1[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,587,877\n",
            "Trainable params: 2,587,877\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-ObDXrThiET",
        "colab_type": "code",
        "outputId": "7f7766b8-3d2e-4b99-f9f4-c020c419d9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_x, train_y, epochs=500, validation_data=(valid_x, valid_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 986 samples, validate on 378 samples\n",
            "Epoch 1/5000\n",
            "986/986 [==============================] - 4s 4ms/step - loss: 669.0556 - acc: 0.5375 - val_loss: 669.0113 - val_acc: 0.5159\n",
            "Epoch 2/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 668.9941 - acc: 0.5487 - val_loss: 668.9504 - val_acc: 0.5212\n",
            "Epoch 3/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 668.9235 - acc: 0.5680 - val_loss: 668.8894 - val_acc: 0.5238\n",
            "Epoch 4/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 668.8685 - acc: 0.5497 - val_loss: 668.8283 - val_acc: 0.5238\n",
            "Epoch 5/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 668.8014 - acc: 0.5456 - val_loss: 668.7674 - val_acc: 0.5265\n",
            "Epoch 6/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 668.7516 - acc: 0.5456 - val_loss: 668.7065 - val_acc: 0.5317\n",
            "Epoch 7/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 668.6929 - acc: 0.5467 - val_loss: 668.6458 - val_acc: 0.5344\n",
            "Epoch 8/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 668.6196 - acc: 0.5609 - val_loss: 668.5851 - val_acc: 0.5370\n",
            "Epoch 9/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 668.5499 - acc: 0.5710 - val_loss: 668.5244 - val_acc: 0.5344\n",
            "Epoch 10/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 668.5050 - acc: 0.5527 - val_loss: 668.4638 - val_acc: 0.5344\n",
            "Epoch 11/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 668.4378 - acc: 0.5659 - val_loss: 668.4033 - val_acc: 0.5370\n",
            "Epoch 12/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 668.3736 - acc: 0.5700 - val_loss: 668.3429 - val_acc: 0.5423\n",
            "Epoch 13/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 668.3196 - acc: 0.5700 - val_loss: 668.2824 - val_acc: 0.5397\n",
            "Epoch 14/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 668.2599 - acc: 0.5568 - val_loss: 668.2221 - val_acc: 0.5450\n",
            "Epoch 15/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 668.1922 - acc: 0.5882 - val_loss: 668.1617 - val_acc: 0.5450\n",
            "Epoch 16/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 668.1212 - acc: 0.6004 - val_loss: 668.1015 - val_acc: 0.5450\n",
            "Epoch 17/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 668.0694 - acc: 0.5822 - val_loss: 668.0412 - val_acc: 0.5476\n",
            "Epoch 18/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 668.0152 - acc: 0.5680 - val_loss: 667.9810 - val_acc: 0.5503\n",
            "Epoch 19/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 667.9534 - acc: 0.5700 - val_loss: 667.9207 - val_acc: 0.5476\n",
            "Epoch 20/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 667.8913 - acc: 0.5882 - val_loss: 667.8604 - val_acc: 0.5556\n",
            "Epoch 21/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 667.8319 - acc: 0.5649 - val_loss: 667.8001 - val_acc: 0.5556\n",
            "Epoch 22/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 667.7638 - acc: 0.5832 - val_loss: 667.7398 - val_acc: 0.5556\n",
            "Epoch 23/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 667.7021 - acc: 0.5903 - val_loss: 667.6795 - val_acc: 0.5556\n",
            "Epoch 24/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 667.6260 - acc: 0.6278 - val_loss: 667.6192 - val_acc: 0.5503\n",
            "Epoch 25/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 667.5835 - acc: 0.5923 - val_loss: 667.5588 - val_acc: 0.5503\n",
            "Epoch 26/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 667.5263 - acc: 0.5923 - val_loss: 667.4986 - val_acc: 0.5529\n",
            "Epoch 27/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 667.4516 - acc: 0.6055 - val_loss: 667.4386 - val_acc: 0.5556\n",
            "Epoch 28/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 667.3967 - acc: 0.6045 - val_loss: 667.3783 - val_acc: 0.5556\n",
            "Epoch 29/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 667.3403 - acc: 0.5903 - val_loss: 667.3181 - val_acc: 0.5529\n",
            "Epoch 30/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 667.2783 - acc: 0.6045 - val_loss: 667.2579 - val_acc: 0.5556\n",
            "Epoch 31/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 667.2116 - acc: 0.6085 - val_loss: 667.1977 - val_acc: 0.5529\n",
            "Epoch 32/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 667.1432 - acc: 0.6308 - val_loss: 667.1375 - val_acc: 0.5529\n",
            "Epoch 33/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 667.0926 - acc: 0.6065 - val_loss: 667.0772 - val_acc: 0.5529\n",
            "Epoch 34/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 667.0334 - acc: 0.5994 - val_loss: 667.0171 - val_acc: 0.5556\n",
            "Epoch 35/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 666.9671 - acc: 0.6258 - val_loss: 666.9567 - val_acc: 0.5556\n",
            "Epoch 36/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 666.9120 - acc: 0.6176 - val_loss: 666.8966 - val_acc: 0.5556\n",
            "Epoch 37/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 666.8441 - acc: 0.6176 - val_loss: 666.8363 - val_acc: 0.5556\n",
            "Epoch 38/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 666.7847 - acc: 0.6379 - val_loss: 666.7760 - val_acc: 0.5556\n",
            "Epoch 39/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 666.7337 - acc: 0.6034 - val_loss: 666.7158 - val_acc: 0.5582\n",
            "Epoch 40/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 666.6595 - acc: 0.6339 - val_loss: 666.6556 - val_acc: 0.5556\n",
            "Epoch 41/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 666.6030 - acc: 0.6369 - val_loss: 666.5952 - val_acc: 0.5556\n",
            "Epoch 42/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 666.5513 - acc: 0.6176 - val_loss: 666.5350 - val_acc: 0.5529\n",
            "Epoch 43/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 666.4890 - acc: 0.6288 - val_loss: 666.4747 - val_acc: 0.5529\n",
            "Epoch 44/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 666.4306 - acc: 0.6024 - val_loss: 666.4146 - val_acc: 0.5582\n",
            "Epoch 45/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 666.3732 - acc: 0.6126 - val_loss: 666.3544 - val_acc: 0.5556\n",
            "Epoch 46/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 666.3084 - acc: 0.6024 - val_loss: 666.2943 - val_acc: 0.5529\n",
            "Epoch 47/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 666.2468 - acc: 0.6329 - val_loss: 666.2340 - val_acc: 0.5582\n",
            "Epoch 48/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 666.1870 - acc: 0.6359 - val_loss: 666.1738 - val_acc: 0.5582\n",
            "Epoch 49/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 666.1232 - acc: 0.6278 - val_loss: 666.1135 - val_acc: 0.5608\n",
            "Epoch 50/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 666.0615 - acc: 0.6420 - val_loss: 666.0534 - val_acc: 0.5635\n",
            "Epoch 51/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 665.9972 - acc: 0.6258 - val_loss: 665.9933 - val_acc: 0.5635\n",
            "Epoch 52/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 665.9430 - acc: 0.6197 - val_loss: 665.9331 - val_acc: 0.5661\n",
            "Epoch 53/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 665.8846 - acc: 0.6410 - val_loss: 665.8731 - val_acc: 0.5635\n",
            "Epoch 54/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 665.8276 - acc: 0.6176 - val_loss: 665.8128 - val_acc: 0.5661\n",
            "Epoch 55/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 665.7552 - acc: 0.6531 - val_loss: 665.7526 - val_acc: 0.5635\n",
            "Epoch 56/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 665.6959 - acc: 0.6501 - val_loss: 665.6926 - val_acc: 0.5608\n",
            "Epoch 57/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 665.6372 - acc: 0.6420 - val_loss: 665.6326 - val_acc: 0.5608\n",
            "Epoch 58/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 665.5818 - acc: 0.6217 - val_loss: 665.5725 - val_acc: 0.5608\n",
            "Epoch 59/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 665.5215 - acc: 0.6278 - val_loss: 665.5124 - val_acc: 0.5635\n",
            "Epoch 60/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 665.4568 - acc: 0.6531 - val_loss: 665.4521 - val_acc: 0.5688\n",
            "Epoch 61/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 665.4067 - acc: 0.6258 - val_loss: 665.3921 - val_acc: 0.5688\n",
            "Epoch 62/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 665.3452 - acc: 0.6116 - val_loss: 665.3319 - val_acc: 0.5714\n",
            "Epoch 63/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 665.2822 - acc: 0.6258 - val_loss: 665.2717 - val_acc: 0.5767\n",
            "Epoch 64/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 665.2248 - acc: 0.6166 - val_loss: 665.2116 - val_acc: 0.5741\n",
            "Epoch 65/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 665.1613 - acc: 0.6460 - val_loss: 665.1514 - val_acc: 0.5714\n",
            "Epoch 66/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 665.0970 - acc: 0.6471 - val_loss: 665.0911 - val_acc: 0.5714\n",
            "Epoch 67/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 665.0290 - acc: 0.6369 - val_loss: 665.0309 - val_acc: 0.5714\n",
            "Epoch 68/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 664.9740 - acc: 0.6501 - val_loss: 664.9705 - val_acc: 0.5741\n",
            "Epoch 69/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 664.9152 - acc: 0.6298 - val_loss: 664.9104 - val_acc: 0.5741\n",
            "Epoch 70/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 664.8559 - acc: 0.6379 - val_loss: 664.8501 - val_acc: 0.5741\n",
            "Epoch 71/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 664.7927 - acc: 0.6369 - val_loss: 664.7900 - val_acc: 0.5794\n",
            "Epoch 72/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 664.7297 - acc: 0.6511 - val_loss: 664.7300 - val_acc: 0.5794\n",
            "Epoch 73/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 664.6765 - acc: 0.6613 - val_loss: 664.6696 - val_acc: 0.5794\n",
            "Epoch 74/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 664.6165 - acc: 0.6410 - val_loss: 664.6094 - val_acc: 0.5794\n",
            "Epoch 75/5000\n",
            "986/986 [==============================] - 0s 302us/step - loss: 664.5549 - acc: 0.6471 - val_loss: 664.5493 - val_acc: 0.5767\n",
            "Epoch 76/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 664.4999 - acc: 0.6339 - val_loss: 664.4892 - val_acc: 0.5767\n",
            "Epoch 77/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 664.4309 - acc: 0.6572 - val_loss: 664.4292 - val_acc: 0.5794\n",
            "Epoch 78/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 664.3667 - acc: 0.6562 - val_loss: 664.3691 - val_acc: 0.5820\n",
            "Epoch 79/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 664.3113 - acc: 0.6531 - val_loss: 664.3090 - val_acc: 0.5820\n",
            "Epoch 80/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 664.2530 - acc: 0.6400 - val_loss: 664.2489 - val_acc: 0.5820\n",
            "Epoch 81/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 664.1799 - acc: 0.6744 - val_loss: 664.1888 - val_acc: 0.5820\n",
            "Epoch 82/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 664.1300 - acc: 0.6572 - val_loss: 664.1286 - val_acc: 0.5820\n",
            "Epoch 83/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 664.0699 - acc: 0.6389 - val_loss: 664.0684 - val_acc: 0.5794\n",
            "Epoch 84/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 664.0122 - acc: 0.6521 - val_loss: 664.0082 - val_acc: 0.5794\n",
            "Epoch 85/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 663.9506 - acc: 0.6602 - val_loss: 663.9481 - val_acc: 0.5794\n",
            "Epoch 86/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 663.8888 - acc: 0.6531 - val_loss: 663.8878 - val_acc: 0.5767\n",
            "Epoch 87/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 663.8304 - acc: 0.6623 - val_loss: 663.8276 - val_acc: 0.5741\n",
            "Epoch 88/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 663.7709 - acc: 0.6420 - val_loss: 663.7673 - val_acc: 0.5767\n",
            "Epoch 89/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 663.7036 - acc: 0.6663 - val_loss: 663.7072 - val_acc: 0.5767\n",
            "Epoch 90/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 663.6467 - acc: 0.6582 - val_loss: 663.6470 - val_acc: 0.5767\n",
            "Epoch 91/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 663.5808 - acc: 0.6613 - val_loss: 663.5866 - val_acc: 0.5741\n",
            "Epoch 92/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 663.5247 - acc: 0.6643 - val_loss: 663.5263 - val_acc: 0.5767\n",
            "Epoch 93/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 663.4618 - acc: 0.6704 - val_loss: 663.4660 - val_acc: 0.5741\n",
            "Epoch 94/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 663.4092 - acc: 0.6339 - val_loss: 663.4059 - val_acc: 0.5741\n",
            "Epoch 95/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 663.3469 - acc: 0.6521 - val_loss: 663.3454 - val_acc: 0.5767\n",
            "Epoch 96/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 663.2906 - acc: 0.6481 - val_loss: 663.2852 - val_acc: 0.5767\n",
            "Epoch 97/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 663.2263 - acc: 0.6410 - val_loss: 663.2249 - val_acc: 0.5741\n",
            "Epoch 98/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 663.1685 - acc: 0.6440 - val_loss: 663.1648 - val_acc: 0.5767\n",
            "Epoch 99/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 663.1017 - acc: 0.6592 - val_loss: 663.1046 - val_acc: 0.5794\n",
            "Epoch 100/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 663.0431 - acc: 0.6582 - val_loss: 663.0443 - val_acc: 0.5767\n",
            "Epoch 101/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 662.9764 - acc: 0.6663 - val_loss: 662.9840 - val_acc: 0.5767\n",
            "Epoch 102/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 662.9185 - acc: 0.6633 - val_loss: 662.9238 - val_acc: 0.5767\n",
            "Epoch 103/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 662.8589 - acc: 0.6542 - val_loss: 662.8636 - val_acc: 0.5767\n",
            "Epoch 104/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 662.7980 - acc: 0.6755 - val_loss: 662.8033 - val_acc: 0.5767\n",
            "Epoch 105/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 662.7468 - acc: 0.6653 - val_loss: 662.7431 - val_acc: 0.5820\n",
            "Epoch 106/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 662.6829 - acc: 0.6562 - val_loss: 662.6827 - val_acc: 0.5767\n",
            "Epoch 107/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 662.6192 - acc: 0.6602 - val_loss: 662.6224 - val_acc: 0.5741\n",
            "Epoch 108/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 662.5538 - acc: 0.6785 - val_loss: 662.5621 - val_acc: 0.5767\n",
            "Epoch 109/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 662.4925 - acc: 0.6653 - val_loss: 662.5017 - val_acc: 0.5741\n",
            "Epoch 110/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 662.4334 - acc: 0.6714 - val_loss: 662.4415 - val_acc: 0.5741\n",
            "Epoch 111/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 662.3777 - acc: 0.6552 - val_loss: 662.3811 - val_acc: 0.5741\n",
            "Epoch 112/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 662.3193 - acc: 0.6552 - val_loss: 662.3210 - val_acc: 0.5794\n",
            "Epoch 113/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 662.2545 - acc: 0.6673 - val_loss: 662.2606 - val_acc: 0.5820\n",
            "Epoch 114/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 662.1965 - acc: 0.6542 - val_loss: 662.2002 - val_acc: 0.5847\n",
            "Epoch 115/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 662.1225 - acc: 0.6998 - val_loss: 662.1400 - val_acc: 0.5794\n",
            "Epoch 116/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 662.0866 - acc: 0.6430 - val_loss: 662.0797 - val_acc: 0.5794\n",
            "Epoch 117/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 662.0104 - acc: 0.6785 - val_loss: 662.0192 - val_acc: 0.5820\n",
            "Epoch 118/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 661.9570 - acc: 0.6613 - val_loss: 661.9589 - val_acc: 0.5847\n",
            "Epoch 119/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 661.8950 - acc: 0.6633 - val_loss: 661.8985 - val_acc: 0.5873\n",
            "Epoch 120/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 661.8312 - acc: 0.6795 - val_loss: 661.8382 - val_acc: 0.5926\n",
            "Epoch 121/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 661.7723 - acc: 0.6491 - val_loss: 661.7778 - val_acc: 0.5873\n",
            "Epoch 122/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 661.7203 - acc: 0.6471 - val_loss: 661.7173 - val_acc: 0.5926\n",
            "Epoch 123/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 661.6509 - acc: 0.6673 - val_loss: 661.6569 - val_acc: 0.5926\n",
            "Epoch 124/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 661.5848 - acc: 0.6755 - val_loss: 661.5966 - val_acc: 0.5926\n",
            "Epoch 125/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 661.5269 - acc: 0.6836 - val_loss: 661.5361 - val_acc: 0.5899\n",
            "Epoch 126/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 661.4653 - acc: 0.6714 - val_loss: 661.4757 - val_acc: 0.5899\n",
            "Epoch 127/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 661.4111 - acc: 0.6471 - val_loss: 661.4154 - val_acc: 0.5899\n",
            "Epoch 128/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 661.3531 - acc: 0.6846 - val_loss: 661.3549 - val_acc: 0.5926\n",
            "Epoch 129/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 661.2860 - acc: 0.6755 - val_loss: 661.2944 - val_acc: 0.5926\n",
            "Epoch 130/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 661.2303 - acc: 0.6704 - val_loss: 661.2340 - val_acc: 0.5926\n",
            "Epoch 131/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 661.1569 - acc: 0.6876 - val_loss: 661.1736 - val_acc: 0.5926\n",
            "Epoch 132/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 661.1144 - acc: 0.6460 - val_loss: 661.1132 - val_acc: 0.5926\n",
            "Epoch 133/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 661.0419 - acc: 0.6775 - val_loss: 661.0527 - val_acc: 0.5926\n",
            "Epoch 134/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 660.9857 - acc: 0.6815 - val_loss: 660.9923 - val_acc: 0.5926\n",
            "Epoch 135/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 660.9205 - acc: 0.6734 - val_loss: 660.9319 - val_acc: 0.5926\n",
            "Epoch 136/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 660.8559 - acc: 0.7008 - val_loss: 660.8714 - val_acc: 0.5952\n",
            "Epoch 137/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 660.7989 - acc: 0.6744 - val_loss: 660.8109 - val_acc: 0.5952\n",
            "Epoch 138/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 660.7445 - acc: 0.6775 - val_loss: 660.7505 - val_acc: 0.5899\n",
            "Epoch 139/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 660.6858 - acc: 0.6714 - val_loss: 660.6900 - val_acc: 0.5873\n",
            "Epoch 140/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 660.6150 - acc: 0.6927 - val_loss: 660.6297 - val_acc: 0.5847\n",
            "Epoch 141/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 660.5662 - acc: 0.6866 - val_loss: 660.5693 - val_acc: 0.5873\n",
            "Epoch 142/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 660.5023 - acc: 0.6755 - val_loss: 660.5089 - val_acc: 0.5899\n",
            "Epoch 143/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 660.4467 - acc: 0.6542 - val_loss: 660.4484 - val_acc: 0.5873\n",
            "Epoch 144/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 660.3771 - acc: 0.6836 - val_loss: 660.3881 - val_acc: 0.5873\n",
            "Epoch 145/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 660.3195 - acc: 0.6704 - val_loss: 660.3276 - val_acc: 0.5873\n",
            "Epoch 146/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 660.2601 - acc: 0.6744 - val_loss: 660.2671 - val_acc: 0.5899\n",
            "Epoch 147/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 660.1951 - acc: 0.6795 - val_loss: 660.2067 - val_acc: 0.5899\n",
            "Epoch 148/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 660.1429 - acc: 0.6704 - val_loss: 660.1463 - val_acc: 0.5873\n",
            "Epoch 149/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 660.0797 - acc: 0.6744 - val_loss: 660.0858 - val_acc: 0.5899\n",
            "Epoch 150/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 660.0263 - acc: 0.6582 - val_loss: 660.0252 - val_acc: 0.5926\n",
            "Epoch 151/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 659.9587 - acc: 0.6653 - val_loss: 659.9649 - val_acc: 0.5899\n",
            "Epoch 152/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 659.9001 - acc: 0.6673 - val_loss: 659.9044 - val_acc: 0.5899\n",
            "Epoch 153/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 659.8474 - acc: 0.6714 - val_loss: 659.8440 - val_acc: 0.5873\n",
            "Epoch 154/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 659.7668 - acc: 0.6978 - val_loss: 659.7836 - val_acc: 0.5873\n",
            "Epoch 155/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 659.7160 - acc: 0.6673 - val_loss: 659.7231 - val_acc: 0.5873\n",
            "Epoch 156/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 659.6501 - acc: 0.6917 - val_loss: 659.6626 - val_acc: 0.5873\n",
            "Epoch 157/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 659.5928 - acc: 0.6897 - val_loss: 659.6021 - val_acc: 0.5899\n",
            "Epoch 158/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 659.5278 - acc: 0.6937 - val_loss: 659.5418 - val_acc: 0.5873\n",
            "Epoch 159/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 659.4710 - acc: 0.6897 - val_loss: 659.4813 - val_acc: 0.5899\n",
            "Epoch 160/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 659.4052 - acc: 0.7069 - val_loss: 659.4208 - val_acc: 0.5899\n",
            "Epoch 161/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 659.3458 - acc: 0.6988 - val_loss: 659.3604 - val_acc: 0.5899\n",
            "Epoch 162/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 659.2817 - acc: 0.7018 - val_loss: 659.2998 - val_acc: 0.5899\n",
            "Epoch 163/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 659.2267 - acc: 0.6795 - val_loss: 659.2394 - val_acc: 0.5899\n",
            "Epoch 164/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 659.1707 - acc: 0.6815 - val_loss: 659.1790 - val_acc: 0.5899\n",
            "Epoch 165/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 659.1162 - acc: 0.6785 - val_loss: 659.1185 - val_acc: 0.5899\n",
            "Epoch 166/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 659.0487 - acc: 0.6947 - val_loss: 659.0580 - val_acc: 0.5899\n",
            "Epoch 167/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 658.9775 - acc: 0.7069 - val_loss: 658.9975 - val_acc: 0.5899\n",
            "Epoch 168/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 658.9284 - acc: 0.6866 - val_loss: 658.9369 - val_acc: 0.5847\n",
            "Epoch 169/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 658.8697 - acc: 0.6826 - val_loss: 658.8766 - val_acc: 0.5873\n",
            "Epoch 170/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 658.8052 - acc: 0.6856 - val_loss: 658.8160 - val_acc: 0.5899\n",
            "Epoch 171/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 658.7502 - acc: 0.6998 - val_loss: 658.7555 - val_acc: 0.5899\n",
            "Epoch 172/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 658.6833 - acc: 0.6968 - val_loss: 658.6951 - val_acc: 0.5899\n",
            "Epoch 173/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 658.6132 - acc: 0.6917 - val_loss: 658.6347 - val_acc: 0.5899\n",
            "Epoch 174/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 658.5697 - acc: 0.6907 - val_loss: 658.5742 - val_acc: 0.5926\n",
            "Epoch 175/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 658.5077 - acc: 0.6815 - val_loss: 658.5136 - val_acc: 0.5952\n",
            "Epoch 176/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 658.4417 - acc: 0.6826 - val_loss: 658.4531 - val_acc: 0.5952\n",
            "Epoch 177/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 658.3794 - acc: 0.6866 - val_loss: 658.3927 - val_acc: 0.5952\n",
            "Epoch 178/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 658.3220 - acc: 0.6957 - val_loss: 658.3321 - val_acc: 0.5952\n",
            "Epoch 179/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 658.2607 - acc: 0.6856 - val_loss: 658.2716 - val_acc: 0.5979\n",
            "Epoch 180/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 658.2018 - acc: 0.6886 - val_loss: 658.2110 - val_acc: 0.5926\n",
            "Epoch 181/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 658.1398 - acc: 0.6765 - val_loss: 658.1504 - val_acc: 0.5926\n",
            "Epoch 182/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 658.0828 - acc: 0.6704 - val_loss: 658.0900 - val_acc: 0.5926\n",
            "Epoch 183/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 658.0221 - acc: 0.6765 - val_loss: 658.0293 - val_acc: 0.5952\n",
            "Epoch 184/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 657.9586 - acc: 0.6866 - val_loss: 657.9689 - val_acc: 0.5926\n",
            "Epoch 185/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 657.8831 - acc: 0.7049 - val_loss: 657.9082 - val_acc: 0.5952\n",
            "Epoch 186/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 657.8365 - acc: 0.6988 - val_loss: 657.8476 - val_acc: 0.5926\n",
            "Epoch 187/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 657.7733 - acc: 0.7028 - val_loss: 657.7872 - val_acc: 0.5952\n",
            "Epoch 188/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 657.7096 - acc: 0.6815 - val_loss: 657.7266 - val_acc: 0.5926\n",
            "Epoch 189/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 657.6531 - acc: 0.6968 - val_loss: 657.6660 - val_acc: 0.5926\n",
            "Epoch 190/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 657.5888 - acc: 0.6795 - val_loss: 657.6056 - val_acc: 0.5952\n",
            "Epoch 191/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 657.5346 - acc: 0.6826 - val_loss: 657.5450 - val_acc: 0.5952\n",
            "Epoch 192/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 657.4710 - acc: 0.6856 - val_loss: 657.4845 - val_acc: 0.5952\n",
            "Epoch 193/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 657.4109 - acc: 0.6957 - val_loss: 657.4240 - val_acc: 0.5926\n",
            "Epoch 194/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 657.3518 - acc: 0.7039 - val_loss: 657.3635 - val_acc: 0.5899\n",
            "Epoch 195/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 657.2861 - acc: 0.7049 - val_loss: 657.3030 - val_acc: 0.5899\n",
            "Epoch 196/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 657.2281 - acc: 0.6988 - val_loss: 657.2425 - val_acc: 0.5899\n",
            "Epoch 197/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 657.1732 - acc: 0.6897 - val_loss: 657.1820 - val_acc: 0.5926\n",
            "Epoch 198/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 657.1109 - acc: 0.6663 - val_loss: 657.1214 - val_acc: 0.5926\n",
            "Epoch 199/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 657.0449 - acc: 0.6907 - val_loss: 657.0610 - val_acc: 0.5926\n",
            "Epoch 200/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 656.9829 - acc: 0.6957 - val_loss: 657.0006 - val_acc: 0.5926\n",
            "Epoch 201/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 656.9374 - acc: 0.6744 - val_loss: 656.9401 - val_acc: 0.5926\n",
            "Epoch 202/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 656.8717 - acc: 0.6755 - val_loss: 656.8795 - val_acc: 0.5926\n",
            "Epoch 203/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 656.8069 - acc: 0.6947 - val_loss: 656.8189 - val_acc: 0.5926\n",
            "Epoch 204/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 656.7413 - acc: 0.6886 - val_loss: 656.7585 - val_acc: 0.5926\n",
            "Epoch 205/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 656.6837 - acc: 0.6978 - val_loss: 656.6979 - val_acc: 0.5926\n",
            "Epoch 206/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 656.6253 - acc: 0.6988 - val_loss: 656.6374 - val_acc: 0.5926\n",
            "Epoch 207/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 656.5653 - acc: 0.6897 - val_loss: 656.5769 - val_acc: 0.5926\n",
            "Epoch 208/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 656.4987 - acc: 0.6978 - val_loss: 656.5164 - val_acc: 0.5926\n",
            "Epoch 209/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 656.4314 - acc: 0.6978 - val_loss: 656.4559 - val_acc: 0.5899\n",
            "Epoch 210/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 656.3919 - acc: 0.6957 - val_loss: 656.3954 - val_acc: 0.5899\n",
            "Epoch 211/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 656.3226 - acc: 0.6947 - val_loss: 656.3348 - val_acc: 0.5899\n",
            "Epoch 212/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 656.2592 - acc: 0.7018 - val_loss: 656.2743 - val_acc: 0.5899\n",
            "Epoch 213/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 656.1872 - acc: 0.7039 - val_loss: 656.2138 - val_acc: 0.5899\n",
            "Epoch 214/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 656.1326 - acc: 0.7130 - val_loss: 656.1532 - val_acc: 0.5899\n",
            "Epoch 215/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 656.0711 - acc: 0.7099 - val_loss: 656.0928 - val_acc: 0.5899\n",
            "Epoch 216/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 656.0190 - acc: 0.6815 - val_loss: 656.0322 - val_acc: 0.5926\n",
            "Epoch 217/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 655.9585 - acc: 0.7008 - val_loss: 655.9715 - val_acc: 0.5979\n",
            "Epoch 218/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 655.8908 - acc: 0.7252 - val_loss: 655.9111 - val_acc: 0.5952\n",
            "Epoch 219/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 655.8399 - acc: 0.6886 - val_loss: 655.8505 - val_acc: 0.5979\n",
            "Epoch 220/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 655.7716 - acc: 0.7039 - val_loss: 655.7900 - val_acc: 0.5979\n",
            "Epoch 221/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 655.7162 - acc: 0.6907 - val_loss: 655.7296 - val_acc: 0.5979\n",
            "Epoch 222/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 655.6585 - acc: 0.6826 - val_loss: 655.6691 - val_acc: 0.5952\n",
            "Epoch 223/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 655.5965 - acc: 0.7049 - val_loss: 655.6084 - val_acc: 0.5952\n",
            "Epoch 224/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 655.5385 - acc: 0.6856 - val_loss: 655.5480 - val_acc: 0.5979\n",
            "Epoch 225/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 655.4733 - acc: 0.6815 - val_loss: 655.4875 - val_acc: 0.5952\n",
            "Epoch 226/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 655.4134 - acc: 0.6876 - val_loss: 655.4269 - val_acc: 0.5952\n",
            "Epoch 227/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 655.3442 - acc: 0.7049 - val_loss: 655.3664 - val_acc: 0.5926\n",
            "Epoch 228/5000\n",
            "986/986 [==============================] - 0s 298us/step - loss: 655.2954 - acc: 0.7039 - val_loss: 655.3059 - val_acc: 0.5952\n",
            "Epoch 229/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 655.2318 - acc: 0.7059 - val_loss: 655.2454 - val_acc: 0.5952\n",
            "Epoch 230/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 655.1769 - acc: 0.6886 - val_loss: 655.1848 - val_acc: 0.5952\n",
            "Epoch 231/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 655.1082 - acc: 0.6978 - val_loss: 655.1243 - val_acc: 0.5979\n",
            "Epoch 232/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 655.0479 - acc: 0.6988 - val_loss: 655.0638 - val_acc: 0.5952\n",
            "Epoch 233/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 654.9841 - acc: 0.6968 - val_loss: 655.0032 - val_acc: 0.5979\n",
            "Epoch 234/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 654.9298 - acc: 0.6927 - val_loss: 654.9428 - val_acc: 0.5952\n",
            "Epoch 235/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 654.8599 - acc: 0.6957 - val_loss: 654.8823 - val_acc: 0.5979\n",
            "Epoch 236/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 654.7994 - acc: 0.7272 - val_loss: 654.8217 - val_acc: 0.5979\n",
            "Epoch 237/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 654.7478 - acc: 0.6947 - val_loss: 654.7612 - val_acc: 0.5979\n",
            "Epoch 238/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 654.6838 - acc: 0.6897 - val_loss: 654.7007 - val_acc: 0.5979\n",
            "Epoch 239/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 654.6230 - acc: 0.7150 - val_loss: 654.6401 - val_acc: 0.5952\n",
            "Epoch 240/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 654.5656 - acc: 0.6866 - val_loss: 654.5798 - val_acc: 0.5926\n",
            "Epoch 241/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 654.5010 - acc: 0.7069 - val_loss: 654.5194 - val_acc: 0.5952\n",
            "Epoch 242/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 654.4304 - acc: 0.7241 - val_loss: 654.4589 - val_acc: 0.5926\n",
            "Epoch 243/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 654.3777 - acc: 0.7008 - val_loss: 654.3983 - val_acc: 0.5926\n",
            "Epoch 244/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 654.3162 - acc: 0.7069 - val_loss: 654.3378 - val_acc: 0.5952\n",
            "Epoch 245/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 654.2643 - acc: 0.6866 - val_loss: 654.2774 - val_acc: 0.5926\n",
            "Epoch 246/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 654.1975 - acc: 0.7049 - val_loss: 654.2170 - val_acc: 0.5899\n",
            "Epoch 247/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 654.1400 - acc: 0.6886 - val_loss: 654.1564 - val_acc: 0.5926\n",
            "Epoch 248/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 654.0807 - acc: 0.7089 - val_loss: 654.0960 - val_acc: 0.5952\n",
            "Epoch 249/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 654.0128 - acc: 0.7120 - val_loss: 654.0356 - val_acc: 0.6032\n",
            "Epoch 250/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 653.9643 - acc: 0.6836 - val_loss: 653.9752 - val_acc: 0.6032\n",
            "Epoch 251/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 653.8918 - acc: 0.6957 - val_loss: 653.9147 - val_acc: 0.6058\n",
            "Epoch 252/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 653.8263 - acc: 0.7333 - val_loss: 653.8544 - val_acc: 0.6058\n",
            "Epoch 253/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 653.7819 - acc: 0.6846 - val_loss: 653.7939 - val_acc: 0.6058\n",
            "Epoch 254/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 653.7192 - acc: 0.6815 - val_loss: 653.7335 - val_acc: 0.5979\n",
            "Epoch 255/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 653.6491 - acc: 0.7140 - val_loss: 653.6730 - val_acc: 0.6032\n",
            "Epoch 256/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 653.5913 - acc: 0.7008 - val_loss: 653.6125 - val_acc: 0.6058\n",
            "Epoch 257/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 653.5346 - acc: 0.6978 - val_loss: 653.5520 - val_acc: 0.6085\n",
            "Epoch 258/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 653.4696 - acc: 0.7191 - val_loss: 653.4916 - val_acc: 0.6058\n",
            "Epoch 259/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 653.4051 - acc: 0.7160 - val_loss: 653.4312 - val_acc: 0.6005\n",
            "Epoch 260/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 653.3489 - acc: 0.7059 - val_loss: 653.3708 - val_acc: 0.6032\n",
            "Epoch 261/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 653.2926 - acc: 0.7008 - val_loss: 653.3103 - val_acc: 0.6032\n",
            "Epoch 262/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 653.2399 - acc: 0.6856 - val_loss: 653.2497 - val_acc: 0.6058\n",
            "Epoch 263/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 653.1696 - acc: 0.6957 - val_loss: 653.1893 - val_acc: 0.6058\n",
            "Epoch 264/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 653.1034 - acc: 0.7110 - val_loss: 653.1288 - val_acc: 0.6058\n",
            "Epoch 265/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 653.0433 - acc: 0.7231 - val_loss: 653.0685 - val_acc: 0.6058\n",
            "Epoch 266/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 652.9843 - acc: 0.7140 - val_loss: 653.0080 - val_acc: 0.6111\n",
            "Epoch 267/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 652.9299 - acc: 0.7039 - val_loss: 652.9475 - val_acc: 0.6058\n",
            "Epoch 268/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 652.8642 - acc: 0.7059 - val_loss: 652.8868 - val_acc: 0.6085\n",
            "Epoch 269/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 652.8093 - acc: 0.7069 - val_loss: 652.8264 - val_acc: 0.6111\n",
            "Epoch 270/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 652.7380 - acc: 0.7069 - val_loss: 652.7659 - val_acc: 0.6085\n",
            "Epoch 271/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 652.6839 - acc: 0.6947 - val_loss: 652.7056 - val_acc: 0.6085\n",
            "Epoch 272/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 652.6280 - acc: 0.7039 - val_loss: 652.6451 - val_acc: 0.6111\n",
            "Epoch 273/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 652.5645 - acc: 0.7140 - val_loss: 652.5845 - val_acc: 0.6164\n",
            "Epoch 274/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 652.4983 - acc: 0.7414 - val_loss: 652.5240 - val_acc: 0.6138\n",
            "Epoch 275/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 652.4429 - acc: 0.7069 - val_loss: 652.4634 - val_acc: 0.6138\n",
            "Epoch 276/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 652.3909 - acc: 0.7089 - val_loss: 652.4031 - val_acc: 0.6138\n",
            "Epoch 277/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 652.3258 - acc: 0.7069 - val_loss: 652.3426 - val_acc: 0.6138\n",
            "Epoch 278/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 652.2596 - acc: 0.7089 - val_loss: 652.2822 - val_acc: 0.6138\n",
            "Epoch 279/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 652.2048 - acc: 0.7059 - val_loss: 652.2217 - val_acc: 0.6138\n",
            "Epoch 280/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 652.1454 - acc: 0.7140 - val_loss: 652.1613 - val_acc: 0.6164\n",
            "Epoch 281/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 652.0794 - acc: 0.6978 - val_loss: 652.1009 - val_acc: 0.6138\n",
            "Epoch 282/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 652.0180 - acc: 0.7201 - val_loss: 652.0405 - val_acc: 0.6164\n",
            "Epoch 283/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 651.9656 - acc: 0.7079 - val_loss: 651.9803 - val_acc: 0.6164\n",
            "Epoch 284/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 651.8959 - acc: 0.7120 - val_loss: 651.9198 - val_acc: 0.6164\n",
            "Epoch 285/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 651.8330 - acc: 0.7221 - val_loss: 651.8594 - val_acc: 0.6164\n",
            "Epoch 286/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 651.7753 - acc: 0.7120 - val_loss: 651.7991 - val_acc: 0.6138\n",
            "Epoch 287/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 651.7187 - acc: 0.6988 - val_loss: 651.7387 - val_acc: 0.6190\n",
            "Epoch 288/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 651.6633 - acc: 0.6978 - val_loss: 651.6782 - val_acc: 0.6138\n",
            "Epoch 289/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 651.5970 - acc: 0.7343 - val_loss: 651.6179 - val_acc: 0.6138\n",
            "Epoch 290/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 651.5258 - acc: 0.7434 - val_loss: 651.5575 - val_acc: 0.6111\n",
            "Epoch 291/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 651.4776 - acc: 0.7302 - val_loss: 651.4972 - val_acc: 0.6085\n",
            "Epoch 292/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 651.4224 - acc: 0.7130 - val_loss: 651.4367 - val_acc: 0.6085\n",
            "Epoch 293/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 651.3522 - acc: 0.7089 - val_loss: 651.3764 - val_acc: 0.6111\n",
            "Epoch 294/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 651.2976 - acc: 0.7170 - val_loss: 651.3160 - val_acc: 0.6085\n",
            "Epoch 295/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 651.2300 - acc: 0.7181 - val_loss: 651.2557 - val_acc: 0.6085\n",
            "Epoch 296/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 651.1669 - acc: 0.7292 - val_loss: 651.1953 - val_acc: 0.6111\n",
            "Epoch 297/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 651.1155 - acc: 0.7140 - val_loss: 651.1349 - val_acc: 0.6111\n",
            "Epoch 298/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 651.0536 - acc: 0.7140 - val_loss: 651.0746 - val_acc: 0.6058\n",
            "Epoch 299/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 650.9852 - acc: 0.7302 - val_loss: 651.0141 - val_acc: 0.6085\n",
            "Epoch 300/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 650.9407 - acc: 0.7028 - val_loss: 650.9538 - val_acc: 0.6058\n",
            "Epoch 301/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 650.8757 - acc: 0.7089 - val_loss: 650.8935 - val_acc: 0.6058\n",
            "Epoch 302/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 650.8198 - acc: 0.7028 - val_loss: 650.8330 - val_acc: 0.6111\n",
            "Epoch 303/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 650.7494 - acc: 0.6988 - val_loss: 650.7727 - val_acc: 0.6111\n",
            "Epoch 304/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 650.6940 - acc: 0.7120 - val_loss: 650.7124 - val_acc: 0.6058\n",
            "Epoch 305/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 650.6309 - acc: 0.7099 - val_loss: 650.6520 - val_acc: 0.6058\n",
            "Epoch 306/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 650.5652 - acc: 0.7170 - val_loss: 650.5916 - val_acc: 0.6085\n",
            "Epoch 307/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 650.5043 - acc: 0.7282 - val_loss: 650.5313 - val_acc: 0.6058\n",
            "Epoch 308/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 650.4493 - acc: 0.7120 - val_loss: 650.4708 - val_acc: 0.6058\n",
            "Epoch 309/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 650.3805 - acc: 0.7312 - val_loss: 650.4105 - val_acc: 0.6085\n",
            "Epoch 310/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 650.3233 - acc: 0.7343 - val_loss: 650.3501 - val_acc: 0.6058\n",
            "Epoch 311/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 650.2704 - acc: 0.7110 - val_loss: 650.2899 - val_acc: 0.6058\n",
            "Epoch 312/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 650.2073 - acc: 0.7160 - val_loss: 650.2295 - val_acc: 0.6058\n",
            "Epoch 313/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 650.1540 - acc: 0.6907 - val_loss: 650.1692 - val_acc: 0.6058\n",
            "Epoch 314/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 650.0873 - acc: 0.7272 - val_loss: 650.1087 - val_acc: 0.6058\n",
            "Epoch 315/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 650.0201 - acc: 0.7333 - val_loss: 650.0483 - val_acc: 0.6085\n",
            "Epoch 316/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 649.9589 - acc: 0.7241 - val_loss: 649.9880 - val_acc: 0.6085\n",
            "Epoch 317/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 649.9088 - acc: 0.6998 - val_loss: 649.9277 - val_acc: 0.6111\n",
            "Epoch 318/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 649.8507 - acc: 0.7099 - val_loss: 649.8674 - val_acc: 0.6085\n",
            "Epoch 319/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 649.7834 - acc: 0.7160 - val_loss: 649.8071 - val_acc: 0.6085\n",
            "Epoch 320/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 649.7277 - acc: 0.7110 - val_loss: 649.7468 - val_acc: 0.6085\n",
            "Epoch 321/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 649.6634 - acc: 0.7089 - val_loss: 649.6865 - val_acc: 0.6085\n",
            "Epoch 322/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 649.6023 - acc: 0.7181 - val_loss: 649.6262 - val_acc: 0.6085\n",
            "Epoch 323/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 649.5413 - acc: 0.7170 - val_loss: 649.5658 - val_acc: 0.6085\n",
            "Epoch 324/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 649.4820 - acc: 0.7252 - val_loss: 649.5055 - val_acc: 0.6111\n",
            "Epoch 325/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 649.4164 - acc: 0.7312 - val_loss: 649.4452 - val_acc: 0.6058\n",
            "Epoch 326/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 649.3655 - acc: 0.7150 - val_loss: 649.3849 - val_acc: 0.6111\n",
            "Epoch 327/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 649.2941 - acc: 0.7394 - val_loss: 649.3245 - val_acc: 0.6058\n",
            "Epoch 328/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 649.2429 - acc: 0.7160 - val_loss: 649.2643 - val_acc: 0.6085\n",
            "Epoch 329/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 649.1830 - acc: 0.7170 - val_loss: 649.2039 - val_acc: 0.6111\n",
            "Epoch 330/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 649.1183 - acc: 0.7130 - val_loss: 649.1436 - val_acc: 0.6085\n",
            "Epoch 331/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 649.0602 - acc: 0.7170 - val_loss: 649.0833 - val_acc: 0.6085\n",
            "Epoch 332/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 648.9932 - acc: 0.7252 - val_loss: 649.0231 - val_acc: 0.6111\n",
            "Epoch 333/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 648.9466 - acc: 0.7069 - val_loss: 648.9628 - val_acc: 0.6111\n",
            "Epoch 334/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 648.8819 - acc: 0.7160 - val_loss: 648.9024 - val_acc: 0.6085\n",
            "Epoch 335/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 648.8163 - acc: 0.7302 - val_loss: 648.8421 - val_acc: 0.6085\n",
            "Epoch 336/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 648.7481 - acc: 0.7302 - val_loss: 648.7817 - val_acc: 0.6085\n",
            "Epoch 337/5000\n",
            "986/986 [==============================] - 0s 299us/step - loss: 648.6977 - acc: 0.6968 - val_loss: 648.7214 - val_acc: 0.6085\n",
            "Epoch 338/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 648.6378 - acc: 0.7120 - val_loss: 648.6611 - val_acc: 0.6058\n",
            "Epoch 339/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 648.5812 - acc: 0.7049 - val_loss: 648.6008 - val_acc: 0.6058\n",
            "Epoch 340/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 648.5101 - acc: 0.7231 - val_loss: 648.5403 - val_acc: 0.6058\n",
            "Epoch 341/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 648.4505 - acc: 0.7434 - val_loss: 648.4802 - val_acc: 0.6085\n",
            "Epoch 342/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 648.3979 - acc: 0.7231 - val_loss: 648.4198 - val_acc: 0.6085\n",
            "Epoch 343/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 648.3291 - acc: 0.7211 - val_loss: 648.3595 - val_acc: 0.6085\n",
            "Epoch 344/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 648.2755 - acc: 0.7181 - val_loss: 648.2993 - val_acc: 0.6085\n",
            "Epoch 345/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 648.2150 - acc: 0.7211 - val_loss: 648.2390 - val_acc: 0.6058\n",
            "Epoch 346/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 648.1556 - acc: 0.7170 - val_loss: 648.1788 - val_acc: 0.6058\n",
            "Epoch 347/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 648.0916 - acc: 0.7110 - val_loss: 648.1186 - val_acc: 0.6058\n",
            "Epoch 348/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 648.0369 - acc: 0.7110 - val_loss: 648.0582 - val_acc: 0.6058\n",
            "Epoch 349/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 647.9744 - acc: 0.7110 - val_loss: 647.9979 - val_acc: 0.6058\n",
            "Epoch 350/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 647.9105 - acc: 0.7363 - val_loss: 647.9376 - val_acc: 0.6296\n",
            "Epoch 351/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 647.8478 - acc: 0.7353 - val_loss: 647.8773 - val_acc: 0.6296\n",
            "Epoch 352/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 647.7877 - acc: 0.7272 - val_loss: 647.8170 - val_acc: 0.6296\n",
            "Epoch 353/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 647.7365 - acc: 0.7130 - val_loss: 647.7567 - val_acc: 0.6323\n",
            "Epoch 354/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 647.6630 - acc: 0.7292 - val_loss: 647.6964 - val_acc: 0.6323\n",
            "Epoch 355/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 647.6134 - acc: 0.7252 - val_loss: 647.6362 - val_acc: 0.6323\n",
            "Epoch 356/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 647.5430 - acc: 0.7150 - val_loss: 647.5759 - val_acc: 0.6349\n",
            "Epoch 357/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 647.4885 - acc: 0.7323 - val_loss: 647.5155 - val_acc: 0.6323\n",
            "Epoch 358/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 647.4259 - acc: 0.7262 - val_loss: 647.4552 - val_acc: 0.6296\n",
            "Epoch 359/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 647.3716 - acc: 0.7140 - val_loss: 647.3950 - val_acc: 0.6270\n",
            "Epoch 360/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 647.3093 - acc: 0.7252 - val_loss: 647.3348 - val_acc: 0.6323\n",
            "Epoch 361/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 647.2476 - acc: 0.7252 - val_loss: 647.2745 - val_acc: 0.6376\n",
            "Epoch 362/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 647.1790 - acc: 0.7302 - val_loss: 647.2143 - val_acc: 0.6349\n",
            "Epoch 363/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 647.1194 - acc: 0.7404 - val_loss: 647.1541 - val_acc: 0.6323\n",
            "Epoch 364/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 647.0613 - acc: 0.7120 - val_loss: 647.0938 - val_acc: 0.6349\n",
            "Epoch 365/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 647.0059 - acc: 0.7434 - val_loss: 647.0336 - val_acc: 0.6349\n",
            "Epoch 366/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 646.9369 - acc: 0.7343 - val_loss: 646.9732 - val_acc: 0.6323\n",
            "Epoch 367/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 646.8904 - acc: 0.7140 - val_loss: 646.9131 - val_acc: 0.6376\n",
            "Epoch 368/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 646.8223 - acc: 0.7454 - val_loss: 646.8527 - val_acc: 0.6349\n",
            "Epoch 369/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 646.7666 - acc: 0.7170 - val_loss: 646.7925 - val_acc: 0.6349\n",
            "Epoch 370/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 646.6992 - acc: 0.7343 - val_loss: 646.7321 - val_acc: 0.6323\n",
            "Epoch 371/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 646.6354 - acc: 0.7333 - val_loss: 646.6719 - val_acc: 0.6349\n",
            "Epoch 372/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 646.5789 - acc: 0.7312 - val_loss: 646.6117 - val_acc: 0.6323\n",
            "Epoch 373/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 646.5111 - acc: 0.7394 - val_loss: 646.5514 - val_acc: 0.6296\n",
            "Epoch 374/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 646.4660 - acc: 0.7252 - val_loss: 646.4912 - val_acc: 0.6323\n",
            "Epoch 375/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 646.4069 - acc: 0.7170 - val_loss: 646.4310 - val_acc: 0.6323\n",
            "Epoch 376/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 646.3385 - acc: 0.7241 - val_loss: 646.3706 - val_acc: 0.6296\n",
            "Epoch 377/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 646.2776 - acc: 0.7434 - val_loss: 646.3104 - val_acc: 0.6296\n",
            "Epoch 378/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 646.2210 - acc: 0.7231 - val_loss: 646.2502 - val_acc: 0.6323\n",
            "Epoch 379/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 646.1526 - acc: 0.7323 - val_loss: 646.1899 - val_acc: 0.6296\n",
            "Epoch 380/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 646.0995 - acc: 0.7414 - val_loss: 646.1297 - val_acc: 0.6296\n",
            "Epoch 381/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 646.0298 - acc: 0.7343 - val_loss: 646.0694 - val_acc: 0.6296\n",
            "Epoch 382/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 645.9810 - acc: 0.7262 - val_loss: 646.0091 - val_acc: 0.6296\n",
            "Epoch 383/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 645.9189 - acc: 0.7424 - val_loss: 645.9488 - val_acc: 0.6296\n",
            "Epoch 384/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 645.8597 - acc: 0.7373 - val_loss: 645.8886 - val_acc: 0.6296\n",
            "Epoch 385/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 645.7974 - acc: 0.7282 - val_loss: 645.8284 - val_acc: 0.6296\n",
            "Epoch 386/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 645.7342 - acc: 0.7485 - val_loss: 645.7681 - val_acc: 0.6296\n",
            "Epoch 387/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 645.6808 - acc: 0.7252 - val_loss: 645.7079 - val_acc: 0.6296\n",
            "Epoch 388/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 645.6228 - acc: 0.7221 - val_loss: 645.6476 - val_acc: 0.6323\n",
            "Epoch 389/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 645.5673 - acc: 0.7049 - val_loss: 645.5875 - val_acc: 0.6296\n",
            "Epoch 390/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 645.5029 - acc: 0.7231 - val_loss: 645.5271 - val_acc: 0.6296\n",
            "Epoch 391/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 645.4356 - acc: 0.7302 - val_loss: 645.4670 - val_acc: 0.6349\n",
            "Epoch 392/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 645.3728 - acc: 0.7363 - val_loss: 645.4068 - val_acc: 0.6323\n",
            "Epoch 393/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 645.3155 - acc: 0.7170 - val_loss: 645.3465 - val_acc: 0.6323\n",
            "Epoch 394/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 645.2563 - acc: 0.7191 - val_loss: 645.2862 - val_acc: 0.6323\n",
            "Epoch 395/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 645.2017 - acc: 0.7170 - val_loss: 645.2261 - val_acc: 0.6323\n",
            "Epoch 396/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 645.1327 - acc: 0.7414 - val_loss: 645.1659 - val_acc: 0.6323\n",
            "Epoch 397/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 645.0780 - acc: 0.7333 - val_loss: 645.1056 - val_acc: 0.6323\n",
            "Epoch 398/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 645.0051 - acc: 0.7394 - val_loss: 645.0455 - val_acc: 0.6323\n",
            "Epoch 399/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 644.9491 - acc: 0.7363 - val_loss: 644.9853 - val_acc: 0.6349\n",
            "Epoch 400/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 644.8952 - acc: 0.7120 - val_loss: 644.9251 - val_acc: 0.6349\n",
            "Epoch 401/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 644.8358 - acc: 0.7262 - val_loss: 644.8649 - val_acc: 0.6376\n",
            "Epoch 402/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 644.7740 - acc: 0.7292 - val_loss: 644.8048 - val_acc: 0.6376\n",
            "Epoch 403/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 644.7102 - acc: 0.7353 - val_loss: 644.7447 - val_acc: 0.6402\n",
            "Epoch 404/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 644.6476 - acc: 0.7475 - val_loss: 644.6845 - val_acc: 0.6376\n",
            "Epoch 405/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 644.5896 - acc: 0.7160 - val_loss: 644.6242 - val_acc: 0.6376\n",
            "Epoch 406/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 644.5470 - acc: 0.7150 - val_loss: 644.5640 - val_acc: 0.6349\n",
            "Epoch 407/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 644.4700 - acc: 0.7444 - val_loss: 644.5039 - val_acc: 0.6349\n",
            "Epoch 408/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 644.4182 - acc: 0.7252 - val_loss: 644.4438 - val_acc: 0.6376\n",
            "Epoch 409/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 644.3553 - acc: 0.7282 - val_loss: 644.3836 - val_acc: 0.6402\n",
            "Epoch 410/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 644.2876 - acc: 0.7424 - val_loss: 644.3235 - val_acc: 0.6376\n",
            "Epoch 411/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 644.2280 - acc: 0.7404 - val_loss: 644.2634 - val_acc: 0.6402\n",
            "Epoch 412/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 644.1771 - acc: 0.7241 - val_loss: 644.2033 - val_acc: 0.6402\n",
            "Epoch 413/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 644.1134 - acc: 0.7211 - val_loss: 644.1431 - val_acc: 0.6402\n",
            "Epoch 414/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 644.0445 - acc: 0.7505 - val_loss: 644.0829 - val_acc: 0.6402\n",
            "Epoch 415/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 643.9922 - acc: 0.7454 - val_loss: 644.0228 - val_acc: 0.6402\n",
            "Epoch 416/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 643.9320 - acc: 0.7363 - val_loss: 643.9626 - val_acc: 0.6402\n",
            "Epoch 417/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 643.8636 - acc: 0.7454 - val_loss: 643.9023 - val_acc: 0.6402\n",
            "Epoch 418/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 643.8098 - acc: 0.7252 - val_loss: 643.8422 - val_acc: 0.6376\n",
            "Epoch 419/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 643.7467 - acc: 0.7373 - val_loss: 643.7821 - val_acc: 0.6376\n",
            "Epoch 420/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 643.7011 - acc: 0.7089 - val_loss: 643.7221 - val_acc: 0.6376\n",
            "Epoch 421/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 643.6293 - acc: 0.7302 - val_loss: 643.6618 - val_acc: 0.6349\n",
            "Epoch 422/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 643.5715 - acc: 0.7383 - val_loss: 643.6018 - val_acc: 0.6376\n",
            "Epoch 423/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 643.5102 - acc: 0.7394 - val_loss: 643.5417 - val_acc: 0.6376\n",
            "Epoch 424/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 643.4494 - acc: 0.7181 - val_loss: 643.4816 - val_acc: 0.6376\n",
            "Epoch 425/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 643.3965 - acc: 0.7312 - val_loss: 643.4216 - val_acc: 0.6376\n",
            "Epoch 426/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 643.3308 - acc: 0.7170 - val_loss: 643.3617 - val_acc: 0.6402\n",
            "Epoch 427/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 643.2683 - acc: 0.7302 - val_loss: 643.3015 - val_acc: 0.6402\n",
            "Epoch 428/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 643.2047 - acc: 0.7454 - val_loss: 643.2413 - val_acc: 0.6402\n",
            "Epoch 429/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 643.1449 - acc: 0.7404 - val_loss: 643.1812 - val_acc: 0.6402\n",
            "Epoch 430/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 643.0864 - acc: 0.7444 - val_loss: 643.1210 - val_acc: 0.6402\n",
            "Epoch 431/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 643.0311 - acc: 0.7262 - val_loss: 643.0609 - val_acc: 0.6402\n",
            "Epoch 432/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 642.9672 - acc: 0.7353 - val_loss: 643.0009 - val_acc: 0.6402\n",
            "Epoch 433/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 642.9024 - acc: 0.7333 - val_loss: 642.9407 - val_acc: 0.6402\n",
            "Epoch 434/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 642.8499 - acc: 0.7211 - val_loss: 642.8807 - val_acc: 0.6402\n",
            "Epoch 435/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 642.7837 - acc: 0.7343 - val_loss: 642.8207 - val_acc: 0.6402\n",
            "Epoch 436/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 642.7213 - acc: 0.7465 - val_loss: 642.7606 - val_acc: 0.6402\n",
            "Epoch 437/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 642.6739 - acc: 0.7353 - val_loss: 642.7005 - val_acc: 0.6376\n",
            "Epoch 438/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 642.6064 - acc: 0.7394 - val_loss: 642.6403 - val_acc: 0.6376\n",
            "Epoch 439/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 642.5495 - acc: 0.7191 - val_loss: 642.5802 - val_acc: 0.6376\n",
            "Epoch 440/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 642.4833 - acc: 0.7465 - val_loss: 642.5202 - val_acc: 0.6349\n",
            "Epoch 441/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 642.4304 - acc: 0.7201 - val_loss: 642.4602 - val_acc: 0.6376\n",
            "Epoch 442/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 642.3705 - acc: 0.7292 - val_loss: 642.4001 - val_acc: 0.6376\n",
            "Epoch 443/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 642.3054 - acc: 0.7282 - val_loss: 642.3401 - val_acc: 0.6376\n",
            "Epoch 444/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 642.2389 - acc: 0.7414 - val_loss: 642.2801 - val_acc: 0.6402\n",
            "Epoch 445/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 642.1890 - acc: 0.7333 - val_loss: 642.2200 - val_acc: 0.6376\n",
            "Epoch 446/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 642.1162 - acc: 0.7424 - val_loss: 642.1600 - val_acc: 0.6402\n",
            "Epoch 447/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 642.0593 - acc: 0.7546 - val_loss: 642.1000 - val_acc: 0.6402\n",
            "Epoch 448/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 642.0053 - acc: 0.7292 - val_loss: 642.0400 - val_acc: 0.6402\n",
            "Epoch 449/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 641.9438 - acc: 0.7343 - val_loss: 641.9801 - val_acc: 0.6402\n",
            "Epoch 450/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 641.8836 - acc: 0.7383 - val_loss: 641.9202 - val_acc: 0.6402\n",
            "Epoch 451/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 641.8307 - acc: 0.7221 - val_loss: 641.8600 - val_acc: 0.6402\n",
            "Epoch 452/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 641.7669 - acc: 0.7343 - val_loss: 641.8000 - val_acc: 0.6402\n",
            "Epoch 453/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 641.7017 - acc: 0.7404 - val_loss: 641.7402 - val_acc: 0.6402\n",
            "Epoch 454/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 641.6423 - acc: 0.7424 - val_loss: 641.6801 - val_acc: 0.6402\n",
            "Epoch 455/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 641.5769 - acc: 0.7495 - val_loss: 641.6201 - val_acc: 0.6402\n",
            "Epoch 456/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 641.5260 - acc: 0.7394 - val_loss: 641.5599 - val_acc: 0.6402\n",
            "Epoch 457/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 641.4648 - acc: 0.7363 - val_loss: 641.5000 - val_acc: 0.6402\n",
            "Epoch 458/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 641.4002 - acc: 0.7454 - val_loss: 641.4400 - val_acc: 0.6402\n",
            "Epoch 459/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 641.3453 - acc: 0.7373 - val_loss: 641.3800 - val_acc: 0.6376\n",
            "Epoch 460/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 641.2808 - acc: 0.7312 - val_loss: 641.3200 - val_acc: 0.6402\n",
            "Epoch 461/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 641.2288 - acc: 0.7394 - val_loss: 641.2599 - val_acc: 0.6402\n",
            "Epoch 462/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 641.1616 - acc: 0.7363 - val_loss: 641.1998 - val_acc: 0.6402\n",
            "Epoch 463/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 641.1067 - acc: 0.7363 - val_loss: 641.1399 - val_acc: 0.6402\n",
            "Epoch 464/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 641.0390 - acc: 0.7556 - val_loss: 641.0800 - val_acc: 0.6402\n",
            "Epoch 465/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 640.9782 - acc: 0.7485 - val_loss: 641.0199 - val_acc: 0.6402\n",
            "Epoch 466/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 640.9257 - acc: 0.7383 - val_loss: 640.9599 - val_acc: 0.6402\n",
            "Epoch 467/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 640.8667 - acc: 0.7252 - val_loss: 640.9000 - val_acc: 0.6402\n",
            "Epoch 468/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 640.8052 - acc: 0.7404 - val_loss: 640.8401 - val_acc: 0.6402\n",
            "Epoch 469/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 640.7450 - acc: 0.7282 - val_loss: 640.7802 - val_acc: 0.6402\n",
            "Epoch 470/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 640.6783 - acc: 0.7323 - val_loss: 640.7202 - val_acc: 0.6402\n",
            "Epoch 471/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 640.6300 - acc: 0.7353 - val_loss: 640.6603 - val_acc: 0.6402\n",
            "Epoch 472/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 640.5647 - acc: 0.7424 - val_loss: 640.6004 - val_acc: 0.6402\n",
            "Epoch 473/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 640.4999 - acc: 0.7495 - val_loss: 640.5404 - val_acc: 0.6402\n",
            "Epoch 474/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 640.4471 - acc: 0.7394 - val_loss: 640.4805 - val_acc: 0.6402\n",
            "Epoch 475/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 640.3845 - acc: 0.7333 - val_loss: 640.4205 - val_acc: 0.6402\n",
            "Epoch 476/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 640.3248 - acc: 0.7434 - val_loss: 640.3606 - val_acc: 0.6402\n",
            "Epoch 477/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 640.2633 - acc: 0.7394 - val_loss: 640.3006 - val_acc: 0.6429\n",
            "Epoch 478/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 640.2002 - acc: 0.7525 - val_loss: 640.2408 - val_acc: 0.6429\n",
            "Epoch 479/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 640.1375 - acc: 0.7627 - val_loss: 640.1809 - val_acc: 0.6429\n",
            "Epoch 480/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 640.0847 - acc: 0.7546 - val_loss: 640.1210 - val_acc: 0.6402\n",
            "Epoch 481/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 640.0302 - acc: 0.7323 - val_loss: 640.0612 - val_acc: 0.6429\n",
            "Epoch 482/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 639.9570 - acc: 0.7525 - val_loss: 640.0013 - val_acc: 0.6429\n",
            "Epoch 483/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 639.8954 - acc: 0.7596 - val_loss: 639.9414 - val_acc: 0.6429\n",
            "Epoch 484/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 639.8439 - acc: 0.7465 - val_loss: 639.8814 - val_acc: 0.6429\n",
            "Epoch 485/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 639.7819 - acc: 0.7525 - val_loss: 639.8213 - val_acc: 0.6429\n",
            "Epoch 486/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 639.7169 - acc: 0.7525 - val_loss: 639.7616 - val_acc: 0.6429\n",
            "Epoch 487/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 639.6544 - acc: 0.7677 - val_loss: 639.7018 - val_acc: 0.6429\n",
            "Epoch 488/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 639.6006 - acc: 0.7465 - val_loss: 639.6418 - val_acc: 0.6402\n",
            "Epoch 489/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 639.5425 - acc: 0.7515 - val_loss: 639.5820 - val_acc: 0.6429\n",
            "Epoch 490/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 639.4823 - acc: 0.7475 - val_loss: 639.5221 - val_acc: 0.6402\n",
            "Epoch 491/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 639.4216 - acc: 0.7465 - val_loss: 639.4623 - val_acc: 0.6402\n",
            "Epoch 492/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 639.3659 - acc: 0.7363 - val_loss: 639.4024 - val_acc: 0.6402\n",
            "Epoch 493/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 639.3012 - acc: 0.7373 - val_loss: 639.3425 - val_acc: 0.6402\n",
            "Epoch 494/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 639.2428 - acc: 0.7414 - val_loss: 639.2827 - val_acc: 0.6402\n",
            "Epoch 495/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 639.1729 - acc: 0.7515 - val_loss: 639.2228 - val_acc: 0.6402\n",
            "Epoch 496/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 639.1192 - acc: 0.7465 - val_loss: 639.1629 - val_acc: 0.6402\n",
            "Epoch 497/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 639.0642 - acc: 0.7414 - val_loss: 639.1031 - val_acc: 0.6402\n",
            "Epoch 498/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 639.0073 - acc: 0.7363 - val_loss: 639.0434 - val_acc: 0.6402\n",
            "Epoch 499/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 638.9453 - acc: 0.7434 - val_loss: 638.9834 - val_acc: 0.6402\n",
            "Epoch 500/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 638.8832 - acc: 0.7525 - val_loss: 638.9236 - val_acc: 0.6402\n",
            "Epoch 501/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 638.8249 - acc: 0.7495 - val_loss: 638.8638 - val_acc: 0.6402\n",
            "Epoch 502/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 638.7611 - acc: 0.7586 - val_loss: 638.8040 - val_acc: 0.6429\n",
            "Epoch 503/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 638.7108 - acc: 0.7312 - val_loss: 638.7441 - val_acc: 0.6429\n",
            "Epoch 504/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 638.6396 - acc: 0.7495 - val_loss: 638.6843 - val_acc: 0.6429\n",
            "Epoch 505/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 638.5894 - acc: 0.7454 - val_loss: 638.6245 - val_acc: 0.6429\n",
            "Epoch 506/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 638.5218 - acc: 0.7566 - val_loss: 638.5648 - val_acc: 0.6429\n",
            "Epoch 507/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 638.4604 - acc: 0.7617 - val_loss: 638.5050 - val_acc: 0.6429\n",
            "Epoch 508/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 638.4094 - acc: 0.7383 - val_loss: 638.4452 - val_acc: 0.6429\n",
            "Epoch 509/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 638.3399 - acc: 0.7444 - val_loss: 638.3855 - val_acc: 0.6429\n",
            "Epoch 510/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 638.2819 - acc: 0.7394 - val_loss: 638.3256 - val_acc: 0.6455\n",
            "Epoch 511/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 638.2311 - acc: 0.7465 - val_loss: 638.2657 - val_acc: 0.6455\n",
            "Epoch 512/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 638.1673 - acc: 0.7465 - val_loss: 638.2061 - val_acc: 0.6455\n",
            "Epoch 513/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 638.1116 - acc: 0.7383 - val_loss: 638.1463 - val_acc: 0.6455\n",
            "Epoch 514/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 638.0496 - acc: 0.7515 - val_loss: 638.0866 - val_acc: 0.6455\n",
            "Epoch 515/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 637.9945 - acc: 0.7312 - val_loss: 638.0268 - val_acc: 0.6455\n",
            "Epoch 516/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 637.9293 - acc: 0.7475 - val_loss: 637.9672 - val_acc: 0.6455\n",
            "Epoch 517/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 637.8699 - acc: 0.7535 - val_loss: 637.9074 - val_acc: 0.6455\n",
            "Epoch 518/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 637.8140 - acc: 0.7292 - val_loss: 637.8476 - val_acc: 0.6455\n",
            "Epoch 519/5000\n",
            "986/986 [==============================] - 0s 260us/step - loss: 637.7456 - acc: 0.7434 - val_loss: 637.7878 - val_acc: 0.6455\n",
            "Epoch 520/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 637.6807 - acc: 0.7566 - val_loss: 637.7282 - val_acc: 0.6455\n",
            "Epoch 521/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 637.6243 - acc: 0.7535 - val_loss: 637.6685 - val_acc: 0.6455\n",
            "Epoch 522/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 637.5697 - acc: 0.7505 - val_loss: 637.6087 - val_acc: 0.6455\n",
            "Epoch 523/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 637.5123 - acc: 0.7323 - val_loss: 637.5490 - val_acc: 0.6455\n",
            "Epoch 524/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 637.4497 - acc: 0.7454 - val_loss: 637.4893 - val_acc: 0.6455\n",
            "Epoch 525/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 637.3829 - acc: 0.7627 - val_loss: 637.4296 - val_acc: 0.6455\n",
            "Epoch 526/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 637.3302 - acc: 0.7475 - val_loss: 637.3697 - val_acc: 0.6455\n",
            "Epoch 527/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 637.2726 - acc: 0.7302 - val_loss: 637.3101 - val_acc: 0.6455\n",
            "Epoch 528/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 637.2072 - acc: 0.7566 - val_loss: 637.2504 - val_acc: 0.6455\n",
            "Epoch 529/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 637.1422 - acc: 0.7586 - val_loss: 637.1908 - val_acc: 0.6455\n",
            "Epoch 530/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 637.0779 - acc: 0.7667 - val_loss: 637.1310 - val_acc: 0.6455\n",
            "Epoch 531/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 637.0312 - acc: 0.7424 - val_loss: 637.0713 - val_acc: 0.6455\n",
            "Epoch 532/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 636.9699 - acc: 0.7495 - val_loss: 637.0115 - val_acc: 0.6455\n",
            "Epoch 533/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 636.9124 - acc: 0.7414 - val_loss: 636.9517 - val_acc: 0.6455\n",
            "Epoch 534/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 636.8545 - acc: 0.7383 - val_loss: 636.8921 - val_acc: 0.6455\n",
            "Epoch 535/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 636.7974 - acc: 0.7525 - val_loss: 636.8324 - val_acc: 0.6455\n",
            "Epoch 536/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 636.7302 - acc: 0.7515 - val_loss: 636.7728 - val_acc: 0.6455\n",
            "Epoch 537/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 636.6642 - acc: 0.7657 - val_loss: 636.7130 - val_acc: 0.6455\n",
            "Epoch 538/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 636.6103 - acc: 0.7515 - val_loss: 636.6534 - val_acc: 0.6455\n",
            "Epoch 539/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 636.5510 - acc: 0.7667 - val_loss: 636.5937 - val_acc: 0.6455\n",
            "Epoch 540/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 636.4888 - acc: 0.7556 - val_loss: 636.5340 - val_acc: 0.6455\n",
            "Epoch 541/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 636.4362 - acc: 0.7404 - val_loss: 636.4743 - val_acc: 0.6455\n",
            "Epoch 542/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 636.3790 - acc: 0.7414 - val_loss: 636.4147 - val_acc: 0.6455\n",
            "Epoch 543/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 636.3139 - acc: 0.7505 - val_loss: 636.3551 - val_acc: 0.6455\n",
            "Epoch 544/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 636.2596 - acc: 0.7383 - val_loss: 636.2955 - val_acc: 0.6455\n",
            "Epoch 545/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 636.1955 - acc: 0.7343 - val_loss: 636.2358 - val_acc: 0.6455\n",
            "Epoch 546/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 636.1262 - acc: 0.7688 - val_loss: 636.1762 - val_acc: 0.6455\n",
            "Epoch 547/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 636.0718 - acc: 0.7505 - val_loss: 636.1165 - val_acc: 0.6455\n",
            "Epoch 548/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 636.0158 - acc: 0.7525 - val_loss: 636.0568 - val_acc: 0.6455\n",
            "Epoch 549/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 635.9577 - acc: 0.7505 - val_loss: 635.9972 - val_acc: 0.6455\n",
            "Epoch 550/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 635.8898 - acc: 0.7617 - val_loss: 635.9376 - val_acc: 0.6455\n",
            "Epoch 551/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 635.8342 - acc: 0.7647 - val_loss: 635.8780 - val_acc: 0.6455\n",
            "Epoch 552/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 635.7688 - acc: 0.7606 - val_loss: 635.8184 - val_acc: 0.6455\n",
            "Epoch 553/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 635.7124 - acc: 0.7444 - val_loss: 635.7588 - val_acc: 0.6455\n",
            "Epoch 554/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 635.6614 - acc: 0.7363 - val_loss: 635.6991 - val_acc: 0.6481\n",
            "Epoch 555/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 635.5956 - acc: 0.7383 - val_loss: 635.6395 - val_acc: 0.6481\n",
            "Epoch 556/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 635.5320 - acc: 0.7606 - val_loss: 635.5800 - val_acc: 0.6508\n",
            "Epoch 557/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 635.4815 - acc: 0.7525 - val_loss: 635.5205 - val_acc: 0.6508\n",
            "Epoch 558/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 635.4150 - acc: 0.7586 - val_loss: 635.4610 - val_acc: 0.6508\n",
            "Epoch 559/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 635.3621 - acc: 0.7454 - val_loss: 635.4014 - val_acc: 0.6508\n",
            "Epoch 560/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 635.2936 - acc: 0.7546 - val_loss: 635.3418 - val_acc: 0.6508\n",
            "Epoch 561/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 635.2379 - acc: 0.7596 - val_loss: 635.2823 - val_acc: 0.6508\n",
            "Epoch 562/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 635.1840 - acc: 0.7454 - val_loss: 635.2228 - val_acc: 0.6508\n",
            "Epoch 563/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 635.1180 - acc: 0.7373 - val_loss: 635.1632 - val_acc: 0.6508\n",
            "Epoch 564/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 635.0619 - acc: 0.7525 - val_loss: 635.1036 - val_acc: 0.6534\n",
            "Epoch 565/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 634.9963 - acc: 0.7556 - val_loss: 635.0441 - val_acc: 0.6508\n",
            "Epoch 566/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 634.9405 - acc: 0.7465 - val_loss: 634.9844 - val_acc: 0.6534\n",
            "Epoch 567/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 634.8760 - acc: 0.7546 - val_loss: 634.9248 - val_acc: 0.6534\n",
            "Epoch 568/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 634.8263 - acc: 0.7424 - val_loss: 634.8653 - val_acc: 0.6534\n",
            "Epoch 569/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 634.7570 - acc: 0.7769 - val_loss: 634.8057 - val_acc: 0.6534\n",
            "Epoch 570/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 634.7048 - acc: 0.7667 - val_loss: 634.7462 - val_acc: 0.6534\n",
            "Epoch 571/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 634.6400 - acc: 0.7596 - val_loss: 634.6866 - val_acc: 0.6534\n",
            "Epoch 572/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 634.5723 - acc: 0.7627 - val_loss: 634.6271 - val_acc: 0.6534\n",
            "Epoch 573/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 634.5207 - acc: 0.7698 - val_loss: 634.5676 - val_acc: 0.6534\n",
            "Epoch 574/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 634.4671 - acc: 0.7404 - val_loss: 634.5079 - val_acc: 0.6534\n",
            "Epoch 575/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 634.3997 - acc: 0.7647 - val_loss: 634.4485 - val_acc: 0.6534\n",
            "Epoch 576/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 634.3361 - acc: 0.7495 - val_loss: 634.3888 - val_acc: 0.6534\n",
            "Epoch 577/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 634.2822 - acc: 0.7535 - val_loss: 634.3294 - val_acc: 0.6534\n",
            "Epoch 578/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 634.2297 - acc: 0.7191 - val_loss: 634.2700 - val_acc: 0.6534\n",
            "Epoch 579/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 634.1684 - acc: 0.7515 - val_loss: 634.2103 - val_acc: 0.6534\n",
            "Epoch 580/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 634.0978 - acc: 0.7546 - val_loss: 634.1509 - val_acc: 0.6534\n",
            "Epoch 581/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 634.0355 - acc: 0.7627 - val_loss: 634.0912 - val_acc: 0.6534\n",
            "Epoch 582/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 633.9865 - acc: 0.7556 - val_loss: 634.0317 - val_acc: 0.6534\n",
            "Epoch 583/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 633.9227 - acc: 0.7627 - val_loss: 633.9724 - val_acc: 0.6534\n",
            "Epoch 584/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 633.8706 - acc: 0.7485 - val_loss: 633.9127 - val_acc: 0.6534\n",
            "Epoch 585/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 633.8105 - acc: 0.7637 - val_loss: 633.8531 - val_acc: 0.6534\n",
            "Epoch 586/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 633.7458 - acc: 0.7596 - val_loss: 633.7937 - val_acc: 0.6561\n",
            "Epoch 587/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 633.6857 - acc: 0.7546 - val_loss: 633.7342 - val_acc: 0.6561\n",
            "Epoch 588/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 633.6278 - acc: 0.7475 - val_loss: 633.6748 - val_acc: 0.6561\n",
            "Epoch 589/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 633.5677 - acc: 0.7556 - val_loss: 633.6152 - val_acc: 0.6561\n",
            "Epoch 590/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 633.5048 - acc: 0.7809 - val_loss: 633.5557 - val_acc: 0.6561\n",
            "Epoch 591/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 633.4550 - acc: 0.7525 - val_loss: 633.4961 - val_acc: 0.6587\n",
            "Epoch 592/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 633.3889 - acc: 0.7444 - val_loss: 633.4366 - val_acc: 0.6561\n",
            "Epoch 593/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 633.3369 - acc: 0.7566 - val_loss: 633.3772 - val_acc: 0.6561\n",
            "Epoch 594/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 633.2650 - acc: 0.7769 - val_loss: 633.3179 - val_acc: 0.6561\n",
            "Epoch 595/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 633.2100 - acc: 0.7759 - val_loss: 633.2584 - val_acc: 0.6587\n",
            "Epoch 596/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 633.1454 - acc: 0.7617 - val_loss: 633.1990 - val_acc: 0.6561\n",
            "Epoch 597/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 633.0947 - acc: 0.7454 - val_loss: 633.1394 - val_acc: 0.6587\n",
            "Epoch 598/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 633.0309 - acc: 0.7576 - val_loss: 633.0800 - val_acc: 0.6614\n",
            "Epoch 599/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 632.9669 - acc: 0.7677 - val_loss: 633.0206 - val_acc: 0.6587\n",
            "Epoch 600/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 632.9107 - acc: 0.7637 - val_loss: 632.9612 - val_acc: 0.6561\n",
            "Epoch 601/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 632.8535 - acc: 0.7606 - val_loss: 632.9018 - val_acc: 0.6587\n",
            "Epoch 602/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 632.7936 - acc: 0.7535 - val_loss: 632.8422 - val_acc: 0.6561\n",
            "Epoch 603/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 632.7335 - acc: 0.7688 - val_loss: 632.7828 - val_acc: 0.6587\n",
            "Epoch 604/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 632.6830 - acc: 0.7525 - val_loss: 632.7235 - val_acc: 0.6561\n",
            "Epoch 605/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 632.6096 - acc: 0.7677 - val_loss: 632.6641 - val_acc: 0.6534\n",
            "Epoch 606/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 632.5545 - acc: 0.7748 - val_loss: 632.6045 - val_acc: 0.6561\n",
            "Epoch 607/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 632.4977 - acc: 0.7718 - val_loss: 632.5450 - val_acc: 0.6561\n",
            "Epoch 608/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 632.4353 - acc: 0.7617 - val_loss: 632.4857 - val_acc: 0.6561\n",
            "Epoch 609/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 632.3781 - acc: 0.7637 - val_loss: 632.4262 - val_acc: 0.6561\n",
            "Epoch 610/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 632.3182 - acc: 0.7637 - val_loss: 632.3669 - val_acc: 0.6561\n",
            "Epoch 611/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 632.2582 - acc: 0.7647 - val_loss: 632.3075 - val_acc: 0.6561\n",
            "Epoch 612/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 632.2013 - acc: 0.7525 - val_loss: 632.2481 - val_acc: 0.6587\n",
            "Epoch 613/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 632.1343 - acc: 0.7738 - val_loss: 632.1888 - val_acc: 0.6561\n",
            "Epoch 614/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 632.0859 - acc: 0.7606 - val_loss: 632.1295 - val_acc: 0.6561\n",
            "Epoch 615/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 632.0169 - acc: 0.7748 - val_loss: 632.0702 - val_acc: 0.6561\n",
            "Epoch 616/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 631.9636 - acc: 0.7475 - val_loss: 632.0107 - val_acc: 0.6561\n",
            "Epoch 617/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 631.9007 - acc: 0.7688 - val_loss: 631.9514 - val_acc: 0.6561\n",
            "Epoch 618/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 631.8483 - acc: 0.7586 - val_loss: 631.8919 - val_acc: 0.6561\n",
            "Epoch 619/5000\n",
            "986/986 [==============================] - 0s 297us/step - loss: 631.7852 - acc: 0.7748 - val_loss: 631.8326 - val_acc: 0.6561\n",
            "Epoch 620/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 631.7195 - acc: 0.7819 - val_loss: 631.7733 - val_acc: 0.6561\n",
            "Epoch 621/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 631.6753 - acc: 0.7444 - val_loss: 631.7139 - val_acc: 0.6561\n",
            "Epoch 622/5000\n",
            "986/986 [==============================] - 0s 298us/step - loss: 631.6072 - acc: 0.7647 - val_loss: 631.6546 - val_acc: 0.6561\n",
            "Epoch 623/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 631.5432 - acc: 0.7748 - val_loss: 631.5953 - val_acc: 0.6561\n",
            "Epoch 624/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 631.4876 - acc: 0.7505 - val_loss: 631.5360 - val_acc: 0.6561\n",
            "Epoch 625/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 631.4285 - acc: 0.7677 - val_loss: 631.4768 - val_acc: 0.6561\n",
            "Epoch 626/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 631.3588 - acc: 0.7890 - val_loss: 631.4174 - val_acc: 0.6534\n",
            "Epoch 627/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 631.3128 - acc: 0.7444 - val_loss: 631.3582 - val_acc: 0.6587\n",
            "Epoch 628/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 631.2430 - acc: 0.7677 - val_loss: 631.2990 - val_acc: 0.6561\n",
            "Epoch 629/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 631.1900 - acc: 0.7789 - val_loss: 631.2396 - val_acc: 0.6561\n",
            "Epoch 630/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 631.1381 - acc: 0.7556 - val_loss: 631.1802 - val_acc: 0.6587\n",
            "Epoch 631/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 631.0690 - acc: 0.7657 - val_loss: 631.1211 - val_acc: 0.6587\n",
            "Epoch 632/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 631.0128 - acc: 0.7748 - val_loss: 631.0619 - val_acc: 0.6587\n",
            "Epoch 633/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 630.9521 - acc: 0.7748 - val_loss: 631.0026 - val_acc: 0.6587\n",
            "Epoch 634/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 630.8913 - acc: 0.7647 - val_loss: 630.9433 - val_acc: 0.6561\n",
            "Epoch 635/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 630.8336 - acc: 0.7647 - val_loss: 630.8839 - val_acc: 0.6561\n",
            "Epoch 636/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 630.7829 - acc: 0.7677 - val_loss: 630.8246 - val_acc: 0.6561\n",
            "Epoch 637/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 630.7170 - acc: 0.7677 - val_loss: 630.7655 - val_acc: 0.6561\n",
            "Epoch 638/5000\n",
            "986/986 [==============================] - 0s 299us/step - loss: 630.6643 - acc: 0.7525 - val_loss: 630.7062 - val_acc: 0.6561\n",
            "Epoch 639/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 630.6050 - acc: 0.7576 - val_loss: 630.6470 - val_acc: 0.6561\n",
            "Epoch 640/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 630.5394 - acc: 0.7454 - val_loss: 630.5878 - val_acc: 0.6561\n",
            "Epoch 641/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 630.4775 - acc: 0.7738 - val_loss: 630.5285 - val_acc: 0.6587\n",
            "Epoch 642/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 630.4234 - acc: 0.7566 - val_loss: 630.4693 - val_acc: 0.6561\n",
            "Epoch 643/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 630.3583 - acc: 0.7688 - val_loss: 630.4102 - val_acc: 0.6561\n",
            "Epoch 644/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 630.2903 - acc: 0.7799 - val_loss: 630.3509 - val_acc: 0.6561\n",
            "Epoch 645/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 630.2457 - acc: 0.7566 - val_loss: 630.2917 - val_acc: 0.6534\n",
            "Epoch 646/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 630.1846 - acc: 0.7627 - val_loss: 630.2326 - val_acc: 0.6508\n",
            "Epoch 647/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 630.1238 - acc: 0.7698 - val_loss: 630.1735 - val_acc: 0.6534\n",
            "Epoch 648/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 630.0554 - acc: 0.7819 - val_loss: 630.1142 - val_acc: 0.6561\n",
            "Epoch 649/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 630.0102 - acc: 0.7718 - val_loss: 630.0550 - val_acc: 0.6561\n",
            "Epoch 650/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 629.9456 - acc: 0.7515 - val_loss: 629.9958 - val_acc: 0.6561\n",
            "Epoch 651/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 629.8866 - acc: 0.7698 - val_loss: 629.9367 - val_acc: 0.6561\n",
            "Epoch 652/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 629.8240 - acc: 0.7860 - val_loss: 629.8775 - val_acc: 0.6561\n",
            "Epoch 653/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 629.7632 - acc: 0.7779 - val_loss: 629.8185 - val_acc: 0.6561\n",
            "Epoch 654/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 629.7005 - acc: 0.7860 - val_loss: 629.7592 - val_acc: 0.6561\n",
            "Epoch 655/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 629.6378 - acc: 0.7860 - val_loss: 629.7000 - val_acc: 0.6561\n",
            "Epoch 656/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 629.5836 - acc: 0.7850 - val_loss: 629.6408 - val_acc: 0.6561\n",
            "Epoch 657/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 629.5203 - acc: 0.7951 - val_loss: 629.5817 - val_acc: 0.6534\n",
            "Epoch 658/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 629.4680 - acc: 0.7566 - val_loss: 629.5224 - val_acc: 0.6534\n",
            "Epoch 659/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 629.4073 - acc: 0.7830 - val_loss: 629.4633 - val_acc: 0.6508\n",
            "Epoch 660/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 629.3465 - acc: 0.7830 - val_loss: 629.4041 - val_acc: 0.6534\n",
            "Epoch 661/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 629.2972 - acc: 0.7718 - val_loss: 629.3451 - val_acc: 0.6508\n",
            "Epoch 662/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 629.2313 - acc: 0.7738 - val_loss: 629.2861 - val_acc: 0.6508\n",
            "Epoch 663/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 629.1775 - acc: 0.7657 - val_loss: 629.2269 - val_acc: 0.6508\n",
            "Epoch 664/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 629.1178 - acc: 0.7465 - val_loss: 629.1677 - val_acc: 0.6508\n",
            "Epoch 665/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 629.0645 - acc: 0.7586 - val_loss: 629.1087 - val_acc: 0.6508\n",
            "Epoch 666/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 628.9887 - acc: 0.7890 - val_loss: 629.0494 - val_acc: 0.6508\n",
            "Epoch 667/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 628.9386 - acc: 0.7698 - val_loss: 628.9902 - val_acc: 0.6534\n",
            "Epoch 668/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 628.8840 - acc: 0.7657 - val_loss: 628.9311 - val_acc: 0.6534\n",
            "Epoch 669/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 628.8195 - acc: 0.7627 - val_loss: 628.8719 - val_acc: 0.6534\n",
            "Epoch 670/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 628.7458 - acc: 0.7830 - val_loss: 628.8128 - val_acc: 0.6534\n",
            "Epoch 671/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 628.7008 - acc: 0.7617 - val_loss: 628.7537 - val_acc: 0.6534\n",
            "Epoch 672/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 628.6411 - acc: 0.7809 - val_loss: 628.6944 - val_acc: 0.6534\n",
            "Epoch 673/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 628.5792 - acc: 0.7779 - val_loss: 628.6354 - val_acc: 0.6534\n",
            "Epoch 674/5000\n",
            "986/986 [==============================] - 0s 297us/step - loss: 628.5256 - acc: 0.7728 - val_loss: 628.5761 - val_acc: 0.6534\n",
            "Epoch 675/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 628.4641 - acc: 0.7840 - val_loss: 628.5170 - val_acc: 0.6534\n",
            "Epoch 676/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 628.4066 - acc: 0.7617 - val_loss: 628.4579 - val_acc: 0.6534\n",
            "Epoch 677/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 628.3548 - acc: 0.7657 - val_loss: 628.3987 - val_acc: 0.6534\n",
            "Epoch 678/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 628.2812 - acc: 0.7698 - val_loss: 628.3395 - val_acc: 0.6561\n",
            "Epoch 679/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 628.2259 - acc: 0.7769 - val_loss: 628.2805 - val_acc: 0.6561\n",
            "Epoch 680/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 628.1632 - acc: 0.7769 - val_loss: 628.2213 - val_acc: 0.6561\n",
            "Epoch 681/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 628.1000 - acc: 0.7728 - val_loss: 628.1622 - val_acc: 0.6561\n",
            "Epoch 682/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 628.0468 - acc: 0.7799 - val_loss: 628.1032 - val_acc: 0.6561\n",
            "Epoch 683/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 628.0003 - acc: 0.7546 - val_loss: 628.0441 - val_acc: 0.6561\n",
            "Epoch 684/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 627.9289 - acc: 0.7627 - val_loss: 627.9850 - val_acc: 0.6561\n",
            "Epoch 685/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 627.8690 - acc: 0.7738 - val_loss: 627.9259 - val_acc: 0.6561\n",
            "Epoch 686/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 627.8145 - acc: 0.7596 - val_loss: 627.8667 - val_acc: 0.6561\n",
            "Epoch 687/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 627.7513 - acc: 0.7738 - val_loss: 627.8077 - val_acc: 0.6561\n",
            "Epoch 688/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 627.6924 - acc: 0.7748 - val_loss: 627.7485 - val_acc: 0.6561\n",
            "Epoch 689/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 627.6303 - acc: 0.7819 - val_loss: 627.6895 - val_acc: 0.6587\n",
            "Epoch 690/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 627.5784 - acc: 0.7718 - val_loss: 627.6304 - val_acc: 0.6561\n",
            "Epoch 691/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 627.5119 - acc: 0.7789 - val_loss: 627.5714 - val_acc: 0.6561\n",
            "Epoch 692/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 627.4524 - acc: 0.7890 - val_loss: 627.5121 - val_acc: 0.6561\n",
            "Epoch 693/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 627.4044 - acc: 0.7546 - val_loss: 627.4532 - val_acc: 0.6561\n",
            "Epoch 694/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 627.3367 - acc: 0.7748 - val_loss: 627.3942 - val_acc: 0.6561\n",
            "Epoch 695/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 627.2828 - acc: 0.7870 - val_loss: 627.3351 - val_acc: 0.6561\n",
            "Epoch 696/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 627.2208 - acc: 0.7759 - val_loss: 627.2760 - val_acc: 0.6561\n",
            "Epoch 697/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 627.1555 - acc: 0.7819 - val_loss: 627.2171 - val_acc: 0.6561\n",
            "Epoch 698/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 627.1100 - acc: 0.7830 - val_loss: 627.1580 - val_acc: 0.6561\n",
            "Epoch 699/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 627.0427 - acc: 0.7941 - val_loss: 627.0987 - val_acc: 0.6561\n",
            "Epoch 700/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 626.9876 - acc: 0.7809 - val_loss: 627.0398 - val_acc: 0.6587\n",
            "Epoch 701/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 626.9251 - acc: 0.7880 - val_loss: 626.9807 - val_acc: 0.6587\n",
            "Epoch 702/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 626.8663 - acc: 0.7870 - val_loss: 626.9217 - val_acc: 0.6587\n",
            "Epoch 703/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 626.7988 - acc: 0.7850 - val_loss: 626.8628 - val_acc: 0.6587\n",
            "Epoch 704/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 626.7468 - acc: 0.7789 - val_loss: 626.8038 - val_acc: 0.6587\n",
            "Epoch 705/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 626.6885 - acc: 0.7738 - val_loss: 626.7448 - val_acc: 0.6614\n",
            "Epoch 706/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 626.6305 - acc: 0.7748 - val_loss: 626.6858 - val_acc: 0.6614\n",
            "Epoch 707/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 626.5739 - acc: 0.7708 - val_loss: 626.6267 - val_acc: 0.6614\n",
            "Epoch 708/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 626.5085 - acc: 0.7677 - val_loss: 626.5677 - val_acc: 0.6587\n",
            "Epoch 709/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 626.4481 - acc: 0.7789 - val_loss: 626.5089 - val_acc: 0.6614\n",
            "Epoch 710/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 626.3946 - acc: 0.7890 - val_loss: 626.4497 - val_acc: 0.6640\n",
            "Epoch 711/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 626.3334 - acc: 0.7890 - val_loss: 626.3908 - val_acc: 0.6640\n",
            "Epoch 712/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 626.2750 - acc: 0.7748 - val_loss: 626.3318 - val_acc: 0.6640\n",
            "Epoch 713/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 626.2157 - acc: 0.7738 - val_loss: 626.2727 - val_acc: 0.6640\n",
            "Epoch 714/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 626.1558 - acc: 0.7708 - val_loss: 626.2139 - val_acc: 0.6640\n",
            "Epoch 715/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 626.0999 - acc: 0.7779 - val_loss: 626.1549 - val_acc: 0.6640\n",
            "Epoch 716/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 626.0422 - acc: 0.7718 - val_loss: 626.0959 - val_acc: 0.6614\n",
            "Epoch 717/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 625.9784 - acc: 0.7789 - val_loss: 626.0370 - val_acc: 0.6640\n",
            "Epoch 718/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 625.9240 - acc: 0.7799 - val_loss: 625.9780 - val_acc: 0.6614\n",
            "Epoch 719/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 625.8643 - acc: 0.7759 - val_loss: 625.9192 - val_acc: 0.6614\n",
            "Epoch 720/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 625.7992 - acc: 0.7708 - val_loss: 625.8601 - val_acc: 0.6614\n",
            "Epoch 721/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 625.7367 - acc: 0.7870 - val_loss: 625.8011 - val_acc: 0.6614\n",
            "Epoch 722/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 625.6896 - acc: 0.7637 - val_loss: 625.7422 - val_acc: 0.6614\n",
            "Epoch 723/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 625.6180 - acc: 0.7880 - val_loss: 625.6832 - val_acc: 0.6614\n",
            "Epoch 724/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 625.5716 - acc: 0.7748 - val_loss: 625.6242 - val_acc: 0.6614\n",
            "Epoch 725/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 625.5137 - acc: 0.7789 - val_loss: 625.5653 - val_acc: 0.6614\n",
            "Epoch 726/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 625.4463 - acc: 0.7850 - val_loss: 625.5063 - val_acc: 0.6614\n",
            "Epoch 727/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 625.3876 - acc: 0.7921 - val_loss: 625.4475 - val_acc: 0.6614\n",
            "Epoch 728/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 625.3330 - acc: 0.7738 - val_loss: 625.3886 - val_acc: 0.6614\n",
            "Epoch 729/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 625.2672 - acc: 0.7799 - val_loss: 625.3296 - val_acc: 0.6614\n",
            "Epoch 730/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 625.2115 - acc: 0.7921 - val_loss: 625.2707 - val_acc: 0.6614\n",
            "Epoch 731/5000\n",
            "986/986 [==============================] - 0s 299us/step - loss: 625.1609 - acc: 0.7546 - val_loss: 625.2118 - val_acc: 0.6614\n",
            "Epoch 732/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 625.1027 - acc: 0.7586 - val_loss: 625.1529 - val_acc: 0.6614\n",
            "Epoch 733/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 625.0370 - acc: 0.7718 - val_loss: 625.0940 - val_acc: 0.6614\n",
            "Epoch 734/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 624.9624 - acc: 0.8022 - val_loss: 625.0351 - val_acc: 0.6614\n",
            "Epoch 735/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 624.9195 - acc: 0.7677 - val_loss: 624.9762 - val_acc: 0.6614\n",
            "Epoch 736/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 624.8575 - acc: 0.7748 - val_loss: 624.9173 - val_acc: 0.6640\n",
            "Epoch 737/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 624.7996 - acc: 0.7840 - val_loss: 624.8584 - val_acc: 0.6640\n",
            "Epoch 738/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 624.7438 - acc: 0.7596 - val_loss: 624.7996 - val_acc: 0.6640\n",
            "Epoch 739/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 624.6827 - acc: 0.7819 - val_loss: 624.7407 - val_acc: 0.6640\n",
            "Epoch 740/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 624.6211 - acc: 0.7799 - val_loss: 624.6818 - val_acc: 0.6640\n",
            "Epoch 741/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 624.5615 - acc: 0.7840 - val_loss: 624.6229 - val_acc: 0.6614\n",
            "Epoch 742/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 624.5106 - acc: 0.7718 - val_loss: 624.5642 - val_acc: 0.6614\n",
            "Epoch 743/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 624.4521 - acc: 0.7698 - val_loss: 624.5053 - val_acc: 0.6614\n",
            "Epoch 744/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 624.3853 - acc: 0.7890 - val_loss: 624.4464 - val_acc: 0.6614\n",
            "Epoch 745/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 624.3261 - acc: 0.7738 - val_loss: 624.3876 - val_acc: 0.6614\n",
            "Epoch 746/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 624.2671 - acc: 0.7901 - val_loss: 624.3287 - val_acc: 0.6614\n",
            "Epoch 747/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 624.2041 - acc: 0.7809 - val_loss: 624.2699 - val_acc: 0.6614\n",
            "Epoch 748/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 624.1516 - acc: 0.7677 - val_loss: 624.2112 - val_acc: 0.6614\n",
            "Epoch 749/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 624.0886 - acc: 0.7840 - val_loss: 624.1521 - val_acc: 0.6614\n",
            "Epoch 750/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 624.0342 - acc: 0.7840 - val_loss: 624.0934 - val_acc: 0.6614\n",
            "Epoch 751/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 623.9782 - acc: 0.7728 - val_loss: 624.0346 - val_acc: 0.6614\n",
            "Epoch 752/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 623.9180 - acc: 0.7830 - val_loss: 623.9758 - val_acc: 0.6614\n",
            "Epoch 753/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 623.8569 - acc: 0.7860 - val_loss: 623.9170 - val_acc: 0.6614\n",
            "Epoch 754/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 623.7923 - acc: 0.7890 - val_loss: 623.8581 - val_acc: 0.6614\n",
            "Epoch 755/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 623.7447 - acc: 0.7738 - val_loss: 623.7995 - val_acc: 0.6614\n",
            "Epoch 756/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 623.6720 - acc: 0.7890 - val_loss: 623.7407 - val_acc: 0.6614\n",
            "Epoch 757/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 623.6184 - acc: 0.7911 - val_loss: 623.6821 - val_acc: 0.6614\n",
            "Epoch 758/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 623.5631 - acc: 0.7911 - val_loss: 623.6233 - val_acc: 0.6614\n",
            "Epoch 759/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 623.5097 - acc: 0.7789 - val_loss: 623.5646 - val_acc: 0.6614\n",
            "Epoch 760/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 623.4439 - acc: 0.7911 - val_loss: 623.5057 - val_acc: 0.6614\n",
            "Epoch 761/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 623.3856 - acc: 0.7860 - val_loss: 623.4469 - val_acc: 0.6614\n",
            "Epoch 762/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 623.3249 - acc: 0.8012 - val_loss: 623.3883 - val_acc: 0.6614\n",
            "Epoch 763/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 623.2713 - acc: 0.7860 - val_loss: 623.3295 - val_acc: 0.6614\n",
            "Epoch 764/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 623.2136 - acc: 0.7830 - val_loss: 623.2707 - val_acc: 0.6614\n",
            "Epoch 765/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 623.1431 - acc: 0.8002 - val_loss: 623.2119 - val_acc: 0.6614\n",
            "Epoch 766/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 623.0919 - acc: 0.7759 - val_loss: 623.1533 - val_acc: 0.6614\n",
            "Epoch 767/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 623.0279 - acc: 0.7931 - val_loss: 623.0946 - val_acc: 0.6614\n",
            "Epoch 768/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 622.9760 - acc: 0.7799 - val_loss: 623.0359 - val_acc: 0.6614\n",
            "Epoch 769/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 622.9103 - acc: 0.7890 - val_loss: 622.9773 - val_acc: 0.6614\n",
            "Epoch 770/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 622.8592 - acc: 0.7789 - val_loss: 622.9185 - val_acc: 0.6614\n",
            "Epoch 771/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 622.8027 - acc: 0.7708 - val_loss: 622.8597 - val_acc: 0.6614\n",
            "Epoch 772/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 622.7454 - acc: 0.7911 - val_loss: 622.8010 - val_acc: 0.6614\n",
            "Epoch 773/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 622.6835 - acc: 0.7880 - val_loss: 622.7423 - val_acc: 0.6614\n",
            "Epoch 774/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 622.6212 - acc: 0.7870 - val_loss: 622.6836 - val_acc: 0.6614\n",
            "Epoch 775/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 622.5677 - acc: 0.7890 - val_loss: 622.6249 - val_acc: 0.6614\n",
            "Epoch 776/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 622.4981 - acc: 0.8032 - val_loss: 622.5662 - val_acc: 0.6640\n",
            "Epoch 777/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 622.4448 - acc: 0.7972 - val_loss: 622.5074 - val_acc: 0.6640\n",
            "Epoch 778/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 622.3909 - acc: 0.7901 - val_loss: 622.4489 - val_acc: 0.6640\n",
            "Epoch 779/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 622.3247 - acc: 0.7911 - val_loss: 622.3902 - val_acc: 0.6640\n",
            "Epoch 780/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 622.2732 - acc: 0.7718 - val_loss: 622.3317 - val_acc: 0.6614\n",
            "Epoch 781/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 622.2100 - acc: 0.7718 - val_loss: 622.2728 - val_acc: 0.6614\n",
            "Epoch 782/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 622.1452 - acc: 0.7982 - val_loss: 622.2142 - val_acc: 0.6561\n",
            "Epoch 783/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 622.0892 - acc: 0.7911 - val_loss: 622.1554 - val_acc: 0.6587\n",
            "Epoch 784/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 622.0320 - acc: 0.7819 - val_loss: 622.0968 - val_acc: 0.6587\n",
            "Epoch 785/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 621.9695 - acc: 0.7941 - val_loss: 622.0381 - val_acc: 0.6587\n",
            "Epoch 786/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 621.9177 - acc: 0.7789 - val_loss: 621.9794 - val_acc: 0.6587\n",
            "Epoch 787/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 621.8642 - acc: 0.7708 - val_loss: 621.9207 - val_acc: 0.6587\n",
            "Epoch 788/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 621.7975 - acc: 0.7992 - val_loss: 621.8621 - val_acc: 0.6561\n",
            "Epoch 789/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 621.7401 - acc: 0.7880 - val_loss: 621.8034 - val_acc: 0.6614\n",
            "Epoch 790/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 621.6781 - acc: 0.7728 - val_loss: 621.7449 - val_acc: 0.6561\n",
            "Epoch 791/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 621.6216 - acc: 0.7880 - val_loss: 621.6862 - val_acc: 0.6587\n",
            "Epoch 792/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 621.5609 - acc: 0.8053 - val_loss: 621.6276 - val_acc: 0.6587\n",
            "Epoch 793/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 621.5020 - acc: 0.7992 - val_loss: 621.5689 - val_acc: 0.6667\n",
            "Epoch 794/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 621.4465 - acc: 0.7860 - val_loss: 621.5103 - val_acc: 0.6587\n",
            "Epoch 795/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 621.3899 - acc: 0.7870 - val_loss: 621.4517 - val_acc: 0.6587\n",
            "Epoch 796/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 621.3322 - acc: 0.7870 - val_loss: 621.3931 - val_acc: 0.6614\n",
            "Epoch 797/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 621.2739 - acc: 0.7718 - val_loss: 621.3344 - val_acc: 0.6587\n",
            "Epoch 798/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 621.2232 - acc: 0.7708 - val_loss: 621.2758 - val_acc: 0.6587\n",
            "Epoch 799/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 621.1568 - acc: 0.7921 - val_loss: 621.2173 - val_acc: 0.6587\n",
            "Epoch 800/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 621.0907 - acc: 0.8022 - val_loss: 621.1586 - val_acc: 0.6561\n",
            "Epoch 801/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 621.0377 - acc: 0.7759 - val_loss: 621.1001 - val_acc: 0.6614\n",
            "Epoch 802/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 620.9799 - acc: 0.7809 - val_loss: 621.0414 - val_acc: 0.6587\n",
            "Epoch 803/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 620.9170 - acc: 0.7830 - val_loss: 620.9828 - val_acc: 0.6614\n",
            "Epoch 804/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 620.8491 - acc: 0.7911 - val_loss: 620.9243 - val_acc: 0.6614\n",
            "Epoch 805/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 620.7979 - acc: 0.7901 - val_loss: 620.8657 - val_acc: 0.6614\n",
            "Epoch 806/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 620.7487 - acc: 0.7830 - val_loss: 620.8071 - val_acc: 0.6614\n",
            "Epoch 807/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 620.6878 - acc: 0.7830 - val_loss: 620.7484 - val_acc: 0.6561\n",
            "Epoch 808/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 620.6243 - acc: 0.7870 - val_loss: 620.6899 - val_acc: 0.6587\n",
            "Epoch 809/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 620.5669 - acc: 0.7748 - val_loss: 620.6315 - val_acc: 0.6561\n",
            "Epoch 810/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 620.5030 - acc: 0.7850 - val_loss: 620.5728 - val_acc: 0.6561\n",
            "Epoch 811/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 620.4469 - acc: 0.8002 - val_loss: 620.5142 - val_acc: 0.6561\n",
            "Epoch 812/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 620.3978 - acc: 0.7738 - val_loss: 620.4557 - val_acc: 0.6534\n",
            "Epoch 813/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 620.3282 - acc: 0.7911 - val_loss: 620.3971 - val_acc: 0.6561\n",
            "Epoch 814/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 620.2690 - acc: 0.7951 - val_loss: 620.3386 - val_acc: 0.6587\n",
            "Epoch 815/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 620.2165 - acc: 0.7880 - val_loss: 620.2799 - val_acc: 0.6587\n",
            "Epoch 816/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 620.1626 - acc: 0.7850 - val_loss: 620.2216 - val_acc: 0.6561\n",
            "Epoch 817/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 620.1041 - acc: 0.7738 - val_loss: 620.1629 - val_acc: 0.6587\n",
            "Epoch 818/5000\n",
            "986/986 [==============================] - 0s 259us/step - loss: 620.0400 - acc: 0.7860 - val_loss: 620.1043 - val_acc: 0.6561\n",
            "Epoch 819/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 619.9841 - acc: 0.7992 - val_loss: 620.0458 - val_acc: 0.6587\n",
            "Epoch 820/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 619.9232 - acc: 0.7860 - val_loss: 619.9872 - val_acc: 0.6587\n",
            "Epoch 821/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 619.8643 - acc: 0.7901 - val_loss: 619.9286 - val_acc: 0.6561\n",
            "Epoch 822/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 619.8077 - acc: 0.7809 - val_loss: 619.8701 - val_acc: 0.6561\n",
            "Epoch 823/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 619.7396 - acc: 0.7982 - val_loss: 619.8116 - val_acc: 0.6561\n",
            "Epoch 824/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 619.6915 - acc: 0.8083 - val_loss: 619.7531 - val_acc: 0.6561\n",
            "Epoch 825/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 619.6346 - acc: 0.7961 - val_loss: 619.6946 - val_acc: 0.6561\n",
            "Epoch 826/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 619.5699 - acc: 0.7911 - val_loss: 619.6362 - val_acc: 0.6561\n",
            "Epoch 827/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 619.5114 - acc: 0.7880 - val_loss: 619.5777 - val_acc: 0.6561\n",
            "Epoch 828/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 619.4469 - acc: 0.7941 - val_loss: 619.5192 - val_acc: 0.6561\n",
            "Epoch 829/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 619.3979 - acc: 0.7860 - val_loss: 619.4606 - val_acc: 0.6561\n",
            "Epoch 830/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 619.3347 - acc: 0.7961 - val_loss: 619.4022 - val_acc: 0.6561\n",
            "Epoch 831/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 619.2833 - acc: 0.7718 - val_loss: 619.3436 - val_acc: 0.6561\n",
            "Epoch 832/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 619.2171 - acc: 0.8093 - val_loss: 619.2851 - val_acc: 0.6561\n",
            "Epoch 833/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 619.1648 - acc: 0.7931 - val_loss: 619.2268 - val_acc: 0.6561\n",
            "Epoch 834/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 619.1048 - acc: 0.7880 - val_loss: 619.1683 - val_acc: 0.6587\n",
            "Epoch 835/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 619.0437 - acc: 0.7870 - val_loss: 619.1100 - val_acc: 0.6587\n",
            "Epoch 836/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 618.9814 - acc: 0.7890 - val_loss: 619.0514 - val_acc: 0.6587\n",
            "Epoch 837/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 618.9250 - acc: 0.7941 - val_loss: 618.9931 - val_acc: 0.6587\n",
            "Epoch 838/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 618.8676 - acc: 0.7921 - val_loss: 618.9347 - val_acc: 0.6587\n",
            "Epoch 839/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 618.8049 - acc: 0.8043 - val_loss: 618.8761 - val_acc: 0.6587\n",
            "Epoch 840/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 618.7504 - acc: 0.7992 - val_loss: 618.8176 - val_acc: 0.6587\n",
            "Epoch 841/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 618.6992 - acc: 0.7931 - val_loss: 618.7591 - val_acc: 0.6587\n",
            "Epoch 842/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 618.6347 - acc: 0.7951 - val_loss: 618.7008 - val_acc: 0.6587\n",
            "Epoch 843/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 618.5805 - acc: 0.7819 - val_loss: 618.6422 - val_acc: 0.6587\n",
            "Epoch 844/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 618.5250 - acc: 0.7921 - val_loss: 618.5840 - val_acc: 0.6561\n",
            "Epoch 845/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 618.4640 - acc: 0.7860 - val_loss: 618.5254 - val_acc: 0.6587\n",
            "Epoch 846/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 618.3958 - acc: 0.8022 - val_loss: 618.4670 - val_acc: 0.6587\n",
            "Epoch 847/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 618.3315 - acc: 0.8144 - val_loss: 618.4085 - val_acc: 0.6614\n",
            "Epoch 848/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 618.2832 - acc: 0.8012 - val_loss: 618.3501 - val_acc: 0.6614\n",
            "Epoch 849/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 618.2280 - acc: 0.7961 - val_loss: 618.2918 - val_acc: 0.6614\n",
            "Epoch 850/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 618.1734 - acc: 0.7809 - val_loss: 618.2333 - val_acc: 0.6614\n",
            "Epoch 851/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 618.1070 - acc: 0.7961 - val_loss: 618.1749 - val_acc: 0.6614\n",
            "Epoch 852/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 618.0400 - acc: 0.8124 - val_loss: 618.1166 - val_acc: 0.6614\n",
            "Epoch 853/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 617.9917 - acc: 0.7840 - val_loss: 618.0582 - val_acc: 0.6587\n",
            "Epoch 854/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 617.9363 - acc: 0.7931 - val_loss: 617.9999 - val_acc: 0.6614\n",
            "Epoch 855/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 617.8745 - acc: 0.7951 - val_loss: 617.9415 - val_acc: 0.6614\n",
            "Epoch 856/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 617.8137 - acc: 0.7941 - val_loss: 617.8829 - val_acc: 0.6587\n",
            "Epoch 857/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 617.7557 - acc: 0.8002 - val_loss: 617.8247 - val_acc: 0.6614\n",
            "Epoch 858/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 617.6900 - acc: 0.8053 - val_loss: 617.7661 - val_acc: 0.6587\n",
            "Epoch 859/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 617.6428 - acc: 0.7911 - val_loss: 617.7078 - val_acc: 0.6587\n",
            "Epoch 860/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 617.5815 - acc: 0.7901 - val_loss: 617.6496 - val_acc: 0.6614\n",
            "Epoch 861/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 617.5261 - acc: 0.7850 - val_loss: 617.5911 - val_acc: 0.6614\n",
            "Epoch 862/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 617.4634 - acc: 0.7951 - val_loss: 617.5327 - val_acc: 0.6587\n",
            "Epoch 863/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 617.4061 - acc: 0.7901 - val_loss: 617.4743 - val_acc: 0.6587\n",
            "Epoch 864/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 617.3439 - acc: 0.8124 - val_loss: 617.4159 - val_acc: 0.6587\n",
            "Epoch 865/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 617.2890 - acc: 0.8124 - val_loss: 617.3576 - val_acc: 0.6587\n",
            "Epoch 866/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 617.2390 - acc: 0.7870 - val_loss: 617.2993 - val_acc: 0.6587\n",
            "Epoch 867/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 617.1682 - acc: 0.8154 - val_loss: 617.2409 - val_acc: 0.6587\n",
            "Epoch 868/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 617.1146 - acc: 0.7992 - val_loss: 617.1826 - val_acc: 0.6587\n",
            "Epoch 869/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 617.0564 - acc: 0.7972 - val_loss: 617.1241 - val_acc: 0.6614\n",
            "Epoch 870/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 617.0014 - acc: 0.7921 - val_loss: 617.0658 - val_acc: 0.6587\n",
            "Epoch 871/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 616.9398 - acc: 0.7921 - val_loss: 617.0075 - val_acc: 0.6614\n",
            "Epoch 872/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 616.8864 - acc: 0.7961 - val_loss: 616.9491 - val_acc: 0.6614\n",
            "Epoch 873/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 616.8306 - acc: 0.7870 - val_loss: 616.8908 - val_acc: 0.6614\n",
            "Epoch 874/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 616.7648 - acc: 0.8002 - val_loss: 616.8325 - val_acc: 0.6614\n",
            "Epoch 875/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 616.7072 - acc: 0.7890 - val_loss: 616.7742 - val_acc: 0.6614\n",
            "Epoch 876/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 616.6528 - acc: 0.7911 - val_loss: 616.7159 - val_acc: 0.6614\n",
            "Epoch 877/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 616.5942 - acc: 0.7961 - val_loss: 616.6576 - val_acc: 0.6614\n",
            "Epoch 878/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 616.5280 - acc: 0.8032 - val_loss: 616.5992 - val_acc: 0.6614\n",
            "Epoch 879/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 616.4760 - acc: 0.7901 - val_loss: 616.5409 - val_acc: 0.6614\n",
            "Epoch 880/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 616.4212 - acc: 0.7860 - val_loss: 616.4825 - val_acc: 0.6614\n",
            "Epoch 881/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 616.3578 - acc: 0.7951 - val_loss: 616.4243 - val_acc: 0.6614\n",
            "Epoch 882/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 616.3001 - acc: 0.7890 - val_loss: 616.3660 - val_acc: 0.6640\n",
            "Epoch 883/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 616.2404 - acc: 0.7901 - val_loss: 616.3078 - val_acc: 0.6640\n",
            "Epoch 884/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 616.1863 - acc: 0.7799 - val_loss: 616.2494 - val_acc: 0.6640\n",
            "Epoch 885/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 616.1246 - acc: 0.7911 - val_loss: 616.1911 - val_acc: 0.6614\n",
            "Epoch 886/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 616.0670 - acc: 0.7860 - val_loss: 616.1327 - val_acc: 0.6614\n",
            "Epoch 887/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 616.0065 - acc: 0.7850 - val_loss: 616.0746 - val_acc: 0.6614\n",
            "Epoch 888/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 615.9522 - acc: 0.8043 - val_loss: 616.0164 - val_acc: 0.6614\n",
            "Epoch 889/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 615.8952 - acc: 0.7941 - val_loss: 615.9581 - val_acc: 0.6614\n",
            "Epoch 890/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 615.8308 - acc: 0.7830 - val_loss: 615.8998 - val_acc: 0.6614\n",
            "Epoch 891/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 615.7681 - acc: 0.7992 - val_loss: 615.8415 - val_acc: 0.6614\n",
            "Epoch 892/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 615.7094 - acc: 0.8053 - val_loss: 615.7832 - val_acc: 0.6614\n",
            "Epoch 893/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 615.6528 - acc: 0.8103 - val_loss: 615.7249 - val_acc: 0.6640\n",
            "Epoch 894/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 615.5982 - acc: 0.8022 - val_loss: 615.6667 - val_acc: 0.6614\n",
            "Epoch 895/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 615.5414 - acc: 0.7992 - val_loss: 615.6084 - val_acc: 0.6614\n",
            "Epoch 896/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 615.4797 - acc: 0.7931 - val_loss: 615.5504 - val_acc: 0.6614\n",
            "Epoch 897/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 615.4284 - acc: 0.7901 - val_loss: 615.4921 - val_acc: 0.6640\n",
            "Epoch 898/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 615.3668 - acc: 0.7931 - val_loss: 615.4339 - val_acc: 0.6640\n",
            "Epoch 899/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 615.3106 - acc: 0.8185 - val_loss: 615.3756 - val_acc: 0.6640\n",
            "Epoch 900/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 615.2423 - acc: 0.8103 - val_loss: 615.3174 - val_acc: 0.6667\n",
            "Epoch 901/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 615.1883 - acc: 0.7921 - val_loss: 615.2593 - val_acc: 0.6614\n",
            "Epoch 902/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 615.1299 - acc: 0.8002 - val_loss: 615.2011 - val_acc: 0.6640\n",
            "Epoch 903/5000\n",
            "986/986 [==============================] - 0s 296us/step - loss: 615.0761 - acc: 0.7890 - val_loss: 615.1428 - val_acc: 0.6640\n",
            "Epoch 904/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 615.0109 - acc: 0.8083 - val_loss: 615.0847 - val_acc: 0.6614\n",
            "Epoch 905/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 614.9574 - acc: 0.8002 - val_loss: 615.0266 - val_acc: 0.6640\n",
            "Epoch 906/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 614.8967 - acc: 0.8022 - val_loss: 614.9682 - val_acc: 0.6667\n",
            "Epoch 907/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 614.8454 - acc: 0.7850 - val_loss: 614.9101 - val_acc: 0.6667\n",
            "Epoch 908/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 614.7790 - acc: 0.7972 - val_loss: 614.8519 - val_acc: 0.6667\n",
            "Epoch 909/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 614.7273 - acc: 0.7972 - val_loss: 614.7937 - val_acc: 0.6667\n",
            "Epoch 910/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 614.6627 - acc: 0.8235 - val_loss: 614.7355 - val_acc: 0.6667\n",
            "Epoch 911/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 614.6143 - acc: 0.7901 - val_loss: 614.6774 - val_acc: 0.6667\n",
            "Epoch 912/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 614.5455 - acc: 0.8063 - val_loss: 614.6194 - val_acc: 0.6667\n",
            "Epoch 913/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 614.4838 - acc: 0.8174 - val_loss: 614.5611 - val_acc: 0.6667\n",
            "Epoch 914/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 614.4330 - acc: 0.7951 - val_loss: 614.5030 - val_acc: 0.6667\n",
            "Epoch 915/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 614.3785 - acc: 0.7972 - val_loss: 614.4448 - val_acc: 0.6667\n",
            "Epoch 916/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 614.3182 - acc: 0.7911 - val_loss: 614.3865 - val_acc: 0.6667\n",
            "Epoch 917/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 614.2537 - acc: 0.8022 - val_loss: 614.3283 - val_acc: 0.6667\n",
            "Epoch 918/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 614.1905 - acc: 0.8063 - val_loss: 614.2703 - val_acc: 0.6667\n",
            "Epoch 919/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 614.1479 - acc: 0.7870 - val_loss: 614.2120 - val_acc: 0.6667\n",
            "Epoch 920/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 614.0769 - acc: 0.8032 - val_loss: 614.1538 - val_acc: 0.6667\n",
            "Epoch 921/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 614.0247 - acc: 0.7951 - val_loss: 614.0957 - val_acc: 0.6667\n",
            "Epoch 922/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 613.9601 - acc: 0.8164 - val_loss: 614.0376 - val_acc: 0.6667\n",
            "Epoch 923/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 613.9093 - acc: 0.7890 - val_loss: 613.9795 - val_acc: 0.6667\n",
            "Epoch 924/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 613.8466 - acc: 0.8053 - val_loss: 613.9213 - val_acc: 0.6667\n",
            "Epoch 925/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 613.7921 - acc: 0.7951 - val_loss: 613.8631 - val_acc: 0.6667\n",
            "Epoch 926/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 613.7349 - acc: 0.8022 - val_loss: 613.8049 - val_acc: 0.6667\n",
            "Epoch 927/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 613.6749 - acc: 0.8012 - val_loss: 613.7467 - val_acc: 0.6667\n",
            "Epoch 928/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 613.6231 - acc: 0.7931 - val_loss: 613.6886 - val_acc: 0.6693\n",
            "Epoch 929/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 613.5595 - acc: 0.8154 - val_loss: 613.6304 - val_acc: 0.6667\n",
            "Epoch 930/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 613.5065 - acc: 0.7901 - val_loss: 613.5723 - val_acc: 0.6667\n",
            "Epoch 931/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 613.4485 - acc: 0.7982 - val_loss: 613.5142 - val_acc: 0.6667\n",
            "Epoch 932/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 613.3843 - acc: 0.7921 - val_loss: 613.4560 - val_acc: 0.6667\n",
            "Epoch 933/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 613.3296 - acc: 0.7972 - val_loss: 613.3979 - val_acc: 0.6667\n",
            "Epoch 934/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 613.2696 - acc: 0.8002 - val_loss: 613.3399 - val_acc: 0.6667\n",
            "Epoch 935/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 613.2072 - acc: 0.8174 - val_loss: 613.2819 - val_acc: 0.6667\n",
            "Epoch 936/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 613.1606 - acc: 0.7880 - val_loss: 613.2238 - val_acc: 0.6667\n",
            "Epoch 937/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 613.0910 - acc: 0.8093 - val_loss: 613.1657 - val_acc: 0.6667\n",
            "Epoch 938/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 613.0352 - acc: 0.7931 - val_loss: 613.1076 - val_acc: 0.6667\n",
            "Epoch 939/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 612.9752 - acc: 0.8185 - val_loss: 613.0496 - val_acc: 0.6667\n",
            "Epoch 940/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 612.9170 - acc: 0.8134 - val_loss: 612.9913 - val_acc: 0.6667\n",
            "Epoch 941/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 612.8622 - acc: 0.8103 - val_loss: 612.9334 - val_acc: 0.6667\n",
            "Epoch 942/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 612.8031 - acc: 0.8032 - val_loss: 612.8753 - val_acc: 0.6667\n",
            "Epoch 943/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 612.7453 - acc: 0.7961 - val_loss: 612.8173 - val_acc: 0.6667\n",
            "Epoch 944/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 612.6904 - acc: 0.8063 - val_loss: 612.7591 - val_acc: 0.6667\n",
            "Epoch 945/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 612.6297 - acc: 0.7951 - val_loss: 612.7011 - val_acc: 0.6667\n",
            "Epoch 946/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 612.5726 - acc: 0.8083 - val_loss: 612.6431 - val_acc: 0.6667\n",
            "Epoch 947/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 612.5081 - acc: 0.8063 - val_loss: 612.5849 - val_acc: 0.6667\n",
            "Epoch 948/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 612.4604 - acc: 0.7890 - val_loss: 612.5268 - val_acc: 0.6667\n",
            "Epoch 949/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 612.3948 - acc: 0.7890 - val_loss: 612.4688 - val_acc: 0.6667\n",
            "Epoch 950/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 612.3385 - acc: 0.8124 - val_loss: 612.4106 - val_acc: 0.6667\n",
            "Epoch 951/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 612.2824 - acc: 0.8002 - val_loss: 612.3526 - val_acc: 0.6667\n",
            "Epoch 952/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 612.2243 - acc: 0.7911 - val_loss: 612.2946 - val_acc: 0.6667\n",
            "Epoch 953/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 612.1648 - acc: 0.8002 - val_loss: 612.2366 - val_acc: 0.6667\n",
            "Epoch 954/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 612.1037 - acc: 0.8134 - val_loss: 612.1786 - val_acc: 0.6667\n",
            "Epoch 955/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 612.0525 - acc: 0.7961 - val_loss: 612.1205 - val_acc: 0.6667\n",
            "Epoch 956/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 611.9884 - acc: 0.7982 - val_loss: 612.0624 - val_acc: 0.6667\n",
            "Epoch 957/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 611.9271 - acc: 0.8144 - val_loss: 612.0044 - val_acc: 0.6667\n",
            "Epoch 958/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 611.8702 - acc: 0.8103 - val_loss: 611.9465 - val_acc: 0.6693\n",
            "Epoch 959/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 611.8083 - acc: 0.8195 - val_loss: 611.8883 - val_acc: 0.6693\n",
            "Epoch 960/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 611.7613 - acc: 0.8032 - val_loss: 611.8305 - val_acc: 0.6693\n",
            "Epoch 961/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 611.6997 - acc: 0.8134 - val_loss: 611.7724 - val_acc: 0.6693\n",
            "Epoch 962/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 611.6452 - acc: 0.7992 - val_loss: 611.7145 - val_acc: 0.6693\n",
            "Epoch 963/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 611.5825 - acc: 0.8022 - val_loss: 611.6565 - val_acc: 0.6693\n",
            "Epoch 964/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 611.5307 - acc: 0.7982 - val_loss: 611.5983 - val_acc: 0.6693\n",
            "Epoch 965/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 611.4659 - acc: 0.8043 - val_loss: 611.5403 - val_acc: 0.6693\n",
            "Epoch 966/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 611.4082 - acc: 0.8103 - val_loss: 611.4825 - val_acc: 0.6693\n",
            "Epoch 967/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 611.3515 - acc: 0.8053 - val_loss: 611.4244 - val_acc: 0.6693\n",
            "Epoch 968/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 611.2959 - acc: 0.8114 - val_loss: 611.3664 - val_acc: 0.6693\n",
            "Epoch 969/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 611.2342 - acc: 0.8022 - val_loss: 611.3084 - val_acc: 0.6693\n",
            "Epoch 970/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 611.1829 - acc: 0.8043 - val_loss: 611.2504 - val_acc: 0.6693\n",
            "Epoch 971/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 611.1140 - acc: 0.8124 - val_loss: 611.1924 - val_acc: 0.6693\n",
            "Epoch 972/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 611.0583 - acc: 0.8144 - val_loss: 611.1345 - val_acc: 0.6693\n",
            "Epoch 973/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 611.0045 - acc: 0.8012 - val_loss: 611.0766 - val_acc: 0.6693\n",
            "Epoch 974/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 610.9507 - acc: 0.7992 - val_loss: 611.0186 - val_acc: 0.6693\n",
            "Epoch 975/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 610.8916 - acc: 0.7961 - val_loss: 610.9608 - val_acc: 0.6693\n",
            "Epoch 976/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 610.8296 - acc: 0.8063 - val_loss: 610.9028 - val_acc: 0.6693\n",
            "Epoch 977/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 610.7672 - acc: 0.8144 - val_loss: 610.8448 - val_acc: 0.6693\n",
            "Epoch 978/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 610.7118 - acc: 0.8215 - val_loss: 610.7869 - val_acc: 0.6693\n",
            "Epoch 979/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 610.6626 - acc: 0.8032 - val_loss: 610.7290 - val_acc: 0.6693\n",
            "Epoch 980/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 610.6004 - acc: 0.7982 - val_loss: 610.6709 - val_acc: 0.6693\n",
            "Epoch 981/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 610.5343 - acc: 0.8053 - val_loss: 610.6131 - val_acc: 0.6693\n",
            "Epoch 982/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 610.4762 - acc: 0.8063 - val_loss: 610.5552 - val_acc: 0.6693\n",
            "Epoch 983/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 610.4250 - acc: 0.8002 - val_loss: 610.4972 - val_acc: 0.6693\n",
            "Epoch 984/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 610.3610 - acc: 0.8022 - val_loss: 610.4393 - val_acc: 0.6693\n",
            "Epoch 985/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 610.2981 - acc: 0.8276 - val_loss: 610.3815 - val_acc: 0.6693\n",
            "Epoch 986/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 610.2480 - acc: 0.7992 - val_loss: 610.3236 - val_acc: 0.6693\n",
            "Epoch 987/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 610.1908 - acc: 0.8103 - val_loss: 610.2655 - val_acc: 0.6693\n",
            "Epoch 988/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 610.1366 - acc: 0.8022 - val_loss: 610.2076 - val_acc: 0.6693\n",
            "Epoch 989/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 610.0720 - acc: 0.8134 - val_loss: 610.1497 - val_acc: 0.6693\n",
            "Epoch 990/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 610.0193 - acc: 0.7982 - val_loss: 610.0918 - val_acc: 0.6693\n",
            "Epoch 991/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 609.9504 - acc: 0.8266 - val_loss: 610.0339 - val_acc: 0.6693\n",
            "Epoch 992/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 609.9019 - acc: 0.8032 - val_loss: 609.9761 - val_acc: 0.6693\n",
            "Epoch 993/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 609.8438 - acc: 0.8063 - val_loss: 609.9182 - val_acc: 0.6693\n",
            "Epoch 994/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 609.7783 - acc: 0.8103 - val_loss: 609.8601 - val_acc: 0.6693\n",
            "Epoch 995/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 609.7278 - acc: 0.8114 - val_loss: 609.8023 - val_acc: 0.6693\n",
            "Epoch 996/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 609.6793 - acc: 0.7951 - val_loss: 609.7444 - val_acc: 0.6693\n",
            "Epoch 997/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 609.6150 - acc: 0.8093 - val_loss: 609.6864 - val_acc: 0.6693\n",
            "Epoch 998/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 609.5505 - acc: 0.8083 - val_loss: 609.6286 - val_acc: 0.6693\n",
            "Epoch 999/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 609.4952 - acc: 0.7941 - val_loss: 609.5707 - val_acc: 0.6693\n",
            "Epoch 1000/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 609.4389 - acc: 0.7951 - val_loss: 609.5130 - val_acc: 0.6693\n",
            "Epoch 1001/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 609.3725 - acc: 0.8073 - val_loss: 609.4549 - val_acc: 0.6693\n",
            "Epoch 1002/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 609.3262 - acc: 0.8002 - val_loss: 609.3971 - val_acc: 0.6693\n",
            "Epoch 1003/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 609.2703 - acc: 0.8144 - val_loss: 609.3392 - val_acc: 0.6693\n",
            "Epoch 1004/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 609.2039 - acc: 0.8225 - val_loss: 609.2814 - val_acc: 0.6693\n",
            "Epoch 1005/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 609.1562 - acc: 0.8144 - val_loss: 609.2235 - val_acc: 0.6693\n",
            "Epoch 1006/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 609.0843 - acc: 0.8144 - val_loss: 609.1657 - val_acc: 0.6667\n",
            "Epoch 1007/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 609.0330 - acc: 0.8164 - val_loss: 609.1079 - val_acc: 0.6667\n",
            "Epoch 1008/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 608.9788 - acc: 0.8032 - val_loss: 609.0501 - val_acc: 0.6693\n",
            "Epoch 1009/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 608.9109 - acc: 0.8144 - val_loss: 608.9922 - val_acc: 0.6693\n",
            "Epoch 1010/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 608.8602 - acc: 0.8043 - val_loss: 608.9344 - val_acc: 0.6667\n",
            "Epoch 1011/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 608.7967 - acc: 0.8185 - val_loss: 608.8765 - val_acc: 0.6667\n",
            "Epoch 1012/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 608.7424 - acc: 0.8073 - val_loss: 608.8188 - val_acc: 0.6667\n",
            "Epoch 1013/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 608.6818 - acc: 0.8205 - val_loss: 608.7610 - val_acc: 0.6667\n",
            "Epoch 1014/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 608.6285 - acc: 0.8154 - val_loss: 608.7032 - val_acc: 0.6667\n",
            "Epoch 1015/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 608.5777 - acc: 0.8053 - val_loss: 608.6455 - val_acc: 0.6667\n",
            "Epoch 1016/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 608.5127 - acc: 0.8063 - val_loss: 608.5877 - val_acc: 0.6667\n",
            "Epoch 1017/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 608.4665 - acc: 0.7870 - val_loss: 608.5300 - val_acc: 0.6693\n",
            "Epoch 1018/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 608.4010 - acc: 0.7972 - val_loss: 608.4722 - val_acc: 0.6667\n",
            "Epoch 1019/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 608.3379 - acc: 0.8134 - val_loss: 608.4145 - val_acc: 0.6667\n",
            "Epoch 1020/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 608.2777 - acc: 0.8012 - val_loss: 608.3568 - val_acc: 0.6667\n",
            "Epoch 1021/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 608.2204 - acc: 0.8063 - val_loss: 608.2989 - val_acc: 0.6667\n",
            "Epoch 1022/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 608.1591 - acc: 0.8073 - val_loss: 608.2412 - val_acc: 0.6667\n",
            "Epoch 1023/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 608.1089 - acc: 0.7941 - val_loss: 608.1834 - val_acc: 0.6667\n",
            "Epoch 1024/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 608.0536 - acc: 0.7911 - val_loss: 608.1257 - val_acc: 0.6667\n",
            "Epoch 1025/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 607.9908 - acc: 0.8215 - val_loss: 608.0679 - val_acc: 0.6667\n",
            "Epoch 1026/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 607.9335 - acc: 0.8195 - val_loss: 608.0101 - val_acc: 0.6693\n",
            "Epoch 1027/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 607.8778 - acc: 0.8063 - val_loss: 607.9524 - val_acc: 0.6693\n",
            "Epoch 1028/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 607.8166 - acc: 0.8134 - val_loss: 607.8948 - val_acc: 0.6693\n",
            "Epoch 1029/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 607.7537 - acc: 0.8093 - val_loss: 607.8372 - val_acc: 0.6667\n",
            "Epoch 1030/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 607.7136 - acc: 0.7972 - val_loss: 607.7794 - val_acc: 0.6667\n",
            "Epoch 1031/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 607.6450 - acc: 0.8032 - val_loss: 607.7217 - val_acc: 0.6667\n",
            "Epoch 1032/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 607.5901 - acc: 0.8134 - val_loss: 607.6639 - val_acc: 0.6667\n",
            "Epoch 1033/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 607.5252 - acc: 0.8235 - val_loss: 607.6062 - val_acc: 0.6667\n",
            "Epoch 1034/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 607.4713 - acc: 0.8174 - val_loss: 607.5484 - val_acc: 0.6667\n",
            "Epoch 1035/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 607.4181 - acc: 0.7931 - val_loss: 607.4907 - val_acc: 0.6667\n",
            "Epoch 1036/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 607.3550 - acc: 0.8093 - val_loss: 607.4331 - val_acc: 0.6667\n",
            "Epoch 1037/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 607.2997 - acc: 0.8022 - val_loss: 607.3755 - val_acc: 0.6667\n",
            "Epoch 1038/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 607.2448 - acc: 0.8296 - val_loss: 607.3178 - val_acc: 0.6667\n",
            "Epoch 1039/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 607.1827 - acc: 0.8154 - val_loss: 607.2600 - val_acc: 0.6667\n",
            "Epoch 1040/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 607.1303 - acc: 0.8073 - val_loss: 607.2021 - val_acc: 0.6667\n",
            "Epoch 1041/5000\n",
            "986/986 [==============================] - 0s 297us/step - loss: 607.0669 - acc: 0.8114 - val_loss: 607.1445 - val_acc: 0.6667\n",
            "Epoch 1042/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 607.0072 - acc: 0.8063 - val_loss: 607.0870 - val_acc: 0.6667\n",
            "Epoch 1043/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 606.9483 - acc: 0.8144 - val_loss: 607.0293 - val_acc: 0.6667\n",
            "Epoch 1044/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 606.8973 - acc: 0.8103 - val_loss: 606.9716 - val_acc: 0.6667\n",
            "Epoch 1045/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 606.8390 - acc: 0.8053 - val_loss: 606.9140 - val_acc: 0.6667\n",
            "Epoch 1046/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 606.7792 - acc: 0.8083 - val_loss: 606.8563 - val_acc: 0.6667\n",
            "Epoch 1047/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 606.7260 - acc: 0.7890 - val_loss: 606.7986 - val_acc: 0.6667\n",
            "Epoch 1048/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 606.6610 - acc: 0.8043 - val_loss: 606.7410 - val_acc: 0.6667\n",
            "Epoch 1049/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 606.6057 - acc: 0.8225 - val_loss: 606.6832 - val_acc: 0.6667\n",
            "Epoch 1050/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 606.5434 - acc: 0.8266 - val_loss: 606.6255 - val_acc: 0.6693\n",
            "Epoch 1051/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 606.4876 - acc: 0.8124 - val_loss: 606.5680 - val_acc: 0.6667\n",
            "Epoch 1052/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 606.4327 - acc: 0.8043 - val_loss: 606.5105 - val_acc: 0.6667\n",
            "Epoch 1053/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 606.3780 - acc: 0.8083 - val_loss: 606.4528 - val_acc: 0.6693\n",
            "Epoch 1054/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 606.3179 - acc: 0.8164 - val_loss: 606.3950 - val_acc: 0.6693\n",
            "Epoch 1055/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 606.2589 - acc: 0.8266 - val_loss: 606.3373 - val_acc: 0.6693\n",
            "Epoch 1056/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 606.2066 - acc: 0.8073 - val_loss: 606.2798 - val_acc: 0.6693\n",
            "Epoch 1057/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 606.1439 - acc: 0.8205 - val_loss: 606.2222 - val_acc: 0.6693\n",
            "Epoch 1058/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 606.0892 - acc: 0.8093 - val_loss: 606.1645 - val_acc: 0.6693\n",
            "Epoch 1059/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 606.0227 - acc: 0.8235 - val_loss: 606.1068 - val_acc: 0.6693\n",
            "Epoch 1060/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 605.9721 - acc: 0.8185 - val_loss: 606.0491 - val_acc: 0.6720\n",
            "Epoch 1061/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 605.9088 - acc: 0.8205 - val_loss: 605.9914 - val_acc: 0.6720\n",
            "Epoch 1062/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 605.8598 - acc: 0.7972 - val_loss: 605.9339 - val_acc: 0.6720\n",
            "Epoch 1063/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 605.8042 - acc: 0.7901 - val_loss: 605.8763 - val_acc: 0.6693\n",
            "Epoch 1064/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 605.7425 - acc: 0.8154 - val_loss: 605.8188 - val_acc: 0.6720\n",
            "Epoch 1065/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 605.6779 - acc: 0.8195 - val_loss: 605.7611 - val_acc: 0.6720\n",
            "Epoch 1066/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 605.6255 - acc: 0.8174 - val_loss: 605.7035 - val_acc: 0.6720\n",
            "Epoch 1067/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 605.5720 - acc: 0.7982 - val_loss: 605.6458 - val_acc: 0.6720\n",
            "Epoch 1068/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 605.5096 - acc: 0.8083 - val_loss: 605.5882 - val_acc: 0.6720\n",
            "Epoch 1069/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 605.4569 - acc: 0.8043 - val_loss: 605.5306 - val_acc: 0.6693\n",
            "Epoch 1070/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 605.3933 - acc: 0.8205 - val_loss: 605.4730 - val_acc: 0.6693\n",
            "Epoch 1071/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 605.3388 - acc: 0.8205 - val_loss: 605.4154 - val_acc: 0.6693\n",
            "Epoch 1072/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 605.2778 - acc: 0.8256 - val_loss: 605.3579 - val_acc: 0.6693\n",
            "Epoch 1073/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 605.2201 - acc: 0.8154 - val_loss: 605.3004 - val_acc: 0.6720\n",
            "Epoch 1074/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 605.1626 - acc: 0.8174 - val_loss: 605.2429 - val_acc: 0.6720\n",
            "Epoch 1075/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 605.1074 - acc: 0.8225 - val_loss: 605.1853 - val_acc: 0.6693\n",
            "Epoch 1076/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 605.0429 - acc: 0.8256 - val_loss: 605.1277 - val_acc: 0.6693\n",
            "Epoch 1077/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 604.9877 - acc: 0.8124 - val_loss: 605.0701 - val_acc: 0.6720\n",
            "Epoch 1078/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 604.9289 - acc: 0.8195 - val_loss: 605.0125 - val_acc: 0.6720\n",
            "Epoch 1079/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 604.8736 - acc: 0.8124 - val_loss: 604.9549 - val_acc: 0.6720\n",
            "Epoch 1080/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 604.8191 - acc: 0.8215 - val_loss: 604.8974 - val_acc: 0.6720\n",
            "Epoch 1081/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 604.7555 - acc: 0.8144 - val_loss: 604.8398 - val_acc: 0.6693\n",
            "Epoch 1082/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 604.7026 - acc: 0.8093 - val_loss: 604.7823 - val_acc: 0.6693\n",
            "Epoch 1083/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 604.6411 - acc: 0.8073 - val_loss: 604.7248 - val_acc: 0.6693\n",
            "Epoch 1084/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 604.5852 - acc: 0.8083 - val_loss: 604.6673 - val_acc: 0.6693\n",
            "Epoch 1085/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 604.5209 - acc: 0.8367 - val_loss: 604.6097 - val_acc: 0.6693\n",
            "Epoch 1086/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 604.4750 - acc: 0.8174 - val_loss: 604.5522 - val_acc: 0.6693\n",
            "Epoch 1087/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 604.4127 - acc: 0.8185 - val_loss: 604.4947 - val_acc: 0.6693\n",
            "Epoch 1088/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 604.3588 - acc: 0.8185 - val_loss: 604.4372 - val_acc: 0.6693\n",
            "Epoch 1089/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 604.2928 - acc: 0.8154 - val_loss: 604.3797 - val_acc: 0.6693\n",
            "Epoch 1090/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 604.2364 - acc: 0.8276 - val_loss: 604.3222 - val_acc: 0.6693\n",
            "Epoch 1091/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 604.1793 - acc: 0.8245 - val_loss: 604.2647 - val_acc: 0.6693\n",
            "Epoch 1092/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 604.1216 - acc: 0.8337 - val_loss: 604.2071 - val_acc: 0.6720\n",
            "Epoch 1093/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 604.0670 - acc: 0.8103 - val_loss: 604.1497 - val_acc: 0.6720\n",
            "Epoch 1094/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 604.0109 - acc: 0.8083 - val_loss: 604.0922 - val_acc: 0.6720\n",
            "Epoch 1095/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 603.9488 - acc: 0.8103 - val_loss: 604.0347 - val_acc: 0.6720\n",
            "Epoch 1096/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 603.9079 - acc: 0.8083 - val_loss: 603.9773 - val_acc: 0.6720\n",
            "Epoch 1097/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 603.8338 - acc: 0.8408 - val_loss: 603.9198 - val_acc: 0.6693\n",
            "Epoch 1098/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 603.7847 - acc: 0.8053 - val_loss: 603.8623 - val_acc: 0.6693\n",
            "Epoch 1099/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 603.7235 - acc: 0.8012 - val_loss: 603.8050 - val_acc: 0.6693\n",
            "Epoch 1100/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 603.6643 - acc: 0.8103 - val_loss: 603.7473 - val_acc: 0.6720\n",
            "Epoch 1101/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 603.6061 - acc: 0.8316 - val_loss: 603.6900 - val_acc: 0.6720\n",
            "Epoch 1102/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 603.5513 - acc: 0.8276 - val_loss: 603.6325 - val_acc: 0.6693\n",
            "Epoch 1103/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 603.4997 - acc: 0.8185 - val_loss: 603.5750 - val_acc: 0.6720\n",
            "Epoch 1104/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 603.4341 - acc: 0.8174 - val_loss: 603.5174 - val_acc: 0.6720\n",
            "Epoch 1105/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 603.3738 - acc: 0.8164 - val_loss: 603.4600 - val_acc: 0.6720\n",
            "Epoch 1106/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 603.3238 - acc: 0.8205 - val_loss: 603.4025 - val_acc: 0.6720\n",
            "Epoch 1107/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 603.2618 - acc: 0.8124 - val_loss: 603.3451 - val_acc: 0.6693\n",
            "Epoch 1108/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 603.2002 - acc: 0.8286 - val_loss: 603.2877 - val_acc: 0.6693\n",
            "Epoch 1109/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 603.1478 - acc: 0.8245 - val_loss: 603.2302 - val_acc: 0.6693\n",
            "Epoch 1110/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 603.0876 - acc: 0.8114 - val_loss: 603.1730 - val_acc: 0.6693\n",
            "Epoch 1111/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 603.0341 - acc: 0.8103 - val_loss: 603.1155 - val_acc: 0.6693\n",
            "Epoch 1112/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 602.9749 - acc: 0.8114 - val_loss: 603.0581 - val_acc: 0.6720\n",
            "Epoch 1113/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 602.9186 - acc: 0.8154 - val_loss: 603.0008 - val_acc: 0.6693\n",
            "Epoch 1114/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 602.8591 - acc: 0.8164 - val_loss: 602.9433 - val_acc: 0.6693\n",
            "Epoch 1115/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 602.7989 - acc: 0.8306 - val_loss: 602.8860 - val_acc: 0.6693\n",
            "Epoch 1116/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 602.7505 - acc: 0.8144 - val_loss: 602.8286 - val_acc: 0.6720\n",
            "Epoch 1117/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 602.6907 - acc: 0.8154 - val_loss: 602.7713 - val_acc: 0.6693\n",
            "Epoch 1118/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 602.6259 - acc: 0.8276 - val_loss: 602.7138 - val_acc: 0.6693\n",
            "Epoch 1119/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 602.5713 - acc: 0.8144 - val_loss: 602.6564 - val_acc: 0.6720\n",
            "Epoch 1120/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 602.5187 - acc: 0.8195 - val_loss: 602.5991 - val_acc: 0.6720\n",
            "Epoch 1121/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 602.4576 - acc: 0.8154 - val_loss: 602.5417 - val_acc: 0.6746\n",
            "Epoch 1122/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 602.4028 - acc: 0.8103 - val_loss: 602.4843 - val_acc: 0.6746\n",
            "Epoch 1123/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 602.3388 - acc: 0.8256 - val_loss: 602.4270 - val_acc: 0.6746\n",
            "Epoch 1124/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 602.2793 - acc: 0.8245 - val_loss: 602.3696 - val_acc: 0.6746\n",
            "Epoch 1125/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 602.2264 - acc: 0.8144 - val_loss: 602.3122 - val_acc: 0.6746\n",
            "Epoch 1126/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 602.1735 - acc: 0.8114 - val_loss: 602.2548 - val_acc: 0.6746\n",
            "Epoch 1127/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 602.1191 - acc: 0.8286 - val_loss: 602.1974 - val_acc: 0.6772\n",
            "Epoch 1128/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 602.0600 - acc: 0.8174 - val_loss: 602.1402 - val_acc: 0.6772\n",
            "Epoch 1129/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 601.9979 - acc: 0.8225 - val_loss: 602.0828 - val_acc: 0.6772\n",
            "Epoch 1130/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 601.9442 - acc: 0.8205 - val_loss: 602.0256 - val_acc: 0.6746\n",
            "Epoch 1131/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 601.8783 - acc: 0.8306 - val_loss: 601.9682 - val_acc: 0.6772\n",
            "Epoch 1132/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 601.8240 - acc: 0.8134 - val_loss: 601.9109 - val_acc: 0.6746\n",
            "Epoch 1133/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 601.7699 - acc: 0.8195 - val_loss: 601.8535 - val_acc: 0.6746\n",
            "Epoch 1134/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 601.7066 - acc: 0.8235 - val_loss: 601.7963 - val_acc: 0.6746\n",
            "Epoch 1135/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 601.6479 - acc: 0.8205 - val_loss: 601.7391 - val_acc: 0.6746\n",
            "Epoch 1136/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 601.6059 - acc: 0.8154 - val_loss: 601.6818 - val_acc: 0.6746\n",
            "Epoch 1137/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 601.5458 - acc: 0.8063 - val_loss: 601.6244 - val_acc: 0.6746\n",
            "Epoch 1138/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 601.4865 - acc: 0.8235 - val_loss: 601.5672 - val_acc: 0.6746\n",
            "Epoch 1139/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 601.4224 - acc: 0.8205 - val_loss: 601.5100 - val_acc: 0.6746\n",
            "Epoch 1140/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 601.3728 - acc: 0.8103 - val_loss: 601.4528 - val_acc: 0.6746\n",
            "Epoch 1141/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 601.3145 - acc: 0.8043 - val_loss: 601.3954 - val_acc: 0.6746\n",
            "Epoch 1142/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 601.2517 - acc: 0.8144 - val_loss: 601.3381 - val_acc: 0.6746\n",
            "Epoch 1143/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 601.1949 - acc: 0.8205 - val_loss: 601.2810 - val_acc: 0.6746\n",
            "Epoch 1144/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 601.1455 - acc: 0.8134 - val_loss: 601.2238 - val_acc: 0.6772\n",
            "Epoch 1145/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 601.0833 - acc: 0.8195 - val_loss: 601.1664 - val_acc: 0.6746\n",
            "Epoch 1146/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 601.0213 - acc: 0.8205 - val_loss: 601.1092 - val_acc: 0.6746\n",
            "Epoch 1147/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 600.9713 - acc: 0.8134 - val_loss: 601.0520 - val_acc: 0.6746\n",
            "Epoch 1148/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 600.9107 - acc: 0.8316 - val_loss: 600.9948 - val_acc: 0.6746\n",
            "Epoch 1149/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 600.8498 - acc: 0.8327 - val_loss: 600.9375 - val_acc: 0.6746\n",
            "Epoch 1150/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 600.7994 - acc: 0.8235 - val_loss: 600.8803 - val_acc: 0.6746\n",
            "Epoch 1151/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 600.7316 - acc: 0.8225 - val_loss: 600.8231 - val_acc: 0.6746\n",
            "Epoch 1152/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 600.6795 - acc: 0.8185 - val_loss: 600.7658 - val_acc: 0.6746\n",
            "Epoch 1153/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 600.6214 - acc: 0.8195 - val_loss: 600.7086 - val_acc: 0.6746\n",
            "Epoch 1154/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 600.5694 - acc: 0.8195 - val_loss: 600.6515 - val_acc: 0.6746\n",
            "Epoch 1155/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 600.5093 - acc: 0.8124 - val_loss: 600.5941 - val_acc: 0.6746\n",
            "Epoch 1156/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 600.4552 - acc: 0.8205 - val_loss: 600.5368 - val_acc: 0.6746\n",
            "Epoch 1157/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 600.3905 - acc: 0.8256 - val_loss: 600.4797 - val_acc: 0.6746\n",
            "Epoch 1158/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 600.3350 - acc: 0.8235 - val_loss: 600.4226 - val_acc: 0.6746\n",
            "Epoch 1159/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 600.2773 - acc: 0.8398 - val_loss: 600.3655 - val_acc: 0.6746\n",
            "Epoch 1160/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 600.2304 - acc: 0.8124 - val_loss: 600.3082 - val_acc: 0.6746\n",
            "Epoch 1161/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 600.1643 - acc: 0.8398 - val_loss: 600.2511 - val_acc: 0.6746\n",
            "Epoch 1162/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 600.1001 - acc: 0.8306 - val_loss: 600.1939 - val_acc: 0.6746\n",
            "Epoch 1163/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 600.0496 - acc: 0.8235 - val_loss: 600.1367 - val_acc: 0.6746\n",
            "Epoch 1164/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 599.9972 - acc: 0.8205 - val_loss: 600.0796 - val_acc: 0.6746\n",
            "Epoch 1165/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 599.9354 - acc: 0.8245 - val_loss: 600.0225 - val_acc: 0.6772\n",
            "Epoch 1166/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 599.8759 - acc: 0.8377 - val_loss: 599.9654 - val_acc: 0.6746\n",
            "Epoch 1167/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 599.8216 - acc: 0.8144 - val_loss: 599.9081 - val_acc: 0.6772\n",
            "Epoch 1168/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 599.7628 - acc: 0.8266 - val_loss: 599.8510 - val_acc: 0.6772\n",
            "Epoch 1169/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 599.7156 - acc: 0.8144 - val_loss: 599.7939 - val_acc: 0.6746\n",
            "Epoch 1170/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 599.6480 - acc: 0.8327 - val_loss: 599.7368 - val_acc: 0.6772\n",
            "Epoch 1171/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 599.5889 - acc: 0.8245 - val_loss: 599.6797 - val_acc: 0.6772\n",
            "Epoch 1172/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 599.5321 - acc: 0.8316 - val_loss: 599.6226 - val_acc: 0.6746\n",
            "Epoch 1173/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 599.4779 - acc: 0.8245 - val_loss: 599.5656 - val_acc: 0.6746\n",
            "Epoch 1174/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 599.4187 - acc: 0.8296 - val_loss: 599.5084 - val_acc: 0.6772\n",
            "Epoch 1175/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 599.3640 - acc: 0.8235 - val_loss: 599.4513 - val_acc: 0.6772\n",
            "Epoch 1176/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 599.3030 - acc: 0.8245 - val_loss: 599.3942 - val_acc: 0.6772\n",
            "Epoch 1177/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 599.2499 - acc: 0.8347 - val_loss: 599.3370 - val_acc: 0.6772\n",
            "Epoch 1178/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 599.1926 - acc: 0.8266 - val_loss: 599.2800 - val_acc: 0.6799\n",
            "Epoch 1179/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 599.1412 - acc: 0.8235 - val_loss: 599.2231 - val_acc: 0.6799\n",
            "Epoch 1180/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 599.0754 - acc: 0.8256 - val_loss: 599.1661 - val_acc: 0.6799\n",
            "Epoch 1181/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 599.0255 - acc: 0.8225 - val_loss: 599.1089 - val_acc: 0.6799\n",
            "Epoch 1182/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 598.9712 - acc: 0.8053 - val_loss: 599.0520 - val_acc: 0.6772\n",
            "Epoch 1183/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 598.9047 - acc: 0.8327 - val_loss: 598.9949 - val_acc: 0.6772\n",
            "Epoch 1184/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 598.8607 - acc: 0.8083 - val_loss: 598.9379 - val_acc: 0.6799\n",
            "Epoch 1185/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 598.7921 - acc: 0.8256 - val_loss: 598.8808 - val_acc: 0.6825\n",
            "Epoch 1186/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 598.7356 - acc: 0.8205 - val_loss: 598.8239 - val_acc: 0.6799\n",
            "Epoch 1187/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 598.6809 - acc: 0.8377 - val_loss: 598.7669 - val_acc: 0.6825\n",
            "Epoch 1188/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 598.6241 - acc: 0.8266 - val_loss: 598.7099 - val_acc: 0.6825\n",
            "Epoch 1189/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 598.5625 - acc: 0.8306 - val_loss: 598.6530 - val_acc: 0.6825\n",
            "Epoch 1190/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 598.5035 - acc: 0.8337 - val_loss: 598.5961 - val_acc: 0.6825\n",
            "Epoch 1191/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 598.4532 - acc: 0.8266 - val_loss: 598.5391 - val_acc: 0.6852\n",
            "Epoch 1192/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 598.3981 - acc: 0.8327 - val_loss: 598.4820 - val_acc: 0.6852\n",
            "Epoch 1193/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 598.3373 - acc: 0.8235 - val_loss: 598.4250 - val_acc: 0.6852\n",
            "Epoch 1194/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 598.2807 - acc: 0.8235 - val_loss: 598.3679 - val_acc: 0.6799\n",
            "Epoch 1195/5000\n",
            "986/986 [==============================] - 0s 297us/step - loss: 598.2177 - acc: 0.8337 - val_loss: 598.3110 - val_acc: 0.6825\n",
            "Epoch 1196/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 598.1673 - acc: 0.8316 - val_loss: 598.2541 - val_acc: 0.6852\n",
            "Epoch 1197/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 598.1115 - acc: 0.8235 - val_loss: 598.1970 - val_acc: 0.6878\n",
            "Epoch 1198/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 598.0531 - acc: 0.8174 - val_loss: 598.1400 - val_acc: 0.6878\n",
            "Epoch 1199/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 597.9953 - acc: 0.8296 - val_loss: 598.0830 - val_acc: 0.6905\n",
            "Epoch 1200/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 597.9363 - acc: 0.8256 - val_loss: 598.0260 - val_acc: 0.6905\n",
            "Epoch 1201/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 597.8810 - acc: 0.8256 - val_loss: 597.9692 - val_acc: 0.6905\n",
            "Epoch 1202/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 597.8255 - acc: 0.8337 - val_loss: 597.9123 - val_acc: 0.6878\n",
            "Epoch 1203/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 597.7650 - acc: 0.8245 - val_loss: 597.8553 - val_acc: 0.6905\n",
            "Epoch 1204/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 597.7104 - acc: 0.8357 - val_loss: 597.7984 - val_acc: 0.6905\n",
            "Epoch 1205/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 597.6582 - acc: 0.8134 - val_loss: 597.7414 - val_acc: 0.6905\n",
            "Epoch 1206/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 597.5989 - acc: 0.8195 - val_loss: 597.6844 - val_acc: 0.6905\n",
            "Epoch 1207/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 597.5390 - acc: 0.8195 - val_loss: 597.6275 - val_acc: 0.6905\n",
            "Epoch 1208/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 597.4794 - acc: 0.8235 - val_loss: 597.5706 - val_acc: 0.6905\n",
            "Epoch 1209/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 597.4267 - acc: 0.8225 - val_loss: 597.5135 - val_acc: 0.6878\n",
            "Epoch 1210/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 597.3598 - acc: 0.8306 - val_loss: 597.4566 - val_acc: 0.6905\n",
            "Epoch 1211/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 597.3139 - acc: 0.8195 - val_loss: 597.3997 - val_acc: 0.6878\n",
            "Epoch 1212/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 597.2473 - acc: 0.8347 - val_loss: 597.3426 - val_acc: 0.6905\n",
            "Epoch 1213/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 597.1975 - acc: 0.8286 - val_loss: 597.2857 - val_acc: 0.6905\n",
            "Epoch 1214/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 597.1378 - acc: 0.8286 - val_loss: 597.2288 - val_acc: 0.6905\n",
            "Epoch 1215/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 597.0827 - acc: 0.8215 - val_loss: 597.1719 - val_acc: 0.6905\n",
            "Epoch 1216/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 597.0276 - acc: 0.8337 - val_loss: 597.1152 - val_acc: 0.6905\n",
            "Epoch 1217/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 596.9638 - acc: 0.8377 - val_loss: 597.0583 - val_acc: 0.6905\n",
            "Epoch 1218/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 596.9134 - acc: 0.8479 - val_loss: 597.0013 - val_acc: 0.6905\n",
            "Epoch 1219/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 596.8543 - acc: 0.8418 - val_loss: 596.9445 - val_acc: 0.6878\n",
            "Epoch 1220/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 596.7959 - acc: 0.8296 - val_loss: 596.8876 - val_acc: 0.6878\n",
            "Epoch 1221/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 596.7393 - acc: 0.8357 - val_loss: 596.8306 - val_acc: 0.6905\n",
            "Epoch 1222/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 596.6815 - acc: 0.8428 - val_loss: 596.7737 - val_acc: 0.6852\n",
            "Epoch 1223/5000\n",
            "986/986 [==============================] - 0s 296us/step - loss: 596.6266 - acc: 0.8276 - val_loss: 596.7169 - val_acc: 0.6878\n",
            "Epoch 1224/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 596.5697 - acc: 0.8316 - val_loss: 596.6602 - val_acc: 0.6852\n",
            "Epoch 1225/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 596.5086 - acc: 0.8428 - val_loss: 596.6033 - val_acc: 0.6852\n",
            "Epoch 1226/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 596.4534 - acc: 0.8347 - val_loss: 596.5464 - val_acc: 0.6878\n",
            "Epoch 1227/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 596.3938 - acc: 0.8509 - val_loss: 596.4895 - val_acc: 0.6878\n",
            "Epoch 1228/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 596.3421 - acc: 0.8225 - val_loss: 596.4327 - val_acc: 0.6905\n",
            "Epoch 1229/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 596.2920 - acc: 0.8185 - val_loss: 596.3760 - val_acc: 0.6878\n",
            "Epoch 1230/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 596.2236 - acc: 0.8377 - val_loss: 596.3191 - val_acc: 0.6878\n",
            "Epoch 1231/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 596.1736 - acc: 0.8296 - val_loss: 596.2622 - val_acc: 0.6878\n",
            "Epoch 1232/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 596.1094 - acc: 0.8499 - val_loss: 596.2054 - val_acc: 0.6878\n",
            "Epoch 1233/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 596.0590 - acc: 0.8276 - val_loss: 596.1486 - val_acc: 0.6931\n",
            "Epoch 1234/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 596.0063 - acc: 0.8185 - val_loss: 596.0918 - val_acc: 0.6878\n",
            "Epoch 1235/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 595.9440 - acc: 0.8316 - val_loss: 596.0350 - val_acc: 0.6878\n",
            "Epoch 1236/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 595.8929 - acc: 0.8256 - val_loss: 595.9782 - val_acc: 0.6905\n",
            "Epoch 1237/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 595.8266 - acc: 0.8276 - val_loss: 595.9214 - val_acc: 0.6931\n",
            "Epoch 1238/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 595.7721 - acc: 0.8225 - val_loss: 595.8646 - val_acc: 0.6931\n",
            "Epoch 1239/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 595.7178 - acc: 0.8235 - val_loss: 595.8078 - val_acc: 0.6905\n",
            "Epoch 1240/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 595.6604 - acc: 0.8245 - val_loss: 595.7511 - val_acc: 0.6931\n",
            "Epoch 1241/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 595.6037 - acc: 0.8276 - val_loss: 595.6941 - val_acc: 0.6905\n",
            "Epoch 1242/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 595.5432 - acc: 0.8367 - val_loss: 595.6373 - val_acc: 0.6905\n",
            "Epoch 1243/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 595.4862 - acc: 0.8296 - val_loss: 595.5805 - val_acc: 0.6905\n",
            "Epoch 1244/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 595.4267 - acc: 0.8428 - val_loss: 595.5238 - val_acc: 0.6905\n",
            "Epoch 1245/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 595.3723 - acc: 0.8438 - val_loss: 595.4669 - val_acc: 0.6905\n",
            "Epoch 1246/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 595.3234 - acc: 0.8266 - val_loss: 595.4101 - val_acc: 0.6905\n",
            "Epoch 1247/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 595.2654 - acc: 0.8276 - val_loss: 595.3534 - val_acc: 0.6905\n",
            "Epoch 1248/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 595.2051 - acc: 0.8266 - val_loss: 595.2967 - val_acc: 0.6905\n",
            "Epoch 1249/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 595.1424 - acc: 0.8357 - val_loss: 595.2400 - val_acc: 0.6905\n",
            "Epoch 1250/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 595.0929 - acc: 0.8235 - val_loss: 595.1832 - val_acc: 0.6905\n",
            "Epoch 1251/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 595.0340 - acc: 0.8367 - val_loss: 595.1264 - val_acc: 0.6931\n",
            "Epoch 1252/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 594.9826 - acc: 0.8276 - val_loss: 595.0696 - val_acc: 0.6905\n",
            "Epoch 1253/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 594.9201 - acc: 0.8327 - val_loss: 595.0129 - val_acc: 0.6931\n",
            "Epoch 1254/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 594.8682 - acc: 0.8266 - val_loss: 594.9561 - val_acc: 0.6905\n",
            "Epoch 1255/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 594.8069 - acc: 0.8296 - val_loss: 594.8994 - val_acc: 0.6878\n",
            "Epoch 1256/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 594.7560 - acc: 0.8256 - val_loss: 594.8427 - val_acc: 0.6878\n",
            "Epoch 1257/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 594.6938 - acc: 0.8286 - val_loss: 594.7860 - val_acc: 0.6878\n",
            "Epoch 1258/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 594.6435 - acc: 0.8266 - val_loss: 594.7292 - val_acc: 0.6878\n",
            "Epoch 1259/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 594.5832 - acc: 0.8225 - val_loss: 594.6725 - val_acc: 0.6878\n",
            "Epoch 1260/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 594.5261 - acc: 0.8387 - val_loss: 594.6157 - val_acc: 0.6878\n",
            "Epoch 1261/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 594.4687 - acc: 0.8266 - val_loss: 594.5591 - val_acc: 0.6878\n",
            "Epoch 1262/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 594.4106 - acc: 0.8225 - val_loss: 594.5024 - val_acc: 0.6905\n",
            "Epoch 1263/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 594.3554 - acc: 0.8286 - val_loss: 594.4455 - val_acc: 0.6905\n",
            "Epoch 1264/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 594.2951 - acc: 0.8174 - val_loss: 594.3889 - val_acc: 0.6905\n",
            "Epoch 1265/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 594.2418 - acc: 0.8134 - val_loss: 594.3322 - val_acc: 0.6905\n",
            "Epoch 1266/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 594.1808 - acc: 0.8428 - val_loss: 594.2756 - val_acc: 0.6905\n",
            "Epoch 1267/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 594.1326 - acc: 0.8296 - val_loss: 594.2188 - val_acc: 0.6905\n",
            "Epoch 1268/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 594.0684 - acc: 0.8387 - val_loss: 594.1621 - val_acc: 0.6905\n",
            "Epoch 1269/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 594.0120 - acc: 0.8357 - val_loss: 594.1053 - val_acc: 0.6878\n",
            "Epoch 1270/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 593.9477 - acc: 0.8540 - val_loss: 594.0486 - val_acc: 0.6878\n",
            "Epoch 1271/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 593.8953 - acc: 0.8408 - val_loss: 593.9919 - val_acc: 0.6905\n",
            "Epoch 1272/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 593.8470 - acc: 0.8327 - val_loss: 593.9353 - val_acc: 0.6878\n",
            "Epoch 1273/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 593.7862 - acc: 0.8347 - val_loss: 593.8788 - val_acc: 0.6905\n",
            "Epoch 1274/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 593.7314 - acc: 0.8174 - val_loss: 593.8220 - val_acc: 0.6905\n",
            "Epoch 1275/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 593.6779 - acc: 0.8164 - val_loss: 593.7654 - val_acc: 0.6905\n",
            "Epoch 1276/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 593.6208 - acc: 0.8327 - val_loss: 593.7088 - val_acc: 0.6905\n",
            "Epoch 1277/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 593.5588 - acc: 0.8316 - val_loss: 593.6521 - val_acc: 0.6905\n",
            "Epoch 1278/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 593.4981 - acc: 0.8398 - val_loss: 593.5953 - val_acc: 0.6878\n",
            "Epoch 1279/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 593.4516 - acc: 0.8174 - val_loss: 593.5387 - val_acc: 0.6905\n",
            "Epoch 1280/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 593.3831 - acc: 0.8398 - val_loss: 593.4821 - val_acc: 0.6931\n",
            "Epoch 1281/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 593.3243 - acc: 0.8357 - val_loss: 593.4254 - val_acc: 0.6931\n",
            "Epoch 1282/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 593.2737 - acc: 0.8387 - val_loss: 593.3688 - val_acc: 0.6905\n",
            "Epoch 1283/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 593.2235 - acc: 0.8225 - val_loss: 593.3122 - val_acc: 0.6931\n",
            "Epoch 1284/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 593.1605 - acc: 0.8306 - val_loss: 593.2556 - val_acc: 0.6905\n",
            "Epoch 1285/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 593.1000 - acc: 0.8398 - val_loss: 593.1989 - val_acc: 0.6931\n",
            "Epoch 1286/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 593.0416 - acc: 0.8367 - val_loss: 593.1423 - val_acc: 0.6905\n",
            "Epoch 1287/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 592.9913 - acc: 0.8235 - val_loss: 593.0856 - val_acc: 0.6931\n",
            "Epoch 1288/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 592.9302 - acc: 0.8316 - val_loss: 593.0291 - val_acc: 0.6931\n",
            "Epoch 1289/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 592.8710 - acc: 0.8550 - val_loss: 592.9724 - val_acc: 0.6931\n",
            "Epoch 1290/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 592.8257 - acc: 0.8387 - val_loss: 592.9159 - val_acc: 0.6931\n",
            "Epoch 1291/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 592.7621 - acc: 0.8316 - val_loss: 592.8594 - val_acc: 0.6931\n",
            "Epoch 1292/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 592.7066 - acc: 0.8479 - val_loss: 592.8026 - val_acc: 0.6905\n",
            "Epoch 1293/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 592.6552 - acc: 0.8327 - val_loss: 592.7460 - val_acc: 0.6905\n",
            "Epoch 1294/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 592.5984 - acc: 0.8327 - val_loss: 592.6894 - val_acc: 0.6905\n",
            "Epoch 1295/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 592.5386 - acc: 0.8367 - val_loss: 592.6328 - val_acc: 0.6905\n",
            "Epoch 1296/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 592.4859 - acc: 0.8235 - val_loss: 592.5762 - val_acc: 0.6905\n",
            "Epoch 1297/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 592.4336 - acc: 0.8134 - val_loss: 592.5197 - val_acc: 0.6878\n",
            "Epoch 1298/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 592.3738 - acc: 0.8124 - val_loss: 592.4630 - val_acc: 0.6878\n",
            "Epoch 1299/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 592.3091 - acc: 0.8357 - val_loss: 592.4066 - val_acc: 0.6878\n",
            "Epoch 1300/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 592.2559 - acc: 0.8266 - val_loss: 592.3499 - val_acc: 0.6878\n",
            "Epoch 1301/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 592.2033 - acc: 0.8256 - val_loss: 592.2935 - val_acc: 0.6905\n",
            "Epoch 1302/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 592.1435 - acc: 0.8327 - val_loss: 592.2369 - val_acc: 0.6852\n",
            "Epoch 1303/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 592.0819 - acc: 0.8387 - val_loss: 592.1804 - val_acc: 0.6878\n",
            "Epoch 1304/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 592.0268 - acc: 0.8357 - val_loss: 592.1240 - val_acc: 0.6905\n",
            "Epoch 1305/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 591.9702 - acc: 0.8337 - val_loss: 592.0673 - val_acc: 0.6905\n",
            "Epoch 1306/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 591.9124 - acc: 0.8357 - val_loss: 592.0108 - val_acc: 0.6905\n",
            "Epoch 1307/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 591.8622 - acc: 0.8306 - val_loss: 591.9542 - val_acc: 0.6905\n",
            "Epoch 1308/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 591.7995 - acc: 0.8337 - val_loss: 591.8977 - val_acc: 0.6905\n",
            "Epoch 1309/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 591.7429 - acc: 0.8347 - val_loss: 591.8412 - val_acc: 0.6878\n",
            "Epoch 1310/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 591.6869 - acc: 0.8469 - val_loss: 591.7846 - val_acc: 0.6878\n",
            "Epoch 1311/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 591.6296 - acc: 0.8245 - val_loss: 591.7282 - val_acc: 0.6878\n",
            "Epoch 1312/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 591.5697 - acc: 0.8448 - val_loss: 591.6717 - val_acc: 0.6878\n",
            "Epoch 1313/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 591.5236 - acc: 0.8215 - val_loss: 591.6152 - val_acc: 0.6878\n",
            "Epoch 1314/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 591.4596 - acc: 0.8367 - val_loss: 591.5588 - val_acc: 0.6852\n",
            "Epoch 1315/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 591.4096 - acc: 0.8286 - val_loss: 591.5022 - val_acc: 0.6878\n",
            "Epoch 1316/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 591.3509 - acc: 0.8225 - val_loss: 591.4456 - val_acc: 0.6878\n",
            "Epoch 1317/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 591.2877 - acc: 0.8398 - val_loss: 591.3892 - val_acc: 0.6878\n",
            "Epoch 1318/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 591.2378 - acc: 0.8215 - val_loss: 591.3327 - val_acc: 0.6878\n",
            "Epoch 1319/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 591.1750 - acc: 0.8458 - val_loss: 591.2762 - val_acc: 0.6852\n",
            "Epoch 1320/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 591.1191 - acc: 0.8438 - val_loss: 591.2196 - val_acc: 0.6852\n",
            "Epoch 1321/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 591.0693 - acc: 0.8195 - val_loss: 591.1631 - val_acc: 0.6852\n",
            "Epoch 1322/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 591.0059 - acc: 0.8448 - val_loss: 591.1066 - val_acc: 0.6852\n",
            "Epoch 1323/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 590.9526 - acc: 0.8428 - val_loss: 591.0500 - val_acc: 0.6878\n",
            "Epoch 1324/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 590.8942 - acc: 0.8377 - val_loss: 590.9935 - val_acc: 0.6852\n",
            "Epoch 1325/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 590.8372 - acc: 0.8337 - val_loss: 590.9370 - val_acc: 0.6878\n",
            "Epoch 1326/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 590.7842 - acc: 0.8387 - val_loss: 590.8806 - val_acc: 0.6878\n",
            "Epoch 1327/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 590.7239 - acc: 0.8489 - val_loss: 590.8242 - val_acc: 0.6878\n",
            "Epoch 1328/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 590.6765 - acc: 0.8327 - val_loss: 590.7676 - val_acc: 0.6878\n",
            "Epoch 1329/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 590.6185 - acc: 0.8408 - val_loss: 590.7113 - val_acc: 0.6878\n",
            "Epoch 1330/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 590.5591 - acc: 0.8438 - val_loss: 590.6548 - val_acc: 0.6878\n",
            "Epoch 1331/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 590.4933 - acc: 0.8469 - val_loss: 590.5983 - val_acc: 0.6905\n",
            "Epoch 1332/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 590.4464 - acc: 0.8408 - val_loss: 590.5420 - val_acc: 0.6852\n",
            "Epoch 1333/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 590.3836 - acc: 0.8367 - val_loss: 590.4855 - val_acc: 0.6905\n",
            "Epoch 1334/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 590.3296 - acc: 0.8509 - val_loss: 590.4291 - val_acc: 0.6905\n",
            "Epoch 1335/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 590.2735 - acc: 0.8387 - val_loss: 590.3727 - val_acc: 0.6905\n",
            "Epoch 1336/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 590.2226 - acc: 0.8357 - val_loss: 590.3162 - val_acc: 0.6931\n",
            "Epoch 1337/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 590.1623 - acc: 0.8347 - val_loss: 590.2599 - val_acc: 0.6905\n",
            "Epoch 1338/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 590.1101 - acc: 0.8438 - val_loss: 590.2037 - val_acc: 0.6905\n",
            "Epoch 1339/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 590.0482 - acc: 0.8357 - val_loss: 590.1471 - val_acc: 0.6905\n",
            "Epoch 1340/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 590.0003 - acc: 0.8387 - val_loss: 590.0906 - val_acc: 0.6905\n",
            "Epoch 1341/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 589.9312 - acc: 0.8398 - val_loss: 590.0343 - val_acc: 0.6905\n",
            "Epoch 1342/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 589.8765 - acc: 0.8387 - val_loss: 589.9779 - val_acc: 0.6905\n",
            "Epoch 1343/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 589.8244 - acc: 0.8448 - val_loss: 589.9216 - val_acc: 0.6905\n",
            "Epoch 1344/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 589.7703 - acc: 0.8286 - val_loss: 589.8650 - val_acc: 0.6905\n",
            "Epoch 1345/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 589.7090 - acc: 0.8398 - val_loss: 589.8088 - val_acc: 0.6905\n",
            "Epoch 1346/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 589.6551 - acc: 0.8337 - val_loss: 589.7524 - val_acc: 0.6905\n",
            "Epoch 1347/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 589.5946 - acc: 0.8367 - val_loss: 589.6961 - val_acc: 0.6905\n",
            "Epoch 1348/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 589.5407 - acc: 0.8519 - val_loss: 589.6396 - val_acc: 0.6905\n",
            "Epoch 1349/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 589.4834 - acc: 0.8398 - val_loss: 589.5834 - val_acc: 0.6905\n",
            "Epoch 1350/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 589.4201 - acc: 0.8469 - val_loss: 589.5271 - val_acc: 0.6905\n",
            "Epoch 1351/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 589.3709 - acc: 0.8357 - val_loss: 589.4707 - val_acc: 0.6931\n",
            "Epoch 1352/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 589.3132 - acc: 0.8479 - val_loss: 589.4143 - val_acc: 0.6931\n",
            "Epoch 1353/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 589.2604 - acc: 0.8398 - val_loss: 589.3580 - val_acc: 0.6931\n",
            "Epoch 1354/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 589.2057 - acc: 0.8347 - val_loss: 589.3016 - val_acc: 0.6931\n",
            "Epoch 1355/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 589.1512 - acc: 0.8489 - val_loss: 589.2453 - val_acc: 0.6931\n",
            "Epoch 1356/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 589.0921 - acc: 0.8428 - val_loss: 589.1890 - val_acc: 0.6931\n",
            "Epoch 1357/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 589.0354 - acc: 0.8469 - val_loss: 589.1327 - val_acc: 0.6931\n",
            "Epoch 1358/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 588.9761 - acc: 0.8256 - val_loss: 589.0764 - val_acc: 0.6931\n",
            "Epoch 1359/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 588.9250 - acc: 0.8286 - val_loss: 589.0201 - val_acc: 0.6931\n",
            "Epoch 1360/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 588.8703 - acc: 0.8499 - val_loss: 588.9637 - val_acc: 0.6931\n",
            "Epoch 1361/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 588.8095 - acc: 0.8357 - val_loss: 588.9075 - val_acc: 0.6931\n",
            "Epoch 1362/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 588.7526 - acc: 0.8256 - val_loss: 588.8512 - val_acc: 0.6931\n",
            "Epoch 1363/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 588.6955 - acc: 0.8316 - val_loss: 588.7950 - val_acc: 0.6931\n",
            "Epoch 1364/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 588.6370 - acc: 0.8438 - val_loss: 588.7388 - val_acc: 0.6931\n",
            "Epoch 1365/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 588.5832 - acc: 0.8377 - val_loss: 588.6824 - val_acc: 0.6931\n",
            "Epoch 1366/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 588.5324 - acc: 0.8387 - val_loss: 588.6261 - val_acc: 0.6931\n",
            "Epoch 1367/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 588.4662 - acc: 0.8387 - val_loss: 588.5699 - val_acc: 0.6931\n",
            "Epoch 1368/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 588.4197 - acc: 0.8306 - val_loss: 588.5137 - val_acc: 0.6931\n",
            "Epoch 1369/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 588.3580 - acc: 0.8438 - val_loss: 588.4573 - val_acc: 0.6931\n",
            "Epoch 1370/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 588.2944 - acc: 0.8387 - val_loss: 588.4011 - val_acc: 0.6931\n",
            "Epoch 1371/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 588.2517 - acc: 0.8387 - val_loss: 588.3448 - val_acc: 0.6931\n",
            "Epoch 1372/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 588.1961 - acc: 0.8296 - val_loss: 588.2886 - val_acc: 0.6931\n",
            "Epoch 1373/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 588.1334 - acc: 0.8357 - val_loss: 588.2323 - val_acc: 0.6958\n",
            "Epoch 1374/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 588.0773 - acc: 0.8469 - val_loss: 588.1761 - val_acc: 0.6931\n",
            "Epoch 1375/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 588.0191 - acc: 0.8408 - val_loss: 588.1199 - val_acc: 0.6905\n",
            "Epoch 1376/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 587.9616 - acc: 0.8448 - val_loss: 588.0635 - val_acc: 0.6905\n",
            "Epoch 1377/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 587.9001 - acc: 0.8499 - val_loss: 588.0075 - val_acc: 0.6931\n",
            "Epoch 1378/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 587.8520 - acc: 0.8357 - val_loss: 587.9512 - val_acc: 0.6905\n",
            "Epoch 1379/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 587.7943 - acc: 0.8377 - val_loss: 587.8949 - val_acc: 0.6905\n",
            "Epoch 1380/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 587.7427 - acc: 0.8438 - val_loss: 587.8387 - val_acc: 0.6905\n",
            "Epoch 1381/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 587.6827 - acc: 0.8387 - val_loss: 587.7824 - val_acc: 0.6931\n",
            "Epoch 1382/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 587.6310 - acc: 0.8306 - val_loss: 587.7263 - val_acc: 0.6931\n",
            "Epoch 1383/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 587.5760 - acc: 0.8296 - val_loss: 587.6701 - val_acc: 0.6931\n",
            "Epoch 1384/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 587.5172 - acc: 0.8316 - val_loss: 587.6139 - val_acc: 0.6958\n",
            "Epoch 1385/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 587.4628 - acc: 0.8469 - val_loss: 587.5578 - val_acc: 0.6931\n",
            "Epoch 1386/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 587.4055 - acc: 0.8276 - val_loss: 587.5016 - val_acc: 0.6905\n",
            "Epoch 1387/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 587.3448 - acc: 0.8387 - val_loss: 587.4453 - val_acc: 0.6905\n",
            "Epoch 1388/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 587.2903 - acc: 0.8438 - val_loss: 587.3891 - val_acc: 0.6931\n",
            "Epoch 1389/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 587.2362 - acc: 0.8448 - val_loss: 587.3329 - val_acc: 0.6931\n",
            "Epoch 1390/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 587.1766 - acc: 0.8469 - val_loss: 587.2768 - val_acc: 0.6905\n",
            "Epoch 1391/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 587.1233 - acc: 0.8347 - val_loss: 587.2206 - val_acc: 0.6905\n",
            "Epoch 1392/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 587.0650 - acc: 0.8316 - val_loss: 587.1646 - val_acc: 0.6905\n",
            "Epoch 1393/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 587.0114 - acc: 0.8337 - val_loss: 587.1083 - val_acc: 0.6905\n",
            "Epoch 1394/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 586.9494 - acc: 0.8377 - val_loss: 587.0522 - val_acc: 0.6905\n",
            "Epoch 1395/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 586.8938 - acc: 0.8316 - val_loss: 586.9960 - val_acc: 0.6931\n",
            "Epoch 1396/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 586.8349 - acc: 0.8499 - val_loss: 586.9398 - val_acc: 0.6931\n",
            "Epoch 1397/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 586.7828 - acc: 0.8357 - val_loss: 586.8837 - val_acc: 0.6931\n",
            "Epoch 1398/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 586.7227 - acc: 0.8418 - val_loss: 586.8275 - val_acc: 0.6931\n",
            "Epoch 1399/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 586.6702 - acc: 0.8377 - val_loss: 586.7714 - val_acc: 0.6931\n",
            "Epoch 1400/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 586.6165 - acc: 0.8296 - val_loss: 586.7153 - val_acc: 0.6931\n",
            "Epoch 1401/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 586.5553 - acc: 0.8408 - val_loss: 586.6591 - val_acc: 0.6931\n",
            "Epoch 1402/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 586.4978 - acc: 0.8387 - val_loss: 586.6031 - val_acc: 0.6931\n",
            "Epoch 1403/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 586.4433 - acc: 0.8408 - val_loss: 586.5468 - val_acc: 0.6931\n",
            "Epoch 1404/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 586.3874 - acc: 0.8489 - val_loss: 586.4908 - val_acc: 0.6931\n",
            "Epoch 1405/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 586.3379 - acc: 0.8357 - val_loss: 586.4346 - val_acc: 0.6931\n",
            "Epoch 1406/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 586.2782 - acc: 0.8398 - val_loss: 586.3784 - val_acc: 0.6931\n",
            "Epoch 1407/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 586.2206 - acc: 0.8367 - val_loss: 586.3222 - val_acc: 0.6931\n",
            "Epoch 1408/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 586.1685 - acc: 0.8256 - val_loss: 586.2662 - val_acc: 0.6905\n",
            "Epoch 1409/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 586.1119 - acc: 0.8327 - val_loss: 586.2101 - val_acc: 0.6905\n",
            "Epoch 1410/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 586.0563 - acc: 0.8347 - val_loss: 586.1539 - val_acc: 0.6931\n",
            "Epoch 1411/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 585.9943 - acc: 0.8418 - val_loss: 586.0979 - val_acc: 0.6931\n",
            "Epoch 1412/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 585.9394 - acc: 0.8296 - val_loss: 586.0420 - val_acc: 0.6931\n",
            "Epoch 1413/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 585.8847 - acc: 0.8408 - val_loss: 585.9859 - val_acc: 0.6931\n",
            "Epoch 1414/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 585.8274 - acc: 0.8458 - val_loss: 585.9298 - val_acc: 0.6931\n",
            "Epoch 1415/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 585.7750 - acc: 0.8357 - val_loss: 585.8737 - val_acc: 0.6931\n",
            "Epoch 1416/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 585.7149 - acc: 0.8377 - val_loss: 585.8176 - val_acc: 0.6931\n",
            "Epoch 1417/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 585.6599 - acc: 0.8387 - val_loss: 585.7616 - val_acc: 0.6931\n",
            "Epoch 1418/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 585.6033 - acc: 0.8448 - val_loss: 585.7056 - val_acc: 0.6958\n",
            "Epoch 1419/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 585.5518 - acc: 0.8337 - val_loss: 585.6496 - val_acc: 0.6931\n",
            "Epoch 1420/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 585.4867 - acc: 0.8479 - val_loss: 585.5936 - val_acc: 0.6931\n",
            "Epoch 1421/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 585.4380 - acc: 0.8327 - val_loss: 585.5374 - val_acc: 0.6958\n",
            "Epoch 1422/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 585.3823 - acc: 0.8367 - val_loss: 585.4814 - val_acc: 0.6931\n",
            "Epoch 1423/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 585.3194 - acc: 0.8499 - val_loss: 585.4253 - val_acc: 0.6931\n",
            "Epoch 1424/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 585.2644 - acc: 0.8428 - val_loss: 585.3693 - val_acc: 0.6931\n",
            "Epoch 1425/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 585.2034 - acc: 0.8590 - val_loss: 585.3132 - val_acc: 0.6931\n",
            "Epoch 1426/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 585.1506 - acc: 0.8448 - val_loss: 585.2572 - val_acc: 0.6958\n",
            "Epoch 1427/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 585.0989 - acc: 0.8377 - val_loss: 585.2011 - val_acc: 0.6931\n",
            "Epoch 1428/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 585.0441 - acc: 0.8377 - val_loss: 585.1452 - val_acc: 0.6958\n",
            "Epoch 1429/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 584.9889 - acc: 0.8245 - val_loss: 585.0891 - val_acc: 0.6931\n",
            "Epoch 1430/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 584.9357 - acc: 0.8387 - val_loss: 585.0331 - val_acc: 0.6931\n",
            "Epoch 1431/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 584.8729 - acc: 0.8367 - val_loss: 584.9771 - val_acc: 0.6931\n",
            "Epoch 1432/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 584.8178 - acc: 0.8387 - val_loss: 584.9212 - val_acc: 0.6931\n",
            "Epoch 1433/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 584.7618 - acc: 0.8387 - val_loss: 584.8652 - val_acc: 0.6931\n",
            "Epoch 1434/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 584.7047 - acc: 0.8428 - val_loss: 584.8092 - val_acc: 0.6931\n",
            "Epoch 1435/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 584.6481 - acc: 0.8479 - val_loss: 584.7531 - val_acc: 0.6905\n",
            "Epoch 1436/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 584.6003 - acc: 0.8316 - val_loss: 584.6972 - val_acc: 0.6931\n",
            "Epoch 1437/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 584.5388 - acc: 0.8570 - val_loss: 584.6411 - val_acc: 0.6931\n",
            "Epoch 1438/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 584.4852 - acc: 0.8398 - val_loss: 584.5851 - val_acc: 0.6931\n",
            "Epoch 1439/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 584.4213 - acc: 0.8469 - val_loss: 584.5289 - val_acc: 0.6931\n",
            "Epoch 1440/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 584.3714 - acc: 0.8469 - val_loss: 584.4730 - val_acc: 0.6931\n",
            "Epoch 1441/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 584.3194 - acc: 0.8357 - val_loss: 584.4169 - val_acc: 0.6931\n",
            "Epoch 1442/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 584.2499 - acc: 0.8580 - val_loss: 584.3609 - val_acc: 0.6931\n",
            "Epoch 1443/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 584.2065 - acc: 0.8347 - val_loss: 584.3050 - val_acc: 0.6931\n",
            "Epoch 1444/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 584.1504 - acc: 0.8367 - val_loss: 584.2491 - val_acc: 0.6931\n",
            "Epoch 1445/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 584.0935 - acc: 0.8296 - val_loss: 584.1930 - val_acc: 0.6931\n",
            "Epoch 1446/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 584.0404 - acc: 0.8398 - val_loss: 584.1371 - val_acc: 0.6958\n",
            "Epoch 1447/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 583.9717 - acc: 0.8509 - val_loss: 584.0811 - val_acc: 0.6958\n",
            "Epoch 1448/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 583.9173 - acc: 0.8357 - val_loss: 584.0252 - val_acc: 0.6905\n",
            "Epoch 1449/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 583.8679 - acc: 0.8438 - val_loss: 583.9692 - val_acc: 0.6958\n",
            "Epoch 1450/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 583.8080 - acc: 0.8469 - val_loss: 583.9132 - val_acc: 0.6931\n",
            "Epoch 1451/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 583.7499 - acc: 0.8408 - val_loss: 583.8573 - val_acc: 0.6931\n",
            "Epoch 1452/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 583.6959 - acc: 0.8377 - val_loss: 583.8014 - val_acc: 0.6931\n",
            "Epoch 1453/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 583.6346 - acc: 0.8489 - val_loss: 583.7454 - val_acc: 0.6931\n",
            "Epoch 1454/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 583.5800 - acc: 0.8357 - val_loss: 583.6895 - val_acc: 0.6931\n",
            "Epoch 1455/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 583.5348 - acc: 0.8377 - val_loss: 583.6336 - val_acc: 0.6931\n",
            "Epoch 1456/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 583.4669 - acc: 0.8540 - val_loss: 583.5777 - val_acc: 0.6931\n",
            "Epoch 1457/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 583.4147 - acc: 0.8499 - val_loss: 583.5218 - val_acc: 0.6931\n",
            "Epoch 1458/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 583.3610 - acc: 0.8560 - val_loss: 583.4658 - val_acc: 0.6931\n",
            "Epoch 1459/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 583.3091 - acc: 0.8398 - val_loss: 583.4099 - val_acc: 0.6905\n",
            "Epoch 1460/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 583.2538 - acc: 0.8479 - val_loss: 583.3541 - val_acc: 0.6931\n",
            "Epoch 1461/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 583.1987 - acc: 0.8286 - val_loss: 583.2981 - val_acc: 0.6958\n",
            "Epoch 1462/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 583.1386 - acc: 0.8489 - val_loss: 583.2422 - val_acc: 0.6958\n",
            "Epoch 1463/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 583.0782 - acc: 0.8509 - val_loss: 583.1863 - val_acc: 0.6931\n",
            "Epoch 1464/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 583.0255 - acc: 0.8418 - val_loss: 583.1304 - val_acc: 0.6905\n",
            "Epoch 1465/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 582.9714 - acc: 0.8489 - val_loss: 583.0745 - val_acc: 0.6931\n",
            "Epoch 1466/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 582.9152 - acc: 0.8316 - val_loss: 583.0188 - val_acc: 0.6905\n",
            "Epoch 1467/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 582.8622 - acc: 0.8367 - val_loss: 582.9627 - val_acc: 0.6905\n",
            "Epoch 1468/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 582.8012 - acc: 0.8448 - val_loss: 582.9069 - val_acc: 0.6931\n",
            "Epoch 1469/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 582.7438 - acc: 0.8499 - val_loss: 582.8511 - val_acc: 0.6931\n",
            "Epoch 1470/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 582.6917 - acc: 0.8377 - val_loss: 582.7951 - val_acc: 0.6931\n",
            "Epoch 1471/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 582.6295 - acc: 0.8540 - val_loss: 582.7394 - val_acc: 0.6931\n",
            "Epoch 1472/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 582.5788 - acc: 0.8367 - val_loss: 582.6834 - val_acc: 0.6931\n",
            "Epoch 1473/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 582.5188 - acc: 0.8448 - val_loss: 582.6275 - val_acc: 0.6931\n",
            "Epoch 1474/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 582.4653 - acc: 0.8469 - val_loss: 582.5716 - val_acc: 0.6931\n",
            "Epoch 1475/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 582.4105 - acc: 0.8479 - val_loss: 582.5157 - val_acc: 0.6931\n",
            "Epoch 1476/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 582.3564 - acc: 0.8327 - val_loss: 582.4599 - val_acc: 0.6931\n",
            "Epoch 1477/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 582.2898 - acc: 0.8600 - val_loss: 582.4040 - val_acc: 0.6931\n",
            "Epoch 1478/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 582.2462 - acc: 0.8387 - val_loss: 582.3482 - val_acc: 0.6931\n",
            "Epoch 1479/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 582.1944 - acc: 0.8398 - val_loss: 582.2923 - val_acc: 0.6931\n",
            "Epoch 1480/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 582.1300 - acc: 0.8398 - val_loss: 582.2365 - val_acc: 0.6931\n",
            "Epoch 1481/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 582.0831 - acc: 0.8387 - val_loss: 582.1807 - val_acc: 0.6931\n",
            "Epoch 1482/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 582.0180 - acc: 0.8428 - val_loss: 582.1249 - val_acc: 0.6931\n",
            "Epoch 1483/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 581.9644 - acc: 0.8377 - val_loss: 582.0691 - val_acc: 0.6931\n",
            "Epoch 1484/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 581.9084 - acc: 0.8560 - val_loss: 582.0132 - val_acc: 0.6931\n",
            "Epoch 1485/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 581.8495 - acc: 0.8347 - val_loss: 581.9576 - val_acc: 0.6931\n",
            "Epoch 1486/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 581.8000 - acc: 0.8347 - val_loss: 581.9018 - val_acc: 0.6931\n",
            "Epoch 1487/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 581.7426 - acc: 0.8479 - val_loss: 581.8459 - val_acc: 0.6931\n",
            "Epoch 1488/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 581.6862 - acc: 0.8469 - val_loss: 581.7902 - val_acc: 0.6931\n",
            "Epoch 1489/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 581.6289 - acc: 0.8408 - val_loss: 581.7344 - val_acc: 0.6931\n",
            "Epoch 1490/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 581.5775 - acc: 0.8327 - val_loss: 581.6788 - val_acc: 0.6931\n",
            "Epoch 1491/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 581.5145 - acc: 0.8458 - val_loss: 581.6229 - val_acc: 0.6931\n",
            "Epoch 1492/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 581.4672 - acc: 0.8347 - val_loss: 581.5672 - val_acc: 0.6958\n",
            "Epoch 1493/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 581.4052 - acc: 0.8428 - val_loss: 581.5114 - val_acc: 0.6958\n",
            "Epoch 1494/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 581.3494 - acc: 0.8479 - val_loss: 581.4556 - val_acc: 0.6931\n",
            "Epoch 1495/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 581.2992 - acc: 0.8438 - val_loss: 581.3999 - val_acc: 0.6905\n",
            "Epoch 1496/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 581.2417 - acc: 0.8408 - val_loss: 581.3442 - val_acc: 0.6931\n",
            "Epoch 1497/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 581.1844 - acc: 0.8509 - val_loss: 581.2884 - val_acc: 0.6931\n",
            "Epoch 1498/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 581.1312 - acc: 0.8428 - val_loss: 581.2327 - val_acc: 0.6931\n",
            "Epoch 1499/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 581.0653 - acc: 0.8590 - val_loss: 581.1768 - val_acc: 0.6905\n",
            "Epoch 1500/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 581.0073 - acc: 0.8641 - val_loss: 581.1213 - val_acc: 0.6931\n",
            "Epoch 1501/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 580.9542 - acc: 0.8458 - val_loss: 581.0657 - val_acc: 0.6931\n",
            "Epoch 1502/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 580.9050 - acc: 0.8489 - val_loss: 581.0099 - val_acc: 0.6931\n",
            "Epoch 1503/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 580.8416 - acc: 0.8590 - val_loss: 580.9541 - val_acc: 0.6931\n",
            "Epoch 1504/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 580.7902 - acc: 0.8458 - val_loss: 580.8983 - val_acc: 0.6931\n",
            "Epoch 1505/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 580.7372 - acc: 0.8418 - val_loss: 580.8426 - val_acc: 0.6931\n",
            "Epoch 1506/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 580.6788 - acc: 0.8479 - val_loss: 580.7869 - val_acc: 0.6931\n",
            "Epoch 1507/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 580.6201 - acc: 0.8428 - val_loss: 580.7311 - val_acc: 0.6931\n",
            "Epoch 1508/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 580.5640 - acc: 0.8448 - val_loss: 580.6754 - val_acc: 0.6931\n",
            "Epoch 1509/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 580.5096 - acc: 0.8499 - val_loss: 580.6197 - val_acc: 0.6931\n",
            "Epoch 1510/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 580.4582 - acc: 0.8519 - val_loss: 580.5639 - val_acc: 0.6931\n",
            "Epoch 1511/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 580.3971 - acc: 0.8550 - val_loss: 580.5083 - val_acc: 0.6931\n",
            "Epoch 1512/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 580.3440 - acc: 0.8469 - val_loss: 580.4525 - val_acc: 0.6931\n",
            "Epoch 1513/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 580.2883 - acc: 0.8408 - val_loss: 580.3970 - val_acc: 0.6931\n",
            "Epoch 1514/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 580.2327 - acc: 0.8438 - val_loss: 580.3413 - val_acc: 0.6958\n",
            "Epoch 1515/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 580.1779 - acc: 0.8469 - val_loss: 580.2855 - val_acc: 0.6958\n",
            "Epoch 1516/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 580.1193 - acc: 0.8499 - val_loss: 580.2298 - val_acc: 0.6931\n",
            "Epoch 1517/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 580.0637 - acc: 0.8509 - val_loss: 580.1742 - val_acc: 0.6931\n",
            "Epoch 1518/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 580.0172 - acc: 0.8489 - val_loss: 580.1184 - val_acc: 0.6958\n",
            "Epoch 1519/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 579.9554 - acc: 0.8458 - val_loss: 580.0628 - val_acc: 0.6958\n",
            "Epoch 1520/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 579.8946 - acc: 0.8550 - val_loss: 580.0072 - val_acc: 0.6958\n",
            "Epoch 1521/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 579.8389 - acc: 0.8479 - val_loss: 579.9516 - val_acc: 0.6958\n",
            "Epoch 1522/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 579.7954 - acc: 0.8347 - val_loss: 579.8960 - val_acc: 0.6958\n",
            "Epoch 1523/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 579.7404 - acc: 0.8489 - val_loss: 579.8403 - val_acc: 0.6958\n",
            "Epoch 1524/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 579.6778 - acc: 0.8489 - val_loss: 579.7844 - val_acc: 0.6958\n",
            "Epoch 1525/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 579.6225 - acc: 0.8611 - val_loss: 579.7289 - val_acc: 0.6931\n",
            "Epoch 1526/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 579.5653 - acc: 0.8519 - val_loss: 579.6732 - val_acc: 0.6931\n",
            "Epoch 1527/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 579.5049 - acc: 0.8590 - val_loss: 579.6175 - val_acc: 0.6931\n",
            "Epoch 1528/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 579.4547 - acc: 0.8499 - val_loss: 579.5620 - val_acc: 0.6905\n",
            "Epoch 1529/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 579.3912 - acc: 0.8560 - val_loss: 579.5063 - val_acc: 0.6931\n",
            "Epoch 1530/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 579.3416 - acc: 0.8489 - val_loss: 579.4507 - val_acc: 0.6958\n",
            "Epoch 1531/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 579.2854 - acc: 0.8448 - val_loss: 579.3951 - val_acc: 0.6958\n",
            "Epoch 1532/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 579.2291 - acc: 0.8438 - val_loss: 579.3394 - val_acc: 0.6931\n",
            "Epoch 1533/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 579.1755 - acc: 0.8428 - val_loss: 579.2838 - val_acc: 0.6931\n",
            "Epoch 1534/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 579.1188 - acc: 0.8499 - val_loss: 579.2283 - val_acc: 0.6958\n",
            "Epoch 1535/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 579.0635 - acc: 0.8580 - val_loss: 579.1728 - val_acc: 0.6958\n",
            "Epoch 1536/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 579.0101 - acc: 0.8509 - val_loss: 579.1171 - val_acc: 0.6958\n",
            "Epoch 1537/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 578.9543 - acc: 0.8479 - val_loss: 579.0617 - val_acc: 0.6958\n",
            "Epoch 1538/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 578.8976 - acc: 0.8529 - val_loss: 579.0061 - val_acc: 0.6931\n",
            "Epoch 1539/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 578.8385 - acc: 0.8448 - val_loss: 578.9504 - val_acc: 0.6958\n",
            "Epoch 1540/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 578.7852 - acc: 0.8509 - val_loss: 578.8948 - val_acc: 0.6958\n",
            "Epoch 1541/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 578.7292 - acc: 0.8560 - val_loss: 578.8394 - val_acc: 0.6958\n",
            "Epoch 1542/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 578.6730 - acc: 0.8408 - val_loss: 578.7838 - val_acc: 0.6931\n",
            "Epoch 1543/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 578.6255 - acc: 0.8337 - val_loss: 578.7282 - val_acc: 0.6931\n",
            "Epoch 1544/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 578.5594 - acc: 0.8377 - val_loss: 578.6726 - val_acc: 0.6958\n",
            "Epoch 1545/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 578.5016 - acc: 0.8570 - val_loss: 578.6171 - val_acc: 0.6958\n",
            "Epoch 1546/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 578.4482 - acc: 0.8570 - val_loss: 578.5615 - val_acc: 0.6958\n",
            "Epoch 1547/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 578.3940 - acc: 0.8550 - val_loss: 578.5060 - val_acc: 0.6931\n",
            "Epoch 1548/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 578.3378 - acc: 0.8479 - val_loss: 578.4504 - val_acc: 0.6958\n",
            "Epoch 1549/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 578.2841 - acc: 0.8458 - val_loss: 578.3948 - val_acc: 0.6931\n",
            "Epoch 1550/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 578.2253 - acc: 0.8651 - val_loss: 578.3393 - val_acc: 0.6958\n",
            "Epoch 1551/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 578.1812 - acc: 0.8387 - val_loss: 578.2837 - val_acc: 0.6931\n",
            "Epoch 1552/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 578.1177 - acc: 0.8580 - val_loss: 578.2281 - val_acc: 0.6931\n",
            "Epoch 1553/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 578.0570 - acc: 0.8509 - val_loss: 578.1725 - val_acc: 0.6931\n",
            "Epoch 1554/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 578.0055 - acc: 0.8570 - val_loss: 578.1171 - val_acc: 0.6905\n",
            "Epoch 1555/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 577.9529 - acc: 0.8469 - val_loss: 578.0615 - val_acc: 0.6905\n",
            "Epoch 1556/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 577.8903 - acc: 0.8509 - val_loss: 578.0058 - val_acc: 0.6958\n",
            "Epoch 1557/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 577.8431 - acc: 0.8469 - val_loss: 577.9503 - val_acc: 0.6958\n",
            "Epoch 1558/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 577.7896 - acc: 0.8458 - val_loss: 577.8951 - val_acc: 0.6931\n",
            "Epoch 1559/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 577.7351 - acc: 0.8316 - val_loss: 577.8394 - val_acc: 0.6905\n",
            "Epoch 1560/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 577.6794 - acc: 0.8458 - val_loss: 577.7839 - val_acc: 0.6905\n",
            "Epoch 1561/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 577.6203 - acc: 0.8428 - val_loss: 577.7285 - val_acc: 0.6931\n",
            "Epoch 1562/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 577.5643 - acc: 0.8418 - val_loss: 577.6729 - val_acc: 0.6905\n",
            "Epoch 1563/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 577.5119 - acc: 0.8347 - val_loss: 577.6174 - val_acc: 0.6905\n",
            "Epoch 1564/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 577.4514 - acc: 0.8499 - val_loss: 577.5621 - val_acc: 0.6905\n",
            "Epoch 1565/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 577.3934 - acc: 0.8509 - val_loss: 577.5065 - val_acc: 0.6905\n",
            "Epoch 1566/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 577.3400 - acc: 0.8560 - val_loss: 577.4510 - val_acc: 0.6905\n",
            "Epoch 1567/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 577.2809 - acc: 0.8519 - val_loss: 577.3954 - val_acc: 0.6931\n",
            "Epoch 1568/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 577.2262 - acc: 0.8651 - val_loss: 577.3399 - val_acc: 0.6905\n",
            "Epoch 1569/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 577.1795 - acc: 0.8296 - val_loss: 577.2847 - val_acc: 0.6931\n",
            "Epoch 1570/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 577.1235 - acc: 0.8570 - val_loss: 577.2292 - val_acc: 0.6931\n",
            "Epoch 1571/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 577.0669 - acc: 0.8509 - val_loss: 577.1738 - val_acc: 0.6931\n",
            "Epoch 1572/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 577.0028 - acc: 0.8631 - val_loss: 577.1183 - val_acc: 0.6958\n",
            "Epoch 1573/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 576.9546 - acc: 0.8286 - val_loss: 577.0627 - val_acc: 0.6958\n",
            "Epoch 1574/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 576.8944 - acc: 0.8509 - val_loss: 577.0074 - val_acc: 0.6931\n",
            "Epoch 1575/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 576.8482 - acc: 0.8408 - val_loss: 576.9520 - val_acc: 0.6931\n",
            "Epoch 1576/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 576.7772 - acc: 0.8580 - val_loss: 576.8966 - val_acc: 0.6931\n",
            "Epoch 1577/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 576.7236 - acc: 0.8631 - val_loss: 576.8411 - val_acc: 0.6931\n",
            "Epoch 1578/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 576.6778 - acc: 0.8357 - val_loss: 576.7857 - val_acc: 0.6905\n",
            "Epoch 1579/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 576.6164 - acc: 0.8580 - val_loss: 576.7304 - val_acc: 0.6905\n",
            "Epoch 1580/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 576.5676 - acc: 0.8357 - val_loss: 576.6750 - val_acc: 0.6931\n",
            "Epoch 1581/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 576.5025 - acc: 0.8641 - val_loss: 576.6196 - val_acc: 0.6931\n",
            "Epoch 1582/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 576.4514 - acc: 0.8499 - val_loss: 576.5641 - val_acc: 0.6958\n",
            "Epoch 1583/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 576.3921 - acc: 0.8458 - val_loss: 576.5087 - val_acc: 0.6931\n",
            "Epoch 1584/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 576.3472 - acc: 0.8469 - val_loss: 576.4533 - val_acc: 0.6958\n",
            "Epoch 1585/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 576.2906 - acc: 0.8428 - val_loss: 576.3980 - val_acc: 0.6931\n",
            "Epoch 1586/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 576.2326 - acc: 0.8509 - val_loss: 576.3426 - val_acc: 0.6931\n",
            "Epoch 1587/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 576.1777 - acc: 0.8489 - val_loss: 576.2874 - val_acc: 0.6905\n",
            "Epoch 1588/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 576.1223 - acc: 0.8550 - val_loss: 576.2319 - val_acc: 0.6905\n",
            "Epoch 1589/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 576.0615 - acc: 0.8499 - val_loss: 576.1765 - val_acc: 0.6958\n",
            "Epoch 1590/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 576.0077 - acc: 0.8479 - val_loss: 576.1212 - val_acc: 0.6931\n",
            "Epoch 1591/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 575.9537 - acc: 0.8509 - val_loss: 576.0659 - val_acc: 0.6931\n",
            "Epoch 1592/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 575.9061 - acc: 0.8367 - val_loss: 576.0106 - val_acc: 0.6905\n",
            "Epoch 1593/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 575.8441 - acc: 0.8458 - val_loss: 575.9553 - val_acc: 0.6905\n",
            "Epoch 1594/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 575.7851 - acc: 0.8408 - val_loss: 575.8999 - val_acc: 0.6905\n",
            "Epoch 1595/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 575.7314 - acc: 0.8570 - val_loss: 575.8446 - val_acc: 0.6931\n",
            "Epoch 1596/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 575.6795 - acc: 0.8428 - val_loss: 575.7893 - val_acc: 0.6931\n",
            "Epoch 1597/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 575.6194 - acc: 0.8489 - val_loss: 575.7340 - val_acc: 0.6931\n",
            "Epoch 1598/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 575.5698 - acc: 0.8448 - val_loss: 575.6786 - val_acc: 0.6931\n",
            "Epoch 1599/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 575.5067 - acc: 0.8560 - val_loss: 575.6234 - val_acc: 0.6984\n",
            "Epoch 1600/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 575.4549 - acc: 0.8611 - val_loss: 575.5680 - val_acc: 0.6958\n",
            "Epoch 1601/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 575.4018 - acc: 0.8590 - val_loss: 575.5127 - val_acc: 0.6958\n",
            "Epoch 1602/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 575.3392 - acc: 0.8479 - val_loss: 575.4574 - val_acc: 0.6958\n",
            "Epoch 1603/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 575.2893 - acc: 0.8590 - val_loss: 575.4020 - val_acc: 0.6958\n",
            "Epoch 1604/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 575.2323 - acc: 0.8499 - val_loss: 575.3468 - val_acc: 0.6931\n",
            "Epoch 1605/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 575.1768 - acc: 0.8469 - val_loss: 575.2915 - val_acc: 0.6931\n",
            "Epoch 1606/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 575.1234 - acc: 0.8469 - val_loss: 575.2361 - val_acc: 0.6958\n",
            "Epoch 1607/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 575.0679 - acc: 0.8418 - val_loss: 575.1808 - val_acc: 0.6958\n",
            "Epoch 1608/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 575.0087 - acc: 0.8580 - val_loss: 575.1256 - val_acc: 0.6958\n",
            "Epoch 1609/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 574.9628 - acc: 0.8438 - val_loss: 575.0703 - val_acc: 0.6958\n",
            "Epoch 1610/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 574.8996 - acc: 0.8550 - val_loss: 575.0150 - val_acc: 0.6958\n",
            "Epoch 1611/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 574.8490 - acc: 0.8580 - val_loss: 574.9597 - val_acc: 0.6958\n",
            "Epoch 1612/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 574.7915 - acc: 0.8529 - val_loss: 574.9045 - val_acc: 0.6931\n",
            "Epoch 1613/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 574.7338 - acc: 0.8550 - val_loss: 574.8493 - val_acc: 0.6958\n",
            "Epoch 1614/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 574.6746 - acc: 0.8499 - val_loss: 574.7940 - val_acc: 0.6958\n",
            "Epoch 1615/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 574.6245 - acc: 0.8631 - val_loss: 574.7387 - val_acc: 0.6931\n",
            "Epoch 1616/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 574.5668 - acc: 0.8651 - val_loss: 574.6835 - val_acc: 0.6931\n",
            "Epoch 1617/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 574.5148 - acc: 0.8458 - val_loss: 574.6282 - val_acc: 0.6958\n",
            "Epoch 1618/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 574.4608 - acc: 0.8489 - val_loss: 574.5730 - val_acc: 0.6984\n",
            "Epoch 1619/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 574.4000 - acc: 0.8580 - val_loss: 574.5178 - val_acc: 0.6931\n",
            "Epoch 1620/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 574.3584 - acc: 0.8418 - val_loss: 574.4626 - val_acc: 0.6931\n",
            "Epoch 1621/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 574.2931 - acc: 0.8529 - val_loss: 574.4072 - val_acc: 0.6931\n",
            "Epoch 1622/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 574.2296 - acc: 0.8570 - val_loss: 574.3521 - val_acc: 0.6931\n",
            "Epoch 1623/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 574.1726 - acc: 0.8661 - val_loss: 574.2969 - val_acc: 0.6958\n",
            "Epoch 1624/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 574.1231 - acc: 0.8651 - val_loss: 574.2416 - val_acc: 0.6931\n",
            "Epoch 1625/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 574.0641 - acc: 0.8600 - val_loss: 574.1864 - val_acc: 0.6931\n",
            "Epoch 1626/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 574.0143 - acc: 0.8479 - val_loss: 574.1311 - val_acc: 0.6931\n",
            "Epoch 1627/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 573.9709 - acc: 0.8408 - val_loss: 574.0758 - val_acc: 0.6931\n",
            "Epoch 1628/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 573.9049 - acc: 0.8529 - val_loss: 574.0207 - val_acc: 0.6931\n",
            "Epoch 1629/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 573.8530 - acc: 0.8438 - val_loss: 573.9653 - val_acc: 0.6958\n",
            "Epoch 1630/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 573.7969 - acc: 0.8499 - val_loss: 573.9101 - val_acc: 0.6958\n",
            "Epoch 1631/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 573.7338 - acc: 0.8580 - val_loss: 573.8550 - val_acc: 0.6958\n",
            "Epoch 1632/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 573.6788 - acc: 0.8641 - val_loss: 573.7999 - val_acc: 0.6931\n",
            "Epoch 1633/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 573.6350 - acc: 0.8408 - val_loss: 573.7449 - val_acc: 0.6931\n",
            "Epoch 1634/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 573.5850 - acc: 0.8408 - val_loss: 573.6895 - val_acc: 0.6958\n",
            "Epoch 1635/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 573.5247 - acc: 0.8458 - val_loss: 573.6343 - val_acc: 0.6931\n",
            "Epoch 1636/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 573.4579 - acc: 0.8509 - val_loss: 573.5792 - val_acc: 0.6958\n",
            "Epoch 1637/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 573.4147 - acc: 0.8479 - val_loss: 573.5240 - val_acc: 0.6931\n",
            "Epoch 1638/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 573.3541 - acc: 0.8469 - val_loss: 573.4688 - val_acc: 0.6958\n",
            "Epoch 1639/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 573.2972 - acc: 0.8489 - val_loss: 573.4136 - val_acc: 0.6984\n",
            "Epoch 1640/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 573.2433 - acc: 0.8489 - val_loss: 573.3584 - val_acc: 0.6931\n",
            "Epoch 1641/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 573.1883 - acc: 0.8611 - val_loss: 573.3032 - val_acc: 0.6958\n",
            "Epoch 1642/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 573.1297 - acc: 0.8570 - val_loss: 573.2480 - val_acc: 0.6958\n",
            "Epoch 1643/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 573.0740 - acc: 0.8600 - val_loss: 573.1931 - val_acc: 0.6958\n",
            "Epoch 1644/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 573.0227 - acc: 0.8529 - val_loss: 573.1379 - val_acc: 0.6984\n",
            "Epoch 1645/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 572.9674 - acc: 0.8550 - val_loss: 573.0827 - val_acc: 0.6984\n",
            "Epoch 1646/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 572.9162 - acc: 0.8448 - val_loss: 573.0276 - val_acc: 0.6984\n",
            "Epoch 1647/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 572.8550 - acc: 0.8671 - val_loss: 572.9724 - val_acc: 0.6984\n",
            "Epoch 1648/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 572.7988 - acc: 0.8580 - val_loss: 572.9174 - val_acc: 0.6984\n",
            "Epoch 1649/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 572.7458 - acc: 0.8611 - val_loss: 572.8622 - val_acc: 0.6984\n",
            "Epoch 1650/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 572.6885 - acc: 0.8712 - val_loss: 572.8073 - val_acc: 0.6984\n",
            "Epoch 1651/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 572.6374 - acc: 0.8540 - val_loss: 572.7523 - val_acc: 0.6984\n",
            "Epoch 1652/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 572.5825 - acc: 0.8509 - val_loss: 572.6971 - val_acc: 0.6984\n",
            "Epoch 1653/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 572.5305 - acc: 0.8479 - val_loss: 572.6421 - val_acc: 0.6984\n",
            "Epoch 1654/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 572.4660 - acc: 0.8600 - val_loss: 572.5869 - val_acc: 0.6984\n",
            "Epoch 1655/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 572.4079 - acc: 0.8692 - val_loss: 572.5317 - val_acc: 0.6984\n",
            "Epoch 1656/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 572.3583 - acc: 0.8682 - val_loss: 572.4767 - val_acc: 0.6984\n",
            "Epoch 1657/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 572.3022 - acc: 0.8641 - val_loss: 572.4216 - val_acc: 0.6984\n",
            "Epoch 1658/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 572.2504 - acc: 0.8489 - val_loss: 572.3666 - val_acc: 0.6984\n",
            "Epoch 1659/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 572.1945 - acc: 0.8469 - val_loss: 572.3115 - val_acc: 0.6984\n",
            "Epoch 1660/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 572.1362 - acc: 0.8540 - val_loss: 572.2565 - val_acc: 0.6958\n",
            "Epoch 1661/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 572.0867 - acc: 0.8570 - val_loss: 572.2014 - val_acc: 0.6984\n",
            "Epoch 1662/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 572.0268 - acc: 0.8560 - val_loss: 572.1463 - val_acc: 0.6984\n",
            "Epoch 1663/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 571.9719 - acc: 0.8611 - val_loss: 572.0912 - val_acc: 0.6984\n",
            "Epoch 1664/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 571.9195 - acc: 0.8651 - val_loss: 572.0361 - val_acc: 0.6958\n",
            "Epoch 1665/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 571.8700 - acc: 0.8489 - val_loss: 571.9812 - val_acc: 0.6958\n",
            "Epoch 1666/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 571.8130 - acc: 0.8590 - val_loss: 571.9261 - val_acc: 0.6984\n",
            "Epoch 1667/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 571.7577 - acc: 0.8540 - val_loss: 571.8711 - val_acc: 0.6984\n",
            "Epoch 1668/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 571.6893 - acc: 0.8763 - val_loss: 571.8160 - val_acc: 0.6958\n",
            "Epoch 1669/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 571.6428 - acc: 0.8519 - val_loss: 571.7611 - val_acc: 0.6958\n",
            "Epoch 1670/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 571.5915 - acc: 0.8590 - val_loss: 571.7061 - val_acc: 0.6958\n",
            "Epoch 1671/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 571.5284 - acc: 0.8600 - val_loss: 571.6510 - val_acc: 0.6958\n",
            "Epoch 1672/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 571.4825 - acc: 0.8509 - val_loss: 571.5959 - val_acc: 0.6958\n",
            "Epoch 1673/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 571.4301 - acc: 0.8377 - val_loss: 571.5409 - val_acc: 0.6958\n",
            "Epoch 1674/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 571.3692 - acc: 0.8631 - val_loss: 571.4859 - val_acc: 0.6958\n",
            "Epoch 1675/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 571.3180 - acc: 0.8560 - val_loss: 571.4309 - val_acc: 0.6984\n",
            "Epoch 1676/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 571.2573 - acc: 0.8509 - val_loss: 571.3759 - val_acc: 0.6958\n",
            "Epoch 1677/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 571.2091 - acc: 0.8489 - val_loss: 571.3210 - val_acc: 0.6958\n",
            "Epoch 1678/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 571.1497 - acc: 0.8570 - val_loss: 571.2661 - val_acc: 0.6958\n",
            "Epoch 1679/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 571.0961 - acc: 0.8448 - val_loss: 571.2112 - val_acc: 0.6958\n",
            "Epoch 1680/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 571.0423 - acc: 0.8570 - val_loss: 571.1560 - val_acc: 0.6958\n",
            "Epoch 1681/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 570.9825 - acc: 0.8641 - val_loss: 571.1013 - val_acc: 0.6958\n",
            "Epoch 1682/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 570.9294 - acc: 0.8519 - val_loss: 571.0462 - val_acc: 0.6958\n",
            "Epoch 1683/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 570.8751 - acc: 0.8499 - val_loss: 570.9912 - val_acc: 0.6958\n",
            "Epoch 1684/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 570.8198 - acc: 0.8469 - val_loss: 570.9362 - val_acc: 0.6958\n",
            "Epoch 1685/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 570.7639 - acc: 0.8590 - val_loss: 570.8811 - val_acc: 0.6958\n",
            "Epoch 1686/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 570.7160 - acc: 0.8469 - val_loss: 570.8261 - val_acc: 0.6958\n",
            "Epoch 1687/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 570.6528 - acc: 0.8499 - val_loss: 570.7712 - val_acc: 0.6958\n",
            "Epoch 1688/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 570.6021 - acc: 0.8387 - val_loss: 570.7163 - val_acc: 0.6984\n",
            "Epoch 1689/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 570.5458 - acc: 0.8499 - val_loss: 570.6613 - val_acc: 0.6984\n",
            "Epoch 1690/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 570.4956 - acc: 0.8428 - val_loss: 570.6064 - val_acc: 0.6984\n",
            "Epoch 1691/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 570.4317 - acc: 0.8529 - val_loss: 570.5515 - val_acc: 0.6958\n",
            "Epoch 1692/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 570.3816 - acc: 0.8458 - val_loss: 570.4966 - val_acc: 0.6958\n",
            "Epoch 1693/5000\n",
            "986/986 [==============================] - 0s 300us/step - loss: 570.3232 - acc: 0.8540 - val_loss: 570.4416 - val_acc: 0.6958\n",
            "Epoch 1694/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 570.2727 - acc: 0.8580 - val_loss: 570.3867 - val_acc: 0.6931\n",
            "Epoch 1695/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 570.2154 - acc: 0.8641 - val_loss: 570.3319 - val_acc: 0.6958\n",
            "Epoch 1696/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 570.1594 - acc: 0.8651 - val_loss: 570.2772 - val_acc: 0.6958\n",
            "Epoch 1697/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 570.1056 - acc: 0.8499 - val_loss: 570.2223 - val_acc: 0.6958\n",
            "Epoch 1698/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 570.0468 - acc: 0.8387 - val_loss: 570.1674 - val_acc: 0.6958\n",
            "Epoch 1699/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 569.9880 - acc: 0.8611 - val_loss: 570.1125 - val_acc: 0.6958\n",
            "Epoch 1700/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 569.9408 - acc: 0.8641 - val_loss: 570.0575 - val_acc: 0.6958\n",
            "Epoch 1701/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 569.8807 - acc: 0.8479 - val_loss: 570.0027 - val_acc: 0.6931\n",
            "Epoch 1702/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 569.8292 - acc: 0.8529 - val_loss: 569.9478 - val_acc: 0.6958\n",
            "Epoch 1703/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 569.7726 - acc: 0.8611 - val_loss: 569.8929 - val_acc: 0.6958\n",
            "Epoch 1704/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 569.7175 - acc: 0.8661 - val_loss: 569.8382 - val_acc: 0.6984\n",
            "Epoch 1705/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 569.6644 - acc: 0.8509 - val_loss: 569.7833 - val_acc: 0.6984\n",
            "Epoch 1706/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 569.6115 - acc: 0.8550 - val_loss: 569.7284 - val_acc: 0.6958\n",
            "Epoch 1707/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 569.5620 - acc: 0.8550 - val_loss: 569.6736 - val_acc: 0.6958\n",
            "Epoch 1708/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 569.5005 - acc: 0.8479 - val_loss: 569.6188 - val_acc: 0.6958\n",
            "Epoch 1709/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 569.4535 - acc: 0.8540 - val_loss: 569.5641 - val_acc: 0.6931\n",
            "Epoch 1710/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 569.3839 - acc: 0.8702 - val_loss: 569.5091 - val_acc: 0.6958\n",
            "Epoch 1711/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 569.3325 - acc: 0.8580 - val_loss: 569.4543 - val_acc: 0.6931\n",
            "Epoch 1712/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 569.2771 - acc: 0.8661 - val_loss: 569.3994 - val_acc: 0.6958\n",
            "Epoch 1713/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 569.2283 - acc: 0.8540 - val_loss: 569.3446 - val_acc: 0.6958\n",
            "Epoch 1714/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 569.1725 - acc: 0.8611 - val_loss: 569.2899 - val_acc: 0.6958\n",
            "Epoch 1715/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 569.1188 - acc: 0.8560 - val_loss: 569.2350 - val_acc: 0.6931\n",
            "Epoch 1716/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 569.0599 - acc: 0.8590 - val_loss: 569.1803 - val_acc: 0.6984\n",
            "Epoch 1717/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 569.0046 - acc: 0.8560 - val_loss: 569.1254 - val_acc: 0.6984\n",
            "Epoch 1718/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 568.9468 - acc: 0.8570 - val_loss: 569.0706 - val_acc: 0.6958\n",
            "Epoch 1719/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 568.8987 - acc: 0.8570 - val_loss: 569.0158 - val_acc: 0.6958\n",
            "Epoch 1720/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 568.8380 - acc: 0.8560 - val_loss: 568.9611 - val_acc: 0.7011\n",
            "Epoch 1721/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 568.7832 - acc: 0.8590 - val_loss: 568.9062 - val_acc: 0.6984\n",
            "Epoch 1722/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 568.7324 - acc: 0.8641 - val_loss: 568.8514 - val_acc: 0.6984\n",
            "Epoch 1723/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 568.6823 - acc: 0.8489 - val_loss: 568.7968 - val_acc: 0.6984\n",
            "Epoch 1724/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 568.6296 - acc: 0.8560 - val_loss: 568.7421 - val_acc: 0.6984\n",
            "Epoch 1725/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 568.5675 - acc: 0.8529 - val_loss: 568.6874 - val_acc: 0.6984\n",
            "Epoch 1726/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 568.5118 - acc: 0.8712 - val_loss: 568.6327 - val_acc: 0.6984\n",
            "Epoch 1727/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 568.4658 - acc: 0.8347 - val_loss: 568.5780 - val_acc: 0.6984\n",
            "Epoch 1728/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 568.4065 - acc: 0.8418 - val_loss: 568.5232 - val_acc: 0.6984\n",
            "Epoch 1729/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 568.3528 - acc: 0.8469 - val_loss: 568.4684 - val_acc: 0.6958\n",
            "Epoch 1730/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 568.2940 - acc: 0.8519 - val_loss: 568.4137 - val_acc: 0.7011\n",
            "Epoch 1731/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 568.2430 - acc: 0.8540 - val_loss: 568.3589 - val_acc: 0.6984\n",
            "Epoch 1732/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 568.1849 - acc: 0.8570 - val_loss: 568.3042 - val_acc: 0.6984\n",
            "Epoch 1733/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 568.1283 - acc: 0.8540 - val_loss: 568.2495 - val_acc: 0.6984\n",
            "Epoch 1734/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 568.0758 - acc: 0.8631 - val_loss: 568.1948 - val_acc: 0.7011\n",
            "Epoch 1735/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 568.0245 - acc: 0.8398 - val_loss: 568.1401 - val_acc: 0.6984\n",
            "Epoch 1736/5000\n",
            "986/986 [==============================] - 0s 300us/step - loss: 567.9685 - acc: 0.8499 - val_loss: 568.0853 - val_acc: 0.6984\n",
            "Epoch 1737/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 567.9070 - acc: 0.8570 - val_loss: 568.0306 - val_acc: 0.6984\n",
            "Epoch 1738/5000\n",
            "986/986 [==============================] - 0s 304us/step - loss: 567.8526 - acc: 0.8570 - val_loss: 567.9758 - val_acc: 0.6984\n",
            "Epoch 1739/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 567.7957 - acc: 0.8600 - val_loss: 567.9211 - val_acc: 0.7011\n",
            "Epoch 1740/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 567.7374 - acc: 0.8682 - val_loss: 567.8664 - val_acc: 0.7011\n",
            "Epoch 1741/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 567.6967 - acc: 0.8529 - val_loss: 567.8116 - val_acc: 0.7011\n",
            "Epoch 1742/5000\n",
            "986/986 [==============================] - 0s 297us/step - loss: 567.6370 - acc: 0.8529 - val_loss: 567.7568 - val_acc: 0.7011\n",
            "Epoch 1743/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 567.5862 - acc: 0.8519 - val_loss: 567.7022 - val_acc: 0.7011\n",
            "Epoch 1744/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 567.5242 - acc: 0.8702 - val_loss: 567.6476 - val_acc: 0.7011\n",
            "Epoch 1745/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 567.4692 - acc: 0.8600 - val_loss: 567.5927 - val_acc: 0.6984\n",
            "Epoch 1746/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 567.4153 - acc: 0.8712 - val_loss: 567.5380 - val_acc: 0.6984\n",
            "Epoch 1747/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 567.3620 - acc: 0.8479 - val_loss: 567.4832 - val_acc: 0.7011\n",
            "Epoch 1748/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 567.3002 - acc: 0.8661 - val_loss: 567.4285 - val_acc: 0.6984\n",
            "Epoch 1749/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 567.2567 - acc: 0.8499 - val_loss: 567.3738 - val_acc: 0.6984\n",
            "Epoch 1750/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 567.2019 - acc: 0.8570 - val_loss: 567.3192 - val_acc: 0.6984\n",
            "Epoch 1751/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 567.1385 - acc: 0.8590 - val_loss: 567.2645 - val_acc: 0.7037\n",
            "Epoch 1752/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 567.0898 - acc: 0.8560 - val_loss: 567.2098 - val_acc: 0.7037\n",
            "Epoch 1753/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 567.0331 - acc: 0.8519 - val_loss: 567.1552 - val_acc: 0.7037\n",
            "Epoch 1754/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 566.9773 - acc: 0.8651 - val_loss: 567.1006 - val_acc: 0.7037\n",
            "Epoch 1755/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 566.9242 - acc: 0.8509 - val_loss: 567.0459 - val_acc: 0.7011\n",
            "Epoch 1756/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 566.8708 - acc: 0.8489 - val_loss: 566.9911 - val_acc: 0.7011\n",
            "Epoch 1757/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 566.8145 - acc: 0.8560 - val_loss: 566.9364 - val_acc: 0.7037\n",
            "Epoch 1758/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 566.7559 - acc: 0.8631 - val_loss: 566.8819 - val_acc: 0.7011\n",
            "Epoch 1759/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 566.6994 - acc: 0.8600 - val_loss: 566.8271 - val_acc: 0.7011\n",
            "Epoch 1760/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 566.6507 - acc: 0.8671 - val_loss: 566.7723 - val_acc: 0.7037\n",
            "Epoch 1761/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 566.5946 - acc: 0.8692 - val_loss: 566.7178 - val_acc: 0.7037\n",
            "Epoch 1762/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 566.5391 - acc: 0.8540 - val_loss: 566.6631 - val_acc: 0.7037\n",
            "Epoch 1763/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 566.4824 - acc: 0.8621 - val_loss: 566.6086 - val_acc: 0.7037\n",
            "Epoch 1764/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 566.4287 - acc: 0.8712 - val_loss: 566.5539 - val_acc: 0.7011\n",
            "Epoch 1765/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 566.3750 - acc: 0.8621 - val_loss: 566.4991 - val_acc: 0.7011\n",
            "Epoch 1766/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 566.3187 - acc: 0.8570 - val_loss: 566.4444 - val_acc: 0.7011\n",
            "Epoch 1767/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 566.2612 - acc: 0.8671 - val_loss: 566.3900 - val_acc: 0.7037\n",
            "Epoch 1768/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 566.2114 - acc: 0.8560 - val_loss: 566.3352 - val_acc: 0.7011\n",
            "Epoch 1769/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 566.1630 - acc: 0.8590 - val_loss: 566.2807 - val_acc: 0.7011\n",
            "Epoch 1770/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 566.1094 - acc: 0.8458 - val_loss: 566.2259 - val_acc: 0.7037\n",
            "Epoch 1771/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 566.0462 - acc: 0.8621 - val_loss: 566.1714 - val_acc: 0.7037\n",
            "Epoch 1772/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 565.9939 - acc: 0.8540 - val_loss: 566.1167 - val_acc: 0.7037\n",
            "Epoch 1773/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 565.9398 - acc: 0.8682 - val_loss: 566.0621 - val_acc: 0.7037\n",
            "Epoch 1774/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 565.8876 - acc: 0.8560 - val_loss: 566.0075 - val_acc: 0.7037\n",
            "Epoch 1775/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 565.8313 - acc: 0.8671 - val_loss: 565.9529 - val_acc: 0.7037\n",
            "Epoch 1776/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 565.7719 - acc: 0.8682 - val_loss: 565.8983 - val_acc: 0.7037\n",
            "Epoch 1777/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 565.7200 - acc: 0.8682 - val_loss: 565.8437 - val_acc: 0.7037\n",
            "Epoch 1778/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 565.6642 - acc: 0.8661 - val_loss: 565.7890 - val_acc: 0.7037\n",
            "Epoch 1779/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 565.6110 - acc: 0.8580 - val_loss: 565.7344 - val_acc: 0.7037\n",
            "Epoch 1780/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 565.5577 - acc: 0.8641 - val_loss: 565.6797 - val_acc: 0.7037\n",
            "Epoch 1781/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 565.5013 - acc: 0.8621 - val_loss: 565.6252 - val_acc: 0.7037\n",
            "Epoch 1782/5000\n",
            "986/986 [==============================] - 0s 298us/step - loss: 565.4463 - acc: 0.8509 - val_loss: 565.5707 - val_acc: 0.7037\n",
            "Epoch 1783/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 565.3914 - acc: 0.8580 - val_loss: 565.5161 - val_acc: 0.7037\n",
            "Epoch 1784/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 565.3386 - acc: 0.8509 - val_loss: 565.4616 - val_acc: 0.7037\n",
            "Epoch 1785/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 565.2800 - acc: 0.8651 - val_loss: 565.4071 - val_acc: 0.7037\n",
            "Epoch 1786/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 565.2286 - acc: 0.8621 - val_loss: 565.3526 - val_acc: 0.7037\n",
            "Epoch 1787/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 565.1788 - acc: 0.8641 - val_loss: 565.2980 - val_acc: 0.7037\n",
            "Epoch 1788/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 565.1278 - acc: 0.8458 - val_loss: 565.2435 - val_acc: 0.7037\n",
            "Epoch 1789/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 565.0676 - acc: 0.8519 - val_loss: 565.1889 - val_acc: 0.7037\n",
            "Epoch 1790/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 565.0174 - acc: 0.8519 - val_loss: 565.1343 - val_acc: 0.7037\n",
            "Epoch 1791/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 564.9507 - acc: 0.8641 - val_loss: 565.0798 - val_acc: 0.7037\n",
            "Epoch 1792/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 564.8998 - acc: 0.8692 - val_loss: 565.0253 - val_acc: 0.7037\n",
            "Epoch 1793/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 564.8486 - acc: 0.8742 - val_loss: 564.9707 - val_acc: 0.7037\n",
            "Epoch 1794/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 564.7860 - acc: 0.8671 - val_loss: 564.9163 - val_acc: 0.7037\n",
            "Epoch 1795/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 564.7460 - acc: 0.8519 - val_loss: 564.8617 - val_acc: 0.7037\n",
            "Epoch 1796/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 564.6907 - acc: 0.8499 - val_loss: 564.8071 - val_acc: 0.7037\n",
            "Epoch 1797/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 564.6258 - acc: 0.8742 - val_loss: 564.7526 - val_acc: 0.7011\n",
            "Epoch 1798/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 564.5771 - acc: 0.8489 - val_loss: 564.6981 - val_acc: 0.7011\n",
            "Epoch 1799/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 564.5109 - acc: 0.8722 - val_loss: 564.6436 - val_acc: 0.7037\n",
            "Epoch 1800/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 564.4635 - acc: 0.8651 - val_loss: 564.5891 - val_acc: 0.7011\n",
            "Epoch 1801/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 564.4140 - acc: 0.8428 - val_loss: 564.5347 - val_acc: 0.7011\n",
            "Epoch 1802/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 564.3542 - acc: 0.8600 - val_loss: 564.4801 - val_acc: 0.7011\n",
            "Epoch 1803/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 564.3093 - acc: 0.8398 - val_loss: 564.4256 - val_acc: 0.7011\n",
            "Epoch 1804/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 564.2436 - acc: 0.8702 - val_loss: 564.3712 - val_acc: 0.7011\n",
            "Epoch 1805/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 564.1916 - acc: 0.8570 - val_loss: 564.3168 - val_acc: 0.7011\n",
            "Epoch 1806/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 564.1327 - acc: 0.8611 - val_loss: 564.2624 - val_acc: 0.7011\n",
            "Epoch 1807/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 564.0809 - acc: 0.8671 - val_loss: 564.2079 - val_acc: 0.7011\n",
            "Epoch 1808/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 564.0310 - acc: 0.8479 - val_loss: 564.1535 - val_acc: 0.7011\n",
            "Epoch 1809/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 563.9806 - acc: 0.8570 - val_loss: 564.0991 - val_acc: 0.7011\n",
            "Epoch 1810/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 563.9183 - acc: 0.8671 - val_loss: 564.0447 - val_acc: 0.7011\n",
            "Epoch 1811/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 563.8711 - acc: 0.8651 - val_loss: 563.9902 - val_acc: 0.7011\n",
            "Epoch 1812/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 563.8148 - acc: 0.8580 - val_loss: 563.9358 - val_acc: 0.7011\n",
            "Epoch 1813/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 563.7575 - acc: 0.8661 - val_loss: 563.8814 - val_acc: 0.7011\n",
            "Epoch 1814/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 563.6987 - acc: 0.8661 - val_loss: 563.8269 - val_acc: 0.7011\n",
            "Epoch 1815/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 563.6477 - acc: 0.8519 - val_loss: 563.7725 - val_acc: 0.7011\n",
            "Epoch 1816/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 563.5921 - acc: 0.8580 - val_loss: 563.7180 - val_acc: 0.7011\n",
            "Epoch 1817/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 563.5363 - acc: 0.8793 - val_loss: 563.6636 - val_acc: 0.7011\n",
            "Epoch 1818/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 563.4791 - acc: 0.8692 - val_loss: 563.6092 - val_acc: 0.7063\n",
            "Epoch 1819/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 563.4302 - acc: 0.8661 - val_loss: 563.5548 - val_acc: 0.7063\n",
            "Epoch 1820/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 563.3704 - acc: 0.8722 - val_loss: 563.5004 - val_acc: 0.7063\n",
            "Epoch 1821/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 563.3191 - acc: 0.8692 - val_loss: 563.4460 - val_acc: 0.7063\n",
            "Epoch 1822/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 563.2696 - acc: 0.8570 - val_loss: 563.3916 - val_acc: 0.7011\n",
            "Epoch 1823/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 563.2077 - acc: 0.8722 - val_loss: 563.3371 - val_acc: 0.7011\n",
            "Epoch 1824/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 563.1655 - acc: 0.8529 - val_loss: 563.2828 - val_acc: 0.7011\n",
            "Epoch 1825/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 563.1056 - acc: 0.8590 - val_loss: 563.2284 - val_acc: 0.7011\n",
            "Epoch 1826/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 563.0499 - acc: 0.8590 - val_loss: 563.1740 - val_acc: 0.7011\n",
            "Epoch 1827/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 562.9955 - acc: 0.8621 - val_loss: 563.1196 - val_acc: 0.7011\n",
            "Epoch 1828/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 562.9374 - acc: 0.8540 - val_loss: 563.0650 - val_acc: 0.7011\n",
            "Epoch 1829/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 562.8890 - acc: 0.8570 - val_loss: 563.0106 - val_acc: 0.7063\n",
            "Epoch 1830/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 562.8284 - acc: 0.8661 - val_loss: 562.9563 - val_acc: 0.7011\n",
            "Epoch 1831/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 562.7763 - acc: 0.8590 - val_loss: 562.9019 - val_acc: 0.7011\n",
            "Epoch 1832/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 562.7255 - acc: 0.8499 - val_loss: 562.8475 - val_acc: 0.7011\n",
            "Epoch 1833/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 562.6719 - acc: 0.8529 - val_loss: 562.7933 - val_acc: 0.7011\n",
            "Epoch 1834/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 562.6128 - acc: 0.8580 - val_loss: 562.7388 - val_acc: 0.7011\n",
            "Epoch 1835/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 562.5559 - acc: 0.8712 - val_loss: 562.6845 - val_acc: 0.7063\n",
            "Epoch 1836/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 562.4998 - acc: 0.8661 - val_loss: 562.6300 - val_acc: 0.7063\n",
            "Epoch 1837/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 562.4517 - acc: 0.8499 - val_loss: 562.5757 - val_acc: 0.7063\n",
            "Epoch 1838/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 562.3959 - acc: 0.8600 - val_loss: 562.5216 - val_acc: 0.7011\n",
            "Epoch 1839/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 562.3419 - acc: 0.8580 - val_loss: 562.4672 - val_acc: 0.7011\n",
            "Epoch 1840/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 562.2851 - acc: 0.8611 - val_loss: 562.4127 - val_acc: 0.7063\n",
            "Epoch 1841/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 562.2429 - acc: 0.8540 - val_loss: 562.3585 - val_acc: 0.7063\n",
            "Epoch 1842/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 562.1792 - acc: 0.8692 - val_loss: 562.3040 - val_acc: 0.7063\n",
            "Epoch 1843/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 562.1323 - acc: 0.8580 - val_loss: 562.2497 - val_acc: 0.7063\n",
            "Epoch 1844/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 562.0731 - acc: 0.8428 - val_loss: 562.1956 - val_acc: 0.7063\n",
            "Epoch 1845/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 562.0158 - acc: 0.8641 - val_loss: 562.1412 - val_acc: 0.7063\n",
            "Epoch 1846/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 561.9626 - acc: 0.8560 - val_loss: 562.0869 - val_acc: 0.7011\n",
            "Epoch 1847/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 561.9098 - acc: 0.8641 - val_loss: 562.0326 - val_acc: 0.7063\n",
            "Epoch 1848/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 561.8515 - acc: 0.8641 - val_loss: 561.9783 - val_acc: 0.7011\n",
            "Epoch 1849/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 561.8013 - acc: 0.8702 - val_loss: 561.9239 - val_acc: 0.7011\n",
            "Epoch 1850/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 561.7424 - acc: 0.8773 - val_loss: 561.8697 - val_acc: 0.7063\n",
            "Epoch 1851/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 561.6915 - acc: 0.8580 - val_loss: 561.8154 - val_acc: 0.7063\n",
            "Epoch 1852/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 561.6280 - acc: 0.8753 - val_loss: 561.7611 - val_acc: 0.7063\n",
            "Epoch 1853/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 561.5785 - acc: 0.8682 - val_loss: 561.7068 - val_acc: 0.7063\n",
            "Epoch 1854/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 561.5301 - acc: 0.8631 - val_loss: 561.6524 - val_acc: 0.7063\n",
            "Epoch 1855/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 561.4700 - acc: 0.8590 - val_loss: 561.5982 - val_acc: 0.7063\n",
            "Epoch 1856/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 561.4197 - acc: 0.8621 - val_loss: 561.5440 - val_acc: 0.7063\n",
            "Epoch 1857/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 561.3658 - acc: 0.8641 - val_loss: 561.4898 - val_acc: 0.7063\n",
            "Epoch 1858/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 561.3120 - acc: 0.8641 - val_loss: 561.4355 - val_acc: 0.7063\n",
            "Epoch 1859/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 561.2592 - acc: 0.8661 - val_loss: 561.3812 - val_acc: 0.7063\n",
            "Epoch 1860/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 561.2049 - acc: 0.8479 - val_loss: 561.3270 - val_acc: 0.7063\n",
            "Epoch 1861/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 561.1469 - acc: 0.8651 - val_loss: 561.2728 - val_acc: 0.7063\n",
            "Epoch 1862/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 561.0938 - acc: 0.8600 - val_loss: 561.2184 - val_acc: 0.7063\n",
            "Epoch 1863/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 561.0415 - acc: 0.8540 - val_loss: 561.1643 - val_acc: 0.7063\n",
            "Epoch 1864/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 560.9799 - acc: 0.8631 - val_loss: 561.1100 - val_acc: 0.7063\n",
            "Epoch 1865/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 560.9191 - acc: 0.8702 - val_loss: 561.0557 - val_acc: 0.7063\n",
            "Epoch 1866/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 560.8694 - acc: 0.8651 - val_loss: 561.0016 - val_acc: 0.7063\n",
            "Epoch 1867/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 560.8206 - acc: 0.8651 - val_loss: 560.9473 - val_acc: 0.7063\n",
            "Epoch 1868/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 560.7694 - acc: 0.8621 - val_loss: 560.8931 - val_acc: 0.7063\n",
            "Epoch 1869/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 560.7122 - acc: 0.8682 - val_loss: 560.8389 - val_acc: 0.7063\n",
            "Epoch 1870/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 560.6607 - acc: 0.8763 - val_loss: 560.7846 - val_acc: 0.7063\n",
            "Epoch 1871/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 560.6048 - acc: 0.8661 - val_loss: 560.7305 - val_acc: 0.7063\n",
            "Epoch 1872/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 560.5513 - acc: 0.8529 - val_loss: 560.6763 - val_acc: 0.7063\n",
            "Epoch 1873/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 560.4951 - acc: 0.8682 - val_loss: 560.6220 - val_acc: 0.7063\n",
            "Epoch 1874/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 560.4401 - acc: 0.8641 - val_loss: 560.5678 - val_acc: 0.7063\n",
            "Epoch 1875/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 560.3847 - acc: 0.8631 - val_loss: 560.5137 - val_acc: 0.7063\n",
            "Epoch 1876/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 560.3312 - acc: 0.8803 - val_loss: 560.4596 - val_acc: 0.7063\n",
            "Epoch 1877/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 560.2785 - acc: 0.8570 - val_loss: 560.4053 - val_acc: 0.7063\n",
            "Epoch 1878/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 560.2248 - acc: 0.8702 - val_loss: 560.3512 - val_acc: 0.7063\n",
            "Epoch 1879/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 560.1723 - acc: 0.8631 - val_loss: 560.2970 - val_acc: 0.7063\n",
            "Epoch 1880/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 560.1096 - acc: 0.8651 - val_loss: 560.2428 - val_acc: 0.7063\n",
            "Epoch 1881/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 560.0540 - acc: 0.8783 - val_loss: 560.1886 - val_acc: 0.7063\n",
            "Epoch 1882/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 560.0090 - acc: 0.8570 - val_loss: 560.1344 - val_acc: 0.7063\n",
            "Epoch 1883/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 559.9530 - acc: 0.8641 - val_loss: 560.0802 - val_acc: 0.7063\n",
            "Epoch 1884/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 559.9001 - acc: 0.8590 - val_loss: 560.0260 - val_acc: 0.7063\n",
            "Epoch 1885/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 559.8424 - acc: 0.8742 - val_loss: 559.9719 - val_acc: 0.7063\n",
            "Epoch 1886/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 559.7831 - acc: 0.8732 - val_loss: 559.9177 - val_acc: 0.7063\n",
            "Epoch 1887/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 559.7410 - acc: 0.8641 - val_loss: 559.8636 - val_acc: 0.7063\n",
            "Epoch 1888/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 559.6833 - acc: 0.8661 - val_loss: 559.8094 - val_acc: 0.7063\n",
            "Epoch 1889/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 559.6354 - acc: 0.8570 - val_loss: 559.7553 - val_acc: 0.7063\n",
            "Epoch 1890/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 559.5716 - acc: 0.8803 - val_loss: 559.7012 - val_acc: 0.7063\n",
            "Epoch 1891/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 559.5181 - acc: 0.8580 - val_loss: 559.6471 - val_acc: 0.7063\n",
            "Epoch 1892/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 559.4620 - acc: 0.8590 - val_loss: 559.5929 - val_acc: 0.7063\n",
            "Epoch 1893/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 559.4140 - acc: 0.8600 - val_loss: 559.5389 - val_acc: 0.7063\n",
            "Epoch 1894/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 559.3551 - acc: 0.8631 - val_loss: 559.4848 - val_acc: 0.7063\n",
            "Epoch 1895/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 559.3043 - acc: 0.8712 - val_loss: 559.4307 - val_acc: 0.7063\n",
            "Epoch 1896/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 559.2442 - acc: 0.8580 - val_loss: 559.3765 - val_acc: 0.7090\n",
            "Epoch 1897/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 559.1940 - acc: 0.8712 - val_loss: 559.3225 - val_acc: 0.7090\n",
            "Epoch 1898/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 559.1394 - acc: 0.8793 - val_loss: 559.2684 - val_acc: 0.7090\n",
            "Epoch 1899/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 559.0873 - acc: 0.8631 - val_loss: 559.2143 - val_acc: 0.7090\n",
            "Epoch 1900/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 559.0366 - acc: 0.8560 - val_loss: 559.1603 - val_acc: 0.7090\n",
            "Epoch 1901/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 558.9787 - acc: 0.8682 - val_loss: 559.1062 - val_acc: 0.7090\n",
            "Epoch 1902/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 558.9244 - acc: 0.8682 - val_loss: 559.0522 - val_acc: 0.7090\n",
            "Epoch 1903/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 558.8661 - acc: 0.8671 - val_loss: 558.9981 - val_acc: 0.7090\n",
            "Epoch 1904/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 558.8100 - acc: 0.8813 - val_loss: 558.9439 - val_acc: 0.7090\n",
            "Epoch 1905/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 558.7595 - acc: 0.8773 - val_loss: 558.8899 - val_acc: 0.7090\n",
            "Epoch 1906/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 558.7062 - acc: 0.8641 - val_loss: 558.8358 - val_acc: 0.7090\n",
            "Epoch 1907/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 558.6503 - acc: 0.8611 - val_loss: 558.7816 - val_acc: 0.7090\n",
            "Epoch 1908/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 558.5947 - acc: 0.8783 - val_loss: 558.7275 - val_acc: 0.7090\n",
            "Epoch 1909/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 558.5334 - acc: 0.8783 - val_loss: 558.6734 - val_acc: 0.7090\n",
            "Epoch 1910/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 558.4834 - acc: 0.8742 - val_loss: 558.6192 - val_acc: 0.7090\n",
            "Epoch 1911/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 558.4372 - acc: 0.8651 - val_loss: 558.5653 - val_acc: 0.7090\n",
            "Epoch 1912/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 558.3831 - acc: 0.8590 - val_loss: 558.5112 - val_acc: 0.7090\n",
            "Epoch 1913/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 558.3254 - acc: 0.8742 - val_loss: 558.4572 - val_acc: 0.7090\n",
            "Epoch 1914/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 558.2828 - acc: 0.8570 - val_loss: 558.4033 - val_acc: 0.7090\n",
            "Epoch 1915/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 558.2215 - acc: 0.8621 - val_loss: 558.3493 - val_acc: 0.7090\n",
            "Epoch 1916/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 558.1552 - acc: 0.8773 - val_loss: 558.2953 - val_acc: 0.7090\n",
            "Epoch 1917/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 558.1078 - acc: 0.8722 - val_loss: 558.2415 - val_acc: 0.7090\n",
            "Epoch 1918/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 558.0555 - acc: 0.8661 - val_loss: 558.1875 - val_acc: 0.7116\n",
            "Epoch 1919/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 557.9985 - acc: 0.8692 - val_loss: 558.1333 - val_acc: 0.7090\n",
            "Epoch 1920/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 557.9515 - acc: 0.8661 - val_loss: 558.0791 - val_acc: 0.7116\n",
            "Epoch 1921/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 557.8982 - acc: 0.8682 - val_loss: 558.0253 - val_acc: 0.7090\n",
            "Epoch 1922/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 557.8439 - acc: 0.8753 - val_loss: 557.9713 - val_acc: 0.7090\n",
            "Epoch 1923/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 557.7831 - acc: 0.8702 - val_loss: 557.9172 - val_acc: 0.7116\n",
            "Epoch 1924/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 557.7355 - acc: 0.8611 - val_loss: 557.8633 - val_acc: 0.7116\n",
            "Epoch 1925/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 557.6790 - acc: 0.8671 - val_loss: 557.8093 - val_acc: 0.7116\n",
            "Epoch 1926/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 557.6217 - acc: 0.8824 - val_loss: 557.7552 - val_acc: 0.7116\n",
            "Epoch 1927/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 557.5714 - acc: 0.8661 - val_loss: 557.7012 - val_acc: 0.7090\n",
            "Epoch 1928/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 557.5234 - acc: 0.8509 - val_loss: 557.6473 - val_acc: 0.7090\n",
            "Epoch 1929/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 557.4566 - acc: 0.8692 - val_loss: 557.5934 - val_acc: 0.7090\n",
            "Epoch 1930/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 557.4093 - acc: 0.8600 - val_loss: 557.5394 - val_acc: 0.7090\n",
            "Epoch 1931/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 557.3615 - acc: 0.8611 - val_loss: 557.4855 - val_acc: 0.7090\n",
            "Epoch 1932/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 557.2973 - acc: 0.8753 - val_loss: 557.4315 - val_acc: 0.7090\n",
            "Epoch 1933/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 557.2438 - acc: 0.8651 - val_loss: 557.3775 - val_acc: 0.7090\n",
            "Epoch 1934/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 557.1860 - acc: 0.8773 - val_loss: 557.3237 - val_acc: 0.7090\n",
            "Epoch 1935/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 557.1409 - acc: 0.8600 - val_loss: 557.2698 - val_acc: 0.7090\n",
            "Epoch 1936/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 557.0865 - acc: 0.8661 - val_loss: 557.2157 - val_acc: 0.7090\n",
            "Epoch 1937/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 557.0351 - acc: 0.8651 - val_loss: 557.1618 - val_acc: 0.7090\n",
            "Epoch 1938/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 556.9753 - acc: 0.8671 - val_loss: 557.1079 - val_acc: 0.7090\n",
            "Epoch 1939/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 556.9223 - acc: 0.8682 - val_loss: 557.0539 - val_acc: 0.7090\n",
            "Epoch 1940/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 556.8668 - acc: 0.8540 - val_loss: 556.9999 - val_acc: 0.7090\n",
            "Epoch 1941/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 556.8255 - acc: 0.8458 - val_loss: 556.9460 - val_acc: 0.7090\n",
            "Epoch 1942/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 556.7654 - acc: 0.8611 - val_loss: 556.8922 - val_acc: 0.7090\n",
            "Epoch 1943/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 556.7045 - acc: 0.8763 - val_loss: 556.8382 - val_acc: 0.7090\n",
            "Epoch 1944/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 556.6465 - acc: 0.8702 - val_loss: 556.7844 - val_acc: 0.7090\n",
            "Epoch 1945/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 556.5981 - acc: 0.8722 - val_loss: 556.7306 - val_acc: 0.7116\n",
            "Epoch 1946/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 556.5433 - acc: 0.8692 - val_loss: 556.6767 - val_acc: 0.7090\n",
            "Epoch 1947/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 556.4889 - acc: 0.8763 - val_loss: 556.6229 - val_acc: 0.7090\n",
            "Epoch 1948/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 556.4341 - acc: 0.8641 - val_loss: 556.5688 - val_acc: 0.7116\n",
            "Epoch 1949/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 556.3828 - acc: 0.8621 - val_loss: 556.5149 - val_acc: 0.7090\n",
            "Epoch 1950/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 556.3306 - acc: 0.8702 - val_loss: 556.4611 - val_acc: 0.7090\n",
            "Epoch 1951/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 556.2736 - acc: 0.8682 - val_loss: 556.4072 - val_acc: 0.7090\n",
            "Epoch 1952/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 556.2203 - acc: 0.8671 - val_loss: 556.3533 - val_acc: 0.7090\n",
            "Epoch 1953/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 556.1665 - acc: 0.8702 - val_loss: 556.2995 - val_acc: 0.7090\n",
            "Epoch 1954/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 556.1123 - acc: 0.8702 - val_loss: 556.2455 - val_acc: 0.7090\n",
            "Epoch 1955/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 556.0524 - acc: 0.8783 - val_loss: 556.1916 - val_acc: 0.7090\n",
            "Epoch 1956/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 556.0068 - acc: 0.8651 - val_loss: 556.1378 - val_acc: 0.7090\n",
            "Epoch 1957/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 555.9541 - acc: 0.8621 - val_loss: 556.0840 - val_acc: 0.7090\n",
            "Epoch 1958/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 555.8963 - acc: 0.8692 - val_loss: 556.0301 - val_acc: 0.7090\n",
            "Epoch 1959/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 555.8400 - acc: 0.8783 - val_loss: 555.9763 - val_acc: 0.7090\n",
            "Epoch 1960/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 555.7865 - acc: 0.8753 - val_loss: 555.9226 - val_acc: 0.7037\n",
            "Epoch 1961/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 555.7381 - acc: 0.8661 - val_loss: 555.8686 - val_acc: 0.7090\n",
            "Epoch 1962/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 555.6804 - acc: 0.8692 - val_loss: 555.8148 - val_acc: 0.7090\n",
            "Epoch 1963/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 555.6256 - acc: 0.8661 - val_loss: 555.7609 - val_acc: 0.7116\n",
            "Epoch 1964/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 555.5805 - acc: 0.8550 - val_loss: 555.7072 - val_acc: 0.7116\n",
            "Epoch 1965/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 555.5215 - acc: 0.8621 - val_loss: 555.6534 - val_acc: 0.7063\n",
            "Epoch 1966/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 555.4700 - acc: 0.8550 - val_loss: 555.5994 - val_acc: 0.7116\n",
            "Epoch 1967/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 555.4167 - acc: 0.8621 - val_loss: 555.5457 - val_acc: 0.7116\n",
            "Epoch 1968/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 555.3605 - acc: 0.8631 - val_loss: 555.4919 - val_acc: 0.7116\n",
            "Epoch 1969/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 555.3026 - acc: 0.8671 - val_loss: 555.4381 - val_acc: 0.7116\n",
            "Epoch 1970/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 555.2534 - acc: 0.8641 - val_loss: 555.3843 - val_acc: 0.7116\n",
            "Epoch 1971/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 555.1961 - acc: 0.8671 - val_loss: 555.3304 - val_acc: 0.7116\n",
            "Epoch 1972/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 555.1471 - acc: 0.8661 - val_loss: 555.2766 - val_acc: 0.7116\n",
            "Epoch 1973/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 555.0900 - acc: 0.8621 - val_loss: 555.2229 - val_acc: 0.7063\n",
            "Epoch 1974/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 555.0323 - acc: 0.8773 - val_loss: 555.1692 - val_acc: 0.7063\n",
            "Epoch 1975/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 554.9882 - acc: 0.8661 - val_loss: 555.1152 - val_acc: 0.7063\n",
            "Epoch 1976/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 554.9279 - acc: 0.8671 - val_loss: 555.0614 - val_acc: 0.7116\n",
            "Epoch 1977/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 554.8727 - acc: 0.8682 - val_loss: 555.0076 - val_acc: 0.7063\n",
            "Epoch 1978/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 554.8225 - acc: 0.8611 - val_loss: 554.9538 - val_acc: 0.7063\n",
            "Epoch 1979/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 554.7679 - acc: 0.8692 - val_loss: 554.9001 - val_acc: 0.7063\n",
            "Epoch 1980/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 554.7106 - acc: 0.8722 - val_loss: 554.8463 - val_acc: 0.7116\n",
            "Epoch 1981/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 554.6610 - acc: 0.8692 - val_loss: 554.7925 - val_acc: 0.7116\n",
            "Epoch 1982/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 554.6074 - acc: 0.8570 - val_loss: 554.7386 - val_acc: 0.7116\n",
            "Epoch 1983/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 554.5545 - acc: 0.8692 - val_loss: 554.6849 - val_acc: 0.7116\n",
            "Epoch 1984/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 554.4988 - acc: 0.8753 - val_loss: 554.6313 - val_acc: 0.7063\n",
            "Epoch 1985/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 554.4450 - acc: 0.8631 - val_loss: 554.5774 - val_acc: 0.7116\n",
            "Epoch 1986/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 554.3827 - acc: 0.8621 - val_loss: 554.5237 - val_acc: 0.7116\n",
            "Epoch 1987/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 554.3329 - acc: 0.8844 - val_loss: 554.4699 - val_acc: 0.7116\n",
            "Epoch 1988/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 554.2836 - acc: 0.8550 - val_loss: 554.4161 - val_acc: 0.7116\n",
            "Epoch 1989/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 554.2312 - acc: 0.8732 - val_loss: 554.3625 - val_acc: 0.7116\n",
            "Epoch 1990/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 554.1770 - acc: 0.8702 - val_loss: 554.3087 - val_acc: 0.7116\n",
            "Epoch 1991/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 554.1227 - acc: 0.8702 - val_loss: 554.2550 - val_acc: 0.7116\n",
            "Epoch 1992/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 554.0602 - acc: 0.8651 - val_loss: 554.2013 - val_acc: 0.7063\n",
            "Epoch 1993/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 554.0108 - acc: 0.8773 - val_loss: 554.1477 - val_acc: 0.7063\n",
            "Epoch 1994/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 553.9601 - acc: 0.8803 - val_loss: 554.0939 - val_acc: 0.7116\n",
            "Epoch 1995/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 553.9074 - acc: 0.8712 - val_loss: 554.0401 - val_acc: 0.7063\n",
            "Epoch 1996/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 553.8529 - acc: 0.8671 - val_loss: 553.9864 - val_acc: 0.7063\n",
            "Epoch 1997/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 553.7956 - acc: 0.8753 - val_loss: 553.9328 - val_acc: 0.7063\n",
            "Epoch 1998/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 553.7427 - acc: 0.8651 - val_loss: 553.8790 - val_acc: 0.7063\n",
            "Epoch 1999/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 553.6888 - acc: 0.8732 - val_loss: 553.8254 - val_acc: 0.7063\n",
            "Epoch 2000/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 553.6434 - acc: 0.8641 - val_loss: 553.7717 - val_acc: 0.7063\n",
            "Epoch 2001/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 553.5832 - acc: 0.8793 - val_loss: 553.7179 - val_acc: 0.7063\n",
            "Epoch 2002/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 553.5291 - acc: 0.8753 - val_loss: 553.6643 - val_acc: 0.7116\n",
            "Epoch 2003/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 553.4796 - acc: 0.8661 - val_loss: 553.6106 - val_acc: 0.7116\n",
            "Epoch 2004/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 553.4238 - acc: 0.8783 - val_loss: 553.5570 - val_acc: 0.7143\n",
            "Epoch 2005/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 553.3672 - acc: 0.8732 - val_loss: 553.5033 - val_acc: 0.7116\n",
            "Epoch 2006/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 553.3168 - acc: 0.8692 - val_loss: 553.4497 - val_acc: 0.7143\n",
            "Epoch 2007/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 553.2607 - acc: 0.8712 - val_loss: 553.3959 - val_acc: 0.7116\n",
            "Epoch 2008/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 553.2121 - acc: 0.8631 - val_loss: 553.3422 - val_acc: 0.7116\n",
            "Epoch 2009/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 553.1545 - acc: 0.8692 - val_loss: 553.2886 - val_acc: 0.7116\n",
            "Epoch 2010/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 553.1043 - acc: 0.8702 - val_loss: 553.2349 - val_acc: 0.7116\n",
            "Epoch 2011/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 553.0484 - acc: 0.8682 - val_loss: 553.1815 - val_acc: 0.7063\n",
            "Epoch 2012/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 552.9914 - acc: 0.8742 - val_loss: 553.1279 - val_acc: 0.7116\n",
            "Epoch 2013/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 552.9390 - acc: 0.8813 - val_loss: 553.0742 - val_acc: 0.7116\n",
            "Epoch 2014/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 552.8862 - acc: 0.8671 - val_loss: 553.0206 - val_acc: 0.7116\n",
            "Epoch 2015/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 552.8303 - acc: 0.8641 - val_loss: 552.9669 - val_acc: 0.7143\n",
            "Epoch 2016/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 552.7760 - acc: 0.8783 - val_loss: 552.9134 - val_acc: 0.7116\n",
            "Epoch 2017/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 552.7303 - acc: 0.8611 - val_loss: 552.8597 - val_acc: 0.7116\n",
            "Epoch 2018/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 552.6744 - acc: 0.8763 - val_loss: 552.8062 - val_acc: 0.7116\n",
            "Epoch 2019/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 552.6184 - acc: 0.8712 - val_loss: 552.7525 - val_acc: 0.7116\n",
            "Epoch 2020/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 552.5638 - acc: 0.8631 - val_loss: 552.6990 - val_acc: 0.7116\n",
            "Epoch 2021/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 552.5109 - acc: 0.8732 - val_loss: 552.6453 - val_acc: 0.7116\n",
            "Epoch 2022/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 552.4553 - acc: 0.8702 - val_loss: 552.5917 - val_acc: 0.7116\n",
            "Epoch 2023/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 552.4078 - acc: 0.8631 - val_loss: 552.5383 - val_acc: 0.7116\n",
            "Epoch 2024/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 552.3461 - acc: 0.8793 - val_loss: 552.4847 - val_acc: 0.7116\n",
            "Epoch 2025/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 552.2966 - acc: 0.8712 - val_loss: 552.4312 - val_acc: 0.7116\n",
            "Epoch 2026/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 552.2386 - acc: 0.8773 - val_loss: 552.3777 - val_acc: 0.7116\n",
            "Epoch 2027/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 552.1949 - acc: 0.8600 - val_loss: 552.3241 - val_acc: 0.7116\n",
            "Epoch 2028/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 552.1389 - acc: 0.8712 - val_loss: 552.2706 - val_acc: 0.7116\n",
            "Epoch 2029/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 552.0847 - acc: 0.8702 - val_loss: 552.2170 - val_acc: 0.7116\n",
            "Epoch 2030/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 552.0314 - acc: 0.8651 - val_loss: 552.1634 - val_acc: 0.7116\n",
            "Epoch 2031/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 551.9729 - acc: 0.8712 - val_loss: 552.1099 - val_acc: 0.7116\n",
            "Epoch 2032/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 551.9257 - acc: 0.8661 - val_loss: 552.0563 - val_acc: 0.7116\n",
            "Epoch 2033/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 551.8669 - acc: 0.8793 - val_loss: 552.0029 - val_acc: 0.7116\n",
            "Epoch 2034/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 551.8113 - acc: 0.8722 - val_loss: 551.9493 - val_acc: 0.7116\n",
            "Epoch 2035/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 551.7613 - acc: 0.8742 - val_loss: 551.8958 - val_acc: 0.7116\n",
            "Epoch 2036/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 551.7036 - acc: 0.8793 - val_loss: 551.8422 - val_acc: 0.7116\n",
            "Epoch 2037/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 551.6554 - acc: 0.8590 - val_loss: 551.7888 - val_acc: 0.7116\n",
            "Epoch 2038/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 551.5964 - acc: 0.8773 - val_loss: 551.7352 - val_acc: 0.7116\n",
            "Epoch 2039/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 551.5386 - acc: 0.8854 - val_loss: 551.6816 - val_acc: 0.7116\n",
            "Epoch 2040/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 551.4999 - acc: 0.8671 - val_loss: 551.6282 - val_acc: 0.7116\n",
            "Epoch 2041/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 551.4423 - acc: 0.8692 - val_loss: 551.5747 - val_acc: 0.7116\n",
            "Epoch 2042/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 551.3829 - acc: 0.8773 - val_loss: 551.5210 - val_acc: 0.7116\n",
            "Epoch 2043/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 551.3359 - acc: 0.8611 - val_loss: 551.4676 - val_acc: 0.7116\n",
            "Epoch 2044/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 551.2802 - acc: 0.8742 - val_loss: 551.4140 - val_acc: 0.7116\n",
            "Epoch 2045/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 551.2211 - acc: 0.8874 - val_loss: 551.3606 - val_acc: 0.7116\n",
            "Epoch 2046/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 551.1670 - acc: 0.8854 - val_loss: 551.3071 - val_acc: 0.7116\n",
            "Epoch 2047/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 551.1112 - acc: 0.8884 - val_loss: 551.2537 - val_acc: 0.7116\n",
            "Epoch 2048/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 551.0601 - acc: 0.8732 - val_loss: 551.2002 - val_acc: 0.7116\n",
            "Epoch 2049/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 551.0152 - acc: 0.8671 - val_loss: 551.1468 - val_acc: 0.7116\n",
            "Epoch 2050/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 550.9524 - acc: 0.8682 - val_loss: 551.0933 - val_acc: 0.7116\n",
            "Epoch 2051/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 550.9025 - acc: 0.8753 - val_loss: 551.0399 - val_acc: 0.7116\n",
            "Epoch 2052/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 550.8555 - acc: 0.8600 - val_loss: 550.9866 - val_acc: 0.7116\n",
            "Epoch 2053/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 550.7978 - acc: 0.8702 - val_loss: 550.9330 - val_acc: 0.7116\n",
            "Epoch 2054/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 550.7351 - acc: 0.8905 - val_loss: 550.8796 - val_acc: 0.7116\n",
            "Epoch 2055/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 550.6924 - acc: 0.8661 - val_loss: 550.8260 - val_acc: 0.7116\n",
            "Epoch 2056/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 550.6397 - acc: 0.8651 - val_loss: 550.7726 - val_acc: 0.7116\n",
            "Epoch 2057/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 550.5830 - acc: 0.8702 - val_loss: 550.7192 - val_acc: 0.7116\n",
            "Epoch 2058/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 550.5357 - acc: 0.8600 - val_loss: 550.6659 - val_acc: 0.7116\n",
            "Epoch 2059/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 550.4781 - acc: 0.8742 - val_loss: 550.6126 - val_acc: 0.7116\n",
            "Epoch 2060/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 550.4235 - acc: 0.8671 - val_loss: 550.5590 - val_acc: 0.7116\n",
            "Epoch 2061/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 550.3681 - acc: 0.8641 - val_loss: 550.5054 - val_acc: 0.7116\n",
            "Epoch 2062/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 550.3181 - acc: 0.8641 - val_loss: 550.4521 - val_acc: 0.7116\n",
            "Epoch 2063/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 550.2562 - acc: 0.8712 - val_loss: 550.3988 - val_acc: 0.7116\n",
            "Epoch 2064/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 550.2095 - acc: 0.8692 - val_loss: 550.3455 - val_acc: 0.7116\n",
            "Epoch 2065/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 550.1536 - acc: 0.8732 - val_loss: 550.2920 - val_acc: 0.7116\n",
            "Epoch 2066/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 550.1060 - acc: 0.8661 - val_loss: 550.2386 - val_acc: 0.7116\n",
            "Epoch 2067/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 550.0497 - acc: 0.8631 - val_loss: 550.1853 - val_acc: 0.7116\n",
            "Epoch 2068/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 549.9899 - acc: 0.8834 - val_loss: 550.1318 - val_acc: 0.7116\n",
            "Epoch 2069/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 549.9407 - acc: 0.8702 - val_loss: 550.0786 - val_acc: 0.7116\n",
            "Epoch 2070/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 549.8842 - acc: 0.8753 - val_loss: 550.0252 - val_acc: 0.7116\n",
            "Epoch 2071/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 549.8407 - acc: 0.8560 - val_loss: 549.9719 - val_acc: 0.7116\n",
            "Epoch 2072/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 549.7838 - acc: 0.8600 - val_loss: 549.9186 - val_acc: 0.7116\n",
            "Epoch 2073/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 549.7305 - acc: 0.8651 - val_loss: 549.8652 - val_acc: 0.7116\n",
            "Epoch 2074/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 549.6782 - acc: 0.8692 - val_loss: 549.8118 - val_acc: 0.7116\n",
            "Epoch 2075/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 549.6149 - acc: 0.8813 - val_loss: 549.7584 - val_acc: 0.7116\n",
            "Epoch 2076/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 549.5736 - acc: 0.8712 - val_loss: 549.7051 - val_acc: 0.7116\n",
            "Epoch 2077/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 549.5200 - acc: 0.8651 - val_loss: 549.6516 - val_acc: 0.7116\n",
            "Epoch 2078/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 549.4645 - acc: 0.8773 - val_loss: 549.5984 - val_acc: 0.7116\n",
            "Epoch 2079/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 549.4072 - acc: 0.8722 - val_loss: 549.5450 - val_acc: 0.7116\n",
            "Epoch 2080/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 549.3619 - acc: 0.8671 - val_loss: 549.4916 - val_acc: 0.7116\n",
            "Epoch 2081/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 549.2960 - acc: 0.8651 - val_loss: 549.4384 - val_acc: 0.7116\n",
            "Epoch 2082/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 549.2511 - acc: 0.8641 - val_loss: 549.3850 - val_acc: 0.7116\n",
            "Epoch 2083/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 549.1897 - acc: 0.8874 - val_loss: 549.3317 - val_acc: 0.7116\n",
            "Epoch 2084/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 549.1404 - acc: 0.8813 - val_loss: 549.2783 - val_acc: 0.7116\n",
            "Epoch 2085/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 549.0864 - acc: 0.8722 - val_loss: 549.2251 - val_acc: 0.7116\n",
            "Epoch 2086/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 549.0322 - acc: 0.8844 - val_loss: 549.1718 - val_acc: 0.7116\n",
            "Epoch 2087/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 548.9758 - acc: 0.8732 - val_loss: 549.1184 - val_acc: 0.7116\n",
            "Epoch 2088/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 548.9222 - acc: 0.8763 - val_loss: 549.0651 - val_acc: 0.7116\n",
            "Epoch 2089/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 548.8747 - acc: 0.8732 - val_loss: 549.0119 - val_acc: 0.7116\n",
            "Epoch 2090/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 548.8271 - acc: 0.8560 - val_loss: 548.9585 - val_acc: 0.7116\n",
            "Epoch 2091/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 548.7632 - acc: 0.8803 - val_loss: 548.9052 - val_acc: 0.7116\n",
            "Epoch 2092/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 548.7099 - acc: 0.8753 - val_loss: 548.8520 - val_acc: 0.7116\n",
            "Epoch 2093/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 548.6605 - acc: 0.8854 - val_loss: 548.7987 - val_acc: 0.7116\n",
            "Epoch 2094/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 548.6015 - acc: 0.8732 - val_loss: 548.7455 - val_acc: 0.7116\n",
            "Epoch 2095/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 548.5559 - acc: 0.8682 - val_loss: 548.6922 - val_acc: 0.7116\n",
            "Epoch 2096/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 548.5038 - acc: 0.8641 - val_loss: 548.6389 - val_acc: 0.7116\n",
            "Epoch 2097/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 548.4470 - acc: 0.8773 - val_loss: 548.5856 - val_acc: 0.7116\n",
            "Epoch 2098/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 548.3923 - acc: 0.8702 - val_loss: 548.5323 - val_acc: 0.7116\n",
            "Epoch 2099/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 548.3379 - acc: 0.8763 - val_loss: 548.4791 - val_acc: 0.7116\n",
            "Epoch 2100/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 548.2883 - acc: 0.8742 - val_loss: 548.4259 - val_acc: 0.7116\n",
            "Epoch 2101/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 548.2307 - acc: 0.8783 - val_loss: 548.3726 - val_acc: 0.7116\n",
            "Epoch 2102/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 548.1839 - acc: 0.8631 - val_loss: 548.3194 - val_acc: 0.7116\n",
            "Epoch 2103/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 548.1329 - acc: 0.8783 - val_loss: 548.2660 - val_acc: 0.7116\n",
            "Epoch 2104/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 548.0739 - acc: 0.8702 - val_loss: 548.2128 - val_acc: 0.7116\n",
            "Epoch 2105/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 548.0189 - acc: 0.8783 - val_loss: 548.1595 - val_acc: 0.7116\n",
            "Epoch 2106/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 547.9742 - acc: 0.8732 - val_loss: 548.1063 - val_acc: 0.7116\n",
            "Epoch 2107/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 547.9132 - acc: 0.8834 - val_loss: 548.0532 - val_acc: 0.7116\n",
            "Epoch 2108/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 547.8611 - acc: 0.8570 - val_loss: 547.9999 - val_acc: 0.7116\n",
            "Epoch 2109/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 547.8091 - acc: 0.8742 - val_loss: 547.9466 - val_acc: 0.7143\n",
            "Epoch 2110/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 547.7589 - acc: 0.8732 - val_loss: 547.8934 - val_acc: 0.7116\n",
            "Epoch 2111/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 547.7016 - acc: 0.8844 - val_loss: 547.8402 - val_acc: 0.7116\n",
            "Epoch 2112/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 547.6473 - acc: 0.8671 - val_loss: 547.7872 - val_acc: 0.7116\n",
            "Epoch 2113/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 547.5993 - acc: 0.8590 - val_loss: 547.7338 - val_acc: 0.7116\n",
            "Epoch 2114/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 547.5493 - acc: 0.8702 - val_loss: 547.6807 - val_acc: 0.7116\n",
            "Epoch 2115/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 547.4914 - acc: 0.8783 - val_loss: 547.6274 - val_acc: 0.7116\n",
            "Epoch 2116/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 547.4409 - acc: 0.8692 - val_loss: 547.5742 - val_acc: 0.7116\n",
            "Epoch 2117/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 547.3854 - acc: 0.8712 - val_loss: 547.5210 - val_acc: 0.7116\n",
            "Epoch 2118/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 547.3290 - acc: 0.8651 - val_loss: 547.4678 - val_acc: 0.7116\n",
            "Epoch 2119/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 547.2810 - acc: 0.8742 - val_loss: 547.4146 - val_acc: 0.7116\n",
            "Epoch 2120/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 547.2263 - acc: 0.8631 - val_loss: 547.3616 - val_acc: 0.7116\n",
            "Epoch 2121/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 547.1747 - acc: 0.8631 - val_loss: 547.3084 - val_acc: 0.7116\n",
            "Epoch 2122/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 547.1116 - acc: 0.8692 - val_loss: 547.2551 - val_acc: 0.7116\n",
            "Epoch 2123/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 547.0579 - acc: 0.8803 - val_loss: 547.2018 - val_acc: 0.7116\n",
            "Epoch 2124/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 547.0140 - acc: 0.8742 - val_loss: 547.1487 - val_acc: 0.7116\n",
            "Epoch 2125/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 546.9557 - acc: 0.8834 - val_loss: 547.0955 - val_acc: 0.7116\n",
            "Epoch 2126/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 546.9086 - acc: 0.8783 - val_loss: 547.0423 - val_acc: 0.7116\n",
            "Epoch 2127/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 546.8492 - acc: 0.8783 - val_loss: 546.9891 - val_acc: 0.7116\n",
            "Epoch 2128/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 546.7956 - acc: 0.8722 - val_loss: 546.9360 - val_acc: 0.7116\n",
            "Epoch 2129/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 546.7437 - acc: 0.8671 - val_loss: 546.8829 - val_acc: 0.7116\n",
            "Epoch 2130/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 546.6916 - acc: 0.8712 - val_loss: 546.8298 - val_acc: 0.7116\n",
            "Epoch 2131/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 546.6326 - acc: 0.8905 - val_loss: 546.7766 - val_acc: 0.7143\n",
            "Epoch 2132/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 546.5862 - acc: 0.8753 - val_loss: 546.7235 - val_acc: 0.7116\n",
            "Epoch 2133/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 546.5253 - acc: 0.8753 - val_loss: 546.6704 - val_acc: 0.7143\n",
            "Epoch 2134/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 546.4742 - acc: 0.8722 - val_loss: 546.6173 - val_acc: 0.7116\n",
            "Epoch 2135/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 546.4200 - acc: 0.8763 - val_loss: 546.5640 - val_acc: 0.7116\n",
            "Epoch 2136/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 546.3681 - acc: 0.8834 - val_loss: 546.5108 - val_acc: 0.7116\n",
            "Epoch 2137/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 546.3151 - acc: 0.8854 - val_loss: 546.4578 - val_acc: 0.7116\n",
            "Epoch 2138/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 546.2651 - acc: 0.8631 - val_loss: 546.4046 - val_acc: 0.7143\n",
            "Epoch 2139/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 546.2152 - acc: 0.8661 - val_loss: 546.3516 - val_acc: 0.7116\n",
            "Epoch 2140/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 546.1559 - acc: 0.8742 - val_loss: 546.2985 - val_acc: 0.7116\n",
            "Epoch 2141/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 546.1092 - acc: 0.8773 - val_loss: 546.2453 - val_acc: 0.7169\n",
            "Epoch 2142/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 546.0496 - acc: 0.8803 - val_loss: 546.1923 - val_acc: 0.7143\n",
            "Epoch 2143/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 545.9938 - acc: 0.8854 - val_loss: 546.1393 - val_acc: 0.7169\n",
            "Epoch 2144/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 545.9460 - acc: 0.8742 - val_loss: 546.0860 - val_acc: 0.7169\n",
            "Epoch 2145/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 545.8892 - acc: 0.8783 - val_loss: 546.0330 - val_acc: 0.7143\n",
            "Epoch 2146/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 545.8398 - acc: 0.8773 - val_loss: 545.9800 - val_acc: 0.7143\n",
            "Epoch 2147/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 545.7840 - acc: 0.8824 - val_loss: 545.9269 - val_acc: 0.7143\n",
            "Epoch 2148/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 545.7307 - acc: 0.8773 - val_loss: 545.8737 - val_acc: 0.7143\n",
            "Epoch 2149/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 545.6802 - acc: 0.8824 - val_loss: 545.8206 - val_acc: 0.7143\n",
            "Epoch 2150/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 545.6240 - acc: 0.8824 - val_loss: 545.7676 - val_acc: 0.7143\n",
            "Epoch 2151/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 545.5707 - acc: 0.8844 - val_loss: 545.7145 - val_acc: 0.7169\n",
            "Epoch 2152/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 545.5246 - acc: 0.8682 - val_loss: 545.6616 - val_acc: 0.7169\n",
            "Epoch 2153/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 545.4638 - acc: 0.8773 - val_loss: 545.6085 - val_acc: 0.7143\n",
            "Epoch 2154/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 545.4149 - acc: 0.8834 - val_loss: 545.5553 - val_acc: 0.7143\n",
            "Epoch 2155/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 545.3592 - acc: 0.8895 - val_loss: 545.5023 - val_acc: 0.7143\n",
            "Epoch 2156/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 545.3037 - acc: 0.8763 - val_loss: 545.4494 - val_acc: 0.7169\n",
            "Epoch 2157/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 545.2590 - acc: 0.8671 - val_loss: 545.3963 - val_acc: 0.7116\n",
            "Epoch 2158/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 545.1972 - acc: 0.8834 - val_loss: 545.3433 - val_acc: 0.7116\n",
            "Epoch 2159/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 545.1432 - acc: 0.8803 - val_loss: 545.2901 - val_acc: 0.7116\n",
            "Epoch 2160/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 545.0939 - acc: 0.8824 - val_loss: 545.2370 - val_acc: 0.7143\n",
            "Epoch 2161/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 545.0438 - acc: 0.8651 - val_loss: 545.1840 - val_acc: 0.7143\n",
            "Epoch 2162/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 544.9927 - acc: 0.8641 - val_loss: 545.1309 - val_acc: 0.7143\n",
            "Epoch 2163/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 544.9345 - acc: 0.8813 - val_loss: 545.0777 - val_acc: 0.7143\n",
            "Epoch 2164/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 544.8876 - acc: 0.8692 - val_loss: 545.0248 - val_acc: 0.7143\n",
            "Epoch 2165/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 544.8348 - acc: 0.8783 - val_loss: 544.9719 - val_acc: 0.7169\n",
            "Epoch 2166/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 544.7753 - acc: 0.8712 - val_loss: 544.9187 - val_acc: 0.7169\n",
            "Epoch 2167/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 544.7237 - acc: 0.8813 - val_loss: 544.8657 - val_acc: 0.7169\n",
            "Epoch 2168/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 544.6772 - acc: 0.8671 - val_loss: 544.8127 - val_acc: 0.7169\n",
            "Epoch 2169/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 544.6201 - acc: 0.8692 - val_loss: 544.7597 - val_acc: 0.7143\n",
            "Epoch 2170/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 544.5610 - acc: 0.8854 - val_loss: 544.7067 - val_acc: 0.7169\n",
            "Epoch 2171/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 544.5122 - acc: 0.8793 - val_loss: 544.6537 - val_acc: 0.7143\n",
            "Epoch 2172/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 544.4602 - acc: 0.8834 - val_loss: 544.6007 - val_acc: 0.7169\n",
            "Epoch 2173/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 544.4086 - acc: 0.8621 - val_loss: 544.5477 - val_acc: 0.7143\n",
            "Epoch 2174/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 544.3569 - acc: 0.8712 - val_loss: 544.4947 - val_acc: 0.7143\n",
            "Epoch 2175/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 544.3093 - acc: 0.8651 - val_loss: 544.4417 - val_acc: 0.7143\n",
            "Epoch 2176/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 544.2395 - acc: 0.8742 - val_loss: 544.3889 - val_acc: 0.7143\n",
            "Epoch 2177/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 544.1953 - acc: 0.8783 - val_loss: 544.3359 - val_acc: 0.7169\n",
            "Epoch 2178/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 544.1433 - acc: 0.8783 - val_loss: 544.2829 - val_acc: 0.7143\n",
            "Epoch 2179/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 544.0892 - acc: 0.8742 - val_loss: 544.2299 - val_acc: 0.7143\n",
            "Epoch 2180/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 544.0349 - acc: 0.8732 - val_loss: 544.1771 - val_acc: 0.7143\n",
            "Epoch 2181/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 543.9817 - acc: 0.8783 - val_loss: 544.1242 - val_acc: 0.7143\n",
            "Epoch 2182/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 543.9291 - acc: 0.8732 - val_loss: 544.0712 - val_acc: 0.7143\n",
            "Epoch 2183/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 543.8818 - acc: 0.8631 - val_loss: 544.0182 - val_acc: 0.7143\n",
            "Epoch 2184/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 543.8191 - acc: 0.8864 - val_loss: 543.9653 - val_acc: 0.7143\n",
            "Epoch 2185/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 543.7671 - acc: 0.8803 - val_loss: 543.9123 - val_acc: 0.7143\n",
            "Epoch 2186/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 543.7212 - acc: 0.8692 - val_loss: 543.8593 - val_acc: 0.7143\n",
            "Epoch 2187/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 543.6594 - acc: 0.8702 - val_loss: 543.8064 - val_acc: 0.7143\n",
            "Epoch 2188/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 543.6102 - acc: 0.8813 - val_loss: 543.7535 - val_acc: 0.7143\n",
            "Epoch 2189/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 543.5525 - acc: 0.8874 - val_loss: 543.7006 - val_acc: 0.7143\n",
            "Epoch 2190/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 543.5064 - acc: 0.8763 - val_loss: 543.6476 - val_acc: 0.7143\n",
            "Epoch 2191/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 543.4496 - acc: 0.8763 - val_loss: 543.5947 - val_acc: 0.7169\n",
            "Epoch 2192/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 543.4054 - acc: 0.8651 - val_loss: 543.5419 - val_acc: 0.7169\n",
            "Epoch 2193/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 543.3472 - acc: 0.8712 - val_loss: 543.4889 - val_acc: 0.7169\n",
            "Epoch 2194/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 543.2962 - acc: 0.8824 - val_loss: 543.4360 - val_acc: 0.7169\n",
            "Epoch 2195/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 543.2371 - acc: 0.8753 - val_loss: 543.3830 - val_acc: 0.7169\n",
            "Epoch 2196/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 543.1953 - acc: 0.8631 - val_loss: 543.3303 - val_acc: 0.7169\n",
            "Epoch 2197/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 543.1391 - acc: 0.8590 - val_loss: 543.2775 - val_acc: 0.7169\n",
            "Epoch 2198/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 543.0872 - acc: 0.8763 - val_loss: 543.2246 - val_acc: 0.7169\n",
            "Epoch 2199/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 543.0289 - acc: 0.8834 - val_loss: 543.1717 - val_acc: 0.7143\n",
            "Epoch 2200/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 542.9725 - acc: 0.8884 - val_loss: 543.1189 - val_acc: 0.7143\n",
            "Epoch 2201/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 542.9254 - acc: 0.8753 - val_loss: 543.0660 - val_acc: 0.7143\n",
            "Epoch 2202/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 542.8767 - acc: 0.8702 - val_loss: 543.0131 - val_acc: 0.7143\n",
            "Epoch 2203/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 542.8129 - acc: 0.8854 - val_loss: 542.9604 - val_acc: 0.7143\n",
            "Epoch 2204/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 542.7692 - acc: 0.8722 - val_loss: 542.9076 - val_acc: 0.7143\n",
            "Epoch 2205/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 542.7096 - acc: 0.8732 - val_loss: 542.8547 - val_acc: 0.7143\n",
            "Epoch 2206/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 542.6600 - acc: 0.8783 - val_loss: 542.8018 - val_acc: 0.7143\n",
            "Epoch 2207/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 542.5980 - acc: 0.8813 - val_loss: 542.7489 - val_acc: 0.7143\n",
            "Epoch 2208/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 542.5555 - acc: 0.8773 - val_loss: 542.6960 - val_acc: 0.7143\n",
            "Epoch 2209/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 542.4959 - acc: 0.8854 - val_loss: 542.6431 - val_acc: 0.7143\n",
            "Epoch 2210/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 542.4486 - acc: 0.8803 - val_loss: 542.5904 - val_acc: 0.7116\n",
            "Epoch 2211/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 542.3967 - acc: 0.8783 - val_loss: 542.5376 - val_acc: 0.7116\n",
            "Epoch 2212/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 542.3380 - acc: 0.8773 - val_loss: 542.4847 - val_acc: 0.7143\n",
            "Epoch 2213/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 542.2890 - acc: 0.8702 - val_loss: 542.4319 - val_acc: 0.7143\n",
            "Epoch 2214/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 542.2385 - acc: 0.8854 - val_loss: 542.3790 - val_acc: 0.7143\n",
            "Epoch 2215/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 542.1844 - acc: 0.8813 - val_loss: 542.3262 - val_acc: 0.7143\n",
            "Epoch 2216/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 542.1355 - acc: 0.8702 - val_loss: 542.2735 - val_acc: 0.7169\n",
            "Epoch 2217/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 542.0809 - acc: 0.8742 - val_loss: 542.2207 - val_acc: 0.7143\n",
            "Epoch 2218/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 542.0244 - acc: 0.8753 - val_loss: 542.1678 - val_acc: 0.7143\n",
            "Epoch 2219/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 541.9726 - acc: 0.8712 - val_loss: 542.1152 - val_acc: 0.7143\n",
            "Epoch 2220/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 541.9186 - acc: 0.8834 - val_loss: 542.0625 - val_acc: 0.7143\n",
            "Epoch 2221/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 541.8668 - acc: 0.8712 - val_loss: 542.0096 - val_acc: 0.7143\n",
            "Epoch 2222/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 541.8087 - acc: 0.8874 - val_loss: 541.9568 - val_acc: 0.7143\n",
            "Epoch 2223/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 541.7596 - acc: 0.8763 - val_loss: 541.9041 - val_acc: 0.7143\n",
            "Epoch 2224/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 541.7075 - acc: 0.8803 - val_loss: 541.8514 - val_acc: 0.7143\n",
            "Epoch 2225/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 541.6594 - acc: 0.8793 - val_loss: 541.7985 - val_acc: 0.7143\n",
            "Epoch 2226/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 541.5956 - acc: 0.8874 - val_loss: 541.7458 - val_acc: 0.7143\n",
            "Epoch 2227/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 541.5441 - acc: 0.8915 - val_loss: 541.6930 - val_acc: 0.7143\n",
            "Epoch 2228/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 541.4988 - acc: 0.8803 - val_loss: 541.6403 - val_acc: 0.7143\n",
            "Epoch 2229/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 541.4427 - acc: 0.8864 - val_loss: 541.5875 - val_acc: 0.7143\n",
            "Epoch 2230/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 541.3889 - acc: 0.8854 - val_loss: 541.5347 - val_acc: 0.7169\n",
            "Epoch 2231/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 541.3366 - acc: 0.8793 - val_loss: 541.4819 - val_acc: 0.7143\n",
            "Epoch 2232/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 541.2748 - acc: 0.8935 - val_loss: 541.4293 - val_acc: 0.7143\n",
            "Epoch 2233/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 541.2306 - acc: 0.8793 - val_loss: 541.3765 - val_acc: 0.7143\n",
            "Epoch 2234/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 541.1830 - acc: 0.8813 - val_loss: 541.3239 - val_acc: 0.7143\n",
            "Epoch 2235/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 541.1257 - acc: 0.8854 - val_loss: 541.2712 - val_acc: 0.7143\n",
            "Epoch 2236/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 541.0792 - acc: 0.8732 - val_loss: 541.2185 - val_acc: 0.7143\n",
            "Epoch 2237/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 541.0301 - acc: 0.8712 - val_loss: 541.1657 - val_acc: 0.7143\n",
            "Epoch 2238/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 540.9766 - acc: 0.8671 - val_loss: 541.1129 - val_acc: 0.7143\n",
            "Epoch 2239/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 540.9112 - acc: 0.8803 - val_loss: 541.0601 - val_acc: 0.7143\n",
            "Epoch 2240/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 540.8642 - acc: 0.8763 - val_loss: 541.0075 - val_acc: 0.7143\n",
            "Epoch 2241/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 540.8097 - acc: 0.8803 - val_loss: 540.9547 - val_acc: 0.7143\n",
            "Epoch 2242/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 540.7626 - acc: 0.8753 - val_loss: 540.9020 - val_acc: 0.7143\n",
            "Epoch 2243/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 540.7057 - acc: 0.8773 - val_loss: 540.8494 - val_acc: 0.7143\n",
            "Epoch 2244/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 540.6511 - acc: 0.8854 - val_loss: 540.7967 - val_acc: 0.7143\n",
            "Epoch 2245/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 540.5964 - acc: 0.8793 - val_loss: 540.7439 - val_acc: 0.7116\n",
            "Epoch 2246/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 540.5482 - acc: 0.8702 - val_loss: 540.6913 - val_acc: 0.7116\n",
            "Epoch 2247/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 540.4934 - acc: 0.8722 - val_loss: 540.6386 - val_acc: 0.7143\n",
            "Epoch 2248/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 540.4445 - acc: 0.8692 - val_loss: 540.5861 - val_acc: 0.7143\n",
            "Epoch 2249/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 540.3886 - acc: 0.8753 - val_loss: 540.5334 - val_acc: 0.7169\n",
            "Epoch 2250/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 540.3396 - acc: 0.8793 - val_loss: 540.4807 - val_acc: 0.7143\n",
            "Epoch 2251/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 540.2838 - acc: 0.8712 - val_loss: 540.4280 - val_acc: 0.7143\n",
            "Epoch 2252/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 540.2331 - acc: 0.8844 - val_loss: 540.3755 - val_acc: 0.7143\n",
            "Epoch 2253/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 540.1721 - acc: 0.8793 - val_loss: 540.3228 - val_acc: 0.7143\n",
            "Epoch 2254/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 540.1255 - acc: 0.8692 - val_loss: 540.2700 - val_acc: 0.7116\n",
            "Epoch 2255/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 540.0760 - acc: 0.8722 - val_loss: 540.2175 - val_acc: 0.7116\n",
            "Epoch 2256/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 540.0150 - acc: 0.8813 - val_loss: 540.1649 - val_acc: 0.7143\n",
            "Epoch 2257/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 539.9656 - acc: 0.8773 - val_loss: 540.1122 - val_acc: 0.7169\n",
            "Epoch 2258/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 539.9181 - acc: 0.8712 - val_loss: 540.0596 - val_acc: 0.7143\n",
            "Epoch 2259/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 539.8599 - acc: 0.8753 - val_loss: 540.0069 - val_acc: 0.7143\n",
            "Epoch 2260/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 539.8119 - acc: 0.8732 - val_loss: 539.9544 - val_acc: 0.7143\n",
            "Epoch 2261/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 539.7519 - acc: 0.8854 - val_loss: 539.9017 - val_acc: 0.7143\n",
            "Epoch 2262/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 539.7040 - acc: 0.8783 - val_loss: 539.8490 - val_acc: 0.7143\n",
            "Epoch 2263/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 539.6443 - acc: 0.8854 - val_loss: 539.7964 - val_acc: 0.7143\n",
            "Epoch 2264/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 539.6028 - acc: 0.8753 - val_loss: 539.7438 - val_acc: 0.7143\n",
            "Epoch 2265/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 539.5445 - acc: 0.8884 - val_loss: 539.6912 - val_acc: 0.7143\n",
            "Epoch 2266/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 539.4897 - acc: 0.8763 - val_loss: 539.6387 - val_acc: 0.7143\n",
            "Epoch 2267/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 539.4405 - acc: 0.8824 - val_loss: 539.5862 - val_acc: 0.7143\n",
            "Epoch 2268/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 539.3950 - acc: 0.8611 - val_loss: 539.5335 - val_acc: 0.7169\n",
            "Epoch 2269/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 539.3342 - acc: 0.8844 - val_loss: 539.4809 - val_acc: 0.7169\n",
            "Epoch 2270/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 539.2817 - acc: 0.8905 - val_loss: 539.4284 - val_acc: 0.7169\n",
            "Epoch 2271/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 539.2338 - acc: 0.8671 - val_loss: 539.3758 - val_acc: 0.7169\n",
            "Epoch 2272/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 539.1823 - acc: 0.8631 - val_loss: 539.3233 - val_acc: 0.7169\n",
            "Epoch 2273/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 539.1276 - acc: 0.8732 - val_loss: 539.2708 - val_acc: 0.7169\n",
            "Epoch 2274/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 539.0624 - acc: 0.8824 - val_loss: 539.2180 - val_acc: 0.7169\n",
            "Epoch 2275/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 539.0181 - acc: 0.8803 - val_loss: 539.1656 - val_acc: 0.7143\n",
            "Epoch 2276/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 538.9649 - acc: 0.8834 - val_loss: 539.1131 - val_acc: 0.7169\n",
            "Epoch 2277/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 538.9135 - acc: 0.8864 - val_loss: 539.0606 - val_acc: 0.7169\n",
            "Epoch 2278/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 538.8646 - acc: 0.8874 - val_loss: 539.0082 - val_acc: 0.7169\n",
            "Epoch 2279/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 538.8142 - acc: 0.8712 - val_loss: 538.9555 - val_acc: 0.7169\n",
            "Epoch 2280/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 538.7570 - acc: 0.8834 - val_loss: 538.9031 - val_acc: 0.7169\n",
            "Epoch 2281/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 538.7009 - acc: 0.8864 - val_loss: 538.8505 - val_acc: 0.7169\n",
            "Epoch 2282/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 538.6507 - acc: 0.8905 - val_loss: 538.7980 - val_acc: 0.7169\n",
            "Epoch 2283/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 538.6032 - acc: 0.8732 - val_loss: 538.7453 - val_acc: 0.7169\n",
            "Epoch 2284/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 538.5429 - acc: 0.8783 - val_loss: 538.6930 - val_acc: 0.7143\n",
            "Epoch 2285/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 538.4950 - acc: 0.8732 - val_loss: 538.6404 - val_acc: 0.7169\n",
            "Epoch 2286/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 538.4430 - acc: 0.8834 - val_loss: 538.5879 - val_acc: 0.7143\n",
            "Epoch 2287/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 538.3841 - acc: 0.8824 - val_loss: 538.5353 - val_acc: 0.7143\n",
            "Epoch 2288/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 538.3350 - acc: 0.8844 - val_loss: 538.4828 - val_acc: 0.7143\n",
            "Epoch 2289/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 538.2882 - acc: 0.8793 - val_loss: 538.4304 - val_acc: 0.7143\n",
            "Epoch 2290/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 538.2303 - acc: 0.8753 - val_loss: 538.3780 - val_acc: 0.7143\n",
            "Epoch 2291/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 538.1772 - acc: 0.8824 - val_loss: 538.3255 - val_acc: 0.7143\n",
            "Epoch 2292/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 538.1223 - acc: 0.8884 - val_loss: 538.2729 - val_acc: 0.7143\n",
            "Epoch 2293/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 538.0657 - acc: 0.8895 - val_loss: 538.2205 - val_acc: 0.7143\n",
            "Epoch 2294/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 538.0139 - acc: 0.8895 - val_loss: 538.1679 - val_acc: 0.7143\n",
            "Epoch 2295/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 537.9678 - acc: 0.8692 - val_loss: 538.1155 - val_acc: 0.7143\n",
            "Epoch 2296/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 537.9192 - acc: 0.8661 - val_loss: 538.0631 - val_acc: 0.7143\n",
            "Epoch 2297/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 537.8607 - acc: 0.8722 - val_loss: 538.0104 - val_acc: 0.7169\n",
            "Epoch 2298/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 537.8152 - acc: 0.8753 - val_loss: 537.9578 - val_acc: 0.7169\n",
            "Epoch 2299/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 537.7535 - acc: 0.8824 - val_loss: 537.9055 - val_acc: 0.7143\n",
            "Epoch 2300/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 537.7058 - acc: 0.8773 - val_loss: 537.8532 - val_acc: 0.7169\n",
            "Epoch 2301/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 537.6602 - acc: 0.8651 - val_loss: 537.8007 - val_acc: 0.7169\n",
            "Epoch 2302/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 537.5989 - acc: 0.8813 - val_loss: 537.7481 - val_acc: 0.7169\n",
            "Epoch 2303/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 537.5459 - acc: 0.8742 - val_loss: 537.6956 - val_acc: 0.7169\n",
            "Epoch 2304/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 537.5031 - acc: 0.8641 - val_loss: 537.6432 - val_acc: 0.7169\n",
            "Epoch 2305/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 537.4478 - acc: 0.8712 - val_loss: 537.5909 - val_acc: 0.7196\n",
            "Epoch 2306/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 537.3909 - acc: 0.8692 - val_loss: 537.5384 - val_acc: 0.7169\n",
            "Epoch 2307/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 537.3371 - acc: 0.8793 - val_loss: 537.4859 - val_acc: 0.7169\n",
            "Epoch 2308/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 537.2844 - acc: 0.8874 - val_loss: 537.4338 - val_acc: 0.7143\n",
            "Epoch 2309/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 537.2310 - acc: 0.8824 - val_loss: 537.3812 - val_acc: 0.7169\n",
            "Epoch 2310/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 537.1709 - acc: 0.8884 - val_loss: 537.3287 - val_acc: 0.7143\n",
            "Epoch 2311/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 537.1279 - acc: 0.8813 - val_loss: 537.2763 - val_acc: 0.7169\n",
            "Epoch 2312/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 537.0739 - acc: 0.8915 - val_loss: 537.2240 - val_acc: 0.7169\n",
            "Epoch 2313/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 537.0193 - acc: 0.8864 - val_loss: 537.1715 - val_acc: 0.7196\n",
            "Epoch 2314/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 536.9759 - acc: 0.8682 - val_loss: 537.1191 - val_acc: 0.7169\n",
            "Epoch 2315/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 536.9183 - acc: 0.8813 - val_loss: 537.0667 - val_acc: 0.7169\n",
            "Epoch 2316/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 536.8687 - acc: 0.8702 - val_loss: 537.0142 - val_acc: 0.7196\n",
            "Epoch 2317/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 536.8094 - acc: 0.8793 - val_loss: 536.9618 - val_acc: 0.7169\n",
            "Epoch 2318/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 536.7696 - acc: 0.8651 - val_loss: 536.9095 - val_acc: 0.7169\n",
            "Epoch 2319/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 536.7041 - acc: 0.8834 - val_loss: 536.8571 - val_acc: 0.7196\n",
            "Epoch 2320/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 536.6598 - acc: 0.8783 - val_loss: 536.8047 - val_acc: 0.7169\n",
            "Epoch 2321/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 536.6020 - acc: 0.8834 - val_loss: 536.7524 - val_acc: 0.7169\n",
            "Epoch 2322/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 536.5489 - acc: 0.8966 - val_loss: 536.7001 - val_acc: 0.7143\n",
            "Epoch 2323/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 536.4939 - acc: 0.8824 - val_loss: 536.6477 - val_acc: 0.7169\n",
            "Epoch 2324/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 536.4441 - acc: 0.8813 - val_loss: 536.5954 - val_acc: 0.7169\n",
            "Epoch 2325/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 536.3956 - acc: 0.8793 - val_loss: 536.5430 - val_acc: 0.7169\n",
            "Epoch 2326/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 536.3378 - acc: 0.8813 - val_loss: 536.4906 - val_acc: 0.7169\n",
            "Epoch 2327/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 536.2879 - acc: 0.8864 - val_loss: 536.4382 - val_acc: 0.7169\n",
            "Epoch 2328/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 536.2344 - acc: 0.8864 - val_loss: 536.3861 - val_acc: 0.7169\n",
            "Epoch 2329/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 536.1893 - acc: 0.8753 - val_loss: 536.3336 - val_acc: 0.7169\n",
            "Epoch 2330/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 536.1324 - acc: 0.8783 - val_loss: 536.2812 - val_acc: 0.7196\n",
            "Epoch 2331/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 536.0836 - acc: 0.8773 - val_loss: 536.2290 - val_acc: 0.7196\n",
            "Epoch 2332/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 536.0258 - acc: 0.8874 - val_loss: 536.1766 - val_acc: 0.7196\n",
            "Epoch 2333/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 535.9808 - acc: 0.8702 - val_loss: 536.1243 - val_acc: 0.7196\n",
            "Epoch 2334/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 535.9208 - acc: 0.8905 - val_loss: 536.0720 - val_acc: 0.7169\n",
            "Epoch 2335/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 535.8743 - acc: 0.8844 - val_loss: 536.0196 - val_acc: 0.7196\n",
            "Epoch 2336/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 535.8191 - acc: 0.8712 - val_loss: 535.9671 - val_acc: 0.7196\n",
            "Epoch 2337/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 535.7623 - acc: 0.8955 - val_loss: 535.9149 - val_acc: 0.7196\n",
            "Epoch 2338/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 535.7175 - acc: 0.8702 - val_loss: 535.8625 - val_acc: 0.7169\n",
            "Epoch 2339/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 535.6614 - acc: 0.8834 - val_loss: 535.8103 - val_acc: 0.7169\n",
            "Epoch 2340/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 535.6119 - acc: 0.8732 - val_loss: 535.7579 - val_acc: 0.7196\n",
            "Epoch 2341/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 535.5613 - acc: 0.8702 - val_loss: 535.7056 - val_acc: 0.7169\n",
            "Epoch 2342/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 535.5017 - acc: 0.8813 - val_loss: 535.6534 - val_acc: 0.7196\n",
            "Epoch 2343/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 535.4560 - acc: 0.8844 - val_loss: 535.6011 - val_acc: 0.7196\n",
            "Epoch 2344/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 535.4022 - acc: 0.8722 - val_loss: 535.5487 - val_acc: 0.7169\n",
            "Epoch 2345/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 535.3473 - acc: 0.8763 - val_loss: 535.4964 - val_acc: 0.7169\n",
            "Epoch 2346/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 535.2933 - acc: 0.8813 - val_loss: 535.4442 - val_acc: 0.7169\n",
            "Epoch 2347/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 535.2468 - acc: 0.8783 - val_loss: 535.3919 - val_acc: 0.7169\n",
            "Epoch 2348/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 535.1906 - acc: 0.8732 - val_loss: 535.3396 - val_acc: 0.7196\n",
            "Epoch 2349/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 535.1338 - acc: 0.8895 - val_loss: 535.2874 - val_acc: 0.7196\n",
            "Epoch 2350/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 535.0812 - acc: 0.8702 - val_loss: 535.2352 - val_acc: 0.7169\n",
            "Epoch 2351/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 535.0342 - acc: 0.8824 - val_loss: 535.1829 - val_acc: 0.7169\n",
            "Epoch 2352/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 534.9776 - acc: 0.8864 - val_loss: 535.1307 - val_acc: 0.7169\n",
            "Epoch 2353/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 534.9279 - acc: 0.8813 - val_loss: 535.0784 - val_acc: 0.7196\n",
            "Epoch 2354/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 534.8680 - acc: 0.8935 - val_loss: 535.0261 - val_acc: 0.7169\n",
            "Epoch 2355/5000\n",
            "986/986 [==============================] - 0s 295us/step - loss: 534.8252 - acc: 0.8793 - val_loss: 534.9739 - val_acc: 0.7169\n",
            "Epoch 2356/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 534.7697 - acc: 0.8773 - val_loss: 534.9216 - val_acc: 0.7196\n",
            "Epoch 2357/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 534.7144 - acc: 0.8925 - val_loss: 534.8694 - val_acc: 0.7196\n",
            "Epoch 2358/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 534.6655 - acc: 0.8905 - val_loss: 534.8174 - val_acc: 0.7196\n",
            "Epoch 2359/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 534.6133 - acc: 0.8844 - val_loss: 534.7649 - val_acc: 0.7196\n",
            "Epoch 2360/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 534.5595 - acc: 0.8864 - val_loss: 534.7128 - val_acc: 0.7196\n",
            "Epoch 2361/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 534.5181 - acc: 0.8651 - val_loss: 534.6605 - val_acc: 0.7196\n",
            "Epoch 2362/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 534.4631 - acc: 0.8742 - val_loss: 534.6082 - val_acc: 0.7196\n",
            "Epoch 2363/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 534.4051 - acc: 0.8824 - val_loss: 534.5560 - val_acc: 0.7169\n",
            "Epoch 2364/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 534.3535 - acc: 0.8824 - val_loss: 534.5037 - val_acc: 0.7196\n",
            "Epoch 2365/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 534.3058 - acc: 0.8864 - val_loss: 534.4514 - val_acc: 0.7196\n",
            "Epoch 2366/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 534.2533 - acc: 0.8732 - val_loss: 534.3994 - val_acc: 0.7196\n",
            "Epoch 2367/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 534.1933 - acc: 0.8925 - val_loss: 534.3472 - val_acc: 0.7196\n",
            "Epoch 2368/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 534.1464 - acc: 0.8834 - val_loss: 534.2950 - val_acc: 0.7196\n",
            "Epoch 2369/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 534.0850 - acc: 0.8935 - val_loss: 534.2427 - val_acc: 0.7196\n",
            "Epoch 2370/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 534.0422 - acc: 0.8712 - val_loss: 534.1905 - val_acc: 0.7196\n",
            "Epoch 2371/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 533.9916 - acc: 0.8763 - val_loss: 534.1382 - val_acc: 0.7196\n",
            "Epoch 2372/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 533.9292 - acc: 0.8895 - val_loss: 534.0860 - val_acc: 0.7196\n",
            "Epoch 2373/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 533.8868 - acc: 0.8682 - val_loss: 534.0339 - val_acc: 0.7196\n",
            "Epoch 2374/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 533.8392 - acc: 0.8793 - val_loss: 533.9817 - val_acc: 0.7169\n",
            "Epoch 2375/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 533.7849 - acc: 0.8682 - val_loss: 533.9295 - val_acc: 0.7196\n",
            "Epoch 2376/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 533.7277 - acc: 0.8793 - val_loss: 533.8775 - val_acc: 0.7196\n",
            "Epoch 2377/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 533.6764 - acc: 0.8813 - val_loss: 533.8253 - val_acc: 0.7169\n",
            "Epoch 2378/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 533.6229 - acc: 0.8864 - val_loss: 533.7731 - val_acc: 0.7169\n",
            "Epoch 2379/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 533.5710 - acc: 0.8905 - val_loss: 533.7210 - val_acc: 0.7169\n",
            "Epoch 2380/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 533.5212 - acc: 0.8793 - val_loss: 533.6689 - val_acc: 0.7169\n",
            "Epoch 2381/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 533.4603 - acc: 0.8824 - val_loss: 533.6166 - val_acc: 0.7196\n",
            "Epoch 2382/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 533.4200 - acc: 0.8763 - val_loss: 533.5645 - val_acc: 0.7169\n",
            "Epoch 2383/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 533.3568 - acc: 0.8783 - val_loss: 533.5123 - val_acc: 0.7196\n",
            "Epoch 2384/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 533.3089 - acc: 0.8773 - val_loss: 533.4601 - val_acc: 0.7196\n",
            "Epoch 2385/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 533.2630 - acc: 0.8824 - val_loss: 533.4082 - val_acc: 0.7196\n",
            "Epoch 2386/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 533.2078 - acc: 0.8824 - val_loss: 533.3559 - val_acc: 0.7196\n",
            "Epoch 2387/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 533.1484 - acc: 0.9006 - val_loss: 533.3038 - val_acc: 0.7196\n",
            "Epoch 2388/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 533.0981 - acc: 0.8783 - val_loss: 533.2519 - val_acc: 0.7169\n",
            "Epoch 2389/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 533.0503 - acc: 0.8732 - val_loss: 533.1997 - val_acc: 0.7196\n",
            "Epoch 2390/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 532.9998 - acc: 0.8783 - val_loss: 533.1477 - val_acc: 0.7169\n",
            "Epoch 2391/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 532.9414 - acc: 0.8915 - val_loss: 533.0956 - val_acc: 0.7169\n",
            "Epoch 2392/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 532.8868 - acc: 0.8955 - val_loss: 533.0435 - val_acc: 0.7169\n",
            "Epoch 2393/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 532.8427 - acc: 0.8773 - val_loss: 532.9913 - val_acc: 0.7169\n",
            "Epoch 2394/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 532.7874 - acc: 0.8813 - val_loss: 532.9392 - val_acc: 0.7169\n",
            "Epoch 2395/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 532.7337 - acc: 0.8854 - val_loss: 532.8870 - val_acc: 0.7196\n",
            "Epoch 2396/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 532.6837 - acc: 0.8763 - val_loss: 532.8350 - val_acc: 0.7196\n",
            "Epoch 2397/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 532.6315 - acc: 0.8773 - val_loss: 532.7827 - val_acc: 0.7196\n",
            "Epoch 2398/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 532.5751 - acc: 0.8986 - val_loss: 532.7307 - val_acc: 0.7169\n",
            "Epoch 2399/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 532.5290 - acc: 0.8824 - val_loss: 532.6785 - val_acc: 0.7169\n",
            "Epoch 2400/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 532.4736 - acc: 0.8854 - val_loss: 532.6266 - val_acc: 0.7169\n",
            "Epoch 2401/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 532.4226 - acc: 0.8813 - val_loss: 532.5747 - val_acc: 0.7169\n",
            "Epoch 2402/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 532.3732 - acc: 0.8722 - val_loss: 532.5225 - val_acc: 0.7196\n",
            "Epoch 2403/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 532.3121 - acc: 0.8915 - val_loss: 532.4705 - val_acc: 0.7196\n",
            "Epoch 2404/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 532.2647 - acc: 0.8854 - val_loss: 532.4185 - val_acc: 0.7196\n",
            "Epoch 2405/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 532.2146 - acc: 0.8712 - val_loss: 532.3663 - val_acc: 0.7196\n",
            "Epoch 2406/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 532.1610 - acc: 0.8773 - val_loss: 532.3143 - val_acc: 0.7196\n",
            "Epoch 2407/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 532.1138 - acc: 0.8915 - val_loss: 532.2622 - val_acc: 0.7169\n",
            "Epoch 2408/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 532.0613 - acc: 0.8732 - val_loss: 532.2102 - val_acc: 0.7196\n",
            "Epoch 2409/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 532.0012 - acc: 0.8864 - val_loss: 532.1581 - val_acc: 0.7196\n",
            "Epoch 2410/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 531.9497 - acc: 0.8884 - val_loss: 532.1061 - val_acc: 0.7169\n",
            "Epoch 2411/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 531.9005 - acc: 0.8864 - val_loss: 532.0541 - val_acc: 0.7196\n",
            "Epoch 2412/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 531.8451 - acc: 0.8895 - val_loss: 532.0021 - val_acc: 0.7196\n",
            "Epoch 2413/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 531.7957 - acc: 0.8783 - val_loss: 531.9500 - val_acc: 0.7196\n",
            "Epoch 2414/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 531.7419 - acc: 0.8884 - val_loss: 531.8980 - val_acc: 0.7196\n",
            "Epoch 2415/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 531.6942 - acc: 0.8773 - val_loss: 531.8459 - val_acc: 0.7196\n",
            "Epoch 2416/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 531.6450 - acc: 0.8682 - val_loss: 531.7939 - val_acc: 0.7196\n",
            "Epoch 2417/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 531.5899 - acc: 0.8773 - val_loss: 531.7419 - val_acc: 0.7196\n",
            "Epoch 2418/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 531.5318 - acc: 0.8834 - val_loss: 531.6900 - val_acc: 0.7196\n",
            "Epoch 2419/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 531.4796 - acc: 0.8905 - val_loss: 531.6381 - val_acc: 0.7196\n",
            "Epoch 2420/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 531.4379 - acc: 0.8824 - val_loss: 531.5860 - val_acc: 0.7196\n",
            "Epoch 2421/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 531.3859 - acc: 0.8742 - val_loss: 531.5339 - val_acc: 0.7196\n",
            "Epoch 2422/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 531.3261 - acc: 0.8813 - val_loss: 531.4820 - val_acc: 0.7196\n",
            "Epoch 2423/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 531.2735 - acc: 0.8783 - val_loss: 531.4300 - val_acc: 0.7196\n",
            "Epoch 2424/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 531.2309 - acc: 0.8864 - val_loss: 531.3782 - val_acc: 0.7196\n",
            "Epoch 2425/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 531.1732 - acc: 0.8783 - val_loss: 531.3262 - val_acc: 0.7196\n",
            "Epoch 2426/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 531.1206 - acc: 0.8895 - val_loss: 531.2743 - val_acc: 0.7169\n",
            "Epoch 2427/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 531.0670 - acc: 0.8763 - val_loss: 531.2222 - val_acc: 0.7169\n",
            "Epoch 2428/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 531.0165 - acc: 0.8763 - val_loss: 531.1701 - val_acc: 0.7196\n",
            "Epoch 2429/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 530.9601 - acc: 0.8874 - val_loss: 531.1182 - val_acc: 0.7169\n",
            "Epoch 2430/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 530.9159 - acc: 0.8844 - val_loss: 531.0662 - val_acc: 0.7196\n",
            "Epoch 2431/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 530.8583 - acc: 0.8824 - val_loss: 531.0143 - val_acc: 0.7196\n",
            "Epoch 2432/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 530.8130 - acc: 0.8874 - val_loss: 530.9624 - val_acc: 0.7196\n",
            "Epoch 2433/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 530.7568 - acc: 0.8854 - val_loss: 530.9105 - val_acc: 0.7196\n",
            "Epoch 2434/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 530.7060 - acc: 0.8813 - val_loss: 530.8587 - val_acc: 0.7196\n",
            "Epoch 2435/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 530.6540 - acc: 0.8773 - val_loss: 530.8066 - val_acc: 0.7196\n",
            "Epoch 2436/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 530.6019 - acc: 0.8824 - val_loss: 530.7547 - val_acc: 0.7196\n",
            "Epoch 2437/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 530.5438 - acc: 0.8763 - val_loss: 530.7029 - val_acc: 0.7196\n",
            "Epoch 2438/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 530.4971 - acc: 0.8844 - val_loss: 530.6511 - val_acc: 0.7196\n",
            "Epoch 2439/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 530.4464 - acc: 0.8803 - val_loss: 530.5993 - val_acc: 0.7196\n",
            "Epoch 2440/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 530.3863 - acc: 0.8834 - val_loss: 530.5473 - val_acc: 0.7196\n",
            "Epoch 2441/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 530.3469 - acc: 0.8753 - val_loss: 530.4955 - val_acc: 0.7196\n",
            "Epoch 2442/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 530.2913 - acc: 0.8793 - val_loss: 530.4435 - val_acc: 0.7196\n",
            "Epoch 2443/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 530.2413 - acc: 0.8793 - val_loss: 530.3916 - val_acc: 0.7196\n",
            "Epoch 2444/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 530.1881 - acc: 0.8813 - val_loss: 530.3397 - val_acc: 0.7196\n",
            "Epoch 2445/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 530.1333 - acc: 0.8925 - val_loss: 530.2878 - val_acc: 0.7196\n",
            "Epoch 2446/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 530.0814 - acc: 0.8935 - val_loss: 530.2357 - val_acc: 0.7196\n",
            "Epoch 2447/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 530.0268 - acc: 0.8803 - val_loss: 530.1838 - val_acc: 0.7196\n",
            "Epoch 2448/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 529.9834 - acc: 0.8813 - val_loss: 530.1320 - val_acc: 0.7196\n",
            "Epoch 2449/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 529.9216 - acc: 0.8905 - val_loss: 530.0801 - val_acc: 0.7196\n",
            "Epoch 2450/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 529.8819 - acc: 0.8712 - val_loss: 530.0284 - val_acc: 0.7169\n",
            "Epoch 2451/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 529.8176 - acc: 0.8793 - val_loss: 529.9764 - val_acc: 0.7169\n",
            "Epoch 2452/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 529.7723 - acc: 0.8793 - val_loss: 529.9248 - val_acc: 0.7169\n",
            "Epoch 2453/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 529.7175 - acc: 0.8915 - val_loss: 529.8730 - val_acc: 0.7169\n",
            "Epoch 2454/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 529.6702 - acc: 0.8763 - val_loss: 529.8212 - val_acc: 0.7169\n",
            "Epoch 2455/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 529.6111 - acc: 0.8955 - val_loss: 529.7694 - val_acc: 0.7169\n",
            "Epoch 2456/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 529.5594 - acc: 0.8824 - val_loss: 529.7176 - val_acc: 0.7169\n",
            "Epoch 2457/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 529.5109 - acc: 0.8986 - val_loss: 529.6657 - val_acc: 0.7169\n",
            "Epoch 2458/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 529.4527 - acc: 0.8966 - val_loss: 529.6137 - val_acc: 0.7169\n",
            "Epoch 2459/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 529.4026 - acc: 0.8976 - val_loss: 529.5620 - val_acc: 0.7169\n",
            "Epoch 2460/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 529.3585 - acc: 0.8753 - val_loss: 529.5103 - val_acc: 0.7169\n",
            "Epoch 2461/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 529.3034 - acc: 0.8864 - val_loss: 529.4585 - val_acc: 0.7169\n",
            "Epoch 2462/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 529.2575 - acc: 0.8824 - val_loss: 529.4067 - val_acc: 0.7169\n",
            "Epoch 2463/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 529.1972 - acc: 0.8844 - val_loss: 529.3549 - val_acc: 0.7196\n",
            "Epoch 2464/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 529.1461 - acc: 0.8945 - val_loss: 529.3030 - val_acc: 0.7169\n",
            "Epoch 2465/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 529.0960 - acc: 0.8813 - val_loss: 529.2512 - val_acc: 0.7196\n",
            "Epoch 2466/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 529.0459 - acc: 0.8773 - val_loss: 529.1993 - val_acc: 0.7196\n",
            "Epoch 2467/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 528.9990 - acc: 0.8783 - val_loss: 529.1475 - val_acc: 0.7196\n",
            "Epoch 2468/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 528.9370 - acc: 0.8884 - val_loss: 529.0959 - val_acc: 0.7169\n",
            "Epoch 2469/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 528.8882 - acc: 0.8813 - val_loss: 529.0442 - val_acc: 0.7196\n",
            "Epoch 2470/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 528.8339 - acc: 0.8803 - val_loss: 528.9924 - val_acc: 0.7169\n",
            "Epoch 2471/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 528.7910 - acc: 0.8834 - val_loss: 528.9406 - val_acc: 0.7169\n",
            "Epoch 2472/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 528.7323 - acc: 0.8874 - val_loss: 528.8887 - val_acc: 0.7169\n",
            "Epoch 2473/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 528.6831 - acc: 0.8753 - val_loss: 528.8369 - val_acc: 0.7169\n",
            "Epoch 2474/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 528.6233 - acc: 0.8884 - val_loss: 528.7852 - val_acc: 0.7169\n",
            "Epoch 2475/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 528.5778 - acc: 0.8803 - val_loss: 528.7334 - val_acc: 0.7169\n",
            "Epoch 2476/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 528.5239 - acc: 0.8895 - val_loss: 528.6817 - val_acc: 0.7169\n",
            "Epoch 2477/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 528.4729 - acc: 0.8976 - val_loss: 528.6299 - val_acc: 0.7169\n",
            "Epoch 2478/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 528.4197 - acc: 0.8824 - val_loss: 528.5783 - val_acc: 0.7196\n",
            "Epoch 2479/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 528.3741 - acc: 0.8763 - val_loss: 528.5265 - val_acc: 0.7196\n",
            "Epoch 2480/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 528.3228 - acc: 0.8803 - val_loss: 528.4748 - val_acc: 0.7196\n",
            "Epoch 2481/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 528.2699 - acc: 0.8864 - val_loss: 528.4228 - val_acc: 0.7196\n",
            "Epoch 2482/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 528.2120 - acc: 0.8905 - val_loss: 528.3712 - val_acc: 0.7169\n",
            "Epoch 2483/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 528.1548 - acc: 0.8874 - val_loss: 528.3195 - val_acc: 0.7169\n",
            "Epoch 2484/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 528.1058 - acc: 0.8955 - val_loss: 528.2678 - val_acc: 0.7169\n",
            "Epoch 2485/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 528.0606 - acc: 0.8895 - val_loss: 528.2160 - val_acc: 0.7196\n",
            "Epoch 2486/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 528.0077 - acc: 0.8722 - val_loss: 528.1643 - val_acc: 0.7196\n",
            "Epoch 2487/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 527.9555 - acc: 0.8935 - val_loss: 528.1127 - val_acc: 0.7196\n",
            "Epoch 2488/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 527.9048 - acc: 0.8925 - val_loss: 528.0610 - val_acc: 0.7196\n",
            "Epoch 2489/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 527.8507 - acc: 0.8824 - val_loss: 528.0094 - val_acc: 0.7196\n",
            "Epoch 2490/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 527.8012 - acc: 0.8793 - val_loss: 527.9577 - val_acc: 0.7196\n",
            "Epoch 2491/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 527.7510 - acc: 0.8803 - val_loss: 527.9058 - val_acc: 0.7196\n",
            "Epoch 2492/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 527.7017 - acc: 0.8803 - val_loss: 527.8539 - val_acc: 0.7196\n",
            "Epoch 2493/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 527.6470 - acc: 0.8773 - val_loss: 527.8022 - val_acc: 0.7196\n",
            "Epoch 2494/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 527.5949 - acc: 0.8925 - val_loss: 527.7507 - val_acc: 0.7196\n",
            "Epoch 2495/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 527.5456 - acc: 0.8824 - val_loss: 527.6992 - val_acc: 0.7196\n",
            "Epoch 2496/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 527.4893 - acc: 0.8824 - val_loss: 527.6475 - val_acc: 0.7169\n",
            "Epoch 2497/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 527.4362 - acc: 0.8915 - val_loss: 527.5959 - val_acc: 0.7169\n",
            "Epoch 2498/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 527.3868 - acc: 0.8732 - val_loss: 527.5443 - val_acc: 0.7169\n",
            "Epoch 2499/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 527.3440 - acc: 0.8854 - val_loss: 527.4926 - val_acc: 0.7169\n",
            "Epoch 2500/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 527.2851 - acc: 0.8834 - val_loss: 527.4409 - val_acc: 0.7169\n",
            "Epoch 2501/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 527.2293 - acc: 0.8813 - val_loss: 527.3892 - val_acc: 0.7169\n",
            "Epoch 2502/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 527.1815 - acc: 0.8793 - val_loss: 527.3376 - val_acc: 0.7169\n",
            "Epoch 2503/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 527.1230 - acc: 0.8884 - val_loss: 527.2858 - val_acc: 0.7169\n",
            "Epoch 2504/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 527.0773 - acc: 0.8915 - val_loss: 527.2342 - val_acc: 0.7169\n",
            "Epoch 2505/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 527.0279 - acc: 0.8864 - val_loss: 527.1825 - val_acc: 0.7169\n",
            "Epoch 2506/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 526.9731 - acc: 0.8854 - val_loss: 527.1310 - val_acc: 0.7169\n",
            "Epoch 2507/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 526.9250 - acc: 0.8854 - val_loss: 527.0794 - val_acc: 0.7169\n",
            "Epoch 2508/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 526.8728 - acc: 0.8793 - val_loss: 527.0277 - val_acc: 0.7169\n",
            "Epoch 2509/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 526.8180 - acc: 0.8874 - val_loss: 526.9761 - val_acc: 0.7169\n",
            "Epoch 2510/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 526.7639 - acc: 0.8915 - val_loss: 526.9244 - val_acc: 0.7169\n",
            "Epoch 2511/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 526.7162 - acc: 0.8884 - val_loss: 526.8728 - val_acc: 0.7169\n",
            "Epoch 2512/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 526.6656 - acc: 0.8864 - val_loss: 526.8213 - val_acc: 0.7169\n",
            "Epoch 2513/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 526.6128 - acc: 0.8895 - val_loss: 526.7697 - val_acc: 0.7169\n",
            "Epoch 2514/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 526.5629 - acc: 0.8793 - val_loss: 526.7180 - val_acc: 0.7169\n",
            "Epoch 2515/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 526.5062 - acc: 0.8905 - val_loss: 526.6664 - val_acc: 0.7169\n",
            "Epoch 2516/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 526.4512 - acc: 0.8966 - val_loss: 526.6148 - val_acc: 0.7169\n",
            "Epoch 2517/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 526.4008 - acc: 0.8925 - val_loss: 526.5632 - val_acc: 0.7169\n",
            "Epoch 2518/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 526.3547 - acc: 0.8813 - val_loss: 526.5116 - val_acc: 0.7169\n",
            "Epoch 2519/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 526.2999 - acc: 0.8834 - val_loss: 526.4600 - val_acc: 0.7169\n",
            "Epoch 2520/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 526.2460 - acc: 0.8966 - val_loss: 526.4084 - val_acc: 0.7169\n",
            "Epoch 2521/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 526.1909 - acc: 0.8915 - val_loss: 526.3570 - val_acc: 0.7169\n",
            "Epoch 2522/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 526.1514 - acc: 0.8732 - val_loss: 526.3054 - val_acc: 0.7169\n",
            "Epoch 2523/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 526.0985 - acc: 0.8945 - val_loss: 526.2539 - val_acc: 0.7169\n",
            "Epoch 2524/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 526.0397 - acc: 0.8905 - val_loss: 526.2022 - val_acc: 0.7169\n",
            "Epoch 2525/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 525.9890 - acc: 0.8884 - val_loss: 526.1508 - val_acc: 0.7169\n",
            "Epoch 2526/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 525.9429 - acc: 0.8864 - val_loss: 526.0990 - val_acc: 0.7169\n",
            "Epoch 2527/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 525.8874 - acc: 0.8854 - val_loss: 526.0475 - val_acc: 0.7169\n",
            "Epoch 2528/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 525.8381 - acc: 0.8945 - val_loss: 525.9960 - val_acc: 0.7169\n",
            "Epoch 2529/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 525.7871 - acc: 0.8884 - val_loss: 525.9443 - val_acc: 0.7169\n",
            "Epoch 2530/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 525.7343 - acc: 0.8844 - val_loss: 525.8928 - val_acc: 0.7169\n",
            "Epoch 2531/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 525.6760 - acc: 0.8905 - val_loss: 525.8413 - val_acc: 0.7169\n",
            "Epoch 2532/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 525.6361 - acc: 0.8905 - val_loss: 525.7897 - val_acc: 0.7169\n",
            "Epoch 2533/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 525.5778 - acc: 0.8976 - val_loss: 525.7383 - val_acc: 0.7169\n",
            "Epoch 2534/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 525.5209 - acc: 0.8996 - val_loss: 525.6867 - val_acc: 0.7169\n",
            "Epoch 2535/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 525.4763 - acc: 0.8905 - val_loss: 525.6353 - val_acc: 0.7169\n",
            "Epoch 2536/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 525.4261 - acc: 0.8834 - val_loss: 525.5838 - val_acc: 0.7169\n",
            "Epoch 2537/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 525.3642 - acc: 0.8976 - val_loss: 525.5322 - val_acc: 0.7169\n",
            "Epoch 2538/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 525.3219 - acc: 0.8925 - val_loss: 525.4807 - val_acc: 0.7169\n",
            "Epoch 2539/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 525.2682 - acc: 0.8874 - val_loss: 525.4290 - val_acc: 0.7169\n",
            "Epoch 2540/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 525.2204 - acc: 0.8824 - val_loss: 525.3775 - val_acc: 0.7169\n",
            "Epoch 2541/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 525.1640 - acc: 0.8763 - val_loss: 525.3262 - val_acc: 0.7169\n",
            "Epoch 2542/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 525.1174 - acc: 0.8905 - val_loss: 525.2746 - val_acc: 0.7169\n",
            "Epoch 2543/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 525.0582 - acc: 0.8864 - val_loss: 525.2233 - val_acc: 0.7169\n",
            "Epoch 2544/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 525.0106 - acc: 0.8915 - val_loss: 525.1719 - val_acc: 0.7169\n",
            "Epoch 2545/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 524.9622 - acc: 0.8864 - val_loss: 525.1204 - val_acc: 0.7169\n",
            "Epoch 2546/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 524.9076 - acc: 0.8976 - val_loss: 525.0690 - val_acc: 0.7169\n",
            "Epoch 2547/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 524.8600 - acc: 0.8803 - val_loss: 525.0175 - val_acc: 0.7169\n",
            "Epoch 2548/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 524.8109 - acc: 0.8803 - val_loss: 524.9660 - val_acc: 0.7169\n",
            "Epoch 2549/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 524.7541 - acc: 0.8864 - val_loss: 524.9144 - val_acc: 0.7169\n",
            "Epoch 2550/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 524.6973 - acc: 0.8925 - val_loss: 524.8629 - val_acc: 0.7169\n",
            "Epoch 2551/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 524.6544 - acc: 0.8844 - val_loss: 524.8114 - val_acc: 0.7169\n",
            "Epoch 2552/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 524.5990 - acc: 0.8824 - val_loss: 524.7598 - val_acc: 0.7169\n",
            "Epoch 2553/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 524.5442 - acc: 0.8884 - val_loss: 524.7085 - val_acc: 0.7169\n",
            "Epoch 2554/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 524.4942 - acc: 0.8935 - val_loss: 524.6571 - val_acc: 0.7169\n",
            "Epoch 2555/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 524.4475 - acc: 0.8854 - val_loss: 524.6057 - val_acc: 0.7169\n",
            "Epoch 2556/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 524.4019 - acc: 0.8813 - val_loss: 524.5541 - val_acc: 0.7169\n",
            "Epoch 2557/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 524.3500 - acc: 0.8824 - val_loss: 524.5028 - val_acc: 0.7169\n",
            "Epoch 2558/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 524.2849 - acc: 0.8986 - val_loss: 524.4513 - val_acc: 0.7169\n",
            "Epoch 2559/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 524.2398 - acc: 0.8925 - val_loss: 524.4000 - val_acc: 0.7169\n",
            "Epoch 2560/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 524.1904 - acc: 0.8884 - val_loss: 524.3485 - val_acc: 0.7169\n",
            "Epoch 2561/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 524.1415 - acc: 0.8742 - val_loss: 524.2972 - val_acc: 0.7169\n",
            "Epoch 2562/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 524.0861 - acc: 0.8925 - val_loss: 524.2460 - val_acc: 0.7169\n",
            "Epoch 2563/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 524.0352 - acc: 0.8966 - val_loss: 524.1946 - val_acc: 0.7169\n",
            "Epoch 2564/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 523.9850 - acc: 0.8763 - val_loss: 524.1431 - val_acc: 0.7169\n",
            "Epoch 2565/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 523.9364 - acc: 0.8915 - val_loss: 524.0918 - val_acc: 0.7169\n",
            "Epoch 2566/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 523.8795 - acc: 0.8905 - val_loss: 524.0405 - val_acc: 0.7169\n",
            "Epoch 2567/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 523.8283 - acc: 0.8895 - val_loss: 523.9891 - val_acc: 0.7169\n",
            "Epoch 2568/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 523.7833 - acc: 0.8813 - val_loss: 523.9377 - val_acc: 0.7169\n",
            "Epoch 2569/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 523.7262 - acc: 0.8986 - val_loss: 523.8864 - val_acc: 0.7169\n",
            "Epoch 2570/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 523.6768 - acc: 0.8864 - val_loss: 523.8350 - val_acc: 0.7169\n",
            "Epoch 2571/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 523.6265 - acc: 0.8834 - val_loss: 523.7840 - val_acc: 0.7169\n",
            "Epoch 2572/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 523.5700 - acc: 0.8824 - val_loss: 523.7327 - val_acc: 0.7169\n",
            "Epoch 2573/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 523.5268 - acc: 0.8844 - val_loss: 523.6813 - val_acc: 0.7169\n",
            "Epoch 2574/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 523.4631 - acc: 0.9006 - val_loss: 523.6300 - val_acc: 0.7169\n",
            "Epoch 2575/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 523.4175 - acc: 0.8945 - val_loss: 523.5787 - val_acc: 0.7169\n",
            "Epoch 2576/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 523.3717 - acc: 0.8864 - val_loss: 523.5274 - val_acc: 0.7169\n",
            "Epoch 2577/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 523.3218 - acc: 0.8702 - val_loss: 523.4761 - val_acc: 0.7169\n",
            "Epoch 2578/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 523.2670 - acc: 0.8854 - val_loss: 523.4249 - val_acc: 0.7169\n",
            "Epoch 2579/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 523.2156 - acc: 0.8803 - val_loss: 523.3736 - val_acc: 0.7169\n",
            "Epoch 2580/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 523.1620 - acc: 0.8864 - val_loss: 523.3223 - val_acc: 0.7169\n",
            "Epoch 2581/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 523.1093 - acc: 0.8813 - val_loss: 523.2710 - val_acc: 0.7169\n",
            "Epoch 2582/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 523.0605 - acc: 0.8773 - val_loss: 523.2197 - val_acc: 0.7169\n",
            "Epoch 2583/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 523.0138 - acc: 0.8742 - val_loss: 523.1686 - val_acc: 0.7169\n",
            "Epoch 2584/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 522.9556 - acc: 0.8915 - val_loss: 523.1174 - val_acc: 0.7169\n",
            "Epoch 2585/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 522.9024 - acc: 0.8966 - val_loss: 523.0661 - val_acc: 0.7169\n",
            "Epoch 2586/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 522.8491 - acc: 0.8925 - val_loss: 523.0150 - val_acc: 0.7169\n",
            "Epoch 2587/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 522.8005 - acc: 0.8945 - val_loss: 522.9638 - val_acc: 0.7169\n",
            "Epoch 2588/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 522.7544 - acc: 0.8834 - val_loss: 522.9126 - val_acc: 0.7169\n",
            "Epoch 2589/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 522.7007 - acc: 0.8935 - val_loss: 522.8614 - val_acc: 0.7169\n",
            "Epoch 2590/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 522.6501 - acc: 0.8874 - val_loss: 522.8101 - val_acc: 0.7169\n",
            "Epoch 2591/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 522.5949 - acc: 0.8966 - val_loss: 522.7590 - val_acc: 0.7169\n",
            "Epoch 2592/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 522.5353 - acc: 0.8996 - val_loss: 522.7079 - val_acc: 0.7169\n",
            "Epoch 2593/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 522.5006 - acc: 0.8813 - val_loss: 522.6566 - val_acc: 0.7169\n",
            "Epoch 2594/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 522.4393 - acc: 0.8935 - val_loss: 522.6054 - val_acc: 0.7169\n",
            "Epoch 2595/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 522.3929 - acc: 0.8925 - val_loss: 522.5542 - val_acc: 0.7169\n",
            "Epoch 2596/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 522.3416 - acc: 0.8773 - val_loss: 522.5030 - val_acc: 0.7169\n",
            "Epoch 2597/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 522.2898 - acc: 0.8874 - val_loss: 522.4520 - val_acc: 0.7169\n",
            "Epoch 2598/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 522.2477 - acc: 0.8844 - val_loss: 522.4008 - val_acc: 0.7169\n",
            "Epoch 2599/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 522.1869 - acc: 0.8783 - val_loss: 522.3497 - val_acc: 0.7169\n",
            "Epoch 2600/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 522.1381 - acc: 0.8895 - val_loss: 522.2984 - val_acc: 0.7169\n",
            "Epoch 2601/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 522.0877 - acc: 0.8773 - val_loss: 522.2471 - val_acc: 0.7169\n",
            "Epoch 2602/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 522.0363 - acc: 0.8864 - val_loss: 522.1959 - val_acc: 0.7169\n",
            "Epoch 2603/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 521.9847 - acc: 0.8793 - val_loss: 522.1449 - val_acc: 0.7169\n",
            "Epoch 2604/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 521.9346 - acc: 0.8915 - val_loss: 522.0936 - val_acc: 0.7169\n",
            "Epoch 2605/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 521.8725 - acc: 0.9047 - val_loss: 522.0426 - val_acc: 0.7169\n",
            "Epoch 2606/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 521.8294 - acc: 0.8884 - val_loss: 521.9914 - val_acc: 0.7169\n",
            "Epoch 2607/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 521.7757 - acc: 0.8955 - val_loss: 521.9404 - val_acc: 0.7169\n",
            "Epoch 2608/5000\n",
            "986/986 [==============================] - 0s 261us/step - loss: 521.7267 - acc: 0.8935 - val_loss: 521.8894 - val_acc: 0.7169\n",
            "Epoch 2609/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 521.6729 - acc: 0.8884 - val_loss: 521.8381 - val_acc: 0.7169\n",
            "Epoch 2610/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 521.6198 - acc: 0.8884 - val_loss: 521.7871 - val_acc: 0.7169\n",
            "Epoch 2611/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 521.5755 - acc: 0.8864 - val_loss: 521.7360 - val_acc: 0.7169\n",
            "Epoch 2612/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 521.5289 - acc: 0.8753 - val_loss: 521.6849 - val_acc: 0.7169\n",
            "Epoch 2613/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 521.4659 - acc: 0.8966 - val_loss: 521.6337 - val_acc: 0.7169\n",
            "Epoch 2614/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 521.4258 - acc: 0.8895 - val_loss: 521.5826 - val_acc: 0.7169\n",
            "Epoch 2615/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 521.3678 - acc: 0.8945 - val_loss: 521.5315 - val_acc: 0.7169\n",
            "Epoch 2616/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 521.3215 - acc: 0.8854 - val_loss: 521.4804 - val_acc: 0.7169\n",
            "Epoch 2617/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 521.2656 - acc: 0.8813 - val_loss: 521.4293 - val_acc: 0.7169\n",
            "Epoch 2618/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 521.2197 - acc: 0.8803 - val_loss: 521.3782 - val_acc: 0.7169\n",
            "Epoch 2619/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 521.1667 - acc: 0.8884 - val_loss: 521.3270 - val_acc: 0.7169\n",
            "Epoch 2620/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 521.1175 - acc: 0.8722 - val_loss: 521.2761 - val_acc: 0.7169\n",
            "Epoch 2621/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 521.0674 - acc: 0.8844 - val_loss: 521.2249 - val_acc: 0.7169\n",
            "Epoch 2622/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 521.0089 - acc: 0.8895 - val_loss: 521.1740 - val_acc: 0.7169\n",
            "Epoch 2623/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 520.9584 - acc: 0.8905 - val_loss: 521.1231 - val_acc: 0.7169\n",
            "Epoch 2624/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 520.9137 - acc: 0.8895 - val_loss: 521.0720 - val_acc: 0.7169\n",
            "Epoch 2625/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 520.8594 - acc: 0.8945 - val_loss: 521.0210 - val_acc: 0.7169\n",
            "Epoch 2626/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 520.8059 - acc: 0.8824 - val_loss: 520.9699 - val_acc: 0.7169\n",
            "Epoch 2627/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 520.7596 - acc: 0.8874 - val_loss: 520.9187 - val_acc: 0.7169\n",
            "Epoch 2628/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 520.7106 - acc: 0.8895 - val_loss: 520.8677 - val_acc: 0.7169\n",
            "Epoch 2629/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 520.6588 - acc: 0.8783 - val_loss: 520.8167 - val_acc: 0.7169\n",
            "Epoch 2630/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 520.6022 - acc: 0.8925 - val_loss: 520.7655 - val_acc: 0.7169\n",
            "Epoch 2631/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 520.5511 - acc: 0.8935 - val_loss: 520.7148 - val_acc: 0.7169\n",
            "Epoch 2632/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 520.4996 - acc: 0.8763 - val_loss: 520.6636 - val_acc: 0.7169\n",
            "Epoch 2633/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 520.4526 - acc: 0.8803 - val_loss: 520.6127 - val_acc: 0.7169\n",
            "Epoch 2634/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 520.3986 - acc: 0.8905 - val_loss: 520.5618 - val_acc: 0.7169\n",
            "Epoch 2635/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 520.3493 - acc: 0.8915 - val_loss: 520.5108 - val_acc: 0.7169\n",
            "Epoch 2636/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 520.2964 - acc: 0.8895 - val_loss: 520.4599 - val_acc: 0.7169\n",
            "Epoch 2637/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 520.2427 - acc: 0.8874 - val_loss: 520.4088 - val_acc: 0.7169\n",
            "Epoch 2638/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 520.1914 - acc: 0.8935 - val_loss: 520.3577 - val_acc: 0.7169\n",
            "Epoch 2639/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 520.1460 - acc: 0.8834 - val_loss: 520.3067 - val_acc: 0.7169\n",
            "Epoch 2640/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 520.0966 - acc: 0.8874 - val_loss: 520.2557 - val_acc: 0.7169\n",
            "Epoch 2641/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 520.0401 - acc: 0.8905 - val_loss: 520.2046 - val_acc: 0.7169\n",
            "Epoch 2642/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 519.9883 - acc: 0.8925 - val_loss: 520.1536 - val_acc: 0.7169\n",
            "Epoch 2643/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 519.9338 - acc: 0.9047 - val_loss: 520.1027 - val_acc: 0.7169\n",
            "Epoch 2644/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 519.8837 - acc: 0.9006 - val_loss: 520.0518 - val_acc: 0.7169\n",
            "Epoch 2645/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 519.8316 - acc: 0.8925 - val_loss: 520.0008 - val_acc: 0.7169\n",
            "Epoch 2646/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 519.7929 - acc: 0.8753 - val_loss: 519.9499 - val_acc: 0.7196\n",
            "Epoch 2647/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 519.7357 - acc: 0.8966 - val_loss: 519.8989 - val_acc: 0.7196\n",
            "Epoch 2648/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 519.6858 - acc: 0.8813 - val_loss: 519.8480 - val_acc: 0.7196\n",
            "Epoch 2649/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 519.6303 - acc: 0.8915 - val_loss: 519.7971 - val_acc: 0.7196\n",
            "Epoch 2650/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 519.5786 - acc: 0.8854 - val_loss: 519.7462 - val_acc: 0.7196\n",
            "Epoch 2651/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 519.5265 - acc: 0.8874 - val_loss: 519.6953 - val_acc: 0.7196\n",
            "Epoch 2652/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 519.4772 - acc: 0.8895 - val_loss: 519.6443 - val_acc: 0.7196\n",
            "Epoch 2653/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 519.4353 - acc: 0.8834 - val_loss: 519.5934 - val_acc: 0.7196\n",
            "Epoch 2654/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 519.3786 - acc: 0.8884 - val_loss: 519.5424 - val_acc: 0.7196\n",
            "Epoch 2655/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 519.3271 - acc: 0.8824 - val_loss: 519.4916 - val_acc: 0.7196\n",
            "Epoch 2656/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 519.2716 - acc: 0.8976 - val_loss: 519.4406 - val_acc: 0.7196\n",
            "Epoch 2657/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 519.2213 - acc: 0.8874 - val_loss: 519.3896 - val_acc: 0.7196\n",
            "Epoch 2658/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 519.1789 - acc: 0.8854 - val_loss: 519.3387 - val_acc: 0.7196\n",
            "Epoch 2659/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 519.1217 - acc: 0.8864 - val_loss: 519.2876 - val_acc: 0.7169\n",
            "Epoch 2660/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 519.0745 - acc: 0.8864 - val_loss: 519.2369 - val_acc: 0.7196\n",
            "Epoch 2661/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 519.0192 - acc: 0.8895 - val_loss: 519.1858 - val_acc: 0.7196\n",
            "Epoch 2662/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 518.9730 - acc: 0.8864 - val_loss: 519.1348 - val_acc: 0.7196\n",
            "Epoch 2663/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 518.9195 - acc: 0.8844 - val_loss: 519.0839 - val_acc: 0.7196\n",
            "Epoch 2664/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 518.8725 - acc: 0.8976 - val_loss: 519.0330 - val_acc: 0.7196\n",
            "Epoch 2665/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 518.8164 - acc: 0.8874 - val_loss: 518.9821 - val_acc: 0.7196\n",
            "Epoch 2666/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 518.7645 - acc: 0.8884 - val_loss: 518.9313 - val_acc: 0.7196\n",
            "Epoch 2667/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 518.7121 - acc: 0.9016 - val_loss: 518.8804 - val_acc: 0.7196\n",
            "Epoch 2668/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 518.6671 - acc: 0.8925 - val_loss: 518.8295 - val_acc: 0.7196\n",
            "Epoch 2669/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 518.6120 - acc: 0.8925 - val_loss: 518.7786 - val_acc: 0.7196\n",
            "Epoch 2670/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 518.5643 - acc: 0.8945 - val_loss: 518.7279 - val_acc: 0.7196\n",
            "Epoch 2671/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 518.5177 - acc: 0.8803 - val_loss: 518.6770 - val_acc: 0.7196\n",
            "Epoch 2672/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 518.4599 - acc: 0.8966 - val_loss: 518.6261 - val_acc: 0.7196\n",
            "Epoch 2673/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 518.4064 - acc: 0.8884 - val_loss: 518.5752 - val_acc: 0.7196\n",
            "Epoch 2674/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 518.3674 - acc: 0.8753 - val_loss: 518.5244 - val_acc: 0.7196\n",
            "Epoch 2675/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 518.3115 - acc: 0.8976 - val_loss: 518.4735 - val_acc: 0.7196\n",
            "Epoch 2676/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 518.2599 - acc: 0.8935 - val_loss: 518.4228 - val_acc: 0.7196\n",
            "Epoch 2677/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 518.2018 - acc: 0.8925 - val_loss: 518.3719 - val_acc: 0.7196\n",
            "Epoch 2678/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 518.1604 - acc: 0.8834 - val_loss: 518.3211 - val_acc: 0.7196\n",
            "Epoch 2679/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 518.1068 - acc: 0.8813 - val_loss: 518.2701 - val_acc: 0.7196\n",
            "Epoch 2680/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 518.0558 - acc: 0.8996 - val_loss: 518.2194 - val_acc: 0.7196\n",
            "Epoch 2681/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 518.0035 - acc: 0.8884 - val_loss: 518.1686 - val_acc: 0.7196\n",
            "Epoch 2682/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 517.9530 - acc: 0.8834 - val_loss: 518.1178 - val_acc: 0.7196\n",
            "Epoch 2683/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 517.8987 - acc: 0.8976 - val_loss: 518.0670 - val_acc: 0.7196\n",
            "Epoch 2684/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 517.8499 - acc: 0.8955 - val_loss: 518.0161 - val_acc: 0.7196\n",
            "Epoch 2685/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 517.7999 - acc: 0.8864 - val_loss: 517.9652 - val_acc: 0.7196\n",
            "Epoch 2686/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 517.7574 - acc: 0.8844 - val_loss: 517.9143 - val_acc: 0.7196\n",
            "Epoch 2687/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 517.7009 - acc: 0.8874 - val_loss: 517.8636 - val_acc: 0.7196\n",
            "Epoch 2688/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 517.6479 - acc: 0.8915 - val_loss: 517.8128 - val_acc: 0.7196\n",
            "Epoch 2689/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 517.5956 - acc: 0.8844 - val_loss: 517.7620 - val_acc: 0.7196\n",
            "Epoch 2690/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 517.5473 - acc: 0.8813 - val_loss: 517.7112 - val_acc: 0.7196\n",
            "Epoch 2691/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 517.4969 - acc: 0.8915 - val_loss: 517.6605 - val_acc: 0.7196\n",
            "Epoch 2692/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 517.4451 - acc: 0.8813 - val_loss: 517.6096 - val_acc: 0.7196\n",
            "Epoch 2693/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 517.3951 - acc: 0.8824 - val_loss: 517.5588 - val_acc: 0.7196\n",
            "Epoch 2694/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 517.3438 - acc: 0.8854 - val_loss: 517.5079 - val_acc: 0.7196\n",
            "Epoch 2695/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 517.2955 - acc: 0.8955 - val_loss: 517.4572 - val_acc: 0.7196\n",
            "Epoch 2696/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 517.2431 - acc: 0.8864 - val_loss: 517.4063 - val_acc: 0.7222\n",
            "Epoch 2697/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 517.1921 - acc: 0.8874 - val_loss: 517.3556 - val_acc: 0.7222\n",
            "Epoch 2698/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 517.1390 - acc: 0.8986 - val_loss: 517.3049 - val_acc: 0.7222\n",
            "Epoch 2699/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 517.0910 - acc: 0.8844 - val_loss: 517.2541 - val_acc: 0.7222\n",
            "Epoch 2700/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 517.0373 - acc: 0.8976 - val_loss: 517.2034 - val_acc: 0.7222\n",
            "Epoch 2701/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 516.9811 - acc: 0.8905 - val_loss: 517.1527 - val_acc: 0.7222\n",
            "Epoch 2702/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 516.9370 - acc: 0.8935 - val_loss: 517.1021 - val_acc: 0.7196\n",
            "Epoch 2703/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 516.8841 - acc: 0.8864 - val_loss: 517.0514 - val_acc: 0.7196\n",
            "Epoch 2704/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 516.8340 - acc: 0.8895 - val_loss: 517.0006 - val_acc: 0.7196\n",
            "Epoch 2705/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 516.7845 - acc: 0.8834 - val_loss: 516.9497 - val_acc: 0.7196\n",
            "Epoch 2706/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 516.7360 - acc: 0.8813 - val_loss: 516.8990 - val_acc: 0.7196\n",
            "Epoch 2707/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 516.6865 - acc: 0.8925 - val_loss: 516.8482 - val_acc: 0.7196\n",
            "Epoch 2708/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 516.6377 - acc: 0.8854 - val_loss: 516.7974 - val_acc: 0.7196\n",
            "Epoch 2709/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 516.5807 - acc: 0.8834 - val_loss: 516.7467 - val_acc: 0.7196\n",
            "Epoch 2710/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 516.5212 - acc: 0.8986 - val_loss: 516.6960 - val_acc: 0.7196\n",
            "Epoch 2711/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 516.4795 - acc: 0.8966 - val_loss: 516.6454 - val_acc: 0.7196\n",
            "Epoch 2712/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 516.4245 - acc: 0.8824 - val_loss: 516.5948 - val_acc: 0.7196\n",
            "Epoch 2713/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 516.3735 - acc: 0.8834 - val_loss: 516.5440 - val_acc: 0.7222\n",
            "Epoch 2714/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 516.3303 - acc: 0.8945 - val_loss: 516.4933 - val_acc: 0.7196\n",
            "Epoch 2715/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 516.2746 - acc: 0.8864 - val_loss: 516.4427 - val_acc: 0.7196\n",
            "Epoch 2716/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 516.2264 - acc: 0.8925 - val_loss: 516.3920 - val_acc: 0.7196\n",
            "Epoch 2717/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 516.1653 - acc: 0.8966 - val_loss: 516.3411 - val_acc: 0.7196\n",
            "Epoch 2718/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 516.1286 - acc: 0.8905 - val_loss: 516.2904 - val_acc: 0.7196\n",
            "Epoch 2719/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 516.0650 - acc: 0.8955 - val_loss: 516.2398 - val_acc: 0.7196\n",
            "Epoch 2720/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 516.0252 - acc: 0.8915 - val_loss: 516.1891 - val_acc: 0.7196\n",
            "Epoch 2721/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 515.9668 - acc: 0.8996 - val_loss: 516.1383 - val_acc: 0.7196\n",
            "Epoch 2722/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 515.9168 - acc: 0.8966 - val_loss: 516.0877 - val_acc: 0.7196\n",
            "Epoch 2723/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 515.8631 - acc: 0.8976 - val_loss: 516.0371 - val_acc: 0.7222\n",
            "Epoch 2724/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 515.8207 - acc: 0.8834 - val_loss: 515.9864 - val_acc: 0.7196\n",
            "Epoch 2725/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 515.7669 - acc: 0.8966 - val_loss: 515.9356 - val_acc: 0.7196\n",
            "Epoch 2726/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 515.7173 - acc: 0.8793 - val_loss: 515.8851 - val_acc: 0.7169\n",
            "Epoch 2727/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 515.6653 - acc: 0.8945 - val_loss: 515.8343 - val_acc: 0.7169\n",
            "Epoch 2728/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 515.6133 - acc: 0.8986 - val_loss: 515.7839 - val_acc: 0.7196\n",
            "Epoch 2729/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 515.5634 - acc: 0.8996 - val_loss: 515.7333 - val_acc: 0.7196\n",
            "Epoch 2730/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 515.5095 - acc: 0.8925 - val_loss: 515.6826 - val_acc: 0.7196\n",
            "Epoch 2731/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 515.4557 - acc: 0.8986 - val_loss: 515.6322 - val_acc: 0.7196\n",
            "Epoch 2732/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 515.4157 - acc: 0.8884 - val_loss: 515.5816 - val_acc: 0.7196\n",
            "Epoch 2733/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 515.3610 - acc: 0.9026 - val_loss: 515.5309 - val_acc: 0.7196\n",
            "Epoch 2734/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 515.3190 - acc: 0.8905 - val_loss: 515.4803 - val_acc: 0.7196\n",
            "Epoch 2735/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 515.2621 - acc: 0.8915 - val_loss: 515.4297 - val_acc: 0.7196\n",
            "Epoch 2736/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 515.2138 - acc: 0.8834 - val_loss: 515.3790 - val_acc: 0.7196\n",
            "Epoch 2737/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 515.1647 - acc: 0.8864 - val_loss: 515.3284 - val_acc: 0.7196\n",
            "Epoch 2738/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 515.1083 - acc: 0.8925 - val_loss: 515.2779 - val_acc: 0.7196\n",
            "Epoch 2739/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 515.0623 - acc: 0.8864 - val_loss: 515.2274 - val_acc: 0.7196\n",
            "Epoch 2740/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 515.0083 - acc: 0.8874 - val_loss: 515.1768 - val_acc: 0.7196\n",
            "Epoch 2741/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 514.9640 - acc: 0.8854 - val_loss: 515.1264 - val_acc: 0.7196\n",
            "Epoch 2742/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 514.9062 - acc: 0.8905 - val_loss: 515.0758 - val_acc: 0.7196\n",
            "Epoch 2743/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 514.8580 - acc: 0.8945 - val_loss: 515.0253 - val_acc: 0.7196\n",
            "Epoch 2744/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 514.8027 - acc: 0.9047 - val_loss: 514.9747 - val_acc: 0.7196\n",
            "Epoch 2745/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 514.7613 - acc: 0.8834 - val_loss: 514.9240 - val_acc: 0.7196\n",
            "Epoch 2746/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 514.7046 - acc: 0.8905 - val_loss: 514.8737 - val_acc: 0.7196\n",
            "Epoch 2747/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 514.6513 - acc: 0.9077 - val_loss: 514.8230 - val_acc: 0.7196\n",
            "Epoch 2748/5000\n",
            "986/986 [==============================] - 0s 259us/step - loss: 514.6015 - acc: 0.8925 - val_loss: 514.7725 - val_acc: 0.7196\n",
            "Epoch 2749/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 514.5537 - acc: 0.8915 - val_loss: 514.7219 - val_acc: 0.7196\n",
            "Epoch 2750/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 514.5021 - acc: 0.8864 - val_loss: 514.6714 - val_acc: 0.7196\n",
            "Epoch 2751/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 514.4589 - acc: 0.8874 - val_loss: 514.6208 - val_acc: 0.7196\n",
            "Epoch 2752/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 514.3962 - acc: 0.8996 - val_loss: 514.5703 - val_acc: 0.7196\n",
            "Epoch 2753/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 514.3497 - acc: 0.9037 - val_loss: 514.5197 - val_acc: 0.7169\n",
            "Epoch 2754/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 514.2998 - acc: 0.8955 - val_loss: 514.4692 - val_acc: 0.7169\n",
            "Epoch 2755/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 514.2477 - acc: 0.8884 - val_loss: 514.4187 - val_acc: 0.7169\n",
            "Epoch 2756/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 514.2026 - acc: 0.8874 - val_loss: 514.3682 - val_acc: 0.7169\n",
            "Epoch 2757/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 514.1448 - acc: 0.9108 - val_loss: 514.3178 - val_acc: 0.7169\n",
            "Epoch 2758/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 514.0946 - acc: 0.8905 - val_loss: 514.2672 - val_acc: 0.7196\n",
            "Epoch 2759/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 514.0464 - acc: 0.8945 - val_loss: 514.2168 - val_acc: 0.7196\n",
            "Epoch 2760/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 513.9987 - acc: 0.8996 - val_loss: 514.1661 - val_acc: 0.7196\n",
            "Epoch 2761/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 513.9472 - acc: 0.8955 - val_loss: 514.1157 - val_acc: 0.7196\n",
            "Epoch 2762/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 513.8917 - acc: 0.8874 - val_loss: 514.0653 - val_acc: 0.7196\n",
            "Epoch 2763/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 513.8453 - acc: 0.8854 - val_loss: 514.0148 - val_acc: 0.7196\n",
            "Epoch 2764/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 513.7906 - acc: 0.8945 - val_loss: 513.9645 - val_acc: 0.7169\n",
            "Epoch 2765/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 513.7470 - acc: 0.8925 - val_loss: 513.9141 - val_acc: 0.7196\n",
            "Epoch 2766/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 513.6954 - acc: 0.8864 - val_loss: 513.8637 - val_acc: 0.7196\n",
            "Epoch 2767/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 513.6436 - acc: 0.8925 - val_loss: 513.8131 - val_acc: 0.7222\n",
            "Epoch 2768/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 513.5901 - acc: 0.8925 - val_loss: 513.7627 - val_acc: 0.7196\n",
            "Epoch 2769/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 513.5337 - acc: 0.8976 - val_loss: 513.7122 - val_acc: 0.7196\n",
            "Epoch 2770/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 513.4938 - acc: 0.8874 - val_loss: 513.6618 - val_acc: 0.7196\n",
            "Epoch 2771/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 513.4412 - acc: 0.9006 - val_loss: 513.6113 - val_acc: 0.7222\n",
            "Epoch 2772/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 513.3951 - acc: 0.8834 - val_loss: 513.5607 - val_acc: 0.7222\n",
            "Epoch 2773/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 513.3428 - acc: 0.8996 - val_loss: 513.5104 - val_acc: 0.7196\n",
            "Epoch 2774/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 513.2921 - acc: 0.8803 - val_loss: 513.4600 - val_acc: 0.7222\n",
            "Epoch 2775/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 513.2401 - acc: 0.8976 - val_loss: 513.4095 - val_acc: 0.7196\n",
            "Epoch 2776/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 513.1907 - acc: 0.8915 - val_loss: 513.3590 - val_acc: 0.7222\n",
            "Epoch 2777/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 513.1444 - acc: 0.8935 - val_loss: 513.3085 - val_acc: 0.7222\n",
            "Epoch 2778/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 513.0909 - acc: 0.8803 - val_loss: 513.2581 - val_acc: 0.7222\n",
            "Epoch 2779/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 513.0410 - acc: 0.8844 - val_loss: 513.2077 - val_acc: 0.7222\n",
            "Epoch 2780/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 512.9860 - acc: 0.8895 - val_loss: 513.1573 - val_acc: 0.7222\n",
            "Epoch 2781/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 512.9401 - acc: 0.9016 - val_loss: 513.1070 - val_acc: 0.7222\n",
            "Epoch 2782/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 512.8872 - acc: 0.8813 - val_loss: 513.0565 - val_acc: 0.7222\n",
            "Epoch 2783/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 512.8339 - acc: 0.8935 - val_loss: 513.0061 - val_acc: 0.7249\n",
            "Epoch 2784/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 512.7862 - acc: 0.8884 - val_loss: 512.9559 - val_acc: 0.7222\n",
            "Epoch 2785/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 512.7353 - acc: 0.8844 - val_loss: 512.9053 - val_acc: 0.7249\n",
            "Epoch 2786/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 512.6794 - acc: 0.8976 - val_loss: 512.8549 - val_acc: 0.7249\n",
            "Epoch 2787/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 512.6289 - acc: 0.8976 - val_loss: 512.8046 - val_acc: 0.7249\n",
            "Epoch 2788/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 512.5876 - acc: 0.8854 - val_loss: 512.7542 - val_acc: 0.7249\n",
            "Epoch 2789/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 512.5310 - acc: 0.8996 - val_loss: 512.7038 - val_acc: 0.7249\n",
            "Epoch 2790/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 512.4830 - acc: 0.8955 - val_loss: 512.6534 - val_acc: 0.7249\n",
            "Epoch 2791/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 512.4267 - acc: 0.8895 - val_loss: 512.6031 - val_acc: 0.7249\n",
            "Epoch 2792/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 512.3817 - acc: 0.8905 - val_loss: 512.5527 - val_acc: 0.7249\n",
            "Epoch 2793/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 512.3287 - acc: 0.8905 - val_loss: 512.5022 - val_acc: 0.7249\n",
            "Epoch 2794/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 512.2760 - acc: 0.8966 - val_loss: 512.4520 - val_acc: 0.7249\n",
            "Epoch 2795/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 512.2350 - acc: 0.8763 - val_loss: 512.4016 - val_acc: 0.7249\n",
            "Epoch 2796/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 512.1819 - acc: 0.8884 - val_loss: 512.3514 - val_acc: 0.7249\n",
            "Epoch 2797/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 512.1303 - acc: 0.8945 - val_loss: 512.3010 - val_acc: 0.7249\n",
            "Epoch 2798/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 512.0728 - acc: 0.9057 - val_loss: 512.2505 - val_acc: 0.7249\n",
            "Epoch 2799/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 512.0259 - acc: 0.8986 - val_loss: 512.2002 - val_acc: 0.7249\n",
            "Epoch 2800/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 511.9771 - acc: 0.8844 - val_loss: 512.1499 - val_acc: 0.7249\n",
            "Epoch 2801/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 511.9302 - acc: 0.8976 - val_loss: 512.0996 - val_acc: 0.7249\n",
            "Epoch 2802/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 511.8829 - acc: 0.8905 - val_loss: 512.0493 - val_acc: 0.7249\n",
            "Epoch 2803/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 511.8317 - acc: 0.8874 - val_loss: 511.9991 - val_acc: 0.7222\n",
            "Epoch 2804/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 511.7790 - acc: 0.8925 - val_loss: 511.9486 - val_acc: 0.7222\n",
            "Epoch 2805/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 511.7268 - acc: 0.8915 - val_loss: 511.8984 - val_acc: 0.7249\n",
            "Epoch 2806/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 511.6816 - acc: 0.8935 - val_loss: 511.8480 - val_acc: 0.7249\n",
            "Epoch 2807/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 511.6233 - acc: 0.8966 - val_loss: 511.7977 - val_acc: 0.7249\n",
            "Epoch 2808/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 511.5815 - acc: 0.8824 - val_loss: 511.7474 - val_acc: 0.7249\n",
            "Epoch 2809/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 511.5270 - acc: 0.8945 - val_loss: 511.6971 - val_acc: 0.7249\n",
            "Epoch 2810/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 511.4689 - acc: 0.9037 - val_loss: 511.6468 - val_acc: 0.7249\n",
            "Epoch 2811/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 511.4190 - acc: 0.8986 - val_loss: 511.5966 - val_acc: 0.7249\n",
            "Epoch 2812/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 511.3765 - acc: 0.8895 - val_loss: 511.5463 - val_acc: 0.7249\n",
            "Epoch 2813/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 511.3239 - acc: 0.8945 - val_loss: 511.4961 - val_acc: 0.7249\n",
            "Epoch 2814/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 511.2735 - acc: 0.8925 - val_loss: 511.4457 - val_acc: 0.7249\n",
            "Epoch 2815/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 511.2237 - acc: 0.8966 - val_loss: 511.3955 - val_acc: 0.7249\n",
            "Epoch 2816/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 511.1630 - acc: 0.8976 - val_loss: 511.3452 - val_acc: 0.7249\n",
            "Epoch 2817/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 511.1235 - acc: 0.8895 - val_loss: 511.2949 - val_acc: 0.7249\n",
            "Epoch 2818/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 511.0796 - acc: 0.8895 - val_loss: 511.2446 - val_acc: 0.7249\n",
            "Epoch 2819/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 511.0222 - acc: 0.8884 - val_loss: 511.1943 - val_acc: 0.7249\n",
            "Epoch 2820/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 510.9767 - acc: 0.8925 - val_loss: 511.1443 - val_acc: 0.7249\n",
            "Epoch 2821/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 510.9205 - acc: 0.9047 - val_loss: 511.0940 - val_acc: 0.7249\n",
            "Epoch 2822/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 510.8745 - acc: 0.8895 - val_loss: 511.0438 - val_acc: 0.7249\n",
            "Epoch 2823/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 510.8212 - acc: 0.8874 - val_loss: 510.9935 - val_acc: 0.7249\n",
            "Epoch 2824/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 510.7658 - acc: 0.8925 - val_loss: 510.9431 - val_acc: 0.7249\n",
            "Epoch 2825/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 510.7237 - acc: 0.8925 - val_loss: 510.8931 - val_acc: 0.7249\n",
            "Epoch 2826/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 510.6721 - acc: 0.8905 - val_loss: 510.8429 - val_acc: 0.7249\n",
            "Epoch 2827/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 510.6171 - acc: 0.9047 - val_loss: 510.7928 - val_acc: 0.7249\n",
            "Epoch 2828/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 510.5705 - acc: 0.8884 - val_loss: 510.7425 - val_acc: 0.7249\n",
            "Epoch 2829/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 510.5252 - acc: 0.8884 - val_loss: 510.6922 - val_acc: 0.7249\n",
            "Epoch 2830/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 510.4690 - acc: 0.8996 - val_loss: 510.6421 - val_acc: 0.7249\n",
            "Epoch 2831/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 510.4182 - acc: 0.8854 - val_loss: 510.5918 - val_acc: 0.7249\n",
            "Epoch 2832/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 510.3703 - acc: 0.8915 - val_loss: 510.5417 - val_acc: 0.7249\n",
            "Epoch 2833/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 510.3194 - acc: 0.8976 - val_loss: 510.4914 - val_acc: 0.7249\n",
            "Epoch 2834/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 510.2721 - acc: 0.8884 - val_loss: 510.4411 - val_acc: 0.7249\n",
            "Epoch 2835/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 510.2180 - acc: 0.8895 - val_loss: 510.3910 - val_acc: 0.7249\n",
            "Epoch 2836/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 510.1623 - acc: 0.8996 - val_loss: 510.3410 - val_acc: 0.7249\n",
            "Epoch 2837/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 510.1186 - acc: 0.8925 - val_loss: 510.2907 - val_acc: 0.7249\n",
            "Epoch 2838/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 510.0674 - acc: 0.8955 - val_loss: 510.2407 - val_acc: 0.7249\n",
            "Epoch 2839/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 510.0184 - acc: 0.8915 - val_loss: 510.1903 - val_acc: 0.7249\n",
            "Epoch 2840/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 509.9706 - acc: 0.8854 - val_loss: 510.1402 - val_acc: 0.7249\n",
            "Epoch 2841/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 509.9115 - acc: 0.9047 - val_loss: 510.0901 - val_acc: 0.7249\n",
            "Epoch 2842/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 509.8690 - acc: 0.8864 - val_loss: 510.0399 - val_acc: 0.7249\n",
            "Epoch 2843/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 509.8141 - acc: 0.8986 - val_loss: 509.9898 - val_acc: 0.7249\n",
            "Epoch 2844/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 509.7602 - acc: 0.8996 - val_loss: 509.9396 - val_acc: 0.7249\n",
            "Epoch 2845/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 509.7177 - acc: 0.8966 - val_loss: 509.8894 - val_acc: 0.7249\n",
            "Epoch 2846/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 509.6718 - acc: 0.8905 - val_loss: 509.8393 - val_acc: 0.7249\n",
            "Epoch 2847/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 509.6135 - acc: 0.8986 - val_loss: 509.7892 - val_acc: 0.7249\n",
            "Epoch 2848/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 509.5691 - acc: 0.8915 - val_loss: 509.7391 - val_acc: 0.7249\n",
            "Epoch 2849/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 509.5183 - acc: 0.8905 - val_loss: 509.6890 - val_acc: 0.7249\n",
            "Epoch 2850/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 509.4571 - acc: 0.9067 - val_loss: 509.6389 - val_acc: 0.7249\n",
            "Epoch 2851/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 509.4152 - acc: 0.8945 - val_loss: 509.5888 - val_acc: 0.7249\n",
            "Epoch 2852/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 509.3595 - acc: 0.8915 - val_loss: 509.5386 - val_acc: 0.7249\n",
            "Epoch 2853/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 509.3081 - acc: 0.9037 - val_loss: 509.4884 - val_acc: 0.7249\n",
            "Epoch 2854/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 509.2641 - acc: 0.9047 - val_loss: 509.4382 - val_acc: 0.7275\n",
            "Epoch 2855/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 509.2128 - acc: 0.8966 - val_loss: 509.3881 - val_acc: 0.7275\n",
            "Epoch 2856/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 509.1632 - acc: 0.9077 - val_loss: 509.3381 - val_acc: 0.7249\n",
            "Epoch 2857/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 509.1213 - acc: 0.8884 - val_loss: 509.2881 - val_acc: 0.7249\n",
            "Epoch 2858/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 509.0691 - acc: 0.8813 - val_loss: 509.2380 - val_acc: 0.7249\n",
            "Epoch 2859/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 509.0179 - acc: 0.8976 - val_loss: 509.1879 - val_acc: 0.7249\n",
            "Epoch 2860/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 508.9744 - acc: 0.8783 - val_loss: 509.1378 - val_acc: 0.7249\n",
            "Epoch 2861/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 508.9181 - acc: 0.8966 - val_loss: 509.0877 - val_acc: 0.7249\n",
            "Epoch 2862/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 508.8606 - acc: 0.8945 - val_loss: 509.0377 - val_acc: 0.7249\n",
            "Epoch 2863/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 508.8232 - acc: 0.8864 - val_loss: 508.9877 - val_acc: 0.7249\n",
            "Epoch 2864/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 508.7613 - acc: 0.8925 - val_loss: 508.9376 - val_acc: 0.7275\n",
            "Epoch 2865/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 508.7138 - acc: 0.8966 - val_loss: 508.8876 - val_acc: 0.7249\n",
            "Epoch 2866/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 508.6627 - acc: 0.8895 - val_loss: 508.8374 - val_acc: 0.7249\n",
            "Epoch 2867/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 508.6125 - acc: 0.8884 - val_loss: 508.7873 - val_acc: 0.7249\n",
            "Epoch 2868/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 508.5548 - acc: 0.9057 - val_loss: 508.7372 - val_acc: 0.7275\n",
            "Epoch 2869/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 508.5184 - acc: 0.8935 - val_loss: 508.6871 - val_acc: 0.7275\n",
            "Epoch 2870/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 508.4626 - acc: 0.8966 - val_loss: 508.6371 - val_acc: 0.7275\n",
            "Epoch 2871/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 508.4120 - acc: 0.9026 - val_loss: 508.5871 - val_acc: 0.7275\n",
            "Epoch 2872/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 508.3615 - acc: 0.8935 - val_loss: 508.5369 - val_acc: 0.7275\n",
            "Epoch 2873/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 508.3130 - acc: 0.8976 - val_loss: 508.4870 - val_acc: 0.7275\n",
            "Epoch 2874/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 508.2665 - acc: 0.8824 - val_loss: 508.4370 - val_acc: 0.7275\n",
            "Epoch 2875/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 508.2058 - acc: 0.9047 - val_loss: 508.3870 - val_acc: 0.7275\n",
            "Epoch 2876/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 508.1652 - acc: 0.8905 - val_loss: 508.3371 - val_acc: 0.7275\n",
            "Epoch 2877/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 508.1132 - acc: 0.8915 - val_loss: 508.2869 - val_acc: 0.7275\n",
            "Epoch 2878/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 508.0618 - acc: 0.8884 - val_loss: 508.2369 - val_acc: 0.7275\n",
            "Epoch 2879/5000\n",
            "986/986 [==============================] - 0s 311us/step - loss: 508.0130 - acc: 0.8895 - val_loss: 508.1869 - val_acc: 0.7275\n",
            "Epoch 2880/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 507.9595 - acc: 0.8925 - val_loss: 508.1369 - val_acc: 0.7275\n",
            "Epoch 2881/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 507.9129 - acc: 0.8925 - val_loss: 508.0870 - val_acc: 0.7275\n",
            "Epoch 2882/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 507.8576 - acc: 0.9016 - val_loss: 508.0371 - val_acc: 0.7275\n",
            "Epoch 2883/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 507.8111 - acc: 0.8955 - val_loss: 507.9870 - val_acc: 0.7275\n",
            "Epoch 2884/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 507.7602 - acc: 0.9006 - val_loss: 507.9371 - val_acc: 0.7275\n",
            "Epoch 2885/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 507.7158 - acc: 0.8884 - val_loss: 507.8869 - val_acc: 0.7275\n",
            "Epoch 2886/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 507.6663 - acc: 0.8905 - val_loss: 507.8370 - val_acc: 0.7275\n",
            "Epoch 2887/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 507.6191 - acc: 0.8935 - val_loss: 507.7871 - val_acc: 0.7275\n",
            "Epoch 2888/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 507.5644 - acc: 0.8935 - val_loss: 507.7372 - val_acc: 0.7275\n",
            "Epoch 2889/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 507.5137 - acc: 0.8976 - val_loss: 507.6873 - val_acc: 0.7275\n",
            "Epoch 2890/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 507.4608 - acc: 0.8986 - val_loss: 507.6373 - val_acc: 0.7275\n",
            "Epoch 2891/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 507.4183 - acc: 0.8915 - val_loss: 507.5873 - val_acc: 0.7275\n",
            "Epoch 2892/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 507.3705 - acc: 0.8864 - val_loss: 507.5377 - val_acc: 0.7275\n",
            "Epoch 2893/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 507.3147 - acc: 0.9006 - val_loss: 507.4878 - val_acc: 0.7275\n",
            "Epoch 2894/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 507.2658 - acc: 0.8854 - val_loss: 507.4377 - val_acc: 0.7275\n",
            "Epoch 2895/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 507.2150 - acc: 0.8895 - val_loss: 507.3878 - val_acc: 0.7275\n",
            "Epoch 2896/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 507.1614 - acc: 0.8935 - val_loss: 507.3378 - val_acc: 0.7275\n",
            "Epoch 2897/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 507.1063 - acc: 0.9016 - val_loss: 507.2880 - val_acc: 0.7275\n",
            "Epoch 2898/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 507.0618 - acc: 0.9097 - val_loss: 507.2382 - val_acc: 0.7275\n",
            "Epoch 2899/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 507.0141 - acc: 0.8854 - val_loss: 507.1884 - val_acc: 0.7275\n",
            "Epoch 2900/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 506.9646 - acc: 0.8925 - val_loss: 507.1385 - val_acc: 0.7275\n",
            "Epoch 2901/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 506.9218 - acc: 0.8844 - val_loss: 507.0887 - val_acc: 0.7275\n",
            "Epoch 2902/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 506.8657 - acc: 0.8905 - val_loss: 507.0388 - val_acc: 0.7275\n",
            "Epoch 2903/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 506.8175 - acc: 0.8915 - val_loss: 506.9891 - val_acc: 0.7275\n",
            "Epoch 2904/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 506.7653 - acc: 0.8976 - val_loss: 506.9393 - val_acc: 0.7275\n",
            "Epoch 2905/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 506.7152 - acc: 0.8935 - val_loss: 506.8894 - val_acc: 0.7275\n",
            "Epoch 2906/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 506.6649 - acc: 0.8925 - val_loss: 506.8395 - val_acc: 0.7275\n",
            "Epoch 2907/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 506.6152 - acc: 0.8905 - val_loss: 506.7896 - val_acc: 0.7275\n",
            "Epoch 2908/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 506.5627 - acc: 0.8966 - val_loss: 506.7399 - val_acc: 0.7275\n",
            "Epoch 2909/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 506.5140 - acc: 0.8976 - val_loss: 506.6901 - val_acc: 0.7275\n",
            "Epoch 2910/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 506.4644 - acc: 0.9067 - val_loss: 506.6402 - val_acc: 0.7275\n",
            "Epoch 2911/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 506.4107 - acc: 0.8996 - val_loss: 506.5905 - val_acc: 0.7275\n",
            "Epoch 2912/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 506.3662 - acc: 0.8925 - val_loss: 506.5406 - val_acc: 0.7275\n",
            "Epoch 2913/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 506.3158 - acc: 0.8895 - val_loss: 506.4910 - val_acc: 0.7275\n",
            "Epoch 2914/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 506.2659 - acc: 0.8813 - val_loss: 506.4411 - val_acc: 0.7275\n",
            "Epoch 2915/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 506.2134 - acc: 0.8915 - val_loss: 506.3912 - val_acc: 0.7275\n",
            "Epoch 2916/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 506.1608 - acc: 0.8976 - val_loss: 506.3415 - val_acc: 0.7275\n",
            "Epoch 2917/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 506.1169 - acc: 0.8925 - val_loss: 506.2916 - val_acc: 0.7275\n",
            "Epoch 2918/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 506.0649 - acc: 0.8986 - val_loss: 506.2417 - val_acc: 0.7275\n",
            "Epoch 2919/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 506.0156 - acc: 0.8996 - val_loss: 506.1919 - val_acc: 0.7275\n",
            "Epoch 2920/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 505.9650 - acc: 0.8884 - val_loss: 506.1421 - val_acc: 0.7275\n",
            "Epoch 2921/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 505.9239 - acc: 0.8925 - val_loss: 506.0925 - val_acc: 0.7275\n",
            "Epoch 2922/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 505.8571 - acc: 0.9067 - val_loss: 506.0427 - val_acc: 0.7275\n",
            "Epoch 2923/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 505.8230 - acc: 0.8996 - val_loss: 505.9929 - val_acc: 0.7275\n",
            "Epoch 2924/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 505.7601 - acc: 0.9057 - val_loss: 505.9430 - val_acc: 0.7275\n",
            "Epoch 2925/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 505.7168 - acc: 0.8854 - val_loss: 505.8933 - val_acc: 0.7275\n",
            "Epoch 2926/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 505.6634 - acc: 0.9047 - val_loss: 505.8434 - val_acc: 0.7275\n",
            "Epoch 2927/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 505.6186 - acc: 0.8955 - val_loss: 505.7938 - val_acc: 0.7302\n",
            "Epoch 2928/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 505.5632 - acc: 0.8966 - val_loss: 505.7442 - val_acc: 0.7275\n",
            "Epoch 2929/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 505.5189 - acc: 0.8854 - val_loss: 505.6944 - val_acc: 0.7275\n",
            "Epoch 2930/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 505.4669 - acc: 0.9047 - val_loss: 505.6447 - val_acc: 0.7275\n",
            "Epoch 2931/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 505.4193 - acc: 0.8925 - val_loss: 505.5949 - val_acc: 0.7275\n",
            "Epoch 2932/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 505.3683 - acc: 0.8895 - val_loss: 505.5451 - val_acc: 0.7275\n",
            "Epoch 2933/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 505.3249 - acc: 0.8874 - val_loss: 505.4954 - val_acc: 0.7275\n",
            "Epoch 2934/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 505.2706 - acc: 0.8915 - val_loss: 505.4457 - val_acc: 0.7275\n",
            "Epoch 2935/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 505.2178 - acc: 0.8915 - val_loss: 505.3960 - val_acc: 0.7275\n",
            "Epoch 2936/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 505.1646 - acc: 0.9006 - val_loss: 505.3462 - val_acc: 0.7275\n",
            "Epoch 2937/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 505.1204 - acc: 0.8895 - val_loss: 505.2966 - val_acc: 0.7275\n",
            "Epoch 2938/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 505.0702 - acc: 0.8955 - val_loss: 505.2470 - val_acc: 0.7275\n",
            "Epoch 2939/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 505.0201 - acc: 0.8976 - val_loss: 505.1972 - val_acc: 0.7275\n",
            "Epoch 2940/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 504.9676 - acc: 0.8925 - val_loss: 505.1475 - val_acc: 0.7275\n",
            "Epoch 2941/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 504.9240 - acc: 0.9006 - val_loss: 505.0976 - val_acc: 0.7275\n",
            "Epoch 2942/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 504.8730 - acc: 0.8955 - val_loss: 505.0479 - val_acc: 0.7275\n",
            "Epoch 2943/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 504.8255 - acc: 0.8976 - val_loss: 504.9984 - val_acc: 0.7275\n",
            "Epoch 2944/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 504.7707 - acc: 0.9006 - val_loss: 504.9487 - val_acc: 0.7302\n",
            "Epoch 2945/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 504.7129 - acc: 0.9087 - val_loss: 504.8990 - val_acc: 0.7275\n",
            "Epoch 2946/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 504.6754 - acc: 0.8996 - val_loss: 504.8495 - val_acc: 0.7275\n",
            "Epoch 2947/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 504.6244 - acc: 0.8915 - val_loss: 504.7998 - val_acc: 0.7275\n",
            "Epoch 2948/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 504.5664 - acc: 0.8955 - val_loss: 504.7501 - val_acc: 0.7275\n",
            "Epoch 2949/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 504.5209 - acc: 0.8925 - val_loss: 504.7004 - val_acc: 0.7275\n",
            "Epoch 2950/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 504.4704 - acc: 0.8966 - val_loss: 504.6508 - val_acc: 0.7275\n",
            "Epoch 2951/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 504.4243 - acc: 0.8935 - val_loss: 504.6012 - val_acc: 0.7275\n",
            "Epoch 2952/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 504.3660 - acc: 0.8945 - val_loss: 504.5516 - val_acc: 0.7302\n",
            "Epoch 2953/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 504.3203 - acc: 0.8945 - val_loss: 504.5019 - val_acc: 0.7302\n",
            "Epoch 2954/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 504.2779 - acc: 0.8854 - val_loss: 504.4523 - val_acc: 0.7275\n",
            "Epoch 2955/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 504.2264 - acc: 0.8895 - val_loss: 504.4026 - val_acc: 0.7275\n",
            "Epoch 2956/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 504.1746 - acc: 0.9047 - val_loss: 504.3529 - val_acc: 0.7302\n",
            "Epoch 2957/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 504.1234 - acc: 0.8895 - val_loss: 504.3034 - val_acc: 0.7275\n",
            "Epoch 2958/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 504.0683 - acc: 0.9108 - val_loss: 504.2536 - val_acc: 0.7302\n",
            "Epoch 2959/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 504.0242 - acc: 0.8935 - val_loss: 504.2040 - val_acc: 0.7302\n",
            "Epoch 2960/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 503.9721 - acc: 0.8986 - val_loss: 504.1543 - val_acc: 0.7275\n",
            "Epoch 2961/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 503.9286 - acc: 0.8986 - val_loss: 504.1046 - val_acc: 0.7302\n",
            "Epoch 2962/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 503.8749 - acc: 0.8884 - val_loss: 504.0550 - val_acc: 0.7302\n",
            "Epoch 2963/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 503.8258 - acc: 0.8996 - val_loss: 504.0055 - val_acc: 0.7302\n",
            "Epoch 2964/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 503.7805 - acc: 0.8874 - val_loss: 503.9559 - val_acc: 0.7275\n",
            "Epoch 2965/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 503.7273 - acc: 0.8996 - val_loss: 503.9064 - val_acc: 0.7302\n",
            "Epoch 2966/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 503.6763 - acc: 0.9067 - val_loss: 503.8568 - val_acc: 0.7302\n",
            "Epoch 2967/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 503.6386 - acc: 0.8884 - val_loss: 503.8072 - val_acc: 0.7275\n",
            "Epoch 2968/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 503.5804 - acc: 0.9026 - val_loss: 503.7577 - val_acc: 0.7302\n",
            "Epoch 2969/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 503.5314 - acc: 0.9016 - val_loss: 503.7081 - val_acc: 0.7302\n",
            "Epoch 2970/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 503.4778 - acc: 0.8925 - val_loss: 503.6585 - val_acc: 0.7302\n",
            "Epoch 2971/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 503.4337 - acc: 0.8945 - val_loss: 503.6091 - val_acc: 0.7302\n",
            "Epoch 2972/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 503.3767 - acc: 0.9047 - val_loss: 503.5596 - val_acc: 0.7302\n",
            "Epoch 2973/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 503.3305 - acc: 0.8986 - val_loss: 503.5100 - val_acc: 0.7302\n",
            "Epoch 2974/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 503.2816 - acc: 0.8986 - val_loss: 503.4603 - val_acc: 0.7302\n",
            "Epoch 2975/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 503.2355 - acc: 0.9057 - val_loss: 503.4108 - val_acc: 0.7302\n",
            "Epoch 2976/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 503.1809 - acc: 0.8996 - val_loss: 503.3614 - val_acc: 0.7302\n",
            "Epoch 2977/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 503.1288 - acc: 0.8966 - val_loss: 503.3119 - val_acc: 0.7275\n",
            "Epoch 2978/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 503.0850 - acc: 0.8925 - val_loss: 503.2622 - val_acc: 0.7275\n",
            "Epoch 2979/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 503.0334 - acc: 0.8925 - val_loss: 503.2128 - val_acc: 0.7275\n",
            "Epoch 2980/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 502.9870 - acc: 0.8915 - val_loss: 503.1632 - val_acc: 0.7275\n",
            "Epoch 2981/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 502.9360 - acc: 0.8945 - val_loss: 503.1137 - val_acc: 0.7275\n",
            "Epoch 2982/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 502.8858 - acc: 0.9006 - val_loss: 503.0641 - val_acc: 0.7275\n",
            "Epoch 2983/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 502.8435 - acc: 0.8884 - val_loss: 503.0146 - val_acc: 0.7275\n",
            "Epoch 2984/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 502.7839 - acc: 0.8976 - val_loss: 502.9651 - val_acc: 0.7275\n",
            "Epoch 2985/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 502.7296 - acc: 0.9026 - val_loss: 502.9157 - val_acc: 0.7302\n",
            "Epoch 2986/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 502.6816 - acc: 0.8996 - val_loss: 502.8661 - val_acc: 0.7302\n",
            "Epoch 2987/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 502.6374 - acc: 0.8935 - val_loss: 502.8167 - val_acc: 0.7302\n",
            "Epoch 2988/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 502.5890 - acc: 0.9016 - val_loss: 502.7672 - val_acc: 0.7328\n",
            "Epoch 2989/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 502.5391 - acc: 0.9067 - val_loss: 502.7177 - val_acc: 0.7302\n",
            "Epoch 2990/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 502.4892 - acc: 0.8874 - val_loss: 502.6682 - val_acc: 0.7275\n",
            "Epoch 2991/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 502.4366 - acc: 0.8966 - val_loss: 502.6186 - val_acc: 0.7275\n",
            "Epoch 2992/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 502.3873 - acc: 0.8945 - val_loss: 502.5693 - val_acc: 0.7275\n",
            "Epoch 2993/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 502.3349 - acc: 0.9037 - val_loss: 502.5198 - val_acc: 0.7302\n",
            "Epoch 2994/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 502.2919 - acc: 0.8996 - val_loss: 502.4702 - val_acc: 0.7302\n",
            "Epoch 2995/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 502.2367 - acc: 0.8945 - val_loss: 502.4208 - val_acc: 0.7328\n",
            "Epoch 2996/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 502.1893 - acc: 0.8884 - val_loss: 502.3714 - val_acc: 0.7302\n",
            "Epoch 2997/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 502.1480 - acc: 0.8803 - val_loss: 502.3218 - val_acc: 0.7302\n",
            "Epoch 2998/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 502.0935 - acc: 0.8854 - val_loss: 502.2723 - val_acc: 0.7328\n",
            "Epoch 2999/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 502.0397 - acc: 0.8935 - val_loss: 502.2227 - val_acc: 0.7328\n",
            "Epoch 3000/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 501.9889 - acc: 0.9026 - val_loss: 502.1732 - val_acc: 0.7328\n",
            "Epoch 3001/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 501.9450 - acc: 0.8986 - val_loss: 502.1238 - val_acc: 0.7328\n",
            "Epoch 3002/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 501.8942 - acc: 0.8996 - val_loss: 502.0744 - val_acc: 0.7328\n",
            "Epoch 3003/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 501.8481 - acc: 0.8884 - val_loss: 502.0250 - val_acc: 0.7328\n",
            "Epoch 3004/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 501.7918 - acc: 0.9016 - val_loss: 501.9754 - val_acc: 0.7328\n",
            "Epoch 3005/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 501.7414 - acc: 0.9016 - val_loss: 501.9261 - val_acc: 0.7328\n",
            "Epoch 3006/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 501.6943 - acc: 0.9057 - val_loss: 501.8767 - val_acc: 0.7328\n",
            "Epoch 3007/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 501.6549 - acc: 0.8905 - val_loss: 501.8274 - val_acc: 0.7328\n",
            "Epoch 3008/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 501.5935 - acc: 0.8955 - val_loss: 501.7780 - val_acc: 0.7328\n",
            "Epoch 3009/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 501.5491 - acc: 0.8925 - val_loss: 501.7285 - val_acc: 0.7328\n",
            "Epoch 3010/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 501.5024 - acc: 0.8966 - val_loss: 501.6790 - val_acc: 0.7328\n",
            "Epoch 3011/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 501.4536 - acc: 0.9006 - val_loss: 501.6296 - val_acc: 0.7328\n",
            "Epoch 3012/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 501.4052 - acc: 0.8955 - val_loss: 501.5802 - val_acc: 0.7328\n",
            "Epoch 3013/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 501.3501 - acc: 0.9006 - val_loss: 501.5309 - val_acc: 0.7328\n",
            "Epoch 3014/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 501.2999 - acc: 0.8966 - val_loss: 501.4815 - val_acc: 0.7328\n",
            "Epoch 3015/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 501.2504 - acc: 0.8986 - val_loss: 501.4322 - val_acc: 0.7328\n",
            "Epoch 3016/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 501.2010 - acc: 0.9067 - val_loss: 501.3829 - val_acc: 0.7328\n",
            "Epoch 3017/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 501.1493 - acc: 0.8955 - val_loss: 501.3334 - val_acc: 0.7328\n",
            "Epoch 3018/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 501.1040 - acc: 0.9016 - val_loss: 501.2840 - val_acc: 0.7328\n",
            "Epoch 3019/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 501.0532 - acc: 0.8864 - val_loss: 501.2347 - val_acc: 0.7328\n",
            "Epoch 3020/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 501.0109 - acc: 0.8773 - val_loss: 501.1852 - val_acc: 0.7328\n",
            "Epoch 3021/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 500.9588 - acc: 0.8996 - val_loss: 501.1359 - val_acc: 0.7328\n",
            "Epoch 3022/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 500.9060 - acc: 0.8834 - val_loss: 501.0865 - val_acc: 0.7328\n",
            "Epoch 3023/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 500.8539 - acc: 0.9097 - val_loss: 501.0372 - val_acc: 0.7328\n",
            "Epoch 3024/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 500.8039 - acc: 0.8996 - val_loss: 500.9878 - val_acc: 0.7328\n",
            "Epoch 3025/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 500.7596 - acc: 0.9108 - val_loss: 500.9385 - val_acc: 0.7328\n",
            "Epoch 3026/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 500.7121 - acc: 0.8844 - val_loss: 500.8889 - val_acc: 0.7328\n",
            "Epoch 3027/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 500.6558 - acc: 0.8986 - val_loss: 500.8396 - val_acc: 0.7328\n",
            "Epoch 3028/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 500.6058 - acc: 0.9037 - val_loss: 500.7903 - val_acc: 0.7328\n",
            "Epoch 3029/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 500.5606 - acc: 0.8945 - val_loss: 500.7411 - val_acc: 0.7328\n",
            "Epoch 3030/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 500.5123 - acc: 0.8905 - val_loss: 500.6918 - val_acc: 0.7328\n",
            "Epoch 3031/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 500.4548 - acc: 0.8955 - val_loss: 500.6424 - val_acc: 0.7328\n",
            "Epoch 3032/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 500.4126 - acc: 0.9006 - val_loss: 500.5931 - val_acc: 0.7328\n",
            "Epoch 3033/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 500.3643 - acc: 0.8915 - val_loss: 500.5437 - val_acc: 0.7328\n",
            "Epoch 3034/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 500.3081 - acc: 0.9006 - val_loss: 500.4943 - val_acc: 0.7328\n",
            "Epoch 3035/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 500.2612 - acc: 0.8955 - val_loss: 500.4450 - val_acc: 0.7328\n",
            "Epoch 3036/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 500.2141 - acc: 0.9037 - val_loss: 500.3957 - val_acc: 0.7328\n",
            "Epoch 3037/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 500.1605 - acc: 0.8986 - val_loss: 500.3464 - val_acc: 0.7328\n",
            "Epoch 3038/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 500.1158 - acc: 0.9006 - val_loss: 500.2971 - val_acc: 0.7328\n",
            "Epoch 3039/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 500.0601 - acc: 0.8996 - val_loss: 500.2477 - val_acc: 0.7328\n",
            "Epoch 3040/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 500.0217 - acc: 0.8915 - val_loss: 500.1986 - val_acc: 0.7328\n",
            "Epoch 3041/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 499.9659 - acc: 0.8996 - val_loss: 500.1492 - val_acc: 0.7328\n",
            "Epoch 3042/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 499.9205 - acc: 0.8925 - val_loss: 500.0999 - val_acc: 0.7328\n",
            "Epoch 3043/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 499.8707 - acc: 0.8884 - val_loss: 500.0506 - val_acc: 0.7328\n",
            "Epoch 3044/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 499.8196 - acc: 0.8915 - val_loss: 500.0014 - val_acc: 0.7328\n",
            "Epoch 3045/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 499.7748 - acc: 0.8966 - val_loss: 499.9520 - val_acc: 0.7328\n",
            "Epoch 3046/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 499.7156 - acc: 0.9037 - val_loss: 499.9027 - val_acc: 0.7328\n",
            "Epoch 3047/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 499.6688 - acc: 0.9026 - val_loss: 499.8535 - val_acc: 0.7328\n",
            "Epoch 3048/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 499.6166 - acc: 0.9037 - val_loss: 499.8042 - val_acc: 0.7328\n",
            "Epoch 3049/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 499.5737 - acc: 0.8895 - val_loss: 499.7548 - val_acc: 0.7328\n",
            "Epoch 3050/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 499.5227 - acc: 0.8884 - val_loss: 499.7054 - val_acc: 0.7328\n",
            "Epoch 3051/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 499.4753 - acc: 0.9026 - val_loss: 499.6563 - val_acc: 0.7328\n",
            "Epoch 3052/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 499.4211 - acc: 0.8966 - val_loss: 499.6071 - val_acc: 0.7328\n",
            "Epoch 3053/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 499.3745 - acc: 0.8884 - val_loss: 499.5576 - val_acc: 0.7328\n",
            "Epoch 3054/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 499.3263 - acc: 0.8976 - val_loss: 499.5085 - val_acc: 0.7328\n",
            "Epoch 3055/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 499.2699 - acc: 0.9006 - val_loss: 499.4593 - val_acc: 0.7328\n",
            "Epoch 3056/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 499.2307 - acc: 0.8915 - val_loss: 499.4100 - val_acc: 0.7328\n",
            "Epoch 3057/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 499.1775 - acc: 0.8915 - val_loss: 499.3607 - val_acc: 0.7328\n",
            "Epoch 3058/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 499.1231 - acc: 0.9016 - val_loss: 499.3117 - val_acc: 0.7328\n",
            "Epoch 3059/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 499.0788 - acc: 0.8915 - val_loss: 499.2624 - val_acc: 0.7328\n",
            "Epoch 3060/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 499.0368 - acc: 0.8895 - val_loss: 499.2132 - val_acc: 0.7328\n",
            "Epoch 3061/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 498.9774 - acc: 0.9057 - val_loss: 499.1639 - val_acc: 0.7328\n",
            "Epoch 3062/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 498.9320 - acc: 0.9016 - val_loss: 499.1147 - val_acc: 0.7328\n",
            "Epoch 3063/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 498.8802 - acc: 0.8996 - val_loss: 499.0655 - val_acc: 0.7328\n",
            "Epoch 3064/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 498.8309 - acc: 0.9057 - val_loss: 499.0163 - val_acc: 0.7328\n",
            "Epoch 3065/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 498.7852 - acc: 0.9037 - val_loss: 498.9670 - val_acc: 0.7328\n",
            "Epoch 3066/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 498.7395 - acc: 0.8986 - val_loss: 498.9177 - val_acc: 0.7328\n",
            "Epoch 3067/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 498.6840 - acc: 0.8955 - val_loss: 498.8685 - val_acc: 0.7328\n",
            "Epoch 3068/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 498.6310 - acc: 0.9087 - val_loss: 498.8193 - val_acc: 0.7328\n",
            "Epoch 3069/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 498.5848 - acc: 0.9006 - val_loss: 498.7699 - val_acc: 0.7328\n",
            "Epoch 3070/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 498.5385 - acc: 0.8996 - val_loss: 498.7209 - val_acc: 0.7328\n",
            "Epoch 3071/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 498.4849 - acc: 0.9077 - val_loss: 498.6716 - val_acc: 0.7328\n",
            "Epoch 3072/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 498.4368 - acc: 0.9037 - val_loss: 498.6223 - val_acc: 0.7328\n",
            "Epoch 3073/5000\n",
            "986/986 [==============================] - 0s 295us/step - loss: 498.3861 - acc: 0.9057 - val_loss: 498.5733 - val_acc: 0.7328\n",
            "Epoch 3074/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 498.3435 - acc: 0.8996 - val_loss: 498.5242 - val_acc: 0.7328\n",
            "Epoch 3075/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 498.2963 - acc: 0.8864 - val_loss: 498.4750 - val_acc: 0.7328\n",
            "Epoch 3076/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 498.2461 - acc: 0.9006 - val_loss: 498.4260 - val_acc: 0.7328\n",
            "Epoch 3077/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 498.1916 - acc: 0.9037 - val_loss: 498.3768 - val_acc: 0.7328\n",
            "Epoch 3078/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 498.1429 - acc: 0.9006 - val_loss: 498.3277 - val_acc: 0.7328\n",
            "Epoch 3079/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 498.0970 - acc: 0.8895 - val_loss: 498.2786 - val_acc: 0.7328\n",
            "Epoch 3080/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 498.0458 - acc: 0.8986 - val_loss: 498.2293 - val_acc: 0.7328\n",
            "Epoch 3081/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 498.0000 - acc: 0.8966 - val_loss: 498.1802 - val_acc: 0.7328\n",
            "Epoch 3082/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 497.9485 - acc: 0.8976 - val_loss: 498.1311 - val_acc: 0.7328\n",
            "Epoch 3083/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 497.8994 - acc: 0.8996 - val_loss: 498.0819 - val_acc: 0.7328\n",
            "Epoch 3084/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 497.8479 - acc: 0.8895 - val_loss: 498.0326 - val_acc: 0.7328\n",
            "Epoch 3085/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 497.7998 - acc: 0.8976 - val_loss: 497.9834 - val_acc: 0.7302\n",
            "Epoch 3086/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 497.7525 - acc: 0.8955 - val_loss: 497.9345 - val_acc: 0.7302\n",
            "Epoch 3087/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 497.7064 - acc: 0.8925 - val_loss: 497.8855 - val_acc: 0.7302\n",
            "Epoch 3088/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 497.6507 - acc: 0.9026 - val_loss: 497.8363 - val_acc: 0.7302\n",
            "Epoch 3089/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 497.5993 - acc: 0.9016 - val_loss: 497.7871 - val_acc: 0.7302\n",
            "Epoch 3090/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 497.5497 - acc: 0.9148 - val_loss: 497.7381 - val_acc: 0.7302\n",
            "Epoch 3091/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 497.5043 - acc: 0.8874 - val_loss: 497.6888 - val_acc: 0.7302\n",
            "Epoch 3092/5000\n",
            "986/986 [==============================] - 0s 295us/step - loss: 497.4612 - acc: 0.8935 - val_loss: 497.6399 - val_acc: 0.7302\n",
            "Epoch 3093/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 497.3994 - acc: 0.9097 - val_loss: 497.5909 - val_acc: 0.7302\n",
            "Epoch 3094/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 497.3575 - acc: 0.9006 - val_loss: 497.5418 - val_acc: 0.7302\n",
            "Epoch 3095/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 497.3078 - acc: 0.8986 - val_loss: 497.4928 - val_acc: 0.7302\n",
            "Epoch 3096/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 497.2629 - acc: 0.8905 - val_loss: 497.4436 - val_acc: 0.7302\n",
            "Epoch 3097/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 497.2135 - acc: 0.8905 - val_loss: 497.3947 - val_acc: 0.7302\n",
            "Epoch 3098/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 497.1649 - acc: 0.8986 - val_loss: 497.3455 - val_acc: 0.7302\n",
            "Epoch 3099/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 497.1104 - acc: 0.9026 - val_loss: 497.2964 - val_acc: 0.7302\n",
            "Epoch 3100/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 497.0569 - acc: 0.9077 - val_loss: 497.2475 - val_acc: 0.7302\n",
            "Epoch 3101/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 497.0149 - acc: 0.9026 - val_loss: 497.1984 - val_acc: 0.7302\n",
            "Epoch 3102/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 496.9636 - acc: 0.8976 - val_loss: 497.1495 - val_acc: 0.7302\n",
            "Epoch 3103/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 496.9211 - acc: 0.8945 - val_loss: 497.1004 - val_acc: 0.7302\n",
            "Epoch 3104/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 496.8706 - acc: 0.9026 - val_loss: 497.0514 - val_acc: 0.7302\n",
            "Epoch 3105/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 496.8180 - acc: 0.8986 - val_loss: 497.0022 - val_acc: 0.7302\n",
            "Epoch 3106/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 496.7645 - acc: 0.8996 - val_loss: 496.9532 - val_acc: 0.7302\n",
            "Epoch 3107/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 496.7167 - acc: 0.9057 - val_loss: 496.9041 - val_acc: 0.7302\n",
            "Epoch 3108/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 496.6706 - acc: 0.8996 - val_loss: 496.8551 - val_acc: 0.7302\n",
            "Epoch 3109/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 496.6160 - acc: 0.9026 - val_loss: 496.8061 - val_acc: 0.7302\n",
            "Epoch 3110/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 496.5715 - acc: 0.8925 - val_loss: 496.7571 - val_acc: 0.7302\n",
            "Epoch 3111/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 496.5243 - acc: 0.9037 - val_loss: 496.7081 - val_acc: 0.7302\n",
            "Epoch 3112/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 496.4786 - acc: 0.8925 - val_loss: 496.6590 - val_acc: 0.7302\n",
            "Epoch 3113/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 496.4266 - acc: 0.8955 - val_loss: 496.6101 - val_acc: 0.7302\n",
            "Epoch 3114/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 496.3799 - acc: 0.8976 - val_loss: 496.5611 - val_acc: 0.7302\n",
            "Epoch 3115/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 496.3253 - acc: 0.9016 - val_loss: 496.5121 - val_acc: 0.7302\n",
            "Epoch 3116/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 496.2834 - acc: 0.8874 - val_loss: 496.4630 - val_acc: 0.7302\n",
            "Epoch 3117/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 496.2274 - acc: 0.9037 - val_loss: 496.4142 - val_acc: 0.7302\n",
            "Epoch 3118/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 496.1730 - acc: 0.9047 - val_loss: 496.3652 - val_acc: 0.7302\n",
            "Epoch 3119/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 496.1314 - acc: 0.8996 - val_loss: 496.3163 - val_acc: 0.7302\n",
            "Epoch 3120/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 496.0782 - acc: 0.8884 - val_loss: 496.2674 - val_acc: 0.7302\n",
            "Epoch 3121/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 496.0367 - acc: 0.8955 - val_loss: 496.2185 - val_acc: 0.7302\n",
            "Epoch 3122/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 495.9882 - acc: 0.9016 - val_loss: 496.1694 - val_acc: 0.7302\n",
            "Epoch 3123/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 495.9357 - acc: 0.8844 - val_loss: 496.1203 - val_acc: 0.7302\n",
            "Epoch 3124/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 495.8873 - acc: 0.9087 - val_loss: 496.0714 - val_acc: 0.7302\n",
            "Epoch 3125/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 495.8394 - acc: 0.8955 - val_loss: 496.0224 - val_acc: 0.7302\n",
            "Epoch 3126/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 495.7824 - acc: 0.9037 - val_loss: 495.9736 - val_acc: 0.7302\n",
            "Epoch 3127/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 495.7407 - acc: 0.8945 - val_loss: 495.9246 - val_acc: 0.7302\n",
            "Epoch 3128/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 495.6920 - acc: 0.8925 - val_loss: 495.8756 - val_acc: 0.7302\n",
            "Epoch 3129/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 495.6421 - acc: 0.8945 - val_loss: 495.8268 - val_acc: 0.7302\n",
            "Epoch 3130/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 495.5970 - acc: 0.8996 - val_loss: 495.7777 - val_acc: 0.7302\n",
            "Epoch 3131/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 495.5400 - acc: 0.9047 - val_loss: 495.7288 - val_acc: 0.7302\n",
            "Epoch 3132/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 495.4935 - acc: 0.8955 - val_loss: 495.6799 - val_acc: 0.7302\n",
            "Epoch 3133/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 495.4503 - acc: 0.8945 - val_loss: 495.6310 - val_acc: 0.7302\n",
            "Epoch 3134/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 495.4012 - acc: 0.8884 - val_loss: 495.5819 - val_acc: 0.7302\n",
            "Epoch 3135/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 495.3467 - acc: 0.9067 - val_loss: 495.5329 - val_acc: 0.7302\n",
            "Epoch 3136/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 495.3091 - acc: 0.8945 - val_loss: 495.4841 - val_acc: 0.7302\n",
            "Epoch 3137/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 495.2435 - acc: 0.9067 - val_loss: 495.4354 - val_acc: 0.7302\n",
            "Epoch 3138/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 495.2033 - acc: 0.9026 - val_loss: 495.3865 - val_acc: 0.7302\n",
            "Epoch 3139/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 495.1526 - acc: 0.8895 - val_loss: 495.3376 - val_acc: 0.7302\n",
            "Epoch 3140/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 495.1003 - acc: 0.9006 - val_loss: 495.2888 - val_acc: 0.7302\n",
            "Epoch 3141/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 495.0474 - acc: 0.8986 - val_loss: 495.2399 - val_acc: 0.7302\n",
            "Epoch 3142/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 494.9997 - acc: 0.9037 - val_loss: 495.1908 - val_acc: 0.7302\n",
            "Epoch 3143/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 494.9545 - acc: 0.8966 - val_loss: 495.1420 - val_acc: 0.7302\n",
            "Epoch 3144/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 494.9092 - acc: 0.8976 - val_loss: 495.0931 - val_acc: 0.7302\n",
            "Epoch 3145/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 494.8558 - acc: 0.9016 - val_loss: 495.0443 - val_acc: 0.7302\n",
            "Epoch 3146/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 494.8058 - acc: 0.9067 - val_loss: 494.9954 - val_acc: 0.7302\n",
            "Epoch 3147/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 494.7615 - acc: 0.9016 - val_loss: 494.9465 - val_acc: 0.7302\n",
            "Epoch 3148/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 494.7135 - acc: 0.8986 - val_loss: 494.8978 - val_acc: 0.7302\n",
            "Epoch 3149/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 494.6653 - acc: 0.8996 - val_loss: 494.8489 - val_acc: 0.7302\n",
            "Epoch 3150/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 494.6151 - acc: 0.8854 - val_loss: 494.8002 - val_acc: 0.7302\n",
            "Epoch 3151/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 494.5686 - acc: 0.8935 - val_loss: 494.7511 - val_acc: 0.7302\n",
            "Epoch 3152/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 494.5121 - acc: 0.9097 - val_loss: 494.7024 - val_acc: 0.7302\n",
            "Epoch 3153/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 494.4612 - acc: 0.9047 - val_loss: 494.6535 - val_acc: 0.7302\n",
            "Epoch 3154/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 494.4167 - acc: 0.8986 - val_loss: 494.6045 - val_acc: 0.7302\n",
            "Epoch 3155/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 494.3762 - acc: 0.8996 - val_loss: 494.5556 - val_acc: 0.7302\n",
            "Epoch 3156/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 494.3231 - acc: 0.8925 - val_loss: 494.5067 - val_acc: 0.7302\n",
            "Epoch 3157/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 494.2669 - acc: 0.9077 - val_loss: 494.4582 - val_acc: 0.7302\n",
            "Epoch 3158/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 494.2229 - acc: 0.9057 - val_loss: 494.4093 - val_acc: 0.7302\n",
            "Epoch 3159/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 494.1683 - acc: 0.9016 - val_loss: 494.3607 - val_acc: 0.7302\n",
            "Epoch 3160/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 494.1323 - acc: 0.8955 - val_loss: 494.3117 - val_acc: 0.7302\n",
            "Epoch 3161/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 494.0775 - acc: 0.9026 - val_loss: 494.2629 - val_acc: 0.7302\n",
            "Epoch 3162/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 494.0232 - acc: 0.8966 - val_loss: 494.2140 - val_acc: 0.7302\n",
            "Epoch 3163/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 493.9830 - acc: 0.8996 - val_loss: 494.1653 - val_acc: 0.7302\n",
            "Epoch 3164/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 493.9279 - acc: 0.9067 - val_loss: 494.1165 - val_acc: 0.7302\n",
            "Epoch 3165/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 493.8754 - acc: 0.9037 - val_loss: 494.0677 - val_acc: 0.7302\n",
            "Epoch 3166/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 493.8305 - acc: 0.8996 - val_loss: 494.0189 - val_acc: 0.7302\n",
            "Epoch 3167/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 493.7855 - acc: 0.8976 - val_loss: 493.9702 - val_acc: 0.7302\n",
            "Epoch 3168/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 493.7336 - acc: 0.8996 - val_loss: 493.9214 - val_acc: 0.7302\n",
            "Epoch 3169/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 493.6794 - acc: 0.9047 - val_loss: 493.8727 - val_acc: 0.7302\n",
            "Epoch 3170/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 493.6339 - acc: 0.9108 - val_loss: 493.8239 - val_acc: 0.7302\n",
            "Epoch 3171/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 493.5886 - acc: 0.9037 - val_loss: 493.7750 - val_acc: 0.7302\n",
            "Epoch 3172/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 493.5350 - acc: 0.9006 - val_loss: 493.7262 - val_acc: 0.7302\n",
            "Epoch 3173/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 493.4856 - acc: 0.9067 - val_loss: 493.6775 - val_acc: 0.7275\n",
            "Epoch 3174/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 493.4446 - acc: 0.8986 - val_loss: 493.6288 - val_acc: 0.7275\n",
            "Epoch 3175/5000\n",
            "986/986 [==============================] - 0s 297us/step - loss: 493.3989 - acc: 0.8996 - val_loss: 493.5799 - val_acc: 0.7275\n",
            "Epoch 3176/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 493.3414 - acc: 0.9077 - val_loss: 493.5311 - val_acc: 0.7275\n",
            "Epoch 3177/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 493.2972 - acc: 0.8905 - val_loss: 493.4825 - val_acc: 0.7302\n",
            "Epoch 3178/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 493.2451 - acc: 0.8986 - val_loss: 493.4336 - val_acc: 0.7302\n",
            "Epoch 3179/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 493.1950 - acc: 0.9006 - val_loss: 493.3848 - val_acc: 0.7302\n",
            "Epoch 3180/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 493.1541 - acc: 0.8935 - val_loss: 493.3360 - val_acc: 0.7302\n",
            "Epoch 3181/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 493.0969 - acc: 0.9016 - val_loss: 493.2872 - val_acc: 0.7302\n",
            "Epoch 3182/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 493.0555 - acc: 0.9016 - val_loss: 493.2387 - val_acc: 0.7302\n",
            "Epoch 3183/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 493.0028 - acc: 0.8996 - val_loss: 493.1899 - val_acc: 0.7302\n",
            "Epoch 3184/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 492.9487 - acc: 0.9037 - val_loss: 493.1412 - val_acc: 0.7302\n",
            "Epoch 3185/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 492.9043 - acc: 0.8966 - val_loss: 493.0924 - val_acc: 0.7302\n",
            "Epoch 3186/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 492.8630 - acc: 0.8935 - val_loss: 493.0436 - val_acc: 0.7302\n",
            "Epoch 3187/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 492.8108 - acc: 0.9026 - val_loss: 492.9949 - val_acc: 0.7275\n",
            "Epoch 3188/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 492.7501 - acc: 0.8945 - val_loss: 492.9462 - val_acc: 0.7302\n",
            "Epoch 3189/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 492.7127 - acc: 0.8935 - val_loss: 492.8976 - val_acc: 0.7302\n",
            "Epoch 3190/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 492.6627 - acc: 0.9016 - val_loss: 492.8488 - val_acc: 0.7302\n",
            "Epoch 3191/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 492.6075 - acc: 0.9047 - val_loss: 492.7998 - val_acc: 0.7302\n",
            "Epoch 3192/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 492.5675 - acc: 0.8986 - val_loss: 492.7513 - val_acc: 0.7302\n",
            "Epoch 3193/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 492.5159 - acc: 0.9016 - val_loss: 492.7026 - val_acc: 0.7302\n",
            "Epoch 3194/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 492.4584 - acc: 0.9087 - val_loss: 492.6540 - val_acc: 0.7302\n",
            "Epoch 3195/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 492.4159 - acc: 0.9047 - val_loss: 492.6053 - val_acc: 0.7302\n",
            "Epoch 3196/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 492.3673 - acc: 0.8945 - val_loss: 492.5564 - val_acc: 0.7302\n",
            "Epoch 3197/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 492.3167 - acc: 0.9006 - val_loss: 492.5077 - val_acc: 0.7302\n",
            "Epoch 3198/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 492.2723 - acc: 0.8955 - val_loss: 492.4592 - val_acc: 0.7302\n",
            "Epoch 3199/5000\n",
            "986/986 [==============================] - 0s 261us/step - loss: 492.2240 - acc: 0.8935 - val_loss: 492.4106 - val_acc: 0.7275\n",
            "Epoch 3200/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 492.1765 - acc: 0.8996 - val_loss: 492.3620 - val_acc: 0.7275\n",
            "Epoch 3201/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 492.1231 - acc: 0.9006 - val_loss: 492.3135 - val_acc: 0.7275\n",
            "Epoch 3202/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 492.0780 - acc: 0.9016 - val_loss: 492.2647 - val_acc: 0.7275\n",
            "Epoch 3203/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 492.0289 - acc: 0.8966 - val_loss: 492.2160 - val_acc: 0.7275\n",
            "Epoch 3204/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 491.9740 - acc: 0.9037 - val_loss: 492.1674 - val_acc: 0.7302\n",
            "Epoch 3205/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 491.9322 - acc: 0.8945 - val_loss: 492.1187 - val_acc: 0.7302\n",
            "Epoch 3206/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 491.8788 - acc: 0.8996 - val_loss: 492.0701 - val_acc: 0.7302\n",
            "Epoch 3207/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 491.8395 - acc: 0.8935 - val_loss: 492.0213 - val_acc: 0.7302\n",
            "Epoch 3208/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 491.7827 - acc: 0.9016 - val_loss: 491.9729 - val_acc: 0.7302\n",
            "Epoch 3209/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 491.7368 - acc: 0.8945 - val_loss: 491.9242 - val_acc: 0.7302\n",
            "Epoch 3210/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 491.6922 - acc: 0.8905 - val_loss: 491.8755 - val_acc: 0.7275\n",
            "Epoch 3211/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 491.6391 - acc: 0.9026 - val_loss: 491.8267 - val_acc: 0.7275\n",
            "Epoch 3212/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 491.5884 - acc: 0.8945 - val_loss: 491.7780 - val_acc: 0.7302\n",
            "Epoch 3213/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 491.5438 - acc: 0.8966 - val_loss: 491.7295 - val_acc: 0.7302\n",
            "Epoch 3214/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 491.4895 - acc: 0.9016 - val_loss: 491.6809 - val_acc: 0.7302\n",
            "Epoch 3215/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 491.4416 - acc: 0.8966 - val_loss: 491.6325 - val_acc: 0.7275\n",
            "Epoch 3216/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 491.3921 - acc: 0.9047 - val_loss: 491.5838 - val_acc: 0.7275\n",
            "Epoch 3217/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 491.3476 - acc: 0.8955 - val_loss: 491.5352 - val_acc: 0.7275\n",
            "Epoch 3218/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 491.2946 - acc: 0.8966 - val_loss: 491.4865 - val_acc: 0.7275\n",
            "Epoch 3219/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 491.2535 - acc: 0.9047 - val_loss: 491.4381 - val_acc: 0.7275\n",
            "Epoch 3220/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 491.1928 - acc: 0.9057 - val_loss: 491.3897 - val_acc: 0.7275\n",
            "Epoch 3221/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 491.1556 - acc: 0.9047 - val_loss: 491.3410 - val_acc: 0.7275\n",
            "Epoch 3222/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 491.1011 - acc: 0.9067 - val_loss: 491.2923 - val_acc: 0.7302\n",
            "Epoch 3223/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 491.0560 - acc: 0.9006 - val_loss: 491.2438 - val_acc: 0.7275\n",
            "Epoch 3224/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 491.0067 - acc: 0.9067 - val_loss: 491.1952 - val_acc: 0.7275\n",
            "Epoch 3225/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 490.9499 - acc: 0.9097 - val_loss: 491.1466 - val_acc: 0.7275\n",
            "Epoch 3226/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 490.9011 - acc: 0.9047 - val_loss: 491.0979 - val_acc: 0.7275\n",
            "Epoch 3227/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 490.8631 - acc: 0.8884 - val_loss: 491.0493 - val_acc: 0.7302\n",
            "Epoch 3228/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 490.8131 - acc: 0.9006 - val_loss: 491.0009 - val_acc: 0.7275\n",
            "Epoch 3229/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 490.7607 - acc: 0.9067 - val_loss: 490.9523 - val_acc: 0.7275\n",
            "Epoch 3230/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 490.7107 - acc: 0.9097 - val_loss: 490.9039 - val_acc: 0.7275\n",
            "Epoch 3231/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 490.6722 - acc: 0.8925 - val_loss: 490.8552 - val_acc: 0.7275\n",
            "Epoch 3232/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 490.6154 - acc: 0.9057 - val_loss: 490.8067 - val_acc: 0.7249\n",
            "Epoch 3233/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 490.5645 - acc: 0.9077 - val_loss: 490.7582 - val_acc: 0.7275\n",
            "Epoch 3234/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 490.5163 - acc: 0.8996 - val_loss: 490.7098 - val_acc: 0.7302\n",
            "Epoch 3235/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 490.4694 - acc: 0.9026 - val_loss: 490.6610 - val_acc: 0.7275\n",
            "Epoch 3236/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 490.4258 - acc: 0.8986 - val_loss: 490.6125 - val_acc: 0.7302\n",
            "Epoch 3237/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 490.3795 - acc: 0.8935 - val_loss: 490.5641 - val_acc: 0.7275\n",
            "Epoch 3238/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 490.3246 - acc: 0.9037 - val_loss: 490.5155 - val_acc: 0.7275\n",
            "Epoch 3239/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 490.2784 - acc: 0.9108 - val_loss: 490.4672 - val_acc: 0.7275\n",
            "Epoch 3240/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 490.2300 - acc: 0.9037 - val_loss: 490.4187 - val_acc: 0.7275\n",
            "Epoch 3241/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 490.1850 - acc: 0.8935 - val_loss: 490.3702 - val_acc: 0.7275\n",
            "Epoch 3242/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 490.1315 - acc: 0.9097 - val_loss: 490.3218 - val_acc: 0.7275\n",
            "Epoch 3243/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 490.0829 - acc: 0.9037 - val_loss: 490.2733 - val_acc: 0.7302\n",
            "Epoch 3244/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 490.0355 - acc: 0.8966 - val_loss: 490.2248 - val_acc: 0.7302\n",
            "Epoch 3245/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 489.9784 - acc: 0.9178 - val_loss: 490.1766 - val_acc: 0.7275\n",
            "Epoch 3246/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 489.9290 - acc: 0.9037 - val_loss: 490.1281 - val_acc: 0.7275\n",
            "Epoch 3247/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 489.8983 - acc: 0.8925 - val_loss: 490.0795 - val_acc: 0.7275\n",
            "Epoch 3248/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 489.8371 - acc: 0.9026 - val_loss: 490.0309 - val_acc: 0.7275\n",
            "Epoch 3249/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 489.7928 - acc: 0.8996 - val_loss: 489.9827 - val_acc: 0.7302\n",
            "Epoch 3250/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 489.7436 - acc: 0.9087 - val_loss: 489.9343 - val_acc: 0.7328\n",
            "Epoch 3251/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 489.6930 - acc: 0.9108 - val_loss: 489.8859 - val_acc: 0.7328\n",
            "Epoch 3252/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 489.6501 - acc: 0.9118 - val_loss: 489.8375 - val_acc: 0.7302\n",
            "Epoch 3253/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 489.6016 - acc: 0.9026 - val_loss: 489.7890 - val_acc: 0.7275\n",
            "Epoch 3254/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 489.5490 - acc: 0.9087 - val_loss: 489.7407 - val_acc: 0.7302\n",
            "Epoch 3255/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 489.5019 - acc: 0.8996 - val_loss: 489.6923 - val_acc: 0.7302\n",
            "Epoch 3256/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 489.4516 - acc: 0.9016 - val_loss: 489.6439 - val_acc: 0.7302\n",
            "Epoch 3257/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 489.3964 - acc: 0.9097 - val_loss: 489.5954 - val_acc: 0.7302\n",
            "Epoch 3258/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 489.3511 - acc: 0.9047 - val_loss: 489.5469 - val_acc: 0.7302\n",
            "Epoch 3259/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 489.3103 - acc: 0.8955 - val_loss: 489.4986 - val_acc: 0.7302\n",
            "Epoch 3260/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 489.2596 - acc: 0.8955 - val_loss: 489.4502 - val_acc: 0.7328\n",
            "Epoch 3261/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 489.2168 - acc: 0.8996 - val_loss: 489.4018 - val_acc: 0.7328\n",
            "Epoch 3262/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 489.1598 - acc: 0.9077 - val_loss: 489.3533 - val_acc: 0.7302\n",
            "Epoch 3263/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 489.1154 - acc: 0.9047 - val_loss: 489.3050 - val_acc: 0.7302\n",
            "Epoch 3264/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 489.0640 - acc: 0.9057 - val_loss: 489.2567 - val_acc: 0.7328\n",
            "Epoch 3265/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 489.0168 - acc: 0.9037 - val_loss: 489.2083 - val_acc: 0.7302\n",
            "Epoch 3266/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 488.9637 - acc: 0.9047 - val_loss: 489.1599 - val_acc: 0.7302\n",
            "Epoch 3267/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 488.9216 - acc: 0.8986 - val_loss: 489.1115 - val_acc: 0.7302\n",
            "Epoch 3268/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 488.8742 - acc: 0.8874 - val_loss: 489.0631 - val_acc: 0.7302\n",
            "Epoch 3269/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 488.8192 - acc: 0.9108 - val_loss: 489.0148 - val_acc: 0.7302\n",
            "Epoch 3270/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 488.7794 - acc: 0.9016 - val_loss: 488.9663 - val_acc: 0.7302\n",
            "Epoch 3271/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 488.7223 - acc: 0.9108 - val_loss: 488.9178 - val_acc: 0.7302\n",
            "Epoch 3272/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 488.6756 - acc: 0.9016 - val_loss: 488.8696 - val_acc: 0.7302\n",
            "Epoch 3273/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 488.6277 - acc: 0.9006 - val_loss: 488.8212 - val_acc: 0.7302\n",
            "Epoch 3274/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 488.5845 - acc: 0.8955 - val_loss: 488.7728 - val_acc: 0.7302\n",
            "Epoch 3275/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 488.5283 - acc: 0.9016 - val_loss: 488.7244 - val_acc: 0.7328\n",
            "Epoch 3276/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 488.4877 - acc: 0.8925 - val_loss: 488.6761 - val_acc: 0.7302\n",
            "Epoch 3277/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 488.4285 - acc: 0.9118 - val_loss: 488.6277 - val_acc: 0.7302\n",
            "Epoch 3278/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 488.3902 - acc: 0.9037 - val_loss: 488.5793 - val_acc: 0.7302\n",
            "Epoch 3279/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 488.3422 - acc: 0.8925 - val_loss: 488.5309 - val_acc: 0.7302\n",
            "Epoch 3280/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 488.2906 - acc: 0.9178 - val_loss: 488.4827 - val_acc: 0.7302\n",
            "Epoch 3281/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 488.2461 - acc: 0.8966 - val_loss: 488.4343 - val_acc: 0.7302\n",
            "Epoch 3282/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 488.1942 - acc: 0.9057 - val_loss: 488.3860 - val_acc: 0.7302\n",
            "Epoch 3283/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 488.1459 - acc: 0.9047 - val_loss: 488.3376 - val_acc: 0.7328\n",
            "Epoch 3284/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 488.0948 - acc: 0.9057 - val_loss: 488.2894 - val_acc: 0.7328\n",
            "Epoch 3285/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 488.0484 - acc: 0.9087 - val_loss: 488.2410 - val_acc: 0.7328\n",
            "Epoch 3286/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 488.0042 - acc: 0.8966 - val_loss: 488.1927 - val_acc: 0.7328\n",
            "Epoch 3287/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 487.9479 - acc: 0.9118 - val_loss: 488.1445 - val_acc: 0.7328\n",
            "Epoch 3288/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 487.9020 - acc: 0.9128 - val_loss: 488.0961 - val_acc: 0.7328\n",
            "Epoch 3289/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 487.8475 - acc: 0.9016 - val_loss: 488.0480 - val_acc: 0.7328\n",
            "Epoch 3290/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 487.8058 - acc: 0.9108 - val_loss: 487.9997 - val_acc: 0.7328\n",
            "Epoch 3291/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 487.7571 - acc: 0.9118 - val_loss: 487.9514 - val_acc: 0.7328\n",
            "Epoch 3292/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 487.7099 - acc: 0.9067 - val_loss: 487.9031 - val_acc: 0.7328\n",
            "Epoch 3293/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 487.6628 - acc: 0.9026 - val_loss: 487.8549 - val_acc: 0.7328\n",
            "Epoch 3294/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 487.6077 - acc: 0.9077 - val_loss: 487.8066 - val_acc: 0.7302\n",
            "Epoch 3295/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 487.5650 - acc: 0.9067 - val_loss: 487.7583 - val_acc: 0.7302\n",
            "Epoch 3296/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 487.5206 - acc: 0.9047 - val_loss: 487.7101 - val_acc: 0.7302\n",
            "Epoch 3297/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 487.4728 - acc: 0.9087 - val_loss: 487.6619 - val_acc: 0.7302\n",
            "Epoch 3298/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 487.4150 - acc: 0.9118 - val_loss: 487.6137 - val_acc: 0.7302\n",
            "Epoch 3299/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 487.3724 - acc: 0.9128 - val_loss: 487.5654 - val_acc: 0.7302\n",
            "Epoch 3300/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 487.3247 - acc: 0.9067 - val_loss: 487.5171 - val_acc: 0.7302\n",
            "Epoch 3301/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 487.2771 - acc: 0.9006 - val_loss: 487.4689 - val_acc: 0.7328\n",
            "Epoch 3302/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 487.2272 - acc: 0.9006 - val_loss: 487.4207 - val_acc: 0.7328\n",
            "Epoch 3303/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 487.1790 - acc: 0.9047 - val_loss: 487.3725 - val_acc: 0.7328\n",
            "Epoch 3304/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 487.1238 - acc: 0.9229 - val_loss: 487.3242 - val_acc: 0.7328\n",
            "Epoch 3305/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 487.0787 - acc: 0.9057 - val_loss: 487.2760 - val_acc: 0.7302\n",
            "Epoch 3306/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 487.0379 - acc: 0.9037 - val_loss: 487.2278 - val_acc: 0.7328\n",
            "Epoch 3307/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 486.9883 - acc: 0.9037 - val_loss: 487.1794 - val_acc: 0.7328\n",
            "Epoch 3308/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 486.9374 - acc: 0.8955 - val_loss: 487.1313 - val_acc: 0.7328\n",
            "Epoch 3309/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 486.8874 - acc: 0.8996 - val_loss: 487.0832 - val_acc: 0.7328\n",
            "Epoch 3310/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 486.8446 - acc: 0.9097 - val_loss: 487.0351 - val_acc: 0.7328\n",
            "Epoch 3311/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 486.7968 - acc: 0.8966 - val_loss: 486.9869 - val_acc: 0.7328\n",
            "Epoch 3312/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 486.7424 - acc: 0.9006 - val_loss: 486.9385 - val_acc: 0.7328\n",
            "Epoch 3313/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 486.6923 - acc: 0.9077 - val_loss: 486.8903 - val_acc: 0.7328\n",
            "Epoch 3314/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 486.6456 - acc: 0.9057 - val_loss: 486.8423 - val_acc: 0.7328\n",
            "Epoch 3315/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 486.5971 - acc: 0.9026 - val_loss: 486.7940 - val_acc: 0.7328\n",
            "Epoch 3316/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 486.5496 - acc: 0.8945 - val_loss: 486.7459 - val_acc: 0.7328\n",
            "Epoch 3317/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 486.5022 - acc: 0.9077 - val_loss: 486.6976 - val_acc: 0.7328\n",
            "Epoch 3318/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 486.4560 - acc: 0.9128 - val_loss: 486.6495 - val_acc: 0.7328\n",
            "Epoch 3319/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 486.4138 - acc: 0.8874 - val_loss: 486.6015 - val_acc: 0.7328\n",
            "Epoch 3320/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 486.3617 - acc: 0.9077 - val_loss: 486.5532 - val_acc: 0.7328\n",
            "Epoch 3321/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 486.3154 - acc: 0.8996 - val_loss: 486.5049 - val_acc: 0.7328\n",
            "Epoch 3322/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 486.2604 - acc: 0.9026 - val_loss: 486.4568 - val_acc: 0.7328\n",
            "Epoch 3323/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 486.2157 - acc: 0.9057 - val_loss: 486.4087 - val_acc: 0.7328\n",
            "Epoch 3324/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 486.1670 - acc: 0.8996 - val_loss: 486.3606 - val_acc: 0.7328\n",
            "Epoch 3325/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 486.1193 - acc: 0.9047 - val_loss: 486.3124 - val_acc: 0.7328\n",
            "Epoch 3326/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 486.0707 - acc: 0.9037 - val_loss: 486.2644 - val_acc: 0.7328\n",
            "Epoch 3327/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 486.0295 - acc: 0.8925 - val_loss: 486.2161 - val_acc: 0.7328\n",
            "Epoch 3328/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 485.9699 - acc: 0.9118 - val_loss: 486.1679 - val_acc: 0.7328\n",
            "Epoch 3329/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 485.9303 - acc: 0.8915 - val_loss: 486.1197 - val_acc: 0.7328\n",
            "Epoch 3330/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 485.8747 - acc: 0.8996 - val_loss: 486.0717 - val_acc: 0.7354\n",
            "Epoch 3331/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 485.8247 - acc: 0.8996 - val_loss: 486.0236 - val_acc: 0.7354\n",
            "Epoch 3332/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 485.7756 - acc: 0.9067 - val_loss: 485.9756 - val_acc: 0.7328\n",
            "Epoch 3333/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 485.7306 - acc: 0.8996 - val_loss: 485.9274 - val_acc: 0.7328\n",
            "Epoch 3334/5000\n",
            "986/986 [==============================] - 0s 260us/step - loss: 485.6875 - acc: 0.8935 - val_loss: 485.8792 - val_acc: 0.7354\n",
            "Epoch 3335/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 485.6381 - acc: 0.9026 - val_loss: 485.8311 - val_acc: 0.7354\n",
            "Epoch 3336/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 485.5860 - acc: 0.9087 - val_loss: 485.7829 - val_acc: 0.7328\n",
            "Epoch 3337/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 485.5403 - acc: 0.8925 - val_loss: 485.7348 - val_acc: 0.7328\n",
            "Epoch 3338/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 485.4864 - acc: 0.9037 - val_loss: 485.6868 - val_acc: 0.7328\n",
            "Epoch 3339/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 485.4396 - acc: 0.9077 - val_loss: 485.6386 - val_acc: 0.7328\n",
            "Epoch 3340/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 485.3926 - acc: 0.9006 - val_loss: 485.5905 - val_acc: 0.7328\n",
            "Epoch 3341/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 485.3472 - acc: 0.9108 - val_loss: 485.5425 - val_acc: 0.7328\n",
            "Epoch 3342/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 485.2995 - acc: 0.9026 - val_loss: 485.4944 - val_acc: 0.7328\n",
            "Epoch 3343/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 485.2539 - acc: 0.8966 - val_loss: 485.4463 - val_acc: 0.7328\n",
            "Epoch 3344/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 485.1972 - acc: 0.9199 - val_loss: 485.3982 - val_acc: 0.7328\n",
            "Epoch 3345/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 485.1520 - acc: 0.8996 - val_loss: 485.3501 - val_acc: 0.7328\n",
            "Epoch 3346/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 485.1121 - acc: 0.9037 - val_loss: 485.3020 - val_acc: 0.7328\n",
            "Epoch 3347/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 485.0597 - acc: 0.9087 - val_loss: 485.2540 - val_acc: 0.7328\n",
            "Epoch 3348/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 485.0150 - acc: 0.8996 - val_loss: 485.2058 - val_acc: 0.7328\n",
            "Epoch 3349/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 484.9601 - acc: 0.9047 - val_loss: 485.1577 - val_acc: 0.7328\n",
            "Epoch 3350/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 484.9071 - acc: 0.9148 - val_loss: 485.1097 - val_acc: 0.7328\n",
            "Epoch 3351/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 484.8680 - acc: 0.9006 - val_loss: 485.0617 - val_acc: 0.7328\n",
            "Epoch 3352/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 484.8193 - acc: 0.9016 - val_loss: 485.0136 - val_acc: 0.7328\n",
            "Epoch 3353/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 484.7675 - acc: 0.9148 - val_loss: 484.9655 - val_acc: 0.7328\n",
            "Epoch 3354/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 484.7208 - acc: 0.9118 - val_loss: 484.9176 - val_acc: 0.7328\n",
            "Epoch 3355/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 484.6726 - acc: 0.8955 - val_loss: 484.8696 - val_acc: 0.7328\n",
            "Epoch 3356/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 484.6293 - acc: 0.8996 - val_loss: 484.8216 - val_acc: 0.7328\n",
            "Epoch 3357/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 484.5738 - acc: 0.9037 - val_loss: 484.7736 - val_acc: 0.7328\n",
            "Epoch 3358/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 484.5255 - acc: 0.9087 - val_loss: 484.7255 - val_acc: 0.7328\n",
            "Epoch 3359/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 484.4810 - acc: 0.9037 - val_loss: 484.6774 - val_acc: 0.7328\n",
            "Epoch 3360/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 484.4336 - acc: 0.9016 - val_loss: 484.6294 - val_acc: 0.7328\n",
            "Epoch 3361/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 484.3952 - acc: 0.9006 - val_loss: 484.5814 - val_acc: 0.7328\n",
            "Epoch 3362/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 484.3370 - acc: 0.8955 - val_loss: 484.5334 - val_acc: 0.7328\n",
            "Epoch 3363/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 484.2874 - acc: 0.9057 - val_loss: 484.4853 - val_acc: 0.7328\n",
            "Epoch 3364/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 484.2427 - acc: 0.8996 - val_loss: 484.4373 - val_acc: 0.7328\n",
            "Epoch 3365/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 484.1922 - acc: 0.9077 - val_loss: 484.3893 - val_acc: 0.7328\n",
            "Epoch 3366/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 484.1458 - acc: 0.9047 - val_loss: 484.3414 - val_acc: 0.7328\n",
            "Epoch 3367/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 484.0962 - acc: 0.9057 - val_loss: 484.2934 - val_acc: 0.7328\n",
            "Epoch 3368/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 484.0517 - acc: 0.9128 - val_loss: 484.2454 - val_acc: 0.7328\n",
            "Epoch 3369/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 483.9957 - acc: 0.9087 - val_loss: 484.1974 - val_acc: 0.7328\n",
            "Epoch 3370/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 483.9538 - acc: 0.9037 - val_loss: 484.1493 - val_acc: 0.7328\n",
            "Epoch 3371/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 483.9045 - acc: 0.9026 - val_loss: 484.1013 - val_acc: 0.7328\n",
            "Epoch 3372/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 483.8546 - acc: 0.9087 - val_loss: 484.0533 - val_acc: 0.7354\n",
            "Epoch 3373/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 483.8049 - acc: 0.9087 - val_loss: 484.0054 - val_acc: 0.7328\n",
            "Epoch 3374/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 483.7599 - acc: 0.9118 - val_loss: 483.9575 - val_acc: 0.7354\n",
            "Epoch 3375/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 483.7125 - acc: 0.9047 - val_loss: 483.9095 - val_acc: 0.7354\n",
            "Epoch 3376/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 483.6638 - acc: 0.9037 - val_loss: 483.8615 - val_acc: 0.7354\n",
            "Epoch 3377/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 483.6163 - acc: 0.9006 - val_loss: 483.8136 - val_acc: 0.7354\n",
            "Epoch 3378/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 483.5679 - acc: 0.9108 - val_loss: 483.7656 - val_acc: 0.7354\n",
            "Epoch 3379/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 483.5213 - acc: 0.9067 - val_loss: 483.7176 - val_acc: 0.7354\n",
            "Epoch 3380/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 483.4697 - acc: 0.9006 - val_loss: 483.6695 - val_acc: 0.7354\n",
            "Epoch 3381/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 483.4269 - acc: 0.9047 - val_loss: 483.6217 - val_acc: 0.7354\n",
            "Epoch 3382/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 483.3749 - acc: 0.9047 - val_loss: 483.5737 - val_acc: 0.7328\n",
            "Epoch 3383/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 483.3288 - acc: 0.9037 - val_loss: 483.5258 - val_acc: 0.7354\n",
            "Epoch 3384/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 483.2824 - acc: 0.8945 - val_loss: 483.4779 - val_acc: 0.7354\n",
            "Epoch 3385/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 483.2314 - acc: 0.9016 - val_loss: 483.4300 - val_acc: 0.7328\n",
            "Epoch 3386/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 483.1861 - acc: 0.9047 - val_loss: 483.3822 - val_acc: 0.7354\n",
            "Epoch 3387/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 483.1359 - acc: 0.9097 - val_loss: 483.3342 - val_acc: 0.7328\n",
            "Epoch 3388/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 483.0894 - acc: 0.9016 - val_loss: 483.2863 - val_acc: 0.7328\n",
            "Epoch 3389/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 483.0413 - acc: 0.9057 - val_loss: 483.2385 - val_acc: 0.7328\n",
            "Epoch 3390/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 482.9924 - acc: 0.9128 - val_loss: 483.1905 - val_acc: 0.7328\n",
            "Epoch 3391/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 482.9445 - acc: 0.9087 - val_loss: 483.1425 - val_acc: 0.7328\n",
            "Epoch 3392/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 482.8971 - acc: 0.8986 - val_loss: 483.0946 - val_acc: 0.7328\n",
            "Epoch 3393/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 482.8471 - acc: 0.9057 - val_loss: 483.0468 - val_acc: 0.7328\n",
            "Epoch 3394/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 482.8030 - acc: 0.9148 - val_loss: 482.9989 - val_acc: 0.7328\n",
            "Epoch 3395/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 482.7593 - acc: 0.8986 - val_loss: 482.9510 - val_acc: 0.7328\n",
            "Epoch 3396/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 482.7036 - acc: 0.9037 - val_loss: 482.9031 - val_acc: 0.7328\n",
            "Epoch 3397/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 482.6550 - acc: 0.9037 - val_loss: 482.8552 - val_acc: 0.7328\n",
            "Epoch 3398/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 482.6147 - acc: 0.9057 - val_loss: 482.8075 - val_acc: 0.7328\n",
            "Epoch 3399/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 482.5603 - acc: 0.8986 - val_loss: 482.7597 - val_acc: 0.7328\n",
            "Epoch 3400/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 482.5165 - acc: 0.9077 - val_loss: 482.7119 - val_acc: 0.7328\n",
            "Epoch 3401/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 482.4620 - acc: 0.8966 - val_loss: 482.6641 - val_acc: 0.7354\n",
            "Epoch 3402/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 482.4143 - acc: 0.9108 - val_loss: 482.6162 - val_acc: 0.7354\n",
            "Epoch 3403/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 482.3727 - acc: 0.9077 - val_loss: 482.5683 - val_acc: 0.7354\n",
            "Epoch 3404/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 482.3170 - acc: 0.9077 - val_loss: 482.5205 - val_acc: 0.7354\n",
            "Epoch 3405/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 482.2726 - acc: 0.9016 - val_loss: 482.4726 - val_acc: 0.7354\n",
            "Epoch 3406/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 482.2259 - acc: 0.9016 - val_loss: 482.4249 - val_acc: 0.7354\n",
            "Epoch 3407/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 482.1770 - acc: 0.9067 - val_loss: 482.3771 - val_acc: 0.7354\n",
            "Epoch 3408/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 482.1291 - acc: 0.9037 - val_loss: 482.3294 - val_acc: 0.7328\n",
            "Epoch 3409/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 482.0856 - acc: 0.9087 - val_loss: 482.2816 - val_acc: 0.7354\n",
            "Epoch 3410/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 482.0375 - acc: 0.9077 - val_loss: 482.2338 - val_acc: 0.7328\n",
            "Epoch 3411/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 481.9903 - acc: 0.9057 - val_loss: 482.1859 - val_acc: 0.7328\n",
            "Epoch 3412/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 481.9375 - acc: 0.9047 - val_loss: 482.1380 - val_acc: 0.7328\n",
            "Epoch 3413/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 481.8883 - acc: 0.9189 - val_loss: 482.0902 - val_acc: 0.7328\n",
            "Epoch 3414/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 481.8446 - acc: 0.9057 - val_loss: 482.0424 - val_acc: 0.7328\n",
            "Epoch 3415/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 481.8016 - acc: 0.9057 - val_loss: 481.9946 - val_acc: 0.7328\n",
            "Epoch 3416/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 481.7521 - acc: 0.9138 - val_loss: 481.9469 - val_acc: 0.7328\n",
            "Epoch 3417/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 481.7008 - acc: 0.9016 - val_loss: 481.8991 - val_acc: 0.7328\n",
            "Epoch 3418/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 481.6584 - acc: 0.8945 - val_loss: 481.8514 - val_acc: 0.7328\n",
            "Epoch 3419/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 481.5959 - acc: 0.9158 - val_loss: 481.8034 - val_acc: 0.7328\n",
            "Epoch 3420/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 481.5601 - acc: 0.9026 - val_loss: 481.7556 - val_acc: 0.7354\n",
            "Epoch 3421/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 481.5106 - acc: 0.9087 - val_loss: 481.7081 - val_acc: 0.7354\n",
            "Epoch 3422/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 481.4659 - acc: 0.9037 - val_loss: 481.6604 - val_acc: 0.7354\n",
            "Epoch 3423/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 481.4119 - acc: 0.9067 - val_loss: 481.6125 - val_acc: 0.7354\n",
            "Epoch 3424/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 481.3712 - acc: 0.8986 - val_loss: 481.5648 - val_acc: 0.7354\n",
            "Epoch 3425/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 481.3130 - acc: 0.9219 - val_loss: 481.5170 - val_acc: 0.7354\n",
            "Epoch 3426/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 481.2757 - acc: 0.8945 - val_loss: 481.4693 - val_acc: 0.7354\n",
            "Epoch 3427/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 481.2212 - acc: 0.8986 - val_loss: 481.4215 - val_acc: 0.7354\n",
            "Epoch 3428/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 481.1721 - acc: 0.9118 - val_loss: 481.3738 - val_acc: 0.7354\n",
            "Epoch 3429/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 481.1327 - acc: 0.9006 - val_loss: 481.3260 - val_acc: 0.7328\n",
            "Epoch 3430/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 481.0800 - acc: 0.9087 - val_loss: 481.2784 - val_acc: 0.7328\n",
            "Epoch 3431/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 481.0339 - acc: 0.9047 - val_loss: 481.2307 - val_acc: 0.7354\n",
            "Epoch 3432/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 480.9878 - acc: 0.8955 - val_loss: 481.1830 - val_acc: 0.7354\n",
            "Epoch 3433/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 480.9355 - acc: 0.9148 - val_loss: 481.1353 - val_acc: 0.7328\n",
            "Epoch 3434/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 480.8833 - acc: 0.9097 - val_loss: 481.0876 - val_acc: 0.7328\n",
            "Epoch 3435/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 480.8402 - acc: 0.9057 - val_loss: 481.0400 - val_acc: 0.7354\n",
            "Epoch 3436/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 480.7948 - acc: 0.9047 - val_loss: 480.9923 - val_acc: 0.7354\n",
            "Epoch 3437/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 480.7504 - acc: 0.9037 - val_loss: 480.9447 - val_acc: 0.7354\n",
            "Epoch 3438/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 480.6932 - acc: 0.9067 - val_loss: 480.8968 - val_acc: 0.7354\n",
            "Epoch 3439/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 480.6505 - acc: 0.9057 - val_loss: 480.8491 - val_acc: 0.7354\n",
            "Epoch 3440/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 480.6132 - acc: 0.8854 - val_loss: 480.8013 - val_acc: 0.7354\n",
            "Epoch 3441/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 480.5528 - acc: 0.9118 - val_loss: 480.7537 - val_acc: 0.7354\n",
            "Epoch 3442/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 480.5114 - acc: 0.8915 - val_loss: 480.7059 - val_acc: 0.7354\n",
            "Epoch 3443/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 480.4566 - acc: 0.9047 - val_loss: 480.6583 - val_acc: 0.7354\n",
            "Epoch 3444/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 480.4142 - acc: 0.9087 - val_loss: 480.6107 - val_acc: 0.7354\n",
            "Epoch 3445/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 480.3602 - acc: 0.8976 - val_loss: 480.5630 - val_acc: 0.7354\n",
            "Epoch 3446/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 480.3122 - acc: 0.9128 - val_loss: 480.5153 - val_acc: 0.7354\n",
            "Epoch 3447/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 480.2663 - acc: 0.9006 - val_loss: 480.4676 - val_acc: 0.7328\n",
            "Epoch 3448/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 480.2198 - acc: 0.9016 - val_loss: 480.4200 - val_acc: 0.7328\n",
            "Epoch 3449/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 480.1692 - acc: 0.9108 - val_loss: 480.3723 - val_acc: 0.7354\n",
            "Epoch 3450/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 480.1257 - acc: 0.9026 - val_loss: 480.3247 - val_acc: 0.7354\n",
            "Epoch 3451/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 480.0835 - acc: 0.9006 - val_loss: 480.2770 - val_acc: 0.7354\n",
            "Epoch 3452/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 480.0312 - acc: 0.9016 - val_loss: 480.2295 - val_acc: 0.7354\n",
            "Epoch 3453/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 479.9852 - acc: 0.9067 - val_loss: 480.1819 - val_acc: 0.7354\n",
            "Epoch 3454/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 479.9428 - acc: 0.8966 - val_loss: 480.1343 - val_acc: 0.7354\n",
            "Epoch 3455/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 479.8899 - acc: 0.8966 - val_loss: 480.0867 - val_acc: 0.7354\n",
            "Epoch 3456/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 479.8382 - acc: 0.9047 - val_loss: 480.0391 - val_acc: 0.7354\n",
            "Epoch 3457/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 479.7963 - acc: 0.8955 - val_loss: 479.9916 - val_acc: 0.7328\n",
            "Epoch 3458/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 479.7415 - acc: 0.9026 - val_loss: 479.9441 - val_acc: 0.7328\n",
            "Epoch 3459/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 479.6932 - acc: 0.9108 - val_loss: 479.8965 - val_acc: 0.7328\n",
            "Epoch 3460/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 479.6438 - acc: 0.9057 - val_loss: 479.8487 - val_acc: 0.7328\n",
            "Epoch 3461/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 479.5940 - acc: 0.9148 - val_loss: 479.8011 - val_acc: 0.7328\n",
            "Epoch 3462/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 479.5509 - acc: 0.9087 - val_loss: 479.7534 - val_acc: 0.7354\n",
            "Epoch 3463/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 479.5032 - acc: 0.9026 - val_loss: 479.7059 - val_acc: 0.7354\n",
            "Epoch 3464/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 479.4594 - acc: 0.8986 - val_loss: 479.6582 - val_acc: 0.7354\n",
            "Epoch 3465/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 479.4053 - acc: 0.9128 - val_loss: 479.6106 - val_acc: 0.7354\n",
            "Epoch 3466/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 479.3641 - acc: 0.9037 - val_loss: 479.5632 - val_acc: 0.7354\n",
            "Epoch 3467/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 479.3164 - acc: 0.8976 - val_loss: 479.5155 - val_acc: 0.7354\n",
            "Epoch 3468/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 479.2725 - acc: 0.9037 - val_loss: 479.4681 - val_acc: 0.7354\n",
            "Epoch 3469/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 479.2146 - acc: 0.9067 - val_loss: 479.4206 - val_acc: 0.7328\n",
            "Epoch 3470/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 479.1728 - acc: 0.8915 - val_loss: 479.3731 - val_acc: 0.7354\n",
            "Epoch 3471/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 479.1230 - acc: 0.9138 - val_loss: 479.3255 - val_acc: 0.7328\n",
            "Epoch 3472/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 479.0808 - acc: 0.9047 - val_loss: 479.2780 - val_acc: 0.7328\n",
            "Epoch 3473/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 479.0277 - acc: 0.9067 - val_loss: 479.2305 - val_acc: 0.7354\n",
            "Epoch 3474/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 478.9794 - acc: 0.9057 - val_loss: 479.1829 - val_acc: 0.7328\n",
            "Epoch 3475/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 478.9309 - acc: 0.9057 - val_loss: 479.1355 - val_acc: 0.7328\n",
            "Epoch 3476/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 478.8818 - acc: 0.9260 - val_loss: 479.0881 - val_acc: 0.7328\n",
            "Epoch 3477/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 478.8399 - acc: 0.9016 - val_loss: 479.0406 - val_acc: 0.7354\n",
            "Epoch 3478/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 478.7925 - acc: 0.9168 - val_loss: 478.9932 - val_acc: 0.7328\n",
            "Epoch 3479/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 478.7390 - acc: 0.9097 - val_loss: 478.9456 - val_acc: 0.7328\n",
            "Epoch 3480/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 478.6964 - acc: 0.9108 - val_loss: 478.8981 - val_acc: 0.7328\n",
            "Epoch 3481/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 478.6518 - acc: 0.8905 - val_loss: 478.8506 - val_acc: 0.7354\n",
            "Epoch 3482/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 478.6036 - acc: 0.9108 - val_loss: 478.8031 - val_acc: 0.7354\n",
            "Epoch 3483/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 478.5565 - acc: 0.9047 - val_loss: 478.7554 - val_acc: 0.7328\n",
            "Epoch 3484/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 478.5104 - acc: 0.9057 - val_loss: 478.7079 - val_acc: 0.7328\n",
            "Epoch 3485/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 478.4584 - acc: 0.9118 - val_loss: 478.6604 - val_acc: 0.7328\n",
            "Epoch 3486/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 478.4142 - acc: 0.9077 - val_loss: 478.6129 - val_acc: 0.7354\n",
            "Epoch 3487/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 478.3684 - acc: 0.9006 - val_loss: 478.5655 - val_acc: 0.7354\n",
            "Epoch 3488/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 478.3250 - acc: 0.8945 - val_loss: 478.5179 - val_acc: 0.7354\n",
            "Epoch 3489/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 478.2703 - acc: 0.9097 - val_loss: 478.4705 - val_acc: 0.7354\n",
            "Epoch 3490/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 478.2212 - acc: 0.9037 - val_loss: 478.4230 - val_acc: 0.7354\n",
            "Epoch 3491/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 478.1698 - acc: 0.9087 - val_loss: 478.3755 - val_acc: 0.7354\n",
            "Epoch 3492/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 478.1351 - acc: 0.8976 - val_loss: 478.3280 - val_acc: 0.7328\n",
            "Epoch 3493/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 478.0767 - acc: 0.8986 - val_loss: 478.2806 - val_acc: 0.7328\n",
            "Epoch 3494/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 478.0305 - acc: 0.8945 - val_loss: 478.2332 - val_acc: 0.7302\n",
            "Epoch 3495/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 477.9889 - acc: 0.8986 - val_loss: 478.1857 - val_acc: 0.7328\n",
            "Epoch 3496/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 477.9310 - acc: 0.9178 - val_loss: 478.1384 - val_acc: 0.7354\n",
            "Epoch 3497/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 477.8898 - acc: 0.8996 - val_loss: 478.0910 - val_acc: 0.7354\n",
            "Epoch 3498/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 477.8448 - acc: 0.9006 - val_loss: 478.0436 - val_acc: 0.7354\n",
            "Epoch 3499/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 477.8008 - acc: 0.8986 - val_loss: 477.9962 - val_acc: 0.7354\n",
            "Epoch 3500/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 477.7459 - acc: 0.9108 - val_loss: 477.9487 - val_acc: 0.7354\n",
            "Epoch 3501/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 477.7017 - acc: 0.9128 - val_loss: 477.9013 - val_acc: 0.7354\n",
            "Epoch 3502/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 477.6506 - acc: 0.9037 - val_loss: 477.8539 - val_acc: 0.7354\n",
            "Epoch 3503/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 477.6085 - acc: 0.9047 - val_loss: 477.8064 - val_acc: 0.7354\n",
            "Epoch 3504/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 477.5549 - acc: 0.9178 - val_loss: 477.7591 - val_acc: 0.7354\n",
            "Epoch 3505/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 477.5166 - acc: 0.8996 - val_loss: 477.7117 - val_acc: 0.7354\n",
            "Epoch 3506/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 477.4663 - acc: 0.9026 - val_loss: 477.6643 - val_acc: 0.7354\n",
            "Epoch 3507/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 477.4211 - acc: 0.9037 - val_loss: 477.6172 - val_acc: 0.7354\n",
            "Epoch 3508/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 477.3666 - acc: 0.9077 - val_loss: 477.5697 - val_acc: 0.7354\n",
            "Epoch 3509/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 477.3148 - acc: 0.9118 - val_loss: 477.5223 - val_acc: 0.7354\n",
            "Epoch 3510/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 477.2729 - acc: 0.9016 - val_loss: 477.4748 - val_acc: 0.7354\n",
            "Epoch 3511/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 477.2272 - acc: 0.9077 - val_loss: 477.4274 - val_acc: 0.7354\n",
            "Epoch 3512/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 477.1803 - acc: 0.9077 - val_loss: 477.3800 - val_acc: 0.7354\n",
            "Epoch 3513/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 477.1367 - acc: 0.9097 - val_loss: 477.3327 - val_acc: 0.7354\n",
            "Epoch 3514/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 477.0852 - acc: 0.9016 - val_loss: 477.2854 - val_acc: 0.7354\n",
            "Epoch 3515/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 477.0353 - acc: 0.9077 - val_loss: 477.2380 - val_acc: 0.7354\n",
            "Epoch 3516/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 476.9847 - acc: 0.9087 - val_loss: 477.1907 - val_acc: 0.7354\n",
            "Epoch 3517/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 476.9408 - acc: 0.9026 - val_loss: 477.1433 - val_acc: 0.7354\n",
            "Epoch 3518/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 476.8945 - acc: 0.9077 - val_loss: 477.0959 - val_acc: 0.7354\n",
            "Epoch 3519/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 476.8464 - acc: 0.9087 - val_loss: 477.0486 - val_acc: 0.7354\n",
            "Epoch 3520/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 476.7981 - acc: 0.8955 - val_loss: 477.0013 - val_acc: 0.7354\n",
            "Epoch 3521/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 476.7528 - acc: 0.9168 - val_loss: 476.9540 - val_acc: 0.7354\n",
            "Epoch 3522/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 476.7062 - acc: 0.9118 - val_loss: 476.9065 - val_acc: 0.7354\n",
            "Epoch 3523/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 476.6583 - acc: 0.9118 - val_loss: 476.8592 - val_acc: 0.7354\n",
            "Epoch 3524/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 476.6099 - acc: 0.9118 - val_loss: 476.8118 - val_acc: 0.7354\n",
            "Epoch 3525/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 476.5666 - acc: 0.9077 - val_loss: 476.7646 - val_acc: 0.7354\n",
            "Epoch 3526/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 476.5122 - acc: 0.9108 - val_loss: 476.7170 - val_acc: 0.7354\n",
            "Epoch 3527/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 476.4682 - acc: 0.9087 - val_loss: 476.6699 - val_acc: 0.7354\n",
            "Epoch 3528/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 476.4184 - acc: 0.9016 - val_loss: 476.6225 - val_acc: 0.7354\n",
            "Epoch 3529/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 476.3736 - acc: 0.9006 - val_loss: 476.5751 - val_acc: 0.7354\n",
            "Epoch 3530/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 476.3285 - acc: 0.8955 - val_loss: 476.5279 - val_acc: 0.7354\n",
            "Epoch 3531/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 476.2755 - acc: 0.9138 - val_loss: 476.4806 - val_acc: 0.7354\n",
            "Epoch 3532/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 476.2296 - acc: 0.9087 - val_loss: 476.4331 - val_acc: 0.7354\n",
            "Epoch 3533/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 476.1828 - acc: 0.9077 - val_loss: 476.3858 - val_acc: 0.7354\n",
            "Epoch 3534/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 476.1343 - acc: 0.9148 - val_loss: 476.3387 - val_acc: 0.7354\n",
            "Epoch 3535/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 476.0894 - acc: 0.9016 - val_loss: 476.2914 - val_acc: 0.7354\n",
            "Epoch 3536/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 476.0427 - acc: 0.9108 - val_loss: 476.2441 - val_acc: 0.7354\n",
            "Epoch 3537/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 475.9978 - acc: 0.9026 - val_loss: 476.1967 - val_acc: 0.7354\n",
            "Epoch 3538/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 475.9449 - acc: 0.9067 - val_loss: 476.1493 - val_acc: 0.7354\n",
            "Epoch 3539/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 475.8984 - acc: 0.9118 - val_loss: 476.1021 - val_acc: 0.7354\n",
            "Epoch 3540/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 475.8485 - acc: 0.9077 - val_loss: 476.0549 - val_acc: 0.7354\n",
            "Epoch 3541/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 475.8064 - acc: 0.9118 - val_loss: 476.0077 - val_acc: 0.7354\n",
            "Epoch 3542/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 475.7613 - acc: 0.9067 - val_loss: 475.9603 - val_acc: 0.7354\n",
            "Epoch 3543/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 475.7058 - acc: 0.9138 - val_loss: 475.9130 - val_acc: 0.7354\n",
            "Epoch 3544/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 475.6637 - acc: 0.9037 - val_loss: 475.8657 - val_acc: 0.7354\n",
            "Epoch 3545/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 475.6177 - acc: 0.9168 - val_loss: 475.8185 - val_acc: 0.7354\n",
            "Epoch 3546/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 475.5746 - acc: 0.9026 - val_loss: 475.7713 - val_acc: 0.7354\n",
            "Epoch 3547/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 475.5236 - acc: 0.9026 - val_loss: 475.7239 - val_acc: 0.7354\n",
            "Epoch 3548/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 475.4724 - acc: 0.9067 - val_loss: 475.6766 - val_acc: 0.7354\n",
            "Epoch 3549/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 475.4264 - acc: 0.9057 - val_loss: 475.6295 - val_acc: 0.7354\n",
            "Epoch 3550/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 475.3794 - acc: 0.9026 - val_loss: 475.5822 - val_acc: 0.7354\n",
            "Epoch 3551/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 475.3327 - acc: 0.9016 - val_loss: 475.5350 - val_acc: 0.7354\n",
            "Epoch 3552/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 475.2833 - acc: 0.9087 - val_loss: 475.4878 - val_acc: 0.7354\n",
            "Epoch 3553/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 475.2343 - acc: 0.9108 - val_loss: 475.4405 - val_acc: 0.7354\n",
            "Epoch 3554/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 475.1863 - acc: 0.9047 - val_loss: 475.3932 - val_acc: 0.7354\n",
            "Epoch 3555/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 475.1445 - acc: 0.9006 - val_loss: 475.3461 - val_acc: 0.7354\n",
            "Epoch 3556/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 475.0959 - acc: 0.9077 - val_loss: 475.2989 - val_acc: 0.7354\n",
            "Epoch 3557/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 475.0534 - acc: 0.8996 - val_loss: 475.2516 - val_acc: 0.7354\n",
            "Epoch 3558/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 475.0000 - acc: 0.9057 - val_loss: 475.2043 - val_acc: 0.7354\n",
            "Epoch 3559/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 474.9580 - acc: 0.9016 - val_loss: 475.1573 - val_acc: 0.7354\n",
            "Epoch 3560/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 474.9102 - acc: 0.9097 - val_loss: 475.1099 - val_acc: 0.7381\n",
            "Epoch 3561/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 474.8580 - acc: 0.9097 - val_loss: 475.0628 - val_acc: 0.7381\n",
            "Epoch 3562/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 474.8056 - acc: 0.9158 - val_loss: 475.0155 - val_acc: 0.7381\n",
            "Epoch 3563/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 474.7625 - acc: 0.9087 - val_loss: 474.9683 - val_acc: 0.7354\n",
            "Epoch 3564/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 474.7189 - acc: 0.9067 - val_loss: 474.9211 - val_acc: 0.7354\n",
            "Epoch 3565/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 474.6754 - acc: 0.8945 - val_loss: 474.8740 - val_acc: 0.7354\n",
            "Epoch 3566/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 474.6254 - acc: 0.9016 - val_loss: 474.8268 - val_acc: 0.7354\n",
            "Epoch 3567/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 474.5743 - acc: 0.9067 - val_loss: 474.7797 - val_acc: 0.7354\n",
            "Epoch 3568/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 474.5302 - acc: 0.9037 - val_loss: 474.7324 - val_acc: 0.7354\n",
            "Epoch 3569/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 474.4888 - acc: 0.8976 - val_loss: 474.6854 - val_acc: 0.7354\n",
            "Epoch 3570/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 474.4380 - acc: 0.9148 - val_loss: 474.6381 - val_acc: 0.7354\n",
            "Epoch 3571/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 474.3920 - acc: 0.9118 - val_loss: 474.5910 - val_acc: 0.7354\n",
            "Epoch 3572/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 474.3347 - acc: 0.9168 - val_loss: 474.5439 - val_acc: 0.7354\n",
            "Epoch 3573/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 474.2869 - acc: 0.9016 - val_loss: 474.4967 - val_acc: 0.7354\n",
            "Epoch 3574/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 474.2446 - acc: 0.9047 - val_loss: 474.4494 - val_acc: 0.7354\n",
            "Epoch 3575/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 474.2013 - acc: 0.9077 - val_loss: 474.4024 - val_acc: 0.7354\n",
            "Epoch 3576/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 474.1539 - acc: 0.9026 - val_loss: 474.3553 - val_acc: 0.7354\n",
            "Epoch 3577/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 474.1015 - acc: 0.9097 - val_loss: 474.3081 - val_acc: 0.7354\n",
            "Epoch 3578/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 474.0512 - acc: 0.9067 - val_loss: 474.2607 - val_acc: 0.7381\n",
            "Epoch 3579/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 474.0068 - acc: 0.9097 - val_loss: 474.2136 - val_acc: 0.7354\n",
            "Epoch 3580/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 473.9622 - acc: 0.9026 - val_loss: 474.1665 - val_acc: 0.7381\n",
            "Epoch 3581/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 473.9214 - acc: 0.9016 - val_loss: 474.1195 - val_acc: 0.7381\n",
            "Epoch 3582/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 473.8710 - acc: 0.9016 - val_loss: 474.0723 - val_acc: 0.7354\n",
            "Epoch 3583/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 473.8175 - acc: 0.9138 - val_loss: 474.0252 - val_acc: 0.7354\n",
            "Epoch 3584/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 473.7721 - acc: 0.9108 - val_loss: 473.9781 - val_acc: 0.7354\n",
            "Epoch 3585/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 473.7258 - acc: 0.9108 - val_loss: 473.9309 - val_acc: 0.7354\n",
            "Epoch 3586/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 473.6837 - acc: 0.8996 - val_loss: 473.8837 - val_acc: 0.7354\n",
            "Epoch 3587/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 473.6306 - acc: 0.9057 - val_loss: 473.8366 - val_acc: 0.7354\n",
            "Epoch 3588/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 473.5911 - acc: 0.9118 - val_loss: 473.7896 - val_acc: 0.7381\n",
            "Epoch 3589/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 473.5438 - acc: 0.9047 - val_loss: 473.7425 - val_acc: 0.7354\n",
            "Epoch 3590/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 473.4893 - acc: 0.9118 - val_loss: 473.6954 - val_acc: 0.7354\n",
            "Epoch 3591/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 473.4416 - acc: 0.9158 - val_loss: 473.6482 - val_acc: 0.7354\n",
            "Epoch 3592/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 473.3969 - acc: 0.9057 - val_loss: 473.6012 - val_acc: 0.7354\n",
            "Epoch 3593/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 473.3514 - acc: 0.9037 - val_loss: 473.5538 - val_acc: 0.7354\n",
            "Epoch 3594/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 473.3052 - acc: 0.9097 - val_loss: 473.5068 - val_acc: 0.7354\n",
            "Epoch 3595/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 473.2577 - acc: 0.9057 - val_loss: 473.4597 - val_acc: 0.7354\n",
            "Epoch 3596/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 473.2123 - acc: 0.8935 - val_loss: 473.4126 - val_acc: 0.7354\n",
            "Epoch 3597/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 473.1607 - acc: 0.9128 - val_loss: 473.3658 - val_acc: 0.7354\n",
            "Epoch 3598/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 473.1239 - acc: 0.9057 - val_loss: 473.3188 - val_acc: 0.7354\n",
            "Epoch 3599/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 473.0685 - acc: 0.9118 - val_loss: 473.2716 - val_acc: 0.7354\n",
            "Epoch 3600/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 473.0169 - acc: 0.9168 - val_loss: 473.2245 - val_acc: 0.7381\n",
            "Epoch 3601/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 472.9674 - acc: 0.9158 - val_loss: 473.1775 - val_acc: 0.7381\n",
            "Epoch 3602/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 472.9300 - acc: 0.9108 - val_loss: 473.1306 - val_acc: 0.7381\n",
            "Epoch 3603/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 472.8851 - acc: 0.9168 - val_loss: 473.0834 - val_acc: 0.7381\n",
            "Epoch 3604/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 472.8301 - acc: 0.9077 - val_loss: 473.0365 - val_acc: 0.7381\n",
            "Epoch 3605/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 472.7832 - acc: 0.9097 - val_loss: 472.9896 - val_acc: 0.7381\n",
            "Epoch 3606/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 472.7396 - acc: 0.9077 - val_loss: 472.9425 - val_acc: 0.7381\n",
            "Epoch 3607/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 472.6931 - acc: 0.9087 - val_loss: 472.8954 - val_acc: 0.7381\n",
            "Epoch 3608/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 472.6404 - acc: 0.9128 - val_loss: 472.8485 - val_acc: 0.7381\n",
            "Epoch 3609/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 472.5967 - acc: 0.9067 - val_loss: 472.8015 - val_acc: 0.7381\n",
            "Epoch 3610/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 472.5489 - acc: 0.9037 - val_loss: 472.7546 - val_acc: 0.7381\n",
            "Epoch 3611/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 472.5031 - acc: 0.9026 - val_loss: 472.7075 - val_acc: 0.7381\n",
            "Epoch 3612/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 472.4531 - acc: 0.9097 - val_loss: 472.6605 - val_acc: 0.7381\n",
            "Epoch 3613/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 472.4116 - acc: 0.8996 - val_loss: 472.6137 - val_acc: 0.7381\n",
            "Epoch 3614/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 472.3610 - acc: 0.9057 - val_loss: 472.5666 - val_acc: 0.7381\n",
            "Epoch 3615/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 472.3179 - acc: 0.9097 - val_loss: 472.5197 - val_acc: 0.7381\n",
            "Epoch 3616/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 472.2693 - acc: 0.9138 - val_loss: 472.4727 - val_acc: 0.7381\n",
            "Epoch 3617/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 472.2187 - acc: 0.9057 - val_loss: 472.4257 - val_acc: 0.7381\n",
            "Epoch 3618/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 472.1832 - acc: 0.8874 - val_loss: 472.3787 - val_acc: 0.7381\n",
            "Epoch 3619/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 472.1305 - acc: 0.9097 - val_loss: 472.3317 - val_acc: 0.7381\n",
            "Epoch 3620/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 472.0874 - acc: 0.8976 - val_loss: 472.2849 - val_acc: 0.7381\n",
            "Epoch 3621/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 472.0344 - acc: 0.9057 - val_loss: 472.2381 - val_acc: 0.7381\n",
            "Epoch 3622/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 471.9849 - acc: 0.9057 - val_loss: 472.1911 - val_acc: 0.7381\n",
            "Epoch 3623/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 471.9325 - acc: 0.9148 - val_loss: 472.1440 - val_acc: 0.7381\n",
            "Epoch 3624/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 471.8931 - acc: 0.9087 - val_loss: 472.0971 - val_acc: 0.7381\n",
            "Epoch 3625/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 471.8461 - acc: 0.9047 - val_loss: 472.0502 - val_acc: 0.7381\n",
            "Epoch 3626/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 471.7980 - acc: 0.9067 - val_loss: 472.0033 - val_acc: 0.7381\n",
            "Epoch 3627/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 471.7574 - acc: 0.9016 - val_loss: 471.9563 - val_acc: 0.7381\n",
            "Epoch 3628/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 471.7041 - acc: 0.8986 - val_loss: 471.9096 - val_acc: 0.7354\n",
            "Epoch 3629/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 471.6637 - acc: 0.8976 - val_loss: 471.8627 - val_acc: 0.7354\n",
            "Epoch 3630/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 471.6101 - acc: 0.9118 - val_loss: 471.8158 - val_acc: 0.7381\n",
            "Epoch 3631/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 471.5648 - acc: 0.9118 - val_loss: 471.7689 - val_acc: 0.7381\n",
            "Epoch 3632/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 471.5116 - acc: 0.9148 - val_loss: 471.7220 - val_acc: 0.7381\n",
            "Epoch 3633/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 471.4721 - acc: 0.9006 - val_loss: 471.6751 - val_acc: 0.7381\n",
            "Epoch 3634/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 471.4248 - acc: 0.9108 - val_loss: 471.6284 - val_acc: 0.7381\n",
            "Epoch 3635/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 471.3789 - acc: 0.9067 - val_loss: 471.5815 - val_acc: 0.7381\n",
            "Epoch 3636/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 471.3298 - acc: 0.8986 - val_loss: 471.5345 - val_acc: 0.7381\n",
            "Epoch 3637/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 471.2793 - acc: 0.9128 - val_loss: 471.4876 - val_acc: 0.7381\n",
            "Epoch 3638/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 471.2314 - acc: 0.9057 - val_loss: 471.4409 - val_acc: 0.7381\n",
            "Epoch 3639/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 471.1887 - acc: 0.9097 - val_loss: 471.3942 - val_acc: 0.7381\n",
            "Epoch 3640/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 471.1436 - acc: 0.8955 - val_loss: 471.3473 - val_acc: 0.7381\n",
            "Epoch 3641/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 471.0864 - acc: 0.9189 - val_loss: 471.3005 - val_acc: 0.7381\n",
            "Epoch 3642/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 471.0469 - acc: 0.9057 - val_loss: 471.2536 - val_acc: 0.7381\n",
            "Epoch 3643/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 471.0014 - acc: 0.8976 - val_loss: 471.2068 - val_acc: 0.7381\n",
            "Epoch 3644/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 470.9537 - acc: 0.9128 - val_loss: 471.1598 - val_acc: 0.7381\n",
            "Epoch 3645/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 470.9043 - acc: 0.9118 - val_loss: 471.1129 - val_acc: 0.7381\n",
            "Epoch 3646/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 470.8625 - acc: 0.9026 - val_loss: 471.0660 - val_acc: 0.7381\n",
            "Epoch 3647/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 470.8143 - acc: 0.9097 - val_loss: 471.0191 - val_acc: 0.7381\n",
            "Epoch 3648/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 470.7648 - acc: 0.9037 - val_loss: 470.9723 - val_acc: 0.7381\n",
            "Epoch 3649/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 470.7186 - acc: 0.9057 - val_loss: 470.9254 - val_acc: 0.7381\n",
            "Epoch 3650/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 470.6744 - acc: 0.9006 - val_loss: 470.8787 - val_acc: 0.7381\n",
            "Epoch 3651/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 470.6281 - acc: 0.9037 - val_loss: 470.8319 - val_acc: 0.7381\n",
            "Epoch 3652/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 470.5733 - acc: 0.9178 - val_loss: 470.7850 - val_acc: 0.7381\n",
            "Epoch 3653/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 470.5330 - acc: 0.8976 - val_loss: 470.7383 - val_acc: 0.7381\n",
            "Epoch 3654/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 470.4822 - acc: 0.9118 - val_loss: 470.6915 - val_acc: 0.7381\n",
            "Epoch 3655/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 470.4421 - acc: 0.9037 - val_loss: 470.6448 - val_acc: 0.7381\n",
            "Epoch 3656/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 470.3940 - acc: 0.9047 - val_loss: 470.5980 - val_acc: 0.7381\n",
            "Epoch 3657/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 470.3460 - acc: 0.9077 - val_loss: 470.5513 - val_acc: 0.7381\n",
            "Epoch 3658/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 470.3053 - acc: 0.8976 - val_loss: 470.5046 - val_acc: 0.7381\n",
            "Epoch 3659/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 470.2472 - acc: 0.9037 - val_loss: 470.4578 - val_acc: 0.7381\n",
            "Epoch 3660/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 470.2082 - acc: 0.8955 - val_loss: 470.4109 - val_acc: 0.7381\n",
            "Epoch 3661/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 470.1551 - acc: 0.9168 - val_loss: 470.3641 - val_acc: 0.7381\n",
            "Epoch 3662/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 470.1099 - acc: 0.9067 - val_loss: 470.3173 - val_acc: 0.7381\n",
            "Epoch 3663/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 470.0663 - acc: 0.9026 - val_loss: 470.2706 - val_acc: 0.7381\n",
            "Epoch 3664/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 470.0182 - acc: 0.9097 - val_loss: 470.2237 - val_acc: 0.7381\n",
            "Epoch 3665/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 469.9663 - acc: 0.9067 - val_loss: 470.1769 - val_acc: 0.7381\n",
            "Epoch 3666/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 469.9177 - acc: 0.9168 - val_loss: 470.1302 - val_acc: 0.7381\n",
            "Epoch 3667/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 469.8814 - acc: 0.9158 - val_loss: 470.0834 - val_acc: 0.7381\n",
            "Epoch 3668/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 469.8262 - acc: 0.9209 - val_loss: 470.0367 - val_acc: 0.7381\n",
            "Epoch 3669/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 469.7878 - acc: 0.9026 - val_loss: 469.9898 - val_acc: 0.7381\n",
            "Epoch 3670/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 469.7350 - acc: 0.9067 - val_loss: 469.9430 - val_acc: 0.7381\n",
            "Epoch 3671/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 469.6908 - acc: 0.8966 - val_loss: 469.8964 - val_acc: 0.7381\n",
            "Epoch 3672/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 469.6408 - acc: 0.9138 - val_loss: 469.8497 - val_acc: 0.7381\n",
            "Epoch 3673/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 469.6011 - acc: 0.9097 - val_loss: 469.8030 - val_acc: 0.7381\n",
            "Epoch 3674/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 469.5494 - acc: 0.9087 - val_loss: 469.7562 - val_acc: 0.7381\n",
            "Epoch 3675/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 469.5039 - acc: 0.9037 - val_loss: 469.7094 - val_acc: 0.7381\n",
            "Epoch 3676/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 469.4558 - acc: 0.9057 - val_loss: 469.6626 - val_acc: 0.7381\n",
            "Epoch 3677/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 469.4085 - acc: 0.9087 - val_loss: 469.6159 - val_acc: 0.7381\n",
            "Epoch 3678/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 469.3634 - acc: 0.9047 - val_loss: 469.5692 - val_acc: 0.7381\n",
            "Epoch 3679/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 469.3109 - acc: 0.9148 - val_loss: 469.5225 - val_acc: 0.7381\n",
            "Epoch 3680/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 469.2699 - acc: 0.9108 - val_loss: 469.4757 - val_acc: 0.7381\n",
            "Epoch 3681/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 469.2192 - acc: 0.9097 - val_loss: 469.4290 - val_acc: 0.7381\n",
            "Epoch 3682/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 469.1751 - acc: 0.9097 - val_loss: 469.3824 - val_acc: 0.7381\n",
            "Epoch 3683/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 469.1299 - acc: 0.9037 - val_loss: 469.3357 - val_acc: 0.7381\n",
            "Epoch 3684/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 469.0814 - acc: 0.9108 - val_loss: 469.2892 - val_acc: 0.7381\n",
            "Epoch 3685/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 469.0363 - acc: 0.9158 - val_loss: 469.2424 - val_acc: 0.7381\n",
            "Epoch 3686/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 468.9877 - acc: 0.9128 - val_loss: 469.1958 - val_acc: 0.7381\n",
            "Epoch 3687/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 468.9369 - acc: 0.9168 - val_loss: 469.1490 - val_acc: 0.7381\n",
            "Epoch 3688/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 468.9037 - acc: 0.8945 - val_loss: 469.1022 - val_acc: 0.7381\n",
            "Epoch 3689/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 468.8512 - acc: 0.9077 - val_loss: 469.0555 - val_acc: 0.7381\n",
            "Epoch 3690/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 468.8017 - acc: 0.9067 - val_loss: 469.0089 - val_acc: 0.7381\n",
            "Epoch 3691/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 468.7548 - acc: 0.9128 - val_loss: 468.9621 - val_acc: 0.7381\n",
            "Epoch 3692/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 468.7080 - acc: 0.9047 - val_loss: 468.9153 - val_acc: 0.7381\n",
            "Epoch 3693/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 468.6608 - acc: 0.9087 - val_loss: 468.8686 - val_acc: 0.7381\n",
            "Epoch 3694/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 468.6136 - acc: 0.9138 - val_loss: 468.8220 - val_acc: 0.7381\n",
            "Epoch 3695/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 468.5672 - acc: 0.9057 - val_loss: 468.7753 - val_acc: 0.7381\n",
            "Epoch 3696/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 468.5236 - acc: 0.8996 - val_loss: 468.7287 - val_acc: 0.7381\n",
            "Epoch 3697/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 468.4748 - acc: 0.8986 - val_loss: 468.6821 - val_acc: 0.7381\n",
            "Epoch 3698/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 468.4234 - acc: 0.9148 - val_loss: 468.6353 - val_acc: 0.7381\n",
            "Epoch 3699/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 468.3824 - acc: 0.9077 - val_loss: 468.5887 - val_acc: 0.7381\n",
            "Epoch 3700/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 468.3343 - acc: 0.9118 - val_loss: 468.5421 - val_acc: 0.7381\n",
            "Epoch 3701/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 468.2814 - acc: 0.9178 - val_loss: 468.4954 - val_acc: 0.7381\n",
            "Epoch 3702/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 468.2372 - acc: 0.9138 - val_loss: 468.4490 - val_acc: 0.7381\n",
            "Epoch 3703/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 468.1954 - acc: 0.9097 - val_loss: 468.4023 - val_acc: 0.7381\n",
            "Epoch 3704/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 468.1471 - acc: 0.9037 - val_loss: 468.3557 - val_acc: 0.7381\n",
            "Epoch 3705/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 468.1026 - acc: 0.9067 - val_loss: 468.3090 - val_acc: 0.7381\n",
            "Epoch 3706/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 468.0590 - acc: 0.9016 - val_loss: 468.2626 - val_acc: 0.7381\n",
            "Epoch 3707/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 468.0053 - acc: 0.9077 - val_loss: 468.2158 - val_acc: 0.7381\n",
            "Epoch 3708/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 467.9602 - acc: 0.9077 - val_loss: 468.1694 - val_acc: 0.7381\n",
            "Epoch 3709/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 467.9204 - acc: 0.9097 - val_loss: 468.1229 - val_acc: 0.7381\n",
            "Epoch 3710/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 467.8733 - acc: 0.9006 - val_loss: 468.0763 - val_acc: 0.7381\n",
            "Epoch 3711/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 467.8193 - acc: 0.9067 - val_loss: 468.0296 - val_acc: 0.7381\n",
            "Epoch 3712/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 467.7764 - acc: 0.9077 - val_loss: 467.9830 - val_acc: 0.7381\n",
            "Epoch 3713/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 467.7265 - acc: 0.9047 - val_loss: 467.9363 - val_acc: 0.7381\n",
            "Epoch 3714/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 467.6847 - acc: 0.9077 - val_loss: 467.8898 - val_acc: 0.7381\n",
            "Epoch 3715/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 467.6351 - acc: 0.9178 - val_loss: 467.8433 - val_acc: 0.7381\n",
            "Epoch 3716/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 467.5896 - acc: 0.9037 - val_loss: 467.7967 - val_acc: 0.7381\n",
            "Epoch 3717/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 467.5407 - acc: 0.9138 - val_loss: 467.7502 - val_acc: 0.7381\n",
            "Epoch 3718/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 467.4937 - acc: 0.9087 - val_loss: 467.7034 - val_acc: 0.7381\n",
            "Epoch 3719/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 467.4440 - acc: 0.9148 - val_loss: 467.6567 - val_acc: 0.7381\n",
            "Epoch 3720/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 467.4009 - acc: 0.9016 - val_loss: 467.6101 - val_acc: 0.7381\n",
            "Epoch 3721/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 467.3576 - acc: 0.9087 - val_loss: 467.5634 - val_acc: 0.7381\n",
            "Epoch 3722/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 467.3072 - acc: 0.9067 - val_loss: 467.5170 - val_acc: 0.7381\n",
            "Epoch 3723/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 467.2584 - acc: 0.9168 - val_loss: 467.4704 - val_acc: 0.7381\n",
            "Epoch 3724/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 467.2106 - acc: 0.9128 - val_loss: 467.4238 - val_acc: 0.7381\n",
            "Epoch 3725/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 467.1709 - acc: 0.9016 - val_loss: 467.3772 - val_acc: 0.7381\n",
            "Epoch 3726/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 467.1194 - acc: 0.9199 - val_loss: 467.3306 - val_acc: 0.7381\n",
            "Epoch 3727/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 467.0756 - acc: 0.8986 - val_loss: 467.2841 - val_acc: 0.7381\n",
            "Epoch 3728/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 467.0324 - acc: 0.9108 - val_loss: 467.2376 - val_acc: 0.7381\n",
            "Epoch 3729/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 466.9798 - acc: 0.9178 - val_loss: 467.1912 - val_acc: 0.7381\n",
            "Epoch 3730/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 466.9339 - acc: 0.9087 - val_loss: 467.1446 - val_acc: 0.7381\n",
            "Epoch 3731/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 466.8900 - acc: 0.9067 - val_loss: 467.0982 - val_acc: 0.7381\n",
            "Epoch 3732/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 466.8403 - acc: 0.9087 - val_loss: 467.0516 - val_acc: 0.7381\n",
            "Epoch 3733/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 466.7890 - acc: 0.9219 - val_loss: 467.0050 - val_acc: 0.7381\n",
            "Epoch 3734/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 466.7545 - acc: 0.9016 - val_loss: 466.9585 - val_acc: 0.7381\n",
            "Epoch 3735/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 466.6968 - acc: 0.9138 - val_loss: 466.9119 - val_acc: 0.7381\n",
            "Epoch 3736/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 466.6527 - acc: 0.9148 - val_loss: 466.8653 - val_acc: 0.7381\n",
            "Epoch 3737/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 466.6048 - acc: 0.9199 - val_loss: 466.8188 - val_acc: 0.7381\n",
            "Epoch 3738/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 466.5638 - acc: 0.9138 - val_loss: 466.7724 - val_acc: 0.7381\n",
            "Epoch 3739/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 466.5230 - acc: 0.8986 - val_loss: 466.7258 - val_acc: 0.7381\n",
            "Epoch 3740/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 466.4723 - acc: 0.9097 - val_loss: 466.6792 - val_acc: 0.7381\n",
            "Epoch 3741/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 466.4278 - acc: 0.8945 - val_loss: 466.6326 - val_acc: 0.7381\n",
            "Epoch 3742/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 466.3830 - acc: 0.9016 - val_loss: 466.5863 - val_acc: 0.7381\n",
            "Epoch 3743/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 466.3307 - acc: 0.9067 - val_loss: 466.5399 - val_acc: 0.7381\n",
            "Epoch 3744/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 466.2921 - acc: 0.8935 - val_loss: 466.4933 - val_acc: 0.7381\n",
            "Epoch 3745/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 466.2359 - acc: 0.9128 - val_loss: 466.4468 - val_acc: 0.7381\n",
            "Epoch 3746/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 466.1805 - acc: 0.9219 - val_loss: 466.4003 - val_acc: 0.7381\n",
            "Epoch 3747/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 466.1464 - acc: 0.8945 - val_loss: 466.3539 - val_acc: 0.7381\n",
            "Epoch 3748/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 466.0997 - acc: 0.9037 - val_loss: 466.3074 - val_acc: 0.7381\n",
            "Epoch 3749/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 466.0572 - acc: 0.9016 - val_loss: 466.2611 - val_acc: 0.7381\n",
            "Epoch 3750/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 466.0057 - acc: 0.9037 - val_loss: 466.2145 - val_acc: 0.7381\n",
            "Epoch 3751/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 465.9579 - acc: 0.9108 - val_loss: 466.1680 - val_acc: 0.7381\n",
            "Epoch 3752/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 465.9202 - acc: 0.8945 - val_loss: 466.1217 - val_acc: 0.7381\n",
            "Epoch 3753/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 465.8555 - acc: 0.9189 - val_loss: 466.0752 - val_acc: 0.7381\n",
            "Epoch 3754/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 465.8193 - acc: 0.9057 - val_loss: 466.0288 - val_acc: 0.7381\n",
            "Epoch 3755/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 465.7648 - acc: 0.9168 - val_loss: 465.9825 - val_acc: 0.7381\n",
            "Epoch 3756/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 465.7210 - acc: 0.9057 - val_loss: 465.9359 - val_acc: 0.7381\n",
            "Epoch 3757/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 465.6784 - acc: 0.9138 - val_loss: 465.8895 - val_acc: 0.7381\n",
            "Epoch 3758/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 465.6290 - acc: 0.9199 - val_loss: 465.8430 - val_acc: 0.7381\n",
            "Epoch 3759/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 465.5824 - acc: 0.9148 - val_loss: 465.7966 - val_acc: 0.7381\n",
            "Epoch 3760/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 465.5390 - acc: 0.9087 - val_loss: 465.7503 - val_acc: 0.7381\n",
            "Epoch 3761/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 465.4917 - acc: 0.8996 - val_loss: 465.7039 - val_acc: 0.7381\n",
            "Epoch 3762/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 465.4548 - acc: 0.9118 - val_loss: 465.6575 - val_acc: 0.7381\n",
            "Epoch 3763/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 465.4074 - acc: 0.9037 - val_loss: 465.6113 - val_acc: 0.7381\n",
            "Epoch 3764/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 465.3626 - acc: 0.8986 - val_loss: 465.5647 - val_acc: 0.7381\n",
            "Epoch 3765/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 465.3156 - acc: 0.9067 - val_loss: 465.5183 - val_acc: 0.7381\n",
            "Epoch 3766/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 465.2679 - acc: 0.9026 - val_loss: 465.4720 - val_acc: 0.7381\n",
            "Epoch 3767/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 465.2165 - acc: 0.9087 - val_loss: 465.4255 - val_acc: 0.7381\n",
            "Epoch 3768/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 465.1646 - acc: 0.9178 - val_loss: 465.3791 - val_acc: 0.7381\n",
            "Epoch 3769/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 465.1242 - acc: 0.9047 - val_loss: 465.3327 - val_acc: 0.7381\n",
            "Epoch 3770/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 465.0717 - acc: 0.9067 - val_loss: 465.2863 - val_acc: 0.7381\n",
            "Epoch 3771/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 465.0260 - acc: 0.9118 - val_loss: 465.2401 - val_acc: 0.7381\n",
            "Epoch 3772/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 464.9799 - acc: 0.9158 - val_loss: 465.1937 - val_acc: 0.7381\n",
            "Epoch 3773/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 464.9396 - acc: 0.9077 - val_loss: 465.1473 - val_acc: 0.7381\n",
            "Epoch 3774/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 464.8888 - acc: 0.9097 - val_loss: 465.1008 - val_acc: 0.7381\n",
            "Epoch 3775/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 464.8487 - acc: 0.9067 - val_loss: 465.0546 - val_acc: 0.7381\n",
            "Epoch 3776/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 464.8019 - acc: 0.9108 - val_loss: 465.0082 - val_acc: 0.7381\n",
            "Epoch 3777/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 464.7553 - acc: 0.8996 - val_loss: 464.9619 - val_acc: 0.7381\n",
            "Epoch 3778/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 464.7017 - acc: 0.9199 - val_loss: 464.9155 - val_acc: 0.7381\n",
            "Epoch 3779/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 464.6532 - acc: 0.9148 - val_loss: 464.8691 - val_acc: 0.7381\n",
            "Epoch 3780/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 464.6094 - acc: 0.9138 - val_loss: 464.8228 - val_acc: 0.7381\n",
            "Epoch 3781/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 464.5622 - acc: 0.9199 - val_loss: 464.7765 - val_acc: 0.7381\n",
            "Epoch 3782/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 464.5199 - acc: 0.9229 - val_loss: 464.7302 - val_acc: 0.7381\n",
            "Epoch 3783/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 464.4703 - acc: 0.9168 - val_loss: 464.6838 - val_acc: 0.7381\n",
            "Epoch 3784/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 464.4244 - acc: 0.9158 - val_loss: 464.6376 - val_acc: 0.7381\n",
            "Epoch 3785/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 464.3799 - acc: 0.9118 - val_loss: 464.5910 - val_acc: 0.7381\n",
            "Epoch 3786/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 464.3340 - acc: 0.9097 - val_loss: 464.5447 - val_acc: 0.7381\n",
            "Epoch 3787/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 464.2931 - acc: 0.8976 - val_loss: 464.4984 - val_acc: 0.7381\n",
            "Epoch 3788/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 464.2387 - acc: 0.9108 - val_loss: 464.4521 - val_acc: 0.7381\n",
            "Epoch 3789/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 464.1891 - acc: 0.9138 - val_loss: 464.4056 - val_acc: 0.7381\n",
            "Epoch 3790/5000\n",
            "986/986 [==============================] - 0s 261us/step - loss: 464.1483 - acc: 0.9178 - val_loss: 464.3592 - val_acc: 0.7381\n",
            "Epoch 3791/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 464.1033 - acc: 0.9006 - val_loss: 464.3131 - val_acc: 0.7381\n",
            "Epoch 3792/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 464.0531 - acc: 0.9158 - val_loss: 464.2667 - val_acc: 0.7381\n",
            "Epoch 3793/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 464.0082 - acc: 0.9097 - val_loss: 464.2203 - val_acc: 0.7381\n",
            "Epoch 3794/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 463.9654 - acc: 0.9077 - val_loss: 464.1740 - val_acc: 0.7381\n",
            "Epoch 3795/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 463.9207 - acc: 0.9097 - val_loss: 464.1277 - val_acc: 0.7381\n",
            "Epoch 3796/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 463.8720 - acc: 0.9067 - val_loss: 464.0815 - val_acc: 0.7381\n",
            "Epoch 3797/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 463.8209 - acc: 0.9128 - val_loss: 464.0352 - val_acc: 0.7407\n",
            "Epoch 3798/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 463.7782 - acc: 0.9037 - val_loss: 463.9890 - val_acc: 0.7381\n",
            "Epoch 3799/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 463.7329 - acc: 0.9108 - val_loss: 463.9428 - val_acc: 0.7381\n",
            "Epoch 3800/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 463.6869 - acc: 0.9087 - val_loss: 463.8964 - val_acc: 0.7407\n",
            "Epoch 3801/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 463.6387 - acc: 0.9047 - val_loss: 463.8501 - val_acc: 0.7407\n",
            "Epoch 3802/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 463.5887 - acc: 0.9209 - val_loss: 463.8037 - val_acc: 0.7407\n",
            "Epoch 3803/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 463.5478 - acc: 0.9067 - val_loss: 463.7576 - val_acc: 0.7381\n",
            "Epoch 3804/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 463.5019 - acc: 0.9067 - val_loss: 463.7115 - val_acc: 0.7407\n",
            "Epoch 3805/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 463.4504 - acc: 0.9077 - val_loss: 463.6653 - val_acc: 0.7407\n",
            "Epoch 3806/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 463.4050 - acc: 0.9057 - val_loss: 463.6189 - val_acc: 0.7407\n",
            "Epoch 3807/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 463.3628 - acc: 0.9087 - val_loss: 463.5726 - val_acc: 0.7407\n",
            "Epoch 3808/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 463.3126 - acc: 0.9108 - val_loss: 463.5263 - val_acc: 0.7407\n",
            "Epoch 3809/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 463.2697 - acc: 0.9087 - val_loss: 463.4802 - val_acc: 0.7407\n",
            "Epoch 3810/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 463.2217 - acc: 0.9108 - val_loss: 463.4340 - val_acc: 0.7407\n",
            "Epoch 3811/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 463.1702 - acc: 0.9138 - val_loss: 463.3879 - val_acc: 0.7407\n",
            "Epoch 3812/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 463.1311 - acc: 0.9128 - val_loss: 463.3416 - val_acc: 0.7407\n",
            "Epoch 3813/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 463.0853 - acc: 0.9047 - val_loss: 463.2955 - val_acc: 0.7407\n",
            "Epoch 3814/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 463.0375 - acc: 0.9118 - val_loss: 463.2492 - val_acc: 0.7407\n",
            "Epoch 3815/5000\n",
            "986/986 [==============================] - 0s 299us/step - loss: 462.9918 - acc: 0.9057 - val_loss: 463.2029 - val_acc: 0.7407\n",
            "Epoch 3816/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 462.9402 - acc: 0.9189 - val_loss: 463.1568 - val_acc: 0.7407\n",
            "Epoch 3817/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 462.8965 - acc: 0.9047 - val_loss: 463.1105 - val_acc: 0.7407\n",
            "Epoch 3818/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 462.8552 - acc: 0.8996 - val_loss: 463.0642 - val_acc: 0.7407\n",
            "Epoch 3819/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 462.8090 - acc: 0.9087 - val_loss: 463.0182 - val_acc: 0.7407\n",
            "Epoch 3820/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 462.7579 - acc: 0.9168 - val_loss: 462.9719 - val_acc: 0.7407\n",
            "Epoch 3821/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 462.7171 - acc: 0.9087 - val_loss: 462.9256 - val_acc: 0.7407\n",
            "Epoch 3822/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 462.6683 - acc: 0.9037 - val_loss: 462.8795 - val_acc: 0.7407\n",
            "Epoch 3823/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 462.6236 - acc: 0.9077 - val_loss: 462.8333 - val_acc: 0.7407\n",
            "Epoch 3824/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 462.5782 - acc: 0.9057 - val_loss: 462.7872 - val_acc: 0.7407\n",
            "Epoch 3825/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 462.5260 - acc: 0.9199 - val_loss: 462.7411 - val_acc: 0.7407\n",
            "Epoch 3826/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 462.4832 - acc: 0.9158 - val_loss: 462.6949 - val_acc: 0.7407\n",
            "Epoch 3827/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 462.4427 - acc: 0.9097 - val_loss: 462.6487 - val_acc: 0.7407\n",
            "Epoch 3828/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 462.3937 - acc: 0.9016 - val_loss: 462.6026 - val_acc: 0.7407\n",
            "Epoch 3829/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 462.3470 - acc: 0.8966 - val_loss: 462.5564 - val_acc: 0.7407\n",
            "Epoch 3830/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 462.2932 - acc: 0.9270 - val_loss: 462.5103 - val_acc: 0.7407\n",
            "Epoch 3831/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 462.2553 - acc: 0.9077 - val_loss: 462.4641 - val_acc: 0.7407\n",
            "Epoch 3832/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 462.2102 - acc: 0.9016 - val_loss: 462.4179 - val_acc: 0.7407\n",
            "Epoch 3833/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 462.1570 - acc: 0.9148 - val_loss: 462.3720 - val_acc: 0.7407\n",
            "Epoch 3834/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 462.1113 - acc: 0.9148 - val_loss: 462.3259 - val_acc: 0.7407\n",
            "Epoch 3835/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 462.0676 - acc: 0.8996 - val_loss: 462.2798 - val_acc: 0.7407\n",
            "Epoch 3836/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 462.0185 - acc: 0.9087 - val_loss: 462.2336 - val_acc: 0.7407\n",
            "Epoch 3837/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 461.9789 - acc: 0.9108 - val_loss: 462.1876 - val_acc: 0.7407\n",
            "Epoch 3838/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 461.9295 - acc: 0.9016 - val_loss: 462.1413 - val_acc: 0.7407\n",
            "Epoch 3839/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 461.8836 - acc: 0.9118 - val_loss: 462.0952 - val_acc: 0.7407\n",
            "Epoch 3840/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 461.8312 - acc: 0.9260 - val_loss: 462.0493 - val_acc: 0.7407\n",
            "Epoch 3841/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 461.7934 - acc: 0.9097 - val_loss: 462.0030 - val_acc: 0.7407\n",
            "Epoch 3842/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 461.7453 - acc: 0.9026 - val_loss: 461.9569 - val_acc: 0.7434\n",
            "Epoch 3843/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 461.7002 - acc: 0.9097 - val_loss: 461.9109 - val_acc: 0.7407\n",
            "Epoch 3844/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 461.6583 - acc: 0.9037 - val_loss: 461.8647 - val_acc: 0.7407\n",
            "Epoch 3845/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 461.6059 - acc: 0.9097 - val_loss: 461.8186 - val_acc: 0.7407\n",
            "Epoch 3846/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 461.5574 - acc: 0.9128 - val_loss: 461.7725 - val_acc: 0.7407\n",
            "Epoch 3847/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 461.5118 - acc: 0.9199 - val_loss: 461.7261 - val_acc: 0.7407\n",
            "Epoch 3848/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 461.4694 - acc: 0.9026 - val_loss: 461.6803 - val_acc: 0.7407\n",
            "Epoch 3849/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 461.4234 - acc: 0.9097 - val_loss: 461.6340 - val_acc: 0.7407\n",
            "Epoch 3850/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 461.3792 - acc: 0.9057 - val_loss: 461.5881 - val_acc: 0.7407\n",
            "Epoch 3851/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 461.3346 - acc: 0.9097 - val_loss: 461.5421 - val_acc: 0.7407\n",
            "Epoch 3852/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 461.2790 - acc: 0.9189 - val_loss: 461.4957 - val_acc: 0.7407\n",
            "Epoch 3853/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 461.2400 - acc: 0.9057 - val_loss: 461.4497 - val_acc: 0.7407\n",
            "Epoch 3854/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 461.1967 - acc: 0.9097 - val_loss: 461.4037 - val_acc: 0.7407\n",
            "Epoch 3855/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 461.1425 - acc: 0.9108 - val_loss: 461.3578 - val_acc: 0.7407\n",
            "Epoch 3856/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 461.1044 - acc: 0.9026 - val_loss: 461.3118 - val_acc: 0.7407\n",
            "Epoch 3857/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 461.0550 - acc: 0.9138 - val_loss: 461.2658 - val_acc: 0.7407\n",
            "Epoch 3858/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 461.0067 - acc: 0.9168 - val_loss: 461.2198 - val_acc: 0.7407\n",
            "Epoch 3859/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 460.9598 - acc: 0.9229 - val_loss: 461.1739 - val_acc: 0.7407\n",
            "Epoch 3860/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 460.9124 - acc: 0.9047 - val_loss: 461.1279 - val_acc: 0.7407\n",
            "Epoch 3861/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 460.8686 - acc: 0.9118 - val_loss: 461.0817 - val_acc: 0.7407\n",
            "Epoch 3862/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 460.8184 - acc: 0.9067 - val_loss: 461.0356 - val_acc: 0.7407\n",
            "Epoch 3863/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 460.7719 - acc: 0.9199 - val_loss: 460.9897 - val_acc: 0.7407\n",
            "Epoch 3864/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 460.7312 - acc: 0.9077 - val_loss: 460.9437 - val_acc: 0.7407\n",
            "Epoch 3865/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 460.6901 - acc: 0.9016 - val_loss: 460.8977 - val_acc: 0.7407\n",
            "Epoch 3866/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 460.6358 - acc: 0.9249 - val_loss: 460.8516 - val_acc: 0.7407\n",
            "Epoch 3867/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 460.5979 - acc: 0.9016 - val_loss: 460.8056 - val_acc: 0.7407\n",
            "Epoch 3868/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 460.5453 - acc: 0.9037 - val_loss: 460.7596 - val_acc: 0.7407\n",
            "Epoch 3869/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 460.4962 - acc: 0.9189 - val_loss: 460.7136 - val_acc: 0.7407\n",
            "Epoch 3870/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 460.4542 - acc: 0.9097 - val_loss: 460.6676 - val_acc: 0.7407\n",
            "Epoch 3871/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 460.4080 - acc: 0.9016 - val_loss: 460.6216 - val_acc: 0.7407\n",
            "Epoch 3872/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 460.3574 - acc: 0.9108 - val_loss: 460.5757 - val_acc: 0.7407\n",
            "Epoch 3873/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 460.3168 - acc: 0.9108 - val_loss: 460.5298 - val_acc: 0.7407\n",
            "Epoch 3874/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 460.2679 - acc: 0.9128 - val_loss: 460.4839 - val_acc: 0.7407\n",
            "Epoch 3875/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 460.2268 - acc: 0.9077 - val_loss: 460.4378 - val_acc: 0.7407\n",
            "Epoch 3876/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 460.1787 - acc: 0.9067 - val_loss: 460.3919 - val_acc: 0.7407\n",
            "Epoch 3877/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 460.1268 - acc: 0.9178 - val_loss: 460.3458 - val_acc: 0.7407\n",
            "Epoch 3878/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 460.0897 - acc: 0.9016 - val_loss: 460.2998 - val_acc: 0.7407\n",
            "Epoch 3879/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 460.0349 - acc: 0.9189 - val_loss: 460.2539 - val_acc: 0.7407\n",
            "Epoch 3880/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 459.9913 - acc: 0.9118 - val_loss: 460.2080 - val_acc: 0.7407\n",
            "Epoch 3881/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 459.9461 - acc: 0.9067 - val_loss: 460.1623 - val_acc: 0.7407\n",
            "Epoch 3882/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 459.9031 - acc: 0.9087 - val_loss: 460.1163 - val_acc: 0.7407\n",
            "Epoch 3883/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 459.8529 - acc: 0.9158 - val_loss: 460.0703 - val_acc: 0.7407\n",
            "Epoch 3884/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 459.8118 - acc: 0.9097 - val_loss: 460.0244 - val_acc: 0.7407\n",
            "Epoch 3885/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 459.7614 - acc: 0.9118 - val_loss: 459.9786 - val_acc: 0.7407\n",
            "Epoch 3886/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 459.7130 - acc: 0.9138 - val_loss: 459.9327 - val_acc: 0.7407\n",
            "Epoch 3887/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 459.6750 - acc: 0.9047 - val_loss: 459.8866 - val_acc: 0.7407\n",
            "Epoch 3888/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 459.6249 - acc: 0.9118 - val_loss: 459.8406 - val_acc: 0.7407\n",
            "Epoch 3889/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 459.5861 - acc: 0.9057 - val_loss: 459.7946 - val_acc: 0.7407\n",
            "Epoch 3890/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 459.5334 - acc: 0.9128 - val_loss: 459.7488 - val_acc: 0.7407\n",
            "Epoch 3891/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 459.4909 - acc: 0.9067 - val_loss: 459.7029 - val_acc: 0.7434\n",
            "Epoch 3892/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 459.4427 - acc: 0.9118 - val_loss: 459.6569 - val_acc: 0.7434\n",
            "Epoch 3893/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 459.3999 - acc: 0.9097 - val_loss: 459.6108 - val_acc: 0.7407\n",
            "Epoch 3894/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 459.3491 - acc: 0.9138 - val_loss: 459.5647 - val_acc: 0.7434\n",
            "Epoch 3895/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 459.3116 - acc: 0.9067 - val_loss: 459.5188 - val_acc: 0.7407\n",
            "Epoch 3896/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 459.2597 - acc: 0.9138 - val_loss: 459.4730 - val_acc: 0.7434\n",
            "Epoch 3897/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 459.2124 - acc: 0.9087 - val_loss: 459.4270 - val_acc: 0.7434\n",
            "Epoch 3898/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 459.1714 - acc: 0.9037 - val_loss: 459.3812 - val_acc: 0.7434\n",
            "Epoch 3899/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 459.1224 - acc: 0.9108 - val_loss: 459.3354 - val_acc: 0.7407\n",
            "Epoch 3900/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 459.0798 - acc: 0.8955 - val_loss: 459.2894 - val_acc: 0.7434\n",
            "Epoch 3901/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 459.0268 - acc: 0.9189 - val_loss: 459.2437 - val_acc: 0.7434\n",
            "Epoch 3902/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 458.9893 - acc: 0.9077 - val_loss: 459.1978 - val_acc: 0.7434\n",
            "Epoch 3903/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 458.9419 - acc: 0.9026 - val_loss: 459.1520 - val_acc: 0.7434\n",
            "Epoch 3904/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 458.8923 - acc: 0.9148 - val_loss: 459.1059 - val_acc: 0.7434\n",
            "Epoch 3905/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 458.8417 - acc: 0.9209 - val_loss: 459.0601 - val_acc: 0.7434\n",
            "Epoch 3906/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 458.7996 - acc: 0.9077 - val_loss: 459.0144 - val_acc: 0.7407\n",
            "Epoch 3907/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 458.7551 - acc: 0.9037 - val_loss: 458.9687 - val_acc: 0.7407\n",
            "Epoch 3908/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 458.7016 - acc: 0.9178 - val_loss: 458.9229 - val_acc: 0.7407\n",
            "Epoch 3909/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 458.6653 - acc: 0.9037 - val_loss: 458.8771 - val_acc: 0.7434\n",
            "Epoch 3910/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 458.6220 - acc: 0.8986 - val_loss: 458.8311 - val_acc: 0.7434\n",
            "Epoch 3911/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 458.5702 - acc: 0.9037 - val_loss: 458.7854 - val_acc: 0.7434\n",
            "Epoch 3912/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 458.5207 - acc: 0.9199 - val_loss: 458.7394 - val_acc: 0.7434\n",
            "Epoch 3913/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 458.4813 - acc: 0.9097 - val_loss: 458.6936 - val_acc: 0.7434\n",
            "Epoch 3914/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 458.4424 - acc: 0.9057 - val_loss: 458.6478 - val_acc: 0.7434\n",
            "Epoch 3915/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 458.3837 - acc: 0.9067 - val_loss: 458.6020 - val_acc: 0.7434\n",
            "Epoch 3916/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 458.3409 - acc: 0.9199 - val_loss: 458.5561 - val_acc: 0.7434\n",
            "Epoch 3917/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 458.2991 - acc: 0.9138 - val_loss: 458.5101 - val_acc: 0.7434\n",
            "Epoch 3918/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 458.2530 - acc: 0.9087 - val_loss: 458.4645 - val_acc: 0.7434\n",
            "Epoch 3919/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 458.2080 - acc: 0.9108 - val_loss: 458.4187 - val_acc: 0.7434\n",
            "Epoch 3920/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 458.1524 - acc: 0.9209 - val_loss: 458.3729 - val_acc: 0.7434\n",
            "Epoch 3921/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 458.1131 - acc: 0.9148 - val_loss: 458.3272 - val_acc: 0.7434\n",
            "Epoch 3922/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 458.0615 - acc: 0.9219 - val_loss: 458.2815 - val_acc: 0.7407\n",
            "Epoch 3923/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 458.0265 - acc: 0.9057 - val_loss: 458.2358 - val_acc: 0.7407\n",
            "Epoch 3924/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 457.9727 - acc: 0.9168 - val_loss: 458.1899 - val_acc: 0.7407\n",
            "Epoch 3925/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 457.9264 - acc: 0.9189 - val_loss: 458.1443 - val_acc: 0.7434\n",
            "Epoch 3926/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 457.8816 - acc: 0.9118 - val_loss: 458.0984 - val_acc: 0.7407\n",
            "Epoch 3927/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 457.8354 - acc: 0.9128 - val_loss: 458.0526 - val_acc: 0.7381\n",
            "Epoch 3928/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 457.7929 - acc: 0.9128 - val_loss: 458.0069 - val_acc: 0.7407\n",
            "Epoch 3929/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 457.7449 - acc: 0.9138 - val_loss: 457.9611 - val_acc: 0.7407\n",
            "Epoch 3930/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 457.6980 - acc: 0.9097 - val_loss: 457.9151 - val_acc: 0.7381\n",
            "Epoch 3931/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 457.6510 - acc: 0.9168 - val_loss: 457.8695 - val_acc: 0.7381\n",
            "Epoch 3932/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 457.6085 - acc: 0.9168 - val_loss: 457.8237 - val_acc: 0.7407\n",
            "Epoch 3933/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 457.5607 - acc: 0.9138 - val_loss: 457.7779 - val_acc: 0.7407\n",
            "Epoch 3934/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 457.5117 - acc: 0.9178 - val_loss: 457.7321 - val_acc: 0.7407\n",
            "Epoch 3935/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 457.4700 - acc: 0.9209 - val_loss: 457.6865 - val_acc: 0.7407\n",
            "Epoch 3936/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 457.4241 - acc: 0.9087 - val_loss: 457.6407 - val_acc: 0.7407\n",
            "Epoch 3937/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 457.3818 - acc: 0.9209 - val_loss: 457.5951 - val_acc: 0.7407\n",
            "Epoch 3938/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 457.3292 - acc: 0.9239 - val_loss: 457.5491 - val_acc: 0.7407\n",
            "Epoch 3939/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 457.2861 - acc: 0.9087 - val_loss: 457.5034 - val_acc: 0.7407\n",
            "Epoch 3940/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 457.2452 - acc: 0.9158 - val_loss: 457.4575 - val_acc: 0.7407\n",
            "Epoch 3941/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 457.1961 - acc: 0.9128 - val_loss: 457.4118 - val_acc: 0.7407\n",
            "Epoch 3942/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 457.1554 - acc: 0.9108 - val_loss: 457.3661 - val_acc: 0.7407\n",
            "Epoch 3943/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 457.1015 - acc: 0.9249 - val_loss: 457.3204 - val_acc: 0.7407\n",
            "Epoch 3944/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 457.0582 - acc: 0.9118 - val_loss: 457.2746 - val_acc: 0.7407\n",
            "Epoch 3945/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 457.0104 - acc: 0.9158 - val_loss: 457.2291 - val_acc: 0.7407\n",
            "Epoch 3946/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 456.9702 - acc: 0.9077 - val_loss: 457.1832 - val_acc: 0.7407\n",
            "Epoch 3947/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 456.9171 - acc: 0.9168 - val_loss: 457.1377 - val_acc: 0.7407\n",
            "Epoch 3948/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 456.8871 - acc: 0.9077 - val_loss: 457.0921 - val_acc: 0.7407\n",
            "Epoch 3949/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 456.8312 - acc: 0.9128 - val_loss: 457.0463 - val_acc: 0.7407\n",
            "Epoch 3950/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 456.7870 - acc: 0.9199 - val_loss: 457.0004 - val_acc: 0.7407\n",
            "Epoch 3951/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 456.7330 - acc: 0.9168 - val_loss: 456.9549 - val_acc: 0.7407\n",
            "Epoch 3952/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 456.6962 - acc: 0.9128 - val_loss: 456.9091 - val_acc: 0.7407\n",
            "Epoch 3953/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 456.6501 - acc: 0.9158 - val_loss: 456.8633 - val_acc: 0.7407\n",
            "Epoch 3954/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 456.6040 - acc: 0.9067 - val_loss: 456.8176 - val_acc: 0.7407\n",
            "Epoch 3955/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 456.5557 - acc: 0.9189 - val_loss: 456.7720 - val_acc: 0.7407\n",
            "Epoch 3956/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 456.5074 - acc: 0.9178 - val_loss: 456.7264 - val_acc: 0.7407\n",
            "Epoch 3957/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 456.4663 - acc: 0.9158 - val_loss: 456.6808 - val_acc: 0.7407\n",
            "Epoch 3958/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 456.4182 - acc: 0.9178 - val_loss: 456.6351 - val_acc: 0.7407\n",
            "Epoch 3959/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 456.3787 - acc: 0.9148 - val_loss: 456.5894 - val_acc: 0.7407\n",
            "Epoch 3960/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 456.3288 - acc: 0.9118 - val_loss: 456.5438 - val_acc: 0.7407\n",
            "Epoch 3961/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 456.2833 - acc: 0.9219 - val_loss: 456.4980 - val_acc: 0.7407\n",
            "Epoch 3962/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 456.2371 - acc: 0.9016 - val_loss: 456.4523 - val_acc: 0.7407\n",
            "Epoch 3963/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 456.1905 - acc: 0.9138 - val_loss: 456.4066 - val_acc: 0.7407\n",
            "Epoch 3964/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 456.1474 - acc: 0.9118 - val_loss: 456.3610 - val_acc: 0.7407\n",
            "Epoch 3965/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 456.0946 - acc: 0.9229 - val_loss: 456.3153 - val_acc: 0.7407\n",
            "Epoch 3966/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 456.0491 - acc: 0.9178 - val_loss: 456.2697 - val_acc: 0.7407\n",
            "Epoch 3967/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 456.0035 - acc: 0.9158 - val_loss: 456.2242 - val_acc: 0.7407\n",
            "Epoch 3968/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 455.9596 - acc: 0.9128 - val_loss: 456.1784 - val_acc: 0.7407\n",
            "Epoch 3969/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 455.9185 - acc: 0.9077 - val_loss: 456.1328 - val_acc: 0.7407\n",
            "Epoch 3970/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 455.8742 - acc: 0.9178 - val_loss: 456.0872 - val_acc: 0.7407\n",
            "Epoch 3971/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 455.8228 - acc: 0.9037 - val_loss: 456.0415 - val_acc: 0.7407\n",
            "Epoch 3972/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 455.7802 - acc: 0.9097 - val_loss: 455.9960 - val_acc: 0.7407\n",
            "Epoch 3973/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 455.7344 - acc: 0.9077 - val_loss: 455.9505 - val_acc: 0.7407\n",
            "Epoch 3974/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 455.6818 - acc: 0.9148 - val_loss: 455.9049 - val_acc: 0.7407\n",
            "Epoch 3975/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 455.6416 - acc: 0.9249 - val_loss: 455.8592 - val_acc: 0.7407\n",
            "Epoch 3976/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 455.5956 - acc: 0.9158 - val_loss: 455.8137 - val_acc: 0.7407\n",
            "Epoch 3977/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 455.5504 - acc: 0.9178 - val_loss: 455.7682 - val_acc: 0.7407\n",
            "Epoch 3978/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 455.5108 - acc: 0.9057 - val_loss: 455.7225 - val_acc: 0.7407\n",
            "Epoch 3979/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 455.4600 - acc: 0.9067 - val_loss: 455.6770 - val_acc: 0.7407\n",
            "Epoch 3980/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 455.4145 - acc: 0.9108 - val_loss: 455.6314 - val_acc: 0.7407\n",
            "Epoch 3981/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 455.3699 - acc: 0.9199 - val_loss: 455.5859 - val_acc: 0.7407\n",
            "Epoch 3982/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 455.3297 - acc: 0.9087 - val_loss: 455.5403 - val_acc: 0.7407\n",
            "Epoch 3983/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 455.2729 - acc: 0.9087 - val_loss: 455.4947 - val_acc: 0.7407\n",
            "Epoch 3984/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 455.2288 - acc: 0.9158 - val_loss: 455.4492 - val_acc: 0.7407\n",
            "Epoch 3985/5000\n",
            "986/986 [==============================] - 0s 259us/step - loss: 455.1878 - acc: 0.9168 - val_loss: 455.4036 - val_acc: 0.7407\n",
            "Epoch 3986/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 455.1429 - acc: 0.9158 - val_loss: 455.3580 - val_acc: 0.7407\n",
            "Epoch 3987/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 455.1000 - acc: 0.9067 - val_loss: 455.3125 - val_acc: 0.7407\n",
            "Epoch 3988/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 455.0495 - acc: 0.9199 - val_loss: 455.2669 - val_acc: 0.7407\n",
            "Epoch 3989/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 455.0006 - acc: 0.9229 - val_loss: 455.2212 - val_acc: 0.7407\n",
            "Epoch 3990/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 454.9613 - acc: 0.9108 - val_loss: 455.1757 - val_acc: 0.7407\n",
            "Epoch 3991/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 454.9150 - acc: 0.9077 - val_loss: 455.1302 - val_acc: 0.7407\n",
            "Epoch 3992/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 454.8661 - acc: 0.9067 - val_loss: 455.0847 - val_acc: 0.7407\n",
            "Epoch 3993/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 454.8224 - acc: 0.9057 - val_loss: 455.0391 - val_acc: 0.7407\n",
            "Epoch 3994/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 454.7799 - acc: 0.9037 - val_loss: 454.9934 - val_acc: 0.7407\n",
            "Epoch 3995/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 454.7352 - acc: 0.9138 - val_loss: 454.9480 - val_acc: 0.7407\n",
            "Epoch 3996/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 454.6816 - acc: 0.9189 - val_loss: 454.9024 - val_acc: 0.7407\n",
            "Epoch 3997/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 454.6384 - acc: 0.9108 - val_loss: 454.8566 - val_acc: 0.7407\n",
            "Epoch 3998/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 454.5995 - acc: 0.9097 - val_loss: 454.8112 - val_acc: 0.7407\n",
            "Epoch 3999/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 454.5452 - acc: 0.9209 - val_loss: 454.7657 - val_acc: 0.7407\n",
            "Epoch 4000/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 454.5054 - acc: 0.9148 - val_loss: 454.7202 - val_acc: 0.7407\n",
            "Epoch 4001/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 454.4571 - acc: 0.9057 - val_loss: 454.6747 - val_acc: 0.7407\n",
            "Epoch 4002/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 454.4125 - acc: 0.9118 - val_loss: 454.6290 - val_acc: 0.7407\n",
            "Epoch 4003/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 454.3671 - acc: 0.9148 - val_loss: 454.5834 - val_acc: 0.7407\n",
            "Epoch 4004/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 454.3143 - acc: 0.9128 - val_loss: 454.5381 - val_acc: 0.7407\n",
            "Epoch 4005/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 454.2788 - acc: 0.9128 - val_loss: 454.4926 - val_acc: 0.7407\n",
            "Epoch 4006/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 454.2316 - acc: 0.9138 - val_loss: 454.4471 - val_acc: 0.7434\n",
            "Epoch 4007/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 454.1855 - acc: 0.9118 - val_loss: 454.4016 - val_acc: 0.7434\n",
            "Epoch 4008/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 454.1376 - acc: 0.9249 - val_loss: 454.3560 - val_acc: 0.7407\n",
            "Epoch 4009/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 454.0935 - acc: 0.9087 - val_loss: 454.3106 - val_acc: 0.7407\n",
            "Epoch 4010/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 454.0484 - acc: 0.9067 - val_loss: 454.2653 - val_acc: 0.7407\n",
            "Epoch 4011/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 454.0035 - acc: 0.9108 - val_loss: 454.2200 - val_acc: 0.7407\n",
            "Epoch 4012/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 453.9663 - acc: 0.9067 - val_loss: 454.1744 - val_acc: 0.7407\n",
            "Epoch 4013/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 453.9073 - acc: 0.9158 - val_loss: 454.1291 - val_acc: 0.7407\n",
            "Epoch 4014/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 453.8727 - acc: 0.9057 - val_loss: 454.0837 - val_acc: 0.7407\n",
            "Epoch 4015/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 453.8265 - acc: 0.9168 - val_loss: 454.0381 - val_acc: 0.7407\n",
            "Epoch 4016/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 453.7752 - acc: 0.9047 - val_loss: 453.9926 - val_acc: 0.7407\n",
            "Epoch 4017/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 453.7260 - acc: 0.9097 - val_loss: 453.9472 - val_acc: 0.7407\n",
            "Epoch 4018/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 453.6765 - acc: 0.9168 - val_loss: 453.9017 - val_acc: 0.7407\n",
            "Epoch 4019/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 453.6414 - acc: 0.9097 - val_loss: 453.8563 - val_acc: 0.7407\n",
            "Epoch 4020/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 453.5956 - acc: 0.9158 - val_loss: 453.8108 - val_acc: 0.7407\n",
            "Epoch 4021/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 453.5469 - acc: 0.9138 - val_loss: 453.7654 - val_acc: 0.7407\n",
            "Epoch 4022/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 453.5015 - acc: 0.9077 - val_loss: 453.7197 - val_acc: 0.7407\n",
            "Epoch 4023/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 453.4548 - acc: 0.9128 - val_loss: 453.6744 - val_acc: 0.7407\n",
            "Epoch 4024/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 453.4082 - acc: 0.9209 - val_loss: 453.6288 - val_acc: 0.7407\n",
            "Epoch 4025/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 453.3669 - acc: 0.9138 - val_loss: 453.5834 - val_acc: 0.7407\n",
            "Epoch 4026/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 453.3219 - acc: 0.9158 - val_loss: 453.5381 - val_acc: 0.7407\n",
            "Epoch 4027/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 453.2762 - acc: 0.9209 - val_loss: 453.4925 - val_acc: 0.7407\n",
            "Epoch 4028/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 453.2256 - acc: 0.9108 - val_loss: 453.4472 - val_acc: 0.7407\n",
            "Epoch 4029/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 453.1839 - acc: 0.9148 - val_loss: 453.4017 - val_acc: 0.7407\n",
            "Epoch 4030/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 453.1483 - acc: 0.9047 - val_loss: 453.3562 - val_acc: 0.7407\n",
            "Epoch 4031/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 453.0877 - acc: 0.9249 - val_loss: 453.3108 - val_acc: 0.7407\n",
            "Epoch 4032/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 453.0442 - acc: 0.9158 - val_loss: 453.2653 - val_acc: 0.7407\n",
            "Epoch 4033/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 453.0058 - acc: 0.9138 - val_loss: 453.2200 - val_acc: 0.7407\n",
            "Epoch 4034/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 452.9531 - acc: 0.9138 - val_loss: 453.1745 - val_acc: 0.7407\n",
            "Epoch 4035/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 452.9116 - acc: 0.9087 - val_loss: 453.1292 - val_acc: 0.7407\n",
            "Epoch 4036/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 452.8668 - acc: 0.9087 - val_loss: 453.0839 - val_acc: 0.7407\n",
            "Epoch 4037/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 452.8182 - acc: 0.9189 - val_loss: 453.0386 - val_acc: 0.7407\n",
            "Epoch 4038/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 452.7746 - acc: 0.9260 - val_loss: 452.9932 - val_acc: 0.7407\n",
            "Epoch 4039/5000\n",
            "986/986 [==============================] - 0s 292us/step - loss: 452.7289 - acc: 0.9178 - val_loss: 452.9479 - val_acc: 0.7407\n",
            "Epoch 4040/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 452.6812 - acc: 0.9148 - val_loss: 452.9026 - val_acc: 0.7407\n",
            "Epoch 4041/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 452.6432 - acc: 0.9158 - val_loss: 452.8572 - val_acc: 0.7407\n",
            "Epoch 4042/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 452.5924 - acc: 0.9148 - val_loss: 452.8117 - val_acc: 0.7407\n",
            "Epoch 4043/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 452.5440 - acc: 0.9148 - val_loss: 452.7662 - val_acc: 0.7407\n",
            "Epoch 4044/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 452.5022 - acc: 0.9138 - val_loss: 452.7209 - val_acc: 0.7407\n",
            "Epoch 4045/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 452.4580 - acc: 0.9138 - val_loss: 452.6756 - val_acc: 0.7407\n",
            "Epoch 4046/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 452.4088 - acc: 0.9158 - val_loss: 452.6302 - val_acc: 0.7407\n",
            "Epoch 4047/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 452.3702 - acc: 0.9108 - val_loss: 452.5850 - val_acc: 0.7407\n",
            "Epoch 4048/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 452.3196 - acc: 0.9178 - val_loss: 452.5396 - val_acc: 0.7407\n",
            "Epoch 4049/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 452.2810 - acc: 0.9097 - val_loss: 452.4942 - val_acc: 0.7407\n",
            "Epoch 4050/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 452.2305 - acc: 0.9108 - val_loss: 452.4487 - val_acc: 0.7407\n",
            "Epoch 4051/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 452.1909 - acc: 0.9077 - val_loss: 452.4035 - val_acc: 0.7407\n",
            "Epoch 4052/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 452.1406 - acc: 0.9118 - val_loss: 452.3582 - val_acc: 0.7407\n",
            "Epoch 4053/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 452.0932 - acc: 0.9148 - val_loss: 452.3128 - val_acc: 0.7407\n",
            "Epoch 4054/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 452.0426 - acc: 0.9229 - val_loss: 452.2674 - val_acc: 0.7407\n",
            "Epoch 4055/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 451.9951 - acc: 0.9178 - val_loss: 452.2222 - val_acc: 0.7407\n",
            "Epoch 4056/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 451.9636 - acc: 0.8935 - val_loss: 452.1768 - val_acc: 0.7407\n",
            "Epoch 4057/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 451.9187 - acc: 0.9108 - val_loss: 452.1314 - val_acc: 0.7407\n",
            "Epoch 4058/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 451.8660 - acc: 0.9108 - val_loss: 452.0861 - val_acc: 0.7407\n",
            "Epoch 4059/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 451.8209 - acc: 0.9270 - val_loss: 452.0407 - val_acc: 0.7407\n",
            "Epoch 4060/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 451.7694 - acc: 0.9209 - val_loss: 451.9955 - val_acc: 0.7407\n",
            "Epoch 4061/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 451.7319 - acc: 0.9219 - val_loss: 451.9502 - val_acc: 0.7407\n",
            "Epoch 4062/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 451.6827 - acc: 0.9168 - val_loss: 451.9048 - val_acc: 0.7407\n",
            "Epoch 4063/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 451.6403 - acc: 0.9138 - val_loss: 451.8596 - val_acc: 0.7407\n",
            "Epoch 4064/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 451.5988 - acc: 0.9148 - val_loss: 451.8143 - val_acc: 0.7407\n",
            "Epoch 4065/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 451.5500 - acc: 0.9178 - val_loss: 451.7689 - val_acc: 0.7407\n",
            "Epoch 4066/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 451.5026 - acc: 0.9047 - val_loss: 451.7236 - val_acc: 0.7407\n",
            "Epoch 4067/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 451.4606 - acc: 0.9178 - val_loss: 451.6783 - val_acc: 0.7407\n",
            "Epoch 4068/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 451.4114 - acc: 0.9249 - val_loss: 451.6330 - val_acc: 0.7407\n",
            "Epoch 4069/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 451.3737 - acc: 0.9037 - val_loss: 451.5877 - val_acc: 0.7407\n",
            "Epoch 4070/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 451.3267 - acc: 0.9037 - val_loss: 451.5424 - val_acc: 0.7407\n",
            "Epoch 4071/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 451.2841 - acc: 0.9148 - val_loss: 451.4971 - val_acc: 0.7407\n",
            "Epoch 4072/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 451.2348 - acc: 0.9178 - val_loss: 451.4518 - val_acc: 0.7407\n",
            "Epoch 4073/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 451.1870 - acc: 0.9057 - val_loss: 451.4065 - val_acc: 0.7407\n",
            "Epoch 4074/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 451.1459 - acc: 0.9219 - val_loss: 451.3611 - val_acc: 0.7407\n",
            "Epoch 4075/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 451.0906 - acc: 0.9148 - val_loss: 451.3160 - val_acc: 0.7407\n",
            "Epoch 4076/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 451.0557 - acc: 0.9077 - val_loss: 451.2709 - val_acc: 0.7407\n",
            "Epoch 4077/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 451.0062 - acc: 0.9158 - val_loss: 451.2257 - val_acc: 0.7407\n",
            "Epoch 4078/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 450.9577 - acc: 0.9148 - val_loss: 451.1805 - val_acc: 0.7407\n",
            "Epoch 4079/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 450.9192 - acc: 0.9158 - val_loss: 451.1353 - val_acc: 0.7407\n",
            "Epoch 4080/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 450.8725 - acc: 0.9097 - val_loss: 451.0901 - val_acc: 0.7407\n",
            "Epoch 4081/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 450.8247 - acc: 0.9118 - val_loss: 451.0447 - val_acc: 0.7407\n",
            "Epoch 4082/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 450.7820 - acc: 0.9118 - val_loss: 450.9995 - val_acc: 0.7407\n",
            "Epoch 4083/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 450.7360 - acc: 0.9158 - val_loss: 450.9542 - val_acc: 0.7407\n",
            "Epoch 4084/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 450.6843 - acc: 0.9148 - val_loss: 450.9090 - val_acc: 0.7407\n",
            "Epoch 4085/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 450.6495 - acc: 0.9087 - val_loss: 450.8637 - val_acc: 0.7407\n",
            "Epoch 4086/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 450.5999 - acc: 0.9006 - val_loss: 450.8186 - val_acc: 0.7407\n",
            "Epoch 4087/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 450.5570 - acc: 0.9189 - val_loss: 450.7734 - val_acc: 0.7407\n",
            "Epoch 4088/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 450.5099 - acc: 0.9118 - val_loss: 450.7282 - val_acc: 0.7407\n",
            "Epoch 4089/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 450.4648 - acc: 0.9158 - val_loss: 450.6831 - val_acc: 0.7407\n",
            "Epoch 4090/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 450.4195 - acc: 0.9189 - val_loss: 450.6379 - val_acc: 0.7407\n",
            "Epoch 4091/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 450.3644 - acc: 0.9199 - val_loss: 450.5927 - val_acc: 0.7407\n",
            "Epoch 4092/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 450.3268 - acc: 0.9209 - val_loss: 450.5475 - val_acc: 0.7407\n",
            "Epoch 4093/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 450.2857 - acc: 0.9047 - val_loss: 450.5023 - val_acc: 0.7407\n",
            "Epoch 4094/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 450.2395 - acc: 0.9168 - val_loss: 450.4571 - val_acc: 0.7407\n",
            "Epoch 4095/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 450.1992 - acc: 0.9097 - val_loss: 450.4120 - val_acc: 0.7407\n",
            "Epoch 4096/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 450.1519 - acc: 0.9189 - val_loss: 450.3669 - val_acc: 0.7407\n",
            "Epoch 4097/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 450.1033 - acc: 0.9087 - val_loss: 450.3218 - val_acc: 0.7407\n",
            "Epoch 4098/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 450.0560 - acc: 0.9209 - val_loss: 450.2766 - val_acc: 0.7407\n",
            "Epoch 4099/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 450.0124 - acc: 0.9128 - val_loss: 450.2313 - val_acc: 0.7407\n",
            "Epoch 4100/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 449.9678 - acc: 0.9097 - val_loss: 450.1863 - val_acc: 0.7407\n",
            "Epoch 4101/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 449.9278 - acc: 0.9047 - val_loss: 450.1411 - val_acc: 0.7407\n",
            "Epoch 4102/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 449.8749 - acc: 0.9077 - val_loss: 450.0958 - val_acc: 0.7407\n",
            "Epoch 4103/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 449.8325 - acc: 0.9178 - val_loss: 450.0507 - val_acc: 0.7407\n",
            "Epoch 4104/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 449.7771 - acc: 0.9300 - val_loss: 450.0057 - val_acc: 0.7407\n",
            "Epoch 4105/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 449.7434 - acc: 0.9128 - val_loss: 449.9605 - val_acc: 0.7407\n",
            "Epoch 4106/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 449.7002 - acc: 0.9087 - val_loss: 449.9154 - val_acc: 0.7407\n",
            "Epoch 4107/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 449.6489 - acc: 0.9199 - val_loss: 449.8703 - val_acc: 0.7407\n",
            "Epoch 4108/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 449.5978 - acc: 0.9239 - val_loss: 449.8252 - val_acc: 0.7407\n",
            "Epoch 4109/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 449.5594 - acc: 0.9087 - val_loss: 449.7801 - val_acc: 0.7407\n",
            "Epoch 4110/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 449.5156 - acc: 0.9138 - val_loss: 449.7350 - val_acc: 0.7407\n",
            "Epoch 4111/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 449.4672 - acc: 0.9178 - val_loss: 449.6899 - val_acc: 0.7407\n",
            "Epoch 4112/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 449.4226 - acc: 0.9178 - val_loss: 449.6449 - val_acc: 0.7407\n",
            "Epoch 4113/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 449.3776 - acc: 0.9067 - val_loss: 449.5999 - val_acc: 0.7407\n",
            "Epoch 4114/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 449.3353 - acc: 0.9067 - val_loss: 449.5547 - val_acc: 0.7407\n",
            "Epoch 4115/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 449.2951 - acc: 0.9158 - val_loss: 449.5097 - val_acc: 0.7407\n",
            "Epoch 4116/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 449.2443 - acc: 0.9168 - val_loss: 449.4646 - val_acc: 0.7407\n",
            "Epoch 4117/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 449.2040 - acc: 0.9118 - val_loss: 449.4196 - val_acc: 0.7407\n",
            "Epoch 4118/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 449.1548 - acc: 0.9219 - val_loss: 449.3746 - val_acc: 0.7407\n",
            "Epoch 4119/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 449.1085 - acc: 0.9128 - val_loss: 449.3295 - val_acc: 0.7407\n",
            "Epoch 4120/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 449.0653 - acc: 0.9097 - val_loss: 449.2845 - val_acc: 0.7407\n",
            "Epoch 4121/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 449.0225 - acc: 0.9128 - val_loss: 449.2395 - val_acc: 0.7407\n",
            "Epoch 4122/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 448.9780 - acc: 0.9108 - val_loss: 449.1946 - val_acc: 0.7407\n",
            "Epoch 4123/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 448.9336 - acc: 0.9108 - val_loss: 449.1495 - val_acc: 0.7407\n",
            "Epoch 4124/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 448.8844 - acc: 0.9168 - val_loss: 449.1043 - val_acc: 0.7407\n",
            "Epoch 4125/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 448.8427 - acc: 0.9128 - val_loss: 449.0592 - val_acc: 0.7407\n",
            "Epoch 4126/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 448.7919 - acc: 0.9199 - val_loss: 449.0142 - val_acc: 0.7407\n",
            "Epoch 4127/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 448.7531 - acc: 0.9057 - val_loss: 448.9693 - val_acc: 0.7407\n",
            "Epoch 4128/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 448.6978 - acc: 0.9280 - val_loss: 448.9243 - val_acc: 0.7407\n",
            "Epoch 4129/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 448.6548 - acc: 0.9199 - val_loss: 448.8793 - val_acc: 0.7407\n",
            "Epoch 4130/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 448.6124 - acc: 0.9158 - val_loss: 448.8343 - val_acc: 0.7407\n",
            "Epoch 4131/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 448.5710 - acc: 0.9087 - val_loss: 448.7894 - val_acc: 0.7407\n",
            "Epoch 4132/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 448.5240 - acc: 0.9097 - val_loss: 448.7444 - val_acc: 0.7407\n",
            "Epoch 4133/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 448.4774 - acc: 0.9108 - val_loss: 448.6993 - val_acc: 0.7407\n",
            "Epoch 4134/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 448.4320 - acc: 0.9178 - val_loss: 448.6542 - val_acc: 0.7407\n",
            "Epoch 4135/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 448.3902 - acc: 0.9128 - val_loss: 448.6093 - val_acc: 0.7407\n",
            "Epoch 4136/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 448.3415 - acc: 0.9148 - val_loss: 448.5642 - val_acc: 0.7407\n",
            "Epoch 4137/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 448.2968 - acc: 0.9260 - val_loss: 448.5192 - val_acc: 0.7407\n",
            "Epoch 4138/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 448.2481 - acc: 0.9209 - val_loss: 448.4744 - val_acc: 0.7407\n",
            "Epoch 4139/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 448.2055 - acc: 0.9209 - val_loss: 448.4293 - val_acc: 0.7407\n",
            "Epoch 4140/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 448.1703 - acc: 0.9067 - val_loss: 448.3843 - val_acc: 0.7407\n",
            "Epoch 4141/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 448.1207 - acc: 0.9118 - val_loss: 448.3393 - val_acc: 0.7407\n",
            "Epoch 4142/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 448.0737 - acc: 0.9067 - val_loss: 448.2942 - val_acc: 0.7407\n",
            "Epoch 4143/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 448.0285 - acc: 0.9067 - val_loss: 448.2492 - val_acc: 0.7407\n",
            "Epoch 4144/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 447.9841 - acc: 0.9178 - val_loss: 448.2042 - val_acc: 0.7407\n",
            "Epoch 4145/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 447.9349 - acc: 0.9108 - val_loss: 448.1593 - val_acc: 0.7407\n",
            "Epoch 4146/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 447.8945 - acc: 0.9189 - val_loss: 448.1144 - val_acc: 0.7407\n",
            "Epoch 4147/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 447.8514 - acc: 0.9108 - val_loss: 448.0695 - val_acc: 0.7407\n",
            "Epoch 4148/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 447.8052 - acc: 0.9097 - val_loss: 448.0244 - val_acc: 0.7407\n",
            "Epoch 4149/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 447.7590 - acc: 0.9097 - val_loss: 447.9796 - val_acc: 0.7407\n",
            "Epoch 4150/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 447.7098 - acc: 0.9199 - val_loss: 447.9347 - val_acc: 0.7407\n",
            "Epoch 4151/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 447.6660 - acc: 0.9168 - val_loss: 447.8897 - val_acc: 0.7407\n",
            "Epoch 4152/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 447.6163 - acc: 0.9219 - val_loss: 447.8446 - val_acc: 0.7407\n",
            "Epoch 4153/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 447.5808 - acc: 0.9178 - val_loss: 447.7997 - val_acc: 0.7407\n",
            "Epoch 4154/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 447.5390 - acc: 0.9097 - val_loss: 447.7547 - val_acc: 0.7407\n",
            "Epoch 4155/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 447.4943 - acc: 0.9097 - val_loss: 447.7098 - val_acc: 0.7407\n",
            "Epoch 4156/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 447.4439 - acc: 0.9128 - val_loss: 447.6649 - val_acc: 0.7407\n",
            "Epoch 4157/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 447.3997 - acc: 0.9138 - val_loss: 447.6198 - val_acc: 0.7407\n",
            "Epoch 4158/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 447.3518 - acc: 0.9189 - val_loss: 447.5749 - val_acc: 0.7407\n",
            "Epoch 4159/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 447.3048 - acc: 0.9148 - val_loss: 447.5301 - val_acc: 0.7407\n",
            "Epoch 4160/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 447.2678 - acc: 0.9118 - val_loss: 447.4853 - val_acc: 0.7407\n",
            "Epoch 4161/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 447.2110 - acc: 0.9280 - val_loss: 447.4405 - val_acc: 0.7407\n",
            "Epoch 4162/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 447.1768 - acc: 0.9128 - val_loss: 447.3955 - val_acc: 0.7407\n",
            "Epoch 4163/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 447.1271 - acc: 0.9199 - val_loss: 447.3506 - val_acc: 0.7407\n",
            "Epoch 4164/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 447.0841 - acc: 0.9219 - val_loss: 447.3058 - val_acc: 0.7407\n",
            "Epoch 4165/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 447.0420 - acc: 0.9178 - val_loss: 447.2608 - val_acc: 0.7407\n",
            "Epoch 4166/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 446.9925 - acc: 0.9178 - val_loss: 447.2160 - val_acc: 0.7407\n",
            "Epoch 4167/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 446.9537 - acc: 0.9037 - val_loss: 447.1712 - val_acc: 0.7407\n",
            "Epoch 4168/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 446.9092 - acc: 0.9128 - val_loss: 447.1263 - val_acc: 0.7407\n",
            "Epoch 4169/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 446.8549 - acc: 0.9158 - val_loss: 447.0814 - val_acc: 0.7407\n",
            "Epoch 4170/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 446.8100 - acc: 0.9178 - val_loss: 447.0364 - val_acc: 0.7407\n",
            "Epoch 4171/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 446.7663 - acc: 0.9067 - val_loss: 446.9915 - val_acc: 0.7407\n",
            "Epoch 4172/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 446.7222 - acc: 0.9199 - val_loss: 446.9467 - val_acc: 0.7407\n",
            "Epoch 4173/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 446.6799 - acc: 0.9077 - val_loss: 446.9018 - val_acc: 0.7407\n",
            "Epoch 4174/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 446.6297 - acc: 0.9219 - val_loss: 446.8570 - val_acc: 0.7407\n",
            "Epoch 4175/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 446.5949 - acc: 0.9199 - val_loss: 446.8121 - val_acc: 0.7407\n",
            "Epoch 4176/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 446.5436 - acc: 0.9118 - val_loss: 446.7671 - val_acc: 0.7407\n",
            "Epoch 4177/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 446.4988 - acc: 0.9158 - val_loss: 446.7223 - val_acc: 0.7407\n",
            "Epoch 4178/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 446.4533 - acc: 0.9148 - val_loss: 446.6774 - val_acc: 0.7407\n",
            "Epoch 4179/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 446.4069 - acc: 0.9178 - val_loss: 446.6325 - val_acc: 0.7407\n",
            "Epoch 4180/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 446.3638 - acc: 0.9168 - val_loss: 446.5876 - val_acc: 0.7407\n",
            "Epoch 4181/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 446.3232 - acc: 0.9118 - val_loss: 446.5427 - val_acc: 0.7407\n",
            "Epoch 4182/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 446.2758 - acc: 0.9178 - val_loss: 446.4979 - val_acc: 0.7407\n",
            "Epoch 4183/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 446.2320 - acc: 0.9128 - val_loss: 446.4531 - val_acc: 0.7407\n",
            "Epoch 4184/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 446.1819 - acc: 0.9290 - val_loss: 446.4082 - val_acc: 0.7407\n",
            "Epoch 4185/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 446.1458 - acc: 0.9189 - val_loss: 446.3635 - val_acc: 0.7407\n",
            "Epoch 4186/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 446.0982 - acc: 0.9006 - val_loss: 446.3186 - val_acc: 0.7407\n",
            "Epoch 4187/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 446.0542 - acc: 0.9108 - val_loss: 446.2738 - val_acc: 0.7407\n",
            "Epoch 4188/5000\n",
            "986/986 [==============================] - 0s 295us/step - loss: 446.0042 - acc: 0.9128 - val_loss: 446.2288 - val_acc: 0.7407\n",
            "Epoch 4189/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 445.9667 - acc: 0.9026 - val_loss: 446.1841 - val_acc: 0.7407\n",
            "Epoch 4190/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 445.9191 - acc: 0.9178 - val_loss: 446.1393 - val_acc: 0.7407\n",
            "Epoch 4191/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 445.8676 - acc: 0.9229 - val_loss: 446.0944 - val_acc: 0.7407\n",
            "Epoch 4192/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 445.8262 - acc: 0.9148 - val_loss: 446.0498 - val_acc: 0.7407\n",
            "Epoch 4193/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 445.7828 - acc: 0.9118 - val_loss: 446.0049 - val_acc: 0.7407\n",
            "Epoch 4194/5000\n",
            "986/986 [==============================] - 0s 293us/step - loss: 445.7400 - acc: 0.9118 - val_loss: 445.9601 - val_acc: 0.7407\n",
            "Epoch 4195/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 445.6947 - acc: 0.9168 - val_loss: 445.9152 - val_acc: 0.7407\n",
            "Epoch 4196/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 445.6469 - acc: 0.9219 - val_loss: 445.8701 - val_acc: 0.7407\n",
            "Epoch 4197/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 445.6018 - acc: 0.9239 - val_loss: 445.8255 - val_acc: 0.7407\n",
            "Epoch 4198/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 445.5609 - acc: 0.9189 - val_loss: 445.7810 - val_acc: 0.7407\n",
            "Epoch 4199/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 445.5161 - acc: 0.9199 - val_loss: 445.7362 - val_acc: 0.7407\n",
            "Epoch 4200/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 445.4716 - acc: 0.9118 - val_loss: 445.6915 - val_acc: 0.7407\n",
            "Epoch 4201/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 445.4262 - acc: 0.9108 - val_loss: 445.6466 - val_acc: 0.7407\n",
            "Epoch 4202/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 445.3772 - acc: 0.9229 - val_loss: 445.6020 - val_acc: 0.7407\n",
            "Epoch 4203/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 445.3320 - acc: 0.9178 - val_loss: 445.5571 - val_acc: 0.7407\n",
            "Epoch 4204/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 445.2845 - acc: 0.9239 - val_loss: 445.5124 - val_acc: 0.7407\n",
            "Epoch 4205/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 445.2486 - acc: 0.9138 - val_loss: 445.4675 - val_acc: 0.7407\n",
            "Epoch 4206/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 445.1981 - acc: 0.9229 - val_loss: 445.4229 - val_acc: 0.7407\n",
            "Epoch 4207/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 445.1522 - acc: 0.9189 - val_loss: 445.3782 - val_acc: 0.7407\n",
            "Epoch 4208/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 445.1110 - acc: 0.9219 - val_loss: 445.3334 - val_acc: 0.7407\n",
            "Epoch 4209/5000\n",
            "986/986 [==============================] - 0s 294us/step - loss: 445.0646 - acc: 0.9077 - val_loss: 445.2886 - val_acc: 0.7407\n",
            "Epoch 4210/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 445.0204 - acc: 0.9037 - val_loss: 445.2438 - val_acc: 0.7407\n",
            "Epoch 4211/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 444.9776 - acc: 0.9168 - val_loss: 445.1991 - val_acc: 0.7407\n",
            "Epoch 4212/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 444.9355 - acc: 0.9158 - val_loss: 445.1543 - val_acc: 0.7407\n",
            "Epoch 4213/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 444.8862 - acc: 0.9067 - val_loss: 445.1095 - val_acc: 0.7407\n",
            "Epoch 4214/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 444.8430 - acc: 0.9118 - val_loss: 445.0648 - val_acc: 0.7407\n",
            "Epoch 4215/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 444.7957 - acc: 0.9118 - val_loss: 445.0201 - val_acc: 0.7407\n",
            "Epoch 4216/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 444.7529 - acc: 0.9108 - val_loss: 444.9753 - val_acc: 0.7407\n",
            "Epoch 4217/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 444.7103 - acc: 0.9209 - val_loss: 444.9306 - val_acc: 0.7407\n",
            "Epoch 4218/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 444.6651 - acc: 0.9138 - val_loss: 444.8859 - val_acc: 0.7407\n",
            "Epoch 4219/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 444.6204 - acc: 0.9168 - val_loss: 444.8411 - val_acc: 0.7407\n",
            "Epoch 4220/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 444.5716 - acc: 0.9320 - val_loss: 444.7963 - val_acc: 0.7407\n",
            "Epoch 4221/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 444.5272 - acc: 0.9138 - val_loss: 444.7517 - val_acc: 0.7407\n",
            "Epoch 4222/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 444.4809 - acc: 0.9158 - val_loss: 444.7070 - val_acc: 0.7407\n",
            "Epoch 4223/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 444.4416 - acc: 0.9168 - val_loss: 444.6624 - val_acc: 0.7407\n",
            "Epoch 4224/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 444.3933 - acc: 0.9219 - val_loss: 444.6177 - val_acc: 0.7407\n",
            "Epoch 4225/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 444.3474 - acc: 0.9118 - val_loss: 444.5730 - val_acc: 0.7407\n",
            "Epoch 4226/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 444.3089 - acc: 0.9097 - val_loss: 444.5284 - val_acc: 0.7407\n",
            "Epoch 4227/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 444.2615 - acc: 0.9148 - val_loss: 444.4837 - val_acc: 0.7407\n",
            "Epoch 4228/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 444.2142 - acc: 0.9138 - val_loss: 444.4388 - val_acc: 0.7407\n",
            "Epoch 4229/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 444.1687 - acc: 0.9118 - val_loss: 444.3941 - val_acc: 0.7407\n",
            "Epoch 4230/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 444.1308 - acc: 0.9026 - val_loss: 444.3496 - val_acc: 0.7407\n",
            "Epoch 4231/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 444.0888 - acc: 0.9128 - val_loss: 444.3050 - val_acc: 0.7407\n",
            "Epoch 4232/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 444.0400 - acc: 0.9077 - val_loss: 444.2603 - val_acc: 0.7407\n",
            "Epoch 4233/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 443.9888 - acc: 0.9199 - val_loss: 444.2157 - val_acc: 0.7407\n",
            "Epoch 4234/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 443.9453 - acc: 0.9189 - val_loss: 444.1712 - val_acc: 0.7407\n",
            "Epoch 4235/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 443.9010 - acc: 0.9158 - val_loss: 444.1266 - val_acc: 0.7407\n",
            "Epoch 4236/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 443.8587 - acc: 0.9067 - val_loss: 444.0820 - val_acc: 0.7407\n",
            "Epoch 4237/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 443.8144 - acc: 0.9199 - val_loss: 444.0373 - val_acc: 0.7407\n",
            "Epoch 4238/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 443.7729 - acc: 0.9037 - val_loss: 443.9927 - val_acc: 0.7407\n",
            "Epoch 4239/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 443.7272 - acc: 0.9199 - val_loss: 443.9479 - val_acc: 0.7407\n",
            "Epoch 4240/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 443.6851 - acc: 0.9158 - val_loss: 443.9033 - val_acc: 0.7407\n",
            "Epoch 4241/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 443.6360 - acc: 0.9108 - val_loss: 443.8587 - val_acc: 0.7407\n",
            "Epoch 4242/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 443.5873 - acc: 0.9118 - val_loss: 443.8140 - val_acc: 0.7407\n",
            "Epoch 4243/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 443.5457 - acc: 0.9138 - val_loss: 443.7693 - val_acc: 0.7407\n",
            "Epoch 4244/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 443.4988 - acc: 0.9158 - val_loss: 443.7248 - val_acc: 0.7407\n",
            "Epoch 4245/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 443.4530 - acc: 0.9168 - val_loss: 443.6802 - val_acc: 0.7407\n",
            "Epoch 4246/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 443.4130 - acc: 0.9077 - val_loss: 443.6355 - val_acc: 0.7407\n",
            "Epoch 4247/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 443.3634 - acc: 0.9239 - val_loss: 443.5910 - val_acc: 0.7407\n",
            "Epoch 4248/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 443.3197 - acc: 0.9249 - val_loss: 443.5464 - val_acc: 0.7407\n",
            "Epoch 4249/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 443.2758 - acc: 0.9178 - val_loss: 443.5018 - val_acc: 0.7407\n",
            "Epoch 4250/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 443.2299 - acc: 0.9199 - val_loss: 443.4572 - val_acc: 0.7407\n",
            "Epoch 4251/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 443.1930 - acc: 0.9087 - val_loss: 443.4127 - val_acc: 0.7407\n",
            "Epoch 4252/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 443.1458 - acc: 0.9077 - val_loss: 443.3679 - val_acc: 0.7407\n",
            "Epoch 4253/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 443.1015 - acc: 0.9057 - val_loss: 443.3233 - val_acc: 0.7407\n",
            "Epoch 4254/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 443.0616 - acc: 0.9067 - val_loss: 443.2787 - val_acc: 0.7407\n",
            "Epoch 4255/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 443.0096 - acc: 0.9178 - val_loss: 443.2342 - val_acc: 0.7407\n",
            "Epoch 4256/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 442.9655 - acc: 0.9189 - val_loss: 443.1896 - val_acc: 0.7407\n",
            "Epoch 4257/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 442.9201 - acc: 0.9118 - val_loss: 443.1451 - val_acc: 0.7407\n",
            "Epoch 4258/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 442.8834 - acc: 0.9138 - val_loss: 443.1005 - val_acc: 0.7407\n",
            "Epoch 4259/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 442.8315 - acc: 0.9077 - val_loss: 443.0559 - val_acc: 0.7407\n",
            "Epoch 4260/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 442.7767 - acc: 0.9260 - val_loss: 443.0113 - val_acc: 0.7407\n",
            "Epoch 4261/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 442.7454 - acc: 0.9128 - val_loss: 442.9668 - val_acc: 0.7407\n",
            "Epoch 4262/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 442.6913 - acc: 0.9249 - val_loss: 442.9220 - val_acc: 0.7407\n",
            "Epoch 4263/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 442.6496 - acc: 0.9209 - val_loss: 442.8777 - val_acc: 0.7407\n",
            "Epoch 4264/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 442.6126 - acc: 0.9168 - val_loss: 442.8331 - val_acc: 0.7407\n",
            "Epoch 4265/5000\n",
            "986/986 [==============================] - 0s 295us/step - loss: 442.5697 - acc: 0.9148 - val_loss: 442.7886 - val_acc: 0.7407\n",
            "Epoch 4266/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 442.5202 - acc: 0.9209 - val_loss: 442.7440 - val_acc: 0.7407\n",
            "Epoch 4267/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 442.4811 - acc: 0.9087 - val_loss: 442.6995 - val_acc: 0.7407\n",
            "Epoch 4268/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 442.4283 - acc: 0.9158 - val_loss: 442.6550 - val_acc: 0.7407\n",
            "Epoch 4269/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 442.3871 - acc: 0.9189 - val_loss: 442.6104 - val_acc: 0.7407\n",
            "Epoch 4270/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 442.3462 - acc: 0.9178 - val_loss: 442.5660 - val_acc: 0.7407\n",
            "Epoch 4271/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 442.2930 - acc: 0.9168 - val_loss: 442.5214 - val_acc: 0.7407\n",
            "Epoch 4272/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 442.2494 - acc: 0.9138 - val_loss: 442.4769 - val_acc: 0.7407\n",
            "Epoch 4273/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 442.2034 - acc: 0.9178 - val_loss: 442.4325 - val_acc: 0.7407\n",
            "Epoch 4274/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 442.1571 - acc: 0.9280 - val_loss: 442.3880 - val_acc: 0.7407\n",
            "Epoch 4275/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 442.1153 - acc: 0.9239 - val_loss: 442.3434 - val_acc: 0.7407\n",
            "Epoch 4276/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 442.0768 - acc: 0.9209 - val_loss: 442.2989 - val_acc: 0.7407\n",
            "Epoch 4277/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 442.0284 - acc: 0.9138 - val_loss: 442.2544 - val_acc: 0.7407\n",
            "Epoch 4278/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 441.9859 - acc: 0.9209 - val_loss: 442.2100 - val_acc: 0.7407\n",
            "Epoch 4279/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 441.9401 - acc: 0.9189 - val_loss: 442.1653 - val_acc: 0.7407\n",
            "Epoch 4280/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 441.8973 - acc: 0.9189 - val_loss: 442.1205 - val_acc: 0.7407\n",
            "Epoch 4281/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 441.8435 - acc: 0.9189 - val_loss: 442.0762 - val_acc: 0.7407\n",
            "Epoch 4282/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 441.8083 - acc: 0.9108 - val_loss: 442.0317 - val_acc: 0.7407\n",
            "Epoch 4283/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 441.7665 - acc: 0.9158 - val_loss: 441.9872 - val_acc: 0.7407\n",
            "Epoch 4284/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 441.7179 - acc: 0.9168 - val_loss: 441.9427 - val_acc: 0.7407\n",
            "Epoch 4285/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 441.6707 - acc: 0.9178 - val_loss: 441.8982 - val_acc: 0.7407\n",
            "Epoch 4286/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 441.6293 - acc: 0.9189 - val_loss: 441.8537 - val_acc: 0.7407\n",
            "Epoch 4287/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 441.5818 - acc: 0.9199 - val_loss: 441.8094 - val_acc: 0.7407\n",
            "Epoch 4288/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 441.5339 - acc: 0.9270 - val_loss: 441.7647 - val_acc: 0.7407\n",
            "Epoch 4289/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 441.4964 - acc: 0.9249 - val_loss: 441.7203 - val_acc: 0.7407\n",
            "Epoch 4290/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 441.4559 - acc: 0.9097 - val_loss: 441.6757 - val_acc: 0.7407\n",
            "Epoch 4291/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 441.4077 - acc: 0.9138 - val_loss: 441.6314 - val_acc: 0.7407\n",
            "Epoch 4292/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 441.3664 - acc: 0.9006 - val_loss: 441.5870 - val_acc: 0.7407\n",
            "Epoch 4293/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 441.3125 - acc: 0.9199 - val_loss: 441.5426 - val_acc: 0.7407\n",
            "Epoch 4294/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 441.2743 - acc: 0.9138 - val_loss: 441.4981 - val_acc: 0.7407\n",
            "Epoch 4295/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 441.2193 - acc: 0.9260 - val_loss: 441.4536 - val_acc: 0.7407\n",
            "Epoch 4296/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 441.1838 - acc: 0.9128 - val_loss: 441.4093 - val_acc: 0.7407\n",
            "Epoch 4297/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 441.1355 - acc: 0.9270 - val_loss: 441.3649 - val_acc: 0.7407\n",
            "Epoch 4298/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 441.0981 - acc: 0.9199 - val_loss: 441.3204 - val_acc: 0.7407\n",
            "Epoch 4299/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 441.0477 - acc: 0.9168 - val_loss: 441.2761 - val_acc: 0.7407\n",
            "Epoch 4300/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 441.0087 - acc: 0.9158 - val_loss: 441.2316 - val_acc: 0.7407\n",
            "Epoch 4301/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 440.9603 - acc: 0.9249 - val_loss: 441.1871 - val_acc: 0.7407\n",
            "Epoch 4302/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 440.9203 - acc: 0.9219 - val_loss: 441.1428 - val_acc: 0.7407\n",
            "Epoch 4303/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 440.8678 - acc: 0.9108 - val_loss: 441.0983 - val_acc: 0.7407\n",
            "Epoch 4304/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 440.8281 - acc: 0.9178 - val_loss: 441.0539 - val_acc: 0.7407\n",
            "Epoch 4305/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 440.7821 - acc: 0.9128 - val_loss: 441.0097 - val_acc: 0.7407\n",
            "Epoch 4306/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 440.7388 - acc: 0.9178 - val_loss: 440.9651 - val_acc: 0.7407\n",
            "Epoch 4307/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 440.6940 - acc: 0.9168 - val_loss: 440.9207 - val_acc: 0.7407\n",
            "Epoch 4308/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 440.6491 - acc: 0.9290 - val_loss: 440.8764 - val_acc: 0.7407\n",
            "Epoch 4309/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 440.6007 - acc: 0.9199 - val_loss: 440.8320 - val_acc: 0.7407\n",
            "Epoch 4310/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 440.5657 - acc: 0.9158 - val_loss: 440.7875 - val_acc: 0.7407\n",
            "Epoch 4311/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 440.5172 - acc: 0.9158 - val_loss: 440.7432 - val_acc: 0.7407\n",
            "Epoch 4312/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 440.4725 - acc: 0.9189 - val_loss: 440.6989 - val_acc: 0.7407\n",
            "Epoch 4313/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 440.4361 - acc: 0.9209 - val_loss: 440.6546 - val_acc: 0.7407\n",
            "Epoch 4314/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 440.3772 - acc: 0.9229 - val_loss: 440.6102 - val_acc: 0.7407\n",
            "Epoch 4315/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 440.3383 - acc: 0.9219 - val_loss: 440.5658 - val_acc: 0.7407\n",
            "Epoch 4316/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 440.2946 - acc: 0.9199 - val_loss: 440.5214 - val_acc: 0.7407\n",
            "Epoch 4317/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 440.2520 - acc: 0.9219 - val_loss: 440.4770 - val_acc: 0.7407\n",
            "Epoch 4318/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 440.1994 - acc: 0.9219 - val_loss: 440.4327 - val_acc: 0.7407\n",
            "Epoch 4319/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 440.1603 - acc: 0.9097 - val_loss: 440.3883 - val_acc: 0.7407\n",
            "Epoch 4320/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 440.1162 - acc: 0.9158 - val_loss: 440.3438 - val_acc: 0.7407\n",
            "Epoch 4321/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 440.0691 - acc: 0.9249 - val_loss: 440.2994 - val_acc: 0.7407\n",
            "Epoch 4322/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 440.0253 - acc: 0.9249 - val_loss: 440.2550 - val_acc: 0.7407\n",
            "Epoch 4323/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 439.9871 - acc: 0.9168 - val_loss: 440.2106 - val_acc: 0.7407\n",
            "Epoch 4324/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 439.9395 - acc: 0.9199 - val_loss: 440.1664 - val_acc: 0.7407\n",
            "Epoch 4325/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 439.8928 - acc: 0.9239 - val_loss: 440.1221 - val_acc: 0.7407\n",
            "Epoch 4326/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 439.8496 - acc: 0.9219 - val_loss: 440.0777 - val_acc: 0.7407\n",
            "Epoch 4327/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 439.8066 - acc: 0.9148 - val_loss: 440.0334 - val_acc: 0.7407\n",
            "Epoch 4328/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 439.7580 - acc: 0.9249 - val_loss: 439.9890 - val_acc: 0.7407\n",
            "Epoch 4329/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 439.7189 - acc: 0.9158 - val_loss: 439.9446 - val_acc: 0.7407\n",
            "Epoch 4330/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 439.6719 - acc: 0.9057 - val_loss: 439.9005 - val_acc: 0.7407\n",
            "Epoch 4331/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 439.6273 - acc: 0.9209 - val_loss: 439.8561 - val_acc: 0.7407\n",
            "Epoch 4332/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 439.5862 - acc: 0.9138 - val_loss: 439.8118 - val_acc: 0.7407\n",
            "Epoch 4333/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 439.5376 - acc: 0.9189 - val_loss: 439.7676 - val_acc: 0.7407\n",
            "Epoch 4334/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 439.4942 - acc: 0.9148 - val_loss: 439.7233 - val_acc: 0.7407\n",
            "Epoch 4335/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 439.4516 - acc: 0.9229 - val_loss: 439.6790 - val_acc: 0.7407\n",
            "Epoch 4336/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 439.4013 - acc: 0.9199 - val_loss: 439.6346 - val_acc: 0.7407\n",
            "Epoch 4337/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 439.3598 - acc: 0.9229 - val_loss: 439.5903 - val_acc: 0.7407\n",
            "Epoch 4338/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 439.3198 - acc: 0.9168 - val_loss: 439.5460 - val_acc: 0.7407\n",
            "Epoch 4339/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 439.2753 - acc: 0.9178 - val_loss: 439.5017 - val_acc: 0.7407\n",
            "Epoch 4340/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 439.2313 - acc: 0.9057 - val_loss: 439.4574 - val_acc: 0.7407\n",
            "Epoch 4341/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 439.1838 - acc: 0.9239 - val_loss: 439.4131 - val_acc: 0.7407\n",
            "Epoch 4342/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 439.1451 - acc: 0.9067 - val_loss: 439.3689 - val_acc: 0.7407\n",
            "Epoch 4343/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 439.0932 - acc: 0.9270 - val_loss: 439.3247 - val_acc: 0.7407\n",
            "Epoch 4344/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 439.0586 - acc: 0.9148 - val_loss: 439.2805 - val_acc: 0.7407\n",
            "Epoch 4345/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 439.0114 - acc: 0.9249 - val_loss: 439.2363 - val_acc: 0.7407\n",
            "Epoch 4346/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 438.9602 - acc: 0.9260 - val_loss: 439.1920 - val_acc: 0.7407\n",
            "Epoch 4347/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 438.9181 - acc: 0.9138 - val_loss: 439.1477 - val_acc: 0.7407\n",
            "Epoch 4348/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 438.8705 - acc: 0.9280 - val_loss: 439.1034 - val_acc: 0.7407\n",
            "Epoch 4349/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 438.8325 - acc: 0.9148 - val_loss: 439.0592 - val_acc: 0.7407\n",
            "Epoch 4350/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 438.7873 - acc: 0.9168 - val_loss: 439.0150 - val_acc: 0.7407\n",
            "Epoch 4351/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 438.7434 - acc: 0.9108 - val_loss: 438.9707 - val_acc: 0.7407\n",
            "Epoch 4352/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 438.7002 - acc: 0.9148 - val_loss: 438.9265 - val_acc: 0.7407\n",
            "Epoch 4353/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 438.6550 - acc: 0.9178 - val_loss: 438.8822 - val_acc: 0.7407\n",
            "Epoch 4354/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 438.6076 - acc: 0.9138 - val_loss: 438.8379 - val_acc: 0.7407\n",
            "Epoch 4355/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 438.5632 - acc: 0.9199 - val_loss: 438.7936 - val_acc: 0.7407\n",
            "Epoch 4356/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 438.5211 - acc: 0.9219 - val_loss: 438.7497 - val_acc: 0.7407\n",
            "Epoch 4357/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 438.4748 - acc: 0.9280 - val_loss: 438.7054 - val_acc: 0.7407\n",
            "Epoch 4358/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 438.4272 - acc: 0.9300 - val_loss: 438.6612 - val_acc: 0.7407\n",
            "Epoch 4359/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 438.3922 - acc: 0.9057 - val_loss: 438.6169 - val_acc: 0.7407\n",
            "Epoch 4360/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 438.3496 - acc: 0.9178 - val_loss: 438.5728 - val_acc: 0.7407\n",
            "Epoch 4361/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 438.3026 - acc: 0.9219 - val_loss: 438.5285 - val_acc: 0.7407\n",
            "Epoch 4362/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 438.2521 - acc: 0.9219 - val_loss: 438.4842 - val_acc: 0.7407\n",
            "Epoch 4363/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 438.2124 - acc: 0.9118 - val_loss: 438.4400 - val_acc: 0.7407\n",
            "Epoch 4364/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 438.1701 - acc: 0.9148 - val_loss: 438.3958 - val_acc: 0.7407\n",
            "Epoch 4365/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 438.1186 - acc: 0.9209 - val_loss: 438.3517 - val_acc: 0.7407\n",
            "Epoch 4366/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 438.0784 - acc: 0.9199 - val_loss: 438.3075 - val_acc: 0.7407\n",
            "Epoch 4367/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 438.0291 - acc: 0.9209 - val_loss: 438.2633 - val_acc: 0.7407\n",
            "Epoch 4368/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 437.9900 - acc: 0.9260 - val_loss: 438.2191 - val_acc: 0.7407\n",
            "Epoch 4369/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 437.9465 - acc: 0.9148 - val_loss: 438.1750 - val_acc: 0.7407\n",
            "Epoch 4370/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 437.9035 - acc: 0.9239 - val_loss: 438.1308 - val_acc: 0.7407\n",
            "Epoch 4371/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 437.8494 - acc: 0.9300 - val_loss: 438.0868 - val_acc: 0.7407\n",
            "Epoch 4372/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 437.8155 - acc: 0.9138 - val_loss: 438.0426 - val_acc: 0.7407\n",
            "Epoch 4373/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 437.7795 - acc: 0.9158 - val_loss: 437.9983 - val_acc: 0.7407\n",
            "Epoch 4374/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 437.7266 - acc: 0.9057 - val_loss: 437.9542 - val_acc: 0.7407\n",
            "Epoch 4375/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 437.6869 - acc: 0.9118 - val_loss: 437.9100 - val_acc: 0.7407\n",
            "Epoch 4376/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 437.6347 - acc: 0.9168 - val_loss: 437.8658 - val_acc: 0.7407\n",
            "Epoch 4377/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 437.5898 - acc: 0.9138 - val_loss: 437.8216 - val_acc: 0.7407\n",
            "Epoch 4378/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 437.5419 - acc: 0.9300 - val_loss: 437.7773 - val_acc: 0.7407\n",
            "Epoch 4379/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 437.5067 - acc: 0.9168 - val_loss: 437.7334 - val_acc: 0.7407\n",
            "Epoch 4380/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 437.4620 - acc: 0.9219 - val_loss: 437.6893 - val_acc: 0.7407\n",
            "Epoch 4381/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 437.4127 - acc: 0.9158 - val_loss: 437.6452 - val_acc: 0.7407\n",
            "Epoch 4382/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 437.3669 - acc: 0.9290 - val_loss: 437.6010 - val_acc: 0.7407\n",
            "Epoch 4383/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 437.3234 - acc: 0.9158 - val_loss: 437.5568 - val_acc: 0.7407\n",
            "Epoch 4384/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 437.2825 - acc: 0.9239 - val_loss: 437.5126 - val_acc: 0.7407\n",
            "Epoch 4385/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 437.2421 - acc: 0.9148 - val_loss: 437.4684 - val_acc: 0.7407\n",
            "Epoch 4386/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 437.1940 - acc: 0.9118 - val_loss: 437.4243 - val_acc: 0.7407\n",
            "Epoch 4387/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 437.1443 - acc: 0.9219 - val_loss: 437.3802 - val_acc: 0.7407\n",
            "Epoch 4388/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 437.1101 - acc: 0.9118 - val_loss: 437.3361 - val_acc: 0.7407\n",
            "Epoch 4389/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 437.0647 - acc: 0.9148 - val_loss: 437.2920 - val_acc: 0.7407\n",
            "Epoch 4390/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 437.0181 - acc: 0.9138 - val_loss: 437.2478 - val_acc: 0.7407\n",
            "Epoch 4391/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 436.9757 - acc: 0.9229 - val_loss: 437.2038 - val_acc: 0.7407\n",
            "Epoch 4392/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 436.9344 - acc: 0.9168 - val_loss: 437.1598 - val_acc: 0.7407\n",
            "Epoch 4393/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 436.8829 - acc: 0.9128 - val_loss: 437.1156 - val_acc: 0.7407\n",
            "Epoch 4394/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 436.8371 - acc: 0.9229 - val_loss: 437.0716 - val_acc: 0.7407\n",
            "Epoch 4395/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 436.8022 - acc: 0.9158 - val_loss: 437.0274 - val_acc: 0.7434\n",
            "Epoch 4396/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 436.7522 - acc: 0.9280 - val_loss: 436.9835 - val_acc: 0.7407\n",
            "Epoch 4397/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 436.7063 - acc: 0.9178 - val_loss: 436.9393 - val_acc: 0.7407\n",
            "Epoch 4398/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 436.6686 - acc: 0.9128 - val_loss: 436.8952 - val_acc: 0.7407\n",
            "Epoch 4399/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 436.6248 - acc: 0.9229 - val_loss: 436.8512 - val_acc: 0.7434\n",
            "Epoch 4400/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 436.5803 - acc: 0.9118 - val_loss: 436.8071 - val_acc: 0.7434\n",
            "Epoch 4401/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 436.5307 - acc: 0.9189 - val_loss: 436.7631 - val_acc: 0.7434\n",
            "Epoch 4402/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 436.4887 - acc: 0.9199 - val_loss: 436.7191 - val_acc: 0.7434\n",
            "Epoch 4403/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 436.4435 - acc: 0.9138 - val_loss: 436.6751 - val_acc: 0.7434\n",
            "Epoch 4404/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 436.3965 - acc: 0.9239 - val_loss: 436.6311 - val_acc: 0.7434\n",
            "Epoch 4405/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 436.3561 - acc: 0.9189 - val_loss: 436.5871 - val_acc: 0.7434\n",
            "Epoch 4406/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 436.3154 - acc: 0.9077 - val_loss: 436.5431 - val_acc: 0.7434\n",
            "Epoch 4407/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 436.2669 - acc: 0.9128 - val_loss: 436.4989 - val_acc: 0.7434\n",
            "Epoch 4408/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 436.2222 - acc: 0.9260 - val_loss: 436.4549 - val_acc: 0.7434\n",
            "Epoch 4409/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 436.1801 - acc: 0.9178 - val_loss: 436.4108 - val_acc: 0.7434\n",
            "Epoch 4410/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 436.1368 - acc: 0.9199 - val_loss: 436.3668 - val_acc: 0.7434\n",
            "Epoch 4411/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 436.0880 - acc: 0.9219 - val_loss: 436.3228 - val_acc: 0.7434\n",
            "Epoch 4412/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 436.0530 - acc: 0.9168 - val_loss: 436.2788 - val_acc: 0.7434\n",
            "Epoch 4413/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 436.0062 - acc: 0.9138 - val_loss: 436.2347 - val_acc: 0.7434\n",
            "Epoch 4414/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 435.9579 - acc: 0.9219 - val_loss: 436.1906 - val_acc: 0.7434\n",
            "Epoch 4415/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 435.9171 - acc: 0.9209 - val_loss: 436.1467 - val_acc: 0.7407\n",
            "Epoch 4416/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 435.8726 - acc: 0.9249 - val_loss: 436.1026 - val_acc: 0.7407\n",
            "Epoch 4417/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 435.8242 - acc: 0.9219 - val_loss: 436.0586 - val_acc: 0.7434\n",
            "Epoch 4418/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 435.7849 - acc: 0.9168 - val_loss: 436.0145 - val_acc: 0.7434\n",
            "Epoch 4419/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 435.7383 - acc: 0.9189 - val_loss: 435.9703 - val_acc: 0.7434\n",
            "Epoch 4420/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 435.6965 - acc: 0.9168 - val_loss: 435.9265 - val_acc: 0.7434\n",
            "Epoch 4421/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 435.6563 - acc: 0.9087 - val_loss: 435.8825 - val_acc: 0.7434\n",
            "Epoch 4422/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 435.6104 - acc: 0.9168 - val_loss: 435.8384 - val_acc: 0.7434\n",
            "Epoch 4423/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 435.5637 - acc: 0.9138 - val_loss: 435.7945 - val_acc: 0.7434\n",
            "Epoch 4424/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 435.5205 - acc: 0.9128 - val_loss: 435.7504 - val_acc: 0.7434\n",
            "Epoch 4425/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 435.4779 - acc: 0.9260 - val_loss: 435.7066 - val_acc: 0.7434\n",
            "Epoch 4426/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 435.4291 - acc: 0.9209 - val_loss: 435.6627 - val_acc: 0.7434\n",
            "Epoch 4427/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 435.3897 - acc: 0.9158 - val_loss: 435.6188 - val_acc: 0.7434\n",
            "Epoch 4428/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 435.3395 - acc: 0.9270 - val_loss: 435.5749 - val_acc: 0.7434\n",
            "Epoch 4429/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 435.3044 - acc: 0.9189 - val_loss: 435.5311 - val_acc: 0.7434\n",
            "Epoch 4430/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 435.2572 - acc: 0.9219 - val_loss: 435.4871 - val_acc: 0.7434\n",
            "Epoch 4431/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 435.2142 - acc: 0.9148 - val_loss: 435.4434 - val_acc: 0.7434\n",
            "Epoch 4432/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 435.1695 - acc: 0.9097 - val_loss: 435.3996 - val_acc: 0.7434\n",
            "Epoch 4433/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 435.1229 - acc: 0.9260 - val_loss: 435.3557 - val_acc: 0.7434\n",
            "Epoch 4434/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 435.0848 - acc: 0.9077 - val_loss: 435.3118 - val_acc: 0.7434\n",
            "Epoch 4435/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 435.0407 - acc: 0.9057 - val_loss: 435.2677 - val_acc: 0.7434\n",
            "Epoch 4436/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 434.9903 - acc: 0.9097 - val_loss: 435.2239 - val_acc: 0.7434\n",
            "Epoch 4437/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 434.9517 - acc: 0.9118 - val_loss: 435.1801 - val_acc: 0.7434\n",
            "Epoch 4438/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 434.8987 - acc: 0.9189 - val_loss: 435.1361 - val_acc: 0.7434\n",
            "Epoch 4439/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 434.8636 - acc: 0.9138 - val_loss: 435.0922 - val_acc: 0.7407\n",
            "Epoch 4440/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 434.8140 - acc: 0.9148 - val_loss: 435.0484 - val_acc: 0.7434\n",
            "Epoch 4441/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 434.7833 - acc: 0.9178 - val_loss: 435.0041 - val_acc: 0.7434\n",
            "Epoch 4442/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 434.7286 - acc: 0.9128 - val_loss: 434.9602 - val_acc: 0.7434\n",
            "Epoch 4443/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 434.6835 - acc: 0.9189 - val_loss: 434.9164 - val_acc: 0.7434\n",
            "Epoch 4444/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 434.6444 - acc: 0.9158 - val_loss: 434.8723 - val_acc: 0.7434\n",
            "Epoch 4445/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 434.5975 - acc: 0.9178 - val_loss: 434.8284 - val_acc: 0.7434\n",
            "Epoch 4446/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 434.5531 - acc: 0.9219 - val_loss: 434.7846 - val_acc: 0.7434\n",
            "Epoch 4447/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 434.5107 - acc: 0.9158 - val_loss: 434.7408 - val_acc: 0.7434\n",
            "Epoch 4448/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 434.4664 - acc: 0.9168 - val_loss: 434.6968 - val_acc: 0.7434\n",
            "Epoch 4449/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 434.4223 - acc: 0.9108 - val_loss: 434.6529 - val_acc: 0.7434\n",
            "Epoch 4450/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 434.3742 - acc: 0.9260 - val_loss: 434.6090 - val_acc: 0.7434\n",
            "Epoch 4451/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 434.3300 - acc: 0.9270 - val_loss: 434.5651 - val_acc: 0.7434\n",
            "Epoch 4452/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 434.2867 - acc: 0.9219 - val_loss: 434.5213 - val_acc: 0.7434\n",
            "Epoch 4453/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 434.2381 - acc: 0.9239 - val_loss: 434.4774 - val_acc: 0.7434\n",
            "Epoch 4454/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 434.2020 - acc: 0.9138 - val_loss: 434.4337 - val_acc: 0.7434\n",
            "Epoch 4455/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 434.1604 - acc: 0.9178 - val_loss: 434.3897 - val_acc: 0.7434\n",
            "Epoch 4456/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 434.1180 - acc: 0.9138 - val_loss: 434.3459 - val_acc: 0.7434\n",
            "Epoch 4457/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 434.0724 - acc: 0.9219 - val_loss: 434.3021 - val_acc: 0.7434\n",
            "Epoch 4458/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 434.0284 - acc: 0.9178 - val_loss: 434.2583 - val_acc: 0.7434\n",
            "Epoch 4459/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 433.9869 - acc: 0.9128 - val_loss: 434.2144 - val_acc: 0.7434\n",
            "Epoch 4460/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 433.9422 - acc: 0.9209 - val_loss: 434.1705 - val_acc: 0.7434\n",
            "Epoch 4461/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 433.8934 - acc: 0.9290 - val_loss: 434.1267 - val_acc: 0.7434\n",
            "Epoch 4462/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 433.8471 - acc: 0.9209 - val_loss: 434.0829 - val_acc: 0.7434\n",
            "Epoch 4463/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 433.8113 - acc: 0.9108 - val_loss: 434.0390 - val_acc: 0.7434\n",
            "Epoch 4464/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 433.7675 - acc: 0.9168 - val_loss: 433.9952 - val_acc: 0.7434\n",
            "Epoch 4465/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 433.7214 - acc: 0.9209 - val_loss: 433.9513 - val_acc: 0.7434\n",
            "Epoch 4466/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 433.6776 - acc: 0.9158 - val_loss: 433.9076 - val_acc: 0.7434\n",
            "Epoch 4467/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 433.6310 - acc: 0.9249 - val_loss: 433.8638 - val_acc: 0.7434\n",
            "Epoch 4468/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 433.5854 - acc: 0.9158 - val_loss: 433.8199 - val_acc: 0.7434\n",
            "Epoch 4469/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 433.5489 - acc: 0.9128 - val_loss: 433.7760 - val_acc: 0.7434\n",
            "Epoch 4470/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 433.4971 - acc: 0.9138 - val_loss: 433.7322 - val_acc: 0.7434\n",
            "Epoch 4471/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 433.4511 - acc: 0.9371 - val_loss: 433.6886 - val_acc: 0.7434\n",
            "Epoch 4472/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 433.4199 - acc: 0.9097 - val_loss: 433.6448 - val_acc: 0.7434\n",
            "Epoch 4473/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 433.3664 - acc: 0.9118 - val_loss: 433.6009 - val_acc: 0.7434\n",
            "Epoch 4474/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 433.3216 - acc: 0.9189 - val_loss: 433.5571 - val_acc: 0.7434\n",
            "Epoch 4475/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 433.2813 - acc: 0.9158 - val_loss: 433.5134 - val_acc: 0.7434\n",
            "Epoch 4476/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 433.2356 - acc: 0.9270 - val_loss: 433.4696 - val_acc: 0.7434\n",
            "Epoch 4477/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 433.1853 - acc: 0.9209 - val_loss: 433.4260 - val_acc: 0.7434\n",
            "Epoch 4478/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 433.1513 - acc: 0.9219 - val_loss: 433.3821 - val_acc: 0.7434\n",
            "Epoch 4479/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 433.1097 - acc: 0.9178 - val_loss: 433.3383 - val_acc: 0.7434\n",
            "Epoch 4480/5000\n",
            "986/986 [==============================] - 0s 288us/step - loss: 433.0613 - acc: 0.9209 - val_loss: 433.2945 - val_acc: 0.7434\n",
            "Epoch 4481/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 433.0163 - acc: 0.9128 - val_loss: 433.2507 - val_acc: 0.7434\n",
            "Epoch 4482/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 432.9798 - acc: 0.9148 - val_loss: 433.2070 - val_acc: 0.7434\n",
            "Epoch 4483/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 432.9227 - acc: 0.9249 - val_loss: 433.1631 - val_acc: 0.7434\n",
            "Epoch 4484/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 432.8877 - acc: 0.9158 - val_loss: 433.1194 - val_acc: 0.7434\n",
            "Epoch 4485/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 432.8457 - acc: 0.9229 - val_loss: 433.0758 - val_acc: 0.7434\n",
            "Epoch 4486/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 432.7967 - acc: 0.9239 - val_loss: 433.0321 - val_acc: 0.7434\n",
            "Epoch 4487/5000\n",
            "986/986 [==============================] - 0s 303us/step - loss: 432.7502 - acc: 0.9239 - val_loss: 432.9884 - val_acc: 0.7434\n",
            "Epoch 4488/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 432.7075 - acc: 0.9280 - val_loss: 432.9447 - val_acc: 0.7434\n",
            "Epoch 4489/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 432.6693 - acc: 0.9168 - val_loss: 432.9008 - val_acc: 0.7434\n",
            "Epoch 4490/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 432.6257 - acc: 0.8996 - val_loss: 432.8571 - val_acc: 0.7434\n",
            "Epoch 4491/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 432.5787 - acc: 0.9118 - val_loss: 432.8135 - val_acc: 0.7434\n",
            "Epoch 4492/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 432.5440 - acc: 0.9148 - val_loss: 432.7697 - val_acc: 0.7434\n",
            "Epoch 4493/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 432.4921 - acc: 0.9199 - val_loss: 432.7259 - val_acc: 0.7434\n",
            "Epoch 4494/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 432.4512 - acc: 0.9209 - val_loss: 432.6822 - val_acc: 0.7434\n",
            "Epoch 4495/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 432.4047 - acc: 0.9219 - val_loss: 432.6386 - val_acc: 0.7434\n",
            "Epoch 4496/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 432.3706 - acc: 0.9016 - val_loss: 432.5948 - val_acc: 0.7434\n",
            "Epoch 4497/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 432.3217 - acc: 0.9077 - val_loss: 432.5511 - val_acc: 0.7434\n",
            "Epoch 4498/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 432.2717 - acc: 0.9280 - val_loss: 432.5073 - val_acc: 0.7434\n",
            "Epoch 4499/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 432.2344 - acc: 0.9168 - val_loss: 432.4636 - val_acc: 0.7434\n",
            "Epoch 4500/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 432.1827 - acc: 0.9219 - val_loss: 432.4200 - val_acc: 0.7434\n",
            "Epoch 4501/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 432.1457 - acc: 0.9097 - val_loss: 432.3764 - val_acc: 0.7460\n",
            "Epoch 4502/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 432.1012 - acc: 0.9168 - val_loss: 432.3327 - val_acc: 0.7460\n",
            "Epoch 4503/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 432.0610 - acc: 0.9108 - val_loss: 432.2890 - val_acc: 0.7460\n",
            "Epoch 4504/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 432.0134 - acc: 0.9209 - val_loss: 432.2454 - val_acc: 0.7460\n",
            "Epoch 4505/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 431.9756 - acc: 0.9158 - val_loss: 432.2017 - val_acc: 0.7460\n",
            "Epoch 4506/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 431.9232 - acc: 0.9320 - val_loss: 432.1581 - val_acc: 0.7460\n",
            "Epoch 4507/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 431.8844 - acc: 0.9118 - val_loss: 432.1145 - val_acc: 0.7460\n",
            "Epoch 4508/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 431.8378 - acc: 0.9128 - val_loss: 432.0708 - val_acc: 0.7460\n",
            "Epoch 4509/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 431.7974 - acc: 0.9118 - val_loss: 432.0271 - val_acc: 0.7460\n",
            "Epoch 4510/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 431.7511 - acc: 0.9138 - val_loss: 431.9835 - val_acc: 0.7460\n",
            "Epoch 4511/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 431.7061 - acc: 0.9148 - val_loss: 431.9399 - val_acc: 0.7460\n",
            "Epoch 4512/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 431.6616 - acc: 0.9280 - val_loss: 431.8961 - val_acc: 0.7434\n",
            "Epoch 4513/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 431.6181 - acc: 0.9067 - val_loss: 431.8526 - val_acc: 0.7460\n",
            "Epoch 4514/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 431.5745 - acc: 0.9189 - val_loss: 431.8090 - val_acc: 0.7434\n",
            "Epoch 4515/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 431.5281 - acc: 0.9199 - val_loss: 431.7653 - val_acc: 0.7460\n",
            "Epoch 4516/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 431.4883 - acc: 0.9097 - val_loss: 431.7218 - val_acc: 0.7434\n",
            "Epoch 4517/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 431.4449 - acc: 0.9310 - val_loss: 431.6780 - val_acc: 0.7434\n",
            "Epoch 4518/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 431.4009 - acc: 0.9128 - val_loss: 431.6344 - val_acc: 0.7434\n",
            "Epoch 4519/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 431.3539 - acc: 0.9249 - val_loss: 431.5907 - val_acc: 0.7434\n",
            "Epoch 4520/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 431.3136 - acc: 0.9260 - val_loss: 431.5470 - val_acc: 0.7434\n",
            "Epoch 4521/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 431.2642 - acc: 0.9320 - val_loss: 431.5035 - val_acc: 0.7434\n",
            "Epoch 4522/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 431.2258 - acc: 0.9148 - val_loss: 431.4598 - val_acc: 0.7460\n",
            "Epoch 4523/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 431.1802 - acc: 0.9249 - val_loss: 431.4162 - val_acc: 0.7434\n",
            "Epoch 4524/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 431.1380 - acc: 0.9239 - val_loss: 431.3725 - val_acc: 0.7434\n",
            "Epoch 4525/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 431.0929 - acc: 0.9148 - val_loss: 431.3290 - val_acc: 0.7460\n",
            "Epoch 4526/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 431.0589 - acc: 0.9087 - val_loss: 431.2854 - val_acc: 0.7434\n",
            "Epoch 4527/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 431.0083 - acc: 0.9199 - val_loss: 431.2419 - val_acc: 0.7434\n",
            "Epoch 4528/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 430.9615 - acc: 0.9209 - val_loss: 431.1984 - val_acc: 0.7460\n",
            "Epoch 4529/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 430.9139 - acc: 0.9219 - val_loss: 431.1549 - val_acc: 0.7434\n",
            "Epoch 4530/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 430.8718 - acc: 0.9199 - val_loss: 431.1113 - val_acc: 0.7434\n",
            "Epoch 4531/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 430.8282 - acc: 0.9341 - val_loss: 431.0678 - val_acc: 0.7434\n",
            "Epoch 4532/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 430.7905 - acc: 0.9097 - val_loss: 431.0242 - val_acc: 0.7434\n",
            "Epoch 4533/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 430.7508 - acc: 0.9189 - val_loss: 430.9807 - val_acc: 0.7460\n",
            "Epoch 4534/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 430.7003 - acc: 0.9270 - val_loss: 430.9370 - val_acc: 0.7460\n",
            "Epoch 4535/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 430.6600 - acc: 0.9178 - val_loss: 430.8934 - val_acc: 0.7460\n",
            "Epoch 4536/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 430.6180 - acc: 0.9118 - val_loss: 430.8498 - val_acc: 0.7460\n",
            "Epoch 4537/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 430.5717 - acc: 0.9219 - val_loss: 430.8061 - val_acc: 0.7460\n",
            "Epoch 4538/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 430.5259 - acc: 0.9239 - val_loss: 430.7627 - val_acc: 0.7434\n",
            "Epoch 4539/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 430.4837 - acc: 0.9168 - val_loss: 430.7192 - val_acc: 0.7434\n",
            "Epoch 4540/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 430.4438 - acc: 0.9199 - val_loss: 430.6756 - val_acc: 0.7434\n",
            "Epoch 4541/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 430.3998 - acc: 0.9047 - val_loss: 430.6319 - val_acc: 0.7460\n",
            "Epoch 4542/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 430.3523 - acc: 0.9178 - val_loss: 430.5884 - val_acc: 0.7460\n",
            "Epoch 4543/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 430.3166 - acc: 0.9219 - val_loss: 430.5449 - val_acc: 0.7434\n",
            "Epoch 4544/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 430.2716 - acc: 0.9108 - val_loss: 430.5015 - val_acc: 0.7460\n",
            "Epoch 4545/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 430.2232 - acc: 0.9219 - val_loss: 430.4579 - val_acc: 0.7434\n",
            "Epoch 4546/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 430.1848 - acc: 0.9178 - val_loss: 430.4144 - val_acc: 0.7434\n",
            "Epoch 4547/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 430.1308 - acc: 0.9209 - val_loss: 430.3709 - val_acc: 0.7434\n",
            "Epoch 4548/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 430.0896 - acc: 0.9168 - val_loss: 430.3273 - val_acc: 0.7434\n",
            "Epoch 4549/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 430.0438 - acc: 0.9320 - val_loss: 430.2838 - val_acc: 0.7434\n",
            "Epoch 4550/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 430.0077 - acc: 0.9108 - val_loss: 430.2403 - val_acc: 0.7434\n",
            "Epoch 4551/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 429.9572 - acc: 0.9189 - val_loss: 430.1967 - val_acc: 0.7434\n",
            "Epoch 4552/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 429.9156 - acc: 0.9219 - val_loss: 430.1533 - val_acc: 0.7434\n",
            "Epoch 4553/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 429.8785 - acc: 0.9118 - val_loss: 430.1099 - val_acc: 0.7460\n",
            "Epoch 4554/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 429.8392 - acc: 0.9199 - val_loss: 430.0663 - val_acc: 0.7434\n",
            "Epoch 4555/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 429.7903 - acc: 0.9087 - val_loss: 430.0228 - val_acc: 0.7434\n",
            "Epoch 4556/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 429.7496 - acc: 0.9118 - val_loss: 429.9794 - val_acc: 0.7434\n",
            "Epoch 4557/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 429.7007 - acc: 0.9209 - val_loss: 429.9357 - val_acc: 0.7460\n",
            "Epoch 4558/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 429.6550 - acc: 0.9249 - val_loss: 429.8922 - val_acc: 0.7460\n",
            "Epoch 4559/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 429.6134 - acc: 0.9229 - val_loss: 429.8488 - val_acc: 0.7460\n",
            "Epoch 4560/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 429.5713 - acc: 0.9108 - val_loss: 429.8052 - val_acc: 0.7460\n",
            "Epoch 4561/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 429.5294 - acc: 0.9138 - val_loss: 429.7619 - val_acc: 0.7434\n",
            "Epoch 4562/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 429.4866 - acc: 0.9168 - val_loss: 429.7184 - val_acc: 0.7434\n",
            "Epoch 4563/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 429.4386 - acc: 0.9239 - val_loss: 429.6748 - val_acc: 0.7434\n",
            "Epoch 4564/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 429.3991 - acc: 0.9249 - val_loss: 429.6314 - val_acc: 0.7434\n",
            "Epoch 4565/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 429.3539 - acc: 0.9229 - val_loss: 429.5880 - val_acc: 0.7434\n",
            "Epoch 4566/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 429.3131 - acc: 0.9199 - val_loss: 429.5446 - val_acc: 0.7434\n",
            "Epoch 4567/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 429.2689 - acc: 0.9108 - val_loss: 429.5011 - val_acc: 0.7434\n",
            "Epoch 4568/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 429.2213 - acc: 0.9209 - val_loss: 429.4576 - val_acc: 0.7434\n",
            "Epoch 4569/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 429.1767 - acc: 0.9229 - val_loss: 429.4142 - val_acc: 0.7434\n",
            "Epoch 4570/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 429.1303 - acc: 0.9270 - val_loss: 429.3708 - val_acc: 0.7434\n",
            "Epoch 4571/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 429.0912 - acc: 0.9189 - val_loss: 429.3272 - val_acc: 0.7460\n",
            "Epoch 4572/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 429.0432 - acc: 0.9270 - val_loss: 429.2840 - val_acc: 0.7460\n",
            "Epoch 4573/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 429.0024 - acc: 0.9178 - val_loss: 429.2406 - val_acc: 0.7460\n",
            "Epoch 4574/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 428.9594 - acc: 0.9270 - val_loss: 429.1970 - val_acc: 0.7460\n",
            "Epoch 4575/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 428.9217 - acc: 0.9037 - val_loss: 429.1536 - val_acc: 0.7434\n",
            "Epoch 4576/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 428.8733 - acc: 0.9229 - val_loss: 429.1100 - val_acc: 0.7434\n",
            "Epoch 4577/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 428.8360 - acc: 0.9189 - val_loss: 429.0665 - val_acc: 0.7460\n",
            "Epoch 4578/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 428.7953 - acc: 0.9108 - val_loss: 429.0232 - val_acc: 0.7460\n",
            "Epoch 4579/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 428.7395 - acc: 0.9209 - val_loss: 428.9796 - val_acc: 0.7460\n",
            "Epoch 4580/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 428.7000 - acc: 0.9148 - val_loss: 428.9362 - val_acc: 0.7460\n",
            "Epoch 4581/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 428.6551 - acc: 0.9087 - val_loss: 428.8928 - val_acc: 0.7460\n",
            "Epoch 4582/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 428.6103 - acc: 0.9128 - val_loss: 428.8495 - val_acc: 0.7460\n",
            "Epoch 4583/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 428.5669 - acc: 0.9158 - val_loss: 428.8062 - val_acc: 0.7460\n",
            "Epoch 4584/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 428.5276 - acc: 0.9158 - val_loss: 428.7627 - val_acc: 0.7460\n",
            "Epoch 4585/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 428.4802 - acc: 0.9249 - val_loss: 428.7194 - val_acc: 0.7460\n",
            "Epoch 4586/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 428.4306 - acc: 0.9270 - val_loss: 428.6760 - val_acc: 0.7460\n",
            "Epoch 4587/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 428.3966 - acc: 0.9219 - val_loss: 428.6325 - val_acc: 0.7460\n",
            "Epoch 4588/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 428.3535 - acc: 0.9189 - val_loss: 428.5891 - val_acc: 0.7460\n",
            "Epoch 4589/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 428.3082 - acc: 0.9209 - val_loss: 428.5457 - val_acc: 0.7460\n",
            "Epoch 4590/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 428.2678 - acc: 0.9138 - val_loss: 428.5024 - val_acc: 0.7460\n",
            "Epoch 4591/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 428.2269 - acc: 0.9158 - val_loss: 428.4589 - val_acc: 0.7460\n",
            "Epoch 4592/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 428.1812 - acc: 0.9168 - val_loss: 428.4155 - val_acc: 0.7460\n",
            "Epoch 4593/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 428.1427 - acc: 0.9108 - val_loss: 428.3724 - val_acc: 0.7460\n",
            "Epoch 4594/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 428.0948 - acc: 0.9290 - val_loss: 428.3290 - val_acc: 0.7460\n",
            "Epoch 4595/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 428.0543 - acc: 0.9108 - val_loss: 428.2857 - val_acc: 0.7460\n",
            "Epoch 4596/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 428.0093 - acc: 0.9148 - val_loss: 428.2424 - val_acc: 0.7460\n",
            "Epoch 4597/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 427.9602 - acc: 0.9168 - val_loss: 428.1989 - val_acc: 0.7460\n",
            "Epoch 4598/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 427.9196 - acc: 0.9178 - val_loss: 428.1556 - val_acc: 0.7460\n",
            "Epoch 4599/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 427.8759 - acc: 0.9178 - val_loss: 428.1122 - val_acc: 0.7460\n",
            "Epoch 4600/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 427.8374 - acc: 0.9158 - val_loss: 428.0689 - val_acc: 0.7460\n",
            "Epoch 4601/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 427.7897 - acc: 0.9168 - val_loss: 428.0257 - val_acc: 0.7460\n",
            "Epoch 4602/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 427.7466 - acc: 0.9189 - val_loss: 427.9824 - val_acc: 0.7460\n",
            "Epoch 4603/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 427.7068 - acc: 0.9189 - val_loss: 427.9391 - val_acc: 0.7460\n",
            "Epoch 4604/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 427.6638 - acc: 0.9158 - val_loss: 427.8958 - val_acc: 0.7460\n",
            "Epoch 4605/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 427.6170 - acc: 0.9300 - val_loss: 427.8526 - val_acc: 0.7460\n",
            "Epoch 4606/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 427.5714 - acc: 0.9168 - val_loss: 427.8093 - val_acc: 0.7487\n",
            "Epoch 4607/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 427.5292 - acc: 0.9189 - val_loss: 427.7659 - val_acc: 0.7487\n",
            "Epoch 4608/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 427.4863 - acc: 0.9219 - val_loss: 427.7227 - val_acc: 0.7487\n",
            "Epoch 4609/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 427.4448 - acc: 0.9209 - val_loss: 427.6793 - val_acc: 0.7487\n",
            "Epoch 4610/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 427.4045 - acc: 0.9178 - val_loss: 427.6361 - val_acc: 0.7487\n",
            "Epoch 4611/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 427.3588 - acc: 0.9249 - val_loss: 427.5928 - val_acc: 0.7487\n",
            "Epoch 4612/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 427.3184 - acc: 0.9168 - val_loss: 427.5495 - val_acc: 0.7487\n",
            "Epoch 4613/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 427.2735 - acc: 0.9209 - val_loss: 427.5062 - val_acc: 0.7487\n",
            "Epoch 4614/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 427.2310 - acc: 0.9189 - val_loss: 427.4629 - val_acc: 0.7487\n",
            "Epoch 4615/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 427.1822 - acc: 0.9239 - val_loss: 427.4197 - val_acc: 0.7487\n",
            "Epoch 4616/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 427.1460 - acc: 0.9158 - val_loss: 427.3764 - val_acc: 0.7487\n",
            "Epoch 4617/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 427.0938 - acc: 0.9138 - val_loss: 427.3333 - val_acc: 0.7487\n",
            "Epoch 4618/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 427.0552 - acc: 0.9290 - val_loss: 427.2899 - val_acc: 0.7487\n",
            "Epoch 4619/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 427.0109 - acc: 0.9199 - val_loss: 427.2466 - val_acc: 0.7487\n",
            "Epoch 4620/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 426.9640 - acc: 0.9199 - val_loss: 427.2035 - val_acc: 0.7487\n",
            "Epoch 4621/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 426.9237 - acc: 0.9209 - val_loss: 427.1604 - val_acc: 0.7487\n",
            "Epoch 4622/5000\n",
            "986/986 [==============================] - 0s 260us/step - loss: 426.8810 - acc: 0.9168 - val_loss: 427.1172 - val_acc: 0.7487\n",
            "Epoch 4623/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 426.8390 - acc: 0.9189 - val_loss: 427.0739 - val_acc: 0.7487\n",
            "Epoch 4624/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 426.7909 - acc: 0.9229 - val_loss: 427.0307 - val_acc: 0.7487\n",
            "Epoch 4625/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 426.7543 - acc: 0.9118 - val_loss: 426.9875 - val_acc: 0.7460\n",
            "Epoch 4626/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 426.7105 - acc: 0.9108 - val_loss: 426.9442 - val_acc: 0.7460\n",
            "Epoch 4627/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 426.6609 - acc: 0.9280 - val_loss: 426.9010 - val_acc: 0.7487\n",
            "Epoch 4628/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 426.6246 - acc: 0.9057 - val_loss: 426.8579 - val_acc: 0.7487\n",
            "Epoch 4629/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 426.5753 - acc: 0.9229 - val_loss: 426.8147 - val_acc: 0.7513\n",
            "Epoch 4630/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 426.5382 - acc: 0.9158 - val_loss: 426.7715 - val_acc: 0.7487\n",
            "Epoch 4631/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 426.4967 - acc: 0.9047 - val_loss: 426.7284 - val_acc: 0.7460\n",
            "Epoch 4632/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 426.4419 - acc: 0.9249 - val_loss: 426.6851 - val_acc: 0.7487\n",
            "Epoch 4633/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 426.4029 - acc: 0.9168 - val_loss: 426.6418 - val_acc: 0.7513\n",
            "Epoch 4634/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 426.3641 - acc: 0.9168 - val_loss: 426.5984 - val_acc: 0.7513\n",
            "Epoch 4635/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 426.3194 - acc: 0.9189 - val_loss: 426.5553 - val_acc: 0.7513\n",
            "Epoch 4636/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 426.2757 - acc: 0.9178 - val_loss: 426.5121 - val_acc: 0.7513\n",
            "Epoch 4637/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 426.2254 - acc: 0.9249 - val_loss: 426.4688 - val_acc: 0.7513\n",
            "Epoch 4638/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 426.1820 - acc: 0.9249 - val_loss: 426.4257 - val_acc: 0.7513\n",
            "Epoch 4639/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 426.1399 - acc: 0.9280 - val_loss: 426.3826 - val_acc: 0.7513\n",
            "Epoch 4640/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 426.1066 - acc: 0.9219 - val_loss: 426.3395 - val_acc: 0.7513\n",
            "Epoch 4641/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 426.0548 - acc: 0.9270 - val_loss: 426.2964 - val_acc: 0.7513\n",
            "Epoch 4642/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 426.0102 - acc: 0.9249 - val_loss: 426.2531 - val_acc: 0.7513\n",
            "Epoch 4643/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 425.9733 - acc: 0.9199 - val_loss: 426.2100 - val_acc: 0.7513\n",
            "Epoch 4644/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 425.9295 - acc: 0.9168 - val_loss: 426.1669 - val_acc: 0.7513\n",
            "Epoch 4645/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 425.8874 - acc: 0.9178 - val_loss: 426.1238 - val_acc: 0.7540\n",
            "Epoch 4646/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 425.8457 - acc: 0.9158 - val_loss: 426.0807 - val_acc: 0.7540\n",
            "Epoch 4647/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 425.7976 - acc: 0.9249 - val_loss: 426.0376 - val_acc: 0.7540\n",
            "Epoch 4648/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 425.7549 - acc: 0.9189 - val_loss: 425.9945 - val_acc: 0.7513\n",
            "Epoch 4649/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 425.7086 - acc: 0.9178 - val_loss: 425.9513 - val_acc: 0.7487\n",
            "Epoch 4650/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 425.6715 - acc: 0.9138 - val_loss: 425.9081 - val_acc: 0.7513\n",
            "Epoch 4651/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 425.6276 - acc: 0.9178 - val_loss: 425.8651 - val_acc: 0.7513\n",
            "Epoch 4652/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 425.5834 - acc: 0.9239 - val_loss: 425.8220 - val_acc: 0.7513\n",
            "Epoch 4653/5000\n",
            "986/986 [==============================] - 0s 290us/step - loss: 425.5439 - acc: 0.9280 - val_loss: 425.7788 - val_acc: 0.7513\n",
            "Epoch 4654/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 425.4998 - acc: 0.9189 - val_loss: 425.7356 - val_acc: 0.7513\n",
            "Epoch 4655/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 425.4480 - acc: 0.9310 - val_loss: 425.6925 - val_acc: 0.7513\n",
            "Epoch 4656/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 425.4065 - acc: 0.9249 - val_loss: 425.6494 - val_acc: 0.7487\n",
            "Epoch 4657/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 425.3690 - acc: 0.9118 - val_loss: 425.6062 - val_acc: 0.7513\n",
            "Epoch 4658/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 425.3215 - acc: 0.9229 - val_loss: 425.5633 - val_acc: 0.7487\n",
            "Epoch 4659/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 425.2794 - acc: 0.9199 - val_loss: 425.5201 - val_acc: 0.7513\n",
            "Epoch 4660/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 425.2376 - acc: 0.9270 - val_loss: 425.4769 - val_acc: 0.7513\n",
            "Epoch 4661/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 425.1969 - acc: 0.9097 - val_loss: 425.4338 - val_acc: 0.7513\n",
            "Epoch 4662/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 425.1517 - acc: 0.9189 - val_loss: 425.3906 - val_acc: 0.7513\n",
            "Epoch 4663/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 425.1095 - acc: 0.9270 - val_loss: 425.3476 - val_acc: 0.7513\n",
            "Epoch 4664/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 425.0670 - acc: 0.9148 - val_loss: 425.3045 - val_acc: 0.7513\n",
            "Epoch 4665/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 425.0224 - acc: 0.9249 - val_loss: 425.2615 - val_acc: 0.7513\n",
            "Epoch 4666/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 424.9808 - acc: 0.9158 - val_loss: 425.2185 - val_acc: 0.7513\n",
            "Epoch 4667/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 424.9430 - acc: 0.9189 - val_loss: 425.1755 - val_acc: 0.7513\n",
            "Epoch 4668/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 424.8885 - acc: 0.9260 - val_loss: 425.1324 - val_acc: 0.7513\n",
            "Epoch 4669/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 424.8487 - acc: 0.9280 - val_loss: 425.0893 - val_acc: 0.7513\n",
            "Epoch 4670/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 424.8025 - acc: 0.9229 - val_loss: 425.0464 - val_acc: 0.7540\n",
            "Epoch 4671/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 424.7649 - acc: 0.9270 - val_loss: 425.0033 - val_acc: 0.7513\n",
            "Epoch 4672/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 424.7212 - acc: 0.9209 - val_loss: 424.9604 - val_acc: 0.7513\n",
            "Epoch 4673/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 424.6782 - acc: 0.9229 - val_loss: 424.9173 - val_acc: 0.7513\n",
            "Epoch 4674/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 424.6322 - acc: 0.9249 - val_loss: 424.8741 - val_acc: 0.7513\n",
            "Epoch 4675/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 424.5977 - acc: 0.9138 - val_loss: 424.8311 - val_acc: 0.7513\n",
            "Epoch 4676/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 424.5500 - acc: 0.9199 - val_loss: 424.7879 - val_acc: 0.7540\n",
            "Epoch 4677/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 424.5036 - acc: 0.9249 - val_loss: 424.7449 - val_acc: 0.7513\n",
            "Epoch 4678/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 424.4605 - acc: 0.9341 - val_loss: 424.7020 - val_acc: 0.7513\n",
            "Epoch 4679/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 424.4188 - acc: 0.9178 - val_loss: 424.6590 - val_acc: 0.7513\n",
            "Epoch 4680/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 424.3792 - acc: 0.9209 - val_loss: 424.6159 - val_acc: 0.7513\n",
            "Epoch 4681/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 424.3371 - acc: 0.9168 - val_loss: 424.5729 - val_acc: 0.7513\n",
            "Epoch 4682/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 424.2867 - acc: 0.9209 - val_loss: 424.5298 - val_acc: 0.7513\n",
            "Epoch 4683/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 424.2490 - acc: 0.9199 - val_loss: 424.4869 - val_acc: 0.7513\n",
            "Epoch 4684/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 424.2002 - acc: 0.9178 - val_loss: 424.4438 - val_acc: 0.7513\n",
            "Epoch 4685/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 424.1582 - acc: 0.9199 - val_loss: 424.4007 - val_acc: 0.7513\n",
            "Epoch 4686/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 424.1241 - acc: 0.9158 - val_loss: 424.3577 - val_acc: 0.7540\n",
            "Epoch 4687/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 424.0767 - acc: 0.9168 - val_loss: 424.3148 - val_acc: 0.7540\n",
            "Epoch 4688/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 424.0336 - acc: 0.9148 - val_loss: 424.2718 - val_acc: 0.7540\n",
            "Epoch 4689/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 423.9943 - acc: 0.9209 - val_loss: 424.2288 - val_acc: 0.7513\n",
            "Epoch 4690/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 423.9471 - acc: 0.9178 - val_loss: 424.1859 - val_acc: 0.7540\n",
            "Epoch 4691/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 423.8996 - acc: 0.9260 - val_loss: 424.1430 - val_acc: 0.7513\n",
            "Epoch 4692/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 423.8588 - acc: 0.9168 - val_loss: 424.1001 - val_acc: 0.7513\n",
            "Epoch 4693/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 423.8150 - acc: 0.9260 - val_loss: 424.0572 - val_acc: 0.7513\n",
            "Epoch 4694/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 423.7711 - acc: 0.9219 - val_loss: 424.0141 - val_acc: 0.7540\n",
            "Epoch 4695/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 423.7272 - acc: 0.9168 - val_loss: 423.9711 - val_acc: 0.7540\n",
            "Epoch 4696/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 423.6907 - acc: 0.9239 - val_loss: 423.9281 - val_acc: 0.7513\n",
            "Epoch 4697/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 423.6452 - acc: 0.9239 - val_loss: 423.8853 - val_acc: 0.7540\n",
            "Epoch 4698/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 423.6063 - acc: 0.9138 - val_loss: 423.8423 - val_acc: 0.7540\n",
            "Epoch 4699/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 423.5654 - acc: 0.9158 - val_loss: 423.7994 - val_acc: 0.7540\n",
            "Epoch 4700/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 423.5130 - acc: 0.9249 - val_loss: 423.7565 - val_acc: 0.7540\n",
            "Epoch 4701/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 423.4702 - acc: 0.9209 - val_loss: 423.7134 - val_acc: 0.7540\n",
            "Epoch 4702/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 423.4274 - acc: 0.9341 - val_loss: 423.6705 - val_acc: 0.7540\n",
            "Epoch 4703/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 423.3881 - acc: 0.9209 - val_loss: 423.6275 - val_acc: 0.7540\n",
            "Epoch 4704/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 423.3492 - acc: 0.9199 - val_loss: 423.5845 - val_acc: 0.7540\n",
            "Epoch 4705/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 423.3010 - acc: 0.9209 - val_loss: 423.5417 - val_acc: 0.7540\n",
            "Epoch 4706/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 423.2621 - acc: 0.9229 - val_loss: 423.4987 - val_acc: 0.7540\n",
            "Epoch 4707/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 423.2198 - acc: 0.9138 - val_loss: 423.4557 - val_acc: 0.7540\n",
            "Epoch 4708/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 423.1737 - acc: 0.9199 - val_loss: 423.4129 - val_acc: 0.7540\n",
            "Epoch 4709/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 423.1331 - acc: 0.9300 - val_loss: 423.3699 - val_acc: 0.7540\n",
            "Epoch 4710/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 423.0835 - acc: 0.9239 - val_loss: 423.3270 - val_acc: 0.7540\n",
            "Epoch 4711/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 423.0418 - acc: 0.9229 - val_loss: 423.2844 - val_acc: 0.7540\n",
            "Epoch 4712/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 423.0018 - acc: 0.9239 - val_loss: 423.2415 - val_acc: 0.7540\n",
            "Epoch 4713/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 422.9507 - acc: 0.9280 - val_loss: 423.1988 - val_acc: 0.7540\n",
            "Epoch 4714/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 422.9202 - acc: 0.9219 - val_loss: 423.1560 - val_acc: 0.7540\n",
            "Epoch 4715/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 422.8706 - acc: 0.9199 - val_loss: 423.1131 - val_acc: 0.7540\n",
            "Epoch 4716/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 422.8295 - acc: 0.9239 - val_loss: 423.0702 - val_acc: 0.7540\n",
            "Epoch 4717/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 422.7877 - acc: 0.9168 - val_loss: 423.0272 - val_acc: 0.7540\n",
            "Epoch 4718/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 422.7455 - acc: 0.9209 - val_loss: 422.9843 - val_acc: 0.7540\n",
            "Epoch 4719/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 422.7000 - acc: 0.9260 - val_loss: 422.9414 - val_acc: 0.7540\n",
            "Epoch 4720/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 422.6590 - acc: 0.9148 - val_loss: 422.8985 - val_acc: 0.7540\n",
            "Epoch 4721/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 422.6213 - acc: 0.9219 - val_loss: 422.8558 - val_acc: 0.7540\n",
            "Epoch 4722/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 422.5714 - acc: 0.9270 - val_loss: 422.8130 - val_acc: 0.7540\n",
            "Epoch 4723/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 422.5263 - acc: 0.9300 - val_loss: 422.7701 - val_acc: 0.7540\n",
            "Epoch 4724/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 422.4848 - acc: 0.9178 - val_loss: 422.7271 - val_acc: 0.7540\n",
            "Epoch 4725/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 422.4449 - acc: 0.9128 - val_loss: 422.6844 - val_acc: 0.7540\n",
            "Epoch 4726/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 422.4003 - acc: 0.9219 - val_loss: 422.6417 - val_acc: 0.7540\n",
            "Epoch 4727/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 422.3548 - acc: 0.9310 - val_loss: 422.5989 - val_acc: 0.7540\n",
            "Epoch 4728/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 422.3073 - acc: 0.9249 - val_loss: 422.5560 - val_acc: 0.7540\n",
            "Epoch 4729/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 422.2701 - acc: 0.9148 - val_loss: 422.5133 - val_acc: 0.7540\n",
            "Epoch 4730/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 422.2328 - acc: 0.9199 - val_loss: 422.4706 - val_acc: 0.7540\n",
            "Epoch 4731/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 422.1894 - acc: 0.9209 - val_loss: 422.4277 - val_acc: 0.7540\n",
            "Epoch 4732/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 422.1450 - acc: 0.9189 - val_loss: 422.3849 - val_acc: 0.7540\n",
            "Epoch 4733/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 422.1046 - acc: 0.9128 - val_loss: 422.3420 - val_acc: 0.7540\n",
            "Epoch 4734/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 422.0612 - acc: 0.9199 - val_loss: 422.2991 - val_acc: 0.7540\n",
            "Epoch 4735/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 422.0148 - acc: 0.9138 - val_loss: 422.2563 - val_acc: 0.7540\n",
            "Epoch 4736/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 421.9728 - acc: 0.9219 - val_loss: 422.2137 - val_acc: 0.7540\n",
            "Epoch 4737/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 421.9287 - acc: 0.9249 - val_loss: 422.1707 - val_acc: 0.7540\n",
            "Epoch 4738/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 421.8892 - acc: 0.9178 - val_loss: 422.1278 - val_acc: 0.7540\n",
            "Epoch 4739/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 421.8468 - acc: 0.9108 - val_loss: 422.0851 - val_acc: 0.7540\n",
            "Epoch 4740/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 421.8077 - acc: 0.9118 - val_loss: 422.0423 - val_acc: 0.7540\n",
            "Epoch 4741/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 421.7569 - acc: 0.9229 - val_loss: 421.9995 - val_acc: 0.7540\n",
            "Epoch 4742/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 421.7163 - acc: 0.9219 - val_loss: 421.9567 - val_acc: 0.7540\n",
            "Epoch 4743/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 421.6736 - acc: 0.9249 - val_loss: 421.9142 - val_acc: 0.7540\n",
            "Epoch 4744/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 421.6379 - acc: 0.9189 - val_loss: 421.8714 - val_acc: 0.7540\n",
            "Epoch 4745/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 421.5881 - acc: 0.9239 - val_loss: 421.8289 - val_acc: 0.7540\n",
            "Epoch 4746/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 421.5438 - acc: 0.9219 - val_loss: 421.7861 - val_acc: 0.7540\n",
            "Epoch 4747/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 421.4992 - acc: 0.9189 - val_loss: 421.7433 - val_acc: 0.7540\n",
            "Epoch 4748/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 421.4619 - acc: 0.9239 - val_loss: 421.7005 - val_acc: 0.7540\n",
            "Epoch 4749/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 421.4193 - acc: 0.9108 - val_loss: 421.6577 - val_acc: 0.7540\n",
            "Epoch 4750/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 421.3761 - acc: 0.9239 - val_loss: 421.6151 - val_acc: 0.7540\n",
            "Epoch 4751/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 421.3311 - acc: 0.9158 - val_loss: 421.5722 - val_acc: 0.7540\n",
            "Epoch 4752/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 421.2924 - acc: 0.9239 - val_loss: 421.5296 - val_acc: 0.7540\n",
            "Epoch 4753/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 421.2446 - acc: 0.9199 - val_loss: 421.4869 - val_acc: 0.7540\n",
            "Epoch 4754/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 421.2040 - acc: 0.9209 - val_loss: 421.4441 - val_acc: 0.7540\n",
            "Epoch 4755/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 421.1631 - acc: 0.9138 - val_loss: 421.4015 - val_acc: 0.7540\n",
            "Epoch 4756/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 421.1185 - acc: 0.9158 - val_loss: 421.3587 - val_acc: 0.7540\n",
            "Epoch 4757/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 421.0746 - acc: 0.9249 - val_loss: 421.3160 - val_acc: 0.7540\n",
            "Epoch 4758/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 421.0360 - acc: 0.9118 - val_loss: 421.2733 - val_acc: 0.7540\n",
            "Epoch 4759/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 420.9824 - acc: 0.9219 - val_loss: 421.2305 - val_acc: 0.7540\n",
            "Epoch 4760/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 420.9490 - acc: 0.9168 - val_loss: 421.1879 - val_acc: 0.7540\n",
            "Epoch 4761/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 420.9047 - acc: 0.9209 - val_loss: 421.1453 - val_acc: 0.7540\n",
            "Epoch 4762/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 420.8686 - acc: 0.9118 - val_loss: 421.1026 - val_acc: 0.7540\n",
            "Epoch 4763/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 420.8198 - acc: 0.9239 - val_loss: 421.0600 - val_acc: 0.7540\n",
            "Epoch 4764/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 420.7754 - acc: 0.9219 - val_loss: 421.0171 - val_acc: 0.7540\n",
            "Epoch 4765/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 420.7321 - acc: 0.9300 - val_loss: 420.9745 - val_acc: 0.7540\n",
            "Epoch 4766/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 420.6910 - acc: 0.9229 - val_loss: 420.9317 - val_acc: 0.7540\n",
            "Epoch 4767/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 420.6456 - acc: 0.9219 - val_loss: 420.8890 - val_acc: 0.7540\n",
            "Epoch 4768/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 420.5987 - acc: 0.9249 - val_loss: 420.8464 - val_acc: 0.7540\n",
            "Epoch 4769/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 420.5663 - acc: 0.9118 - val_loss: 420.8037 - val_acc: 0.7540\n",
            "Epoch 4770/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 420.5132 - acc: 0.9290 - val_loss: 420.7610 - val_acc: 0.7540\n",
            "Epoch 4771/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 420.4751 - acc: 0.9270 - val_loss: 420.7183 - val_acc: 0.7540\n",
            "Epoch 4772/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 420.4280 - acc: 0.9270 - val_loss: 420.6758 - val_acc: 0.7540\n",
            "Epoch 4773/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 420.3899 - acc: 0.9178 - val_loss: 420.6332 - val_acc: 0.7540\n",
            "Epoch 4774/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 420.3464 - acc: 0.9300 - val_loss: 420.5905 - val_acc: 0.7540\n",
            "Epoch 4775/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 420.3041 - acc: 0.9219 - val_loss: 420.5478 - val_acc: 0.7540\n",
            "Epoch 4776/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 420.2643 - acc: 0.9290 - val_loss: 420.5052 - val_acc: 0.7540\n",
            "Epoch 4777/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 420.2203 - acc: 0.9260 - val_loss: 420.4625 - val_acc: 0.7540\n",
            "Epoch 4778/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 420.1761 - acc: 0.9229 - val_loss: 420.4198 - val_acc: 0.7540\n",
            "Epoch 4779/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 420.1389 - acc: 0.9249 - val_loss: 420.3772 - val_acc: 0.7540\n",
            "Epoch 4780/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 420.0982 - acc: 0.9168 - val_loss: 420.3346 - val_acc: 0.7540\n",
            "Epoch 4781/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 420.0544 - acc: 0.9148 - val_loss: 420.2920 - val_acc: 0.7540\n",
            "Epoch 4782/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 420.0062 - acc: 0.9108 - val_loss: 420.2493 - val_acc: 0.7540\n",
            "Epoch 4783/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 419.9683 - acc: 0.9178 - val_loss: 420.2067 - val_acc: 0.7540\n",
            "Epoch 4784/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 419.9264 - acc: 0.9148 - val_loss: 420.1640 - val_acc: 0.7540\n",
            "Epoch 4785/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 419.8772 - acc: 0.9249 - val_loss: 420.1212 - val_acc: 0.7540\n",
            "Epoch 4786/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 419.8339 - acc: 0.9249 - val_loss: 420.0785 - val_acc: 0.7540\n",
            "Epoch 4787/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 419.7939 - acc: 0.9260 - val_loss: 420.0360 - val_acc: 0.7540\n",
            "Epoch 4788/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 419.7517 - acc: 0.9178 - val_loss: 419.9934 - val_acc: 0.7540\n",
            "Epoch 4789/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 419.7115 - acc: 0.9229 - val_loss: 419.9511 - val_acc: 0.7540\n",
            "Epoch 4790/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 419.6705 - acc: 0.9219 - val_loss: 419.9084 - val_acc: 0.7540\n",
            "Epoch 4791/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 419.6305 - acc: 0.9178 - val_loss: 419.8658 - val_acc: 0.7540\n",
            "Epoch 4792/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 419.5778 - acc: 0.9239 - val_loss: 419.8231 - val_acc: 0.7540\n",
            "Epoch 4793/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 419.5314 - acc: 0.9280 - val_loss: 419.7806 - val_acc: 0.7540\n",
            "Epoch 4794/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 419.4929 - acc: 0.9168 - val_loss: 419.7382 - val_acc: 0.7540\n",
            "Epoch 4795/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 419.4614 - acc: 0.9158 - val_loss: 419.6957 - val_acc: 0.7540\n",
            "Epoch 4796/5000\n",
            "986/986 [==============================] - 0s 289us/step - loss: 419.4105 - acc: 0.9280 - val_loss: 419.6531 - val_acc: 0.7540\n",
            "Epoch 4797/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 419.3646 - acc: 0.9158 - val_loss: 419.6105 - val_acc: 0.7540\n",
            "Epoch 4798/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 419.3269 - acc: 0.9229 - val_loss: 419.5679 - val_acc: 0.7540\n",
            "Epoch 4799/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 419.2787 - acc: 0.9239 - val_loss: 419.5254 - val_acc: 0.7540\n",
            "Epoch 4800/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 419.2429 - acc: 0.9178 - val_loss: 419.4828 - val_acc: 0.7540\n",
            "Epoch 4801/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 419.2031 - acc: 0.9199 - val_loss: 419.4404 - val_acc: 0.7540\n",
            "Epoch 4802/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 419.1646 - acc: 0.9219 - val_loss: 419.3978 - val_acc: 0.7540\n",
            "Epoch 4803/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 419.1117 - acc: 0.9249 - val_loss: 419.3550 - val_acc: 0.7540\n",
            "Epoch 4804/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 419.0680 - acc: 0.9209 - val_loss: 419.3125 - val_acc: 0.7540\n",
            "Epoch 4805/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 419.0254 - acc: 0.9239 - val_loss: 419.2700 - val_acc: 0.7540\n",
            "Epoch 4806/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 418.9845 - acc: 0.9260 - val_loss: 419.2275 - val_acc: 0.7540\n",
            "Epoch 4807/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 418.9351 - acc: 0.9310 - val_loss: 419.1849 - val_acc: 0.7540\n",
            "Epoch 4808/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 418.9045 - acc: 0.9189 - val_loss: 419.1426 - val_acc: 0.7540\n",
            "Epoch 4809/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 418.8547 - acc: 0.9189 - val_loss: 419.1001 - val_acc: 0.7540\n",
            "Epoch 4810/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 418.8166 - acc: 0.9158 - val_loss: 419.0574 - val_acc: 0.7540\n",
            "Epoch 4811/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 418.7754 - acc: 0.9118 - val_loss: 419.0148 - val_acc: 0.7540\n",
            "Epoch 4812/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 418.7279 - acc: 0.9229 - val_loss: 418.9725 - val_acc: 0.7540\n",
            "Epoch 4813/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 418.6825 - acc: 0.9189 - val_loss: 418.9299 - val_acc: 0.7540\n",
            "Epoch 4814/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 418.6448 - acc: 0.9219 - val_loss: 418.8874 - val_acc: 0.7540\n",
            "Epoch 4815/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 418.6030 - acc: 0.9199 - val_loss: 418.8447 - val_acc: 0.7540\n",
            "Epoch 4816/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 418.5604 - acc: 0.9087 - val_loss: 418.8023 - val_acc: 0.7540\n",
            "Epoch 4817/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 418.5218 - acc: 0.9219 - val_loss: 418.7599 - val_acc: 0.7540\n",
            "Epoch 4818/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 418.4660 - acc: 0.9351 - val_loss: 418.7173 - val_acc: 0.7540\n",
            "Epoch 4819/5000\n",
            "986/986 [==============================] - 0s 296us/step - loss: 418.4352 - acc: 0.9148 - val_loss: 418.6748 - val_acc: 0.7540\n",
            "Epoch 4820/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 418.3885 - acc: 0.9249 - val_loss: 418.6322 - val_acc: 0.7540\n",
            "Epoch 4821/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 418.3485 - acc: 0.9249 - val_loss: 418.5898 - val_acc: 0.7540\n",
            "Epoch 4822/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 418.3066 - acc: 0.9229 - val_loss: 418.5474 - val_acc: 0.7540\n",
            "Epoch 4823/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 418.2630 - acc: 0.9249 - val_loss: 418.5048 - val_acc: 0.7540\n",
            "Epoch 4824/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 418.2217 - acc: 0.9199 - val_loss: 418.4623 - val_acc: 0.7540\n",
            "Epoch 4825/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 418.1817 - acc: 0.9158 - val_loss: 418.4198 - val_acc: 0.7540\n",
            "Epoch 4826/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 418.1365 - acc: 0.9280 - val_loss: 418.3774 - val_acc: 0.7540\n",
            "Epoch 4827/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 418.0921 - acc: 0.9280 - val_loss: 418.3352 - val_acc: 0.7540\n",
            "Epoch 4828/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 418.0500 - acc: 0.9209 - val_loss: 418.2927 - val_acc: 0.7540\n",
            "Epoch 4829/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 418.0044 - acc: 0.9229 - val_loss: 418.2503 - val_acc: 0.7540\n",
            "Epoch 4830/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 417.9584 - acc: 0.9270 - val_loss: 418.2079 - val_acc: 0.7540\n",
            "Epoch 4831/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 417.9253 - acc: 0.9128 - val_loss: 418.1655 - val_acc: 0.7540\n",
            "Epoch 4832/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 417.8810 - acc: 0.9178 - val_loss: 418.1230 - val_acc: 0.7540\n",
            "Epoch 4833/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 417.8343 - acc: 0.9219 - val_loss: 418.0806 - val_acc: 0.7540\n",
            "Epoch 4834/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 417.7998 - acc: 0.9138 - val_loss: 418.0382 - val_acc: 0.7540\n",
            "Epoch 4835/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 417.7571 - acc: 0.9108 - val_loss: 417.9958 - val_acc: 0.7540\n",
            "Epoch 4836/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 417.7062 - acc: 0.9239 - val_loss: 417.9533 - val_acc: 0.7540\n",
            "Epoch 4837/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 417.6658 - acc: 0.9249 - val_loss: 417.9108 - val_acc: 0.7540\n",
            "Epoch 4838/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 417.6258 - acc: 0.9249 - val_loss: 417.8684 - val_acc: 0.7540\n",
            "Epoch 4839/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 417.5862 - acc: 0.9239 - val_loss: 417.8258 - val_acc: 0.7540\n",
            "Epoch 4840/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 417.5425 - acc: 0.9199 - val_loss: 417.7835 - val_acc: 0.7540\n",
            "Epoch 4841/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 417.5003 - acc: 0.9168 - val_loss: 417.7410 - val_acc: 0.7540\n",
            "Epoch 4842/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 417.4515 - acc: 0.9189 - val_loss: 417.6988 - val_acc: 0.7540\n",
            "Epoch 4843/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 417.4129 - acc: 0.9239 - val_loss: 417.6563 - val_acc: 0.7540\n",
            "Epoch 4844/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 417.3687 - acc: 0.9249 - val_loss: 417.6140 - val_acc: 0.7540\n",
            "Epoch 4845/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 417.3216 - acc: 0.9320 - val_loss: 417.5715 - val_acc: 0.7540\n",
            "Epoch 4846/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 417.2848 - acc: 0.9249 - val_loss: 417.5291 - val_acc: 0.7540\n",
            "Epoch 4847/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 417.2420 - acc: 0.9239 - val_loss: 417.4867 - val_acc: 0.7540\n",
            "Epoch 4848/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 417.2021 - acc: 0.9209 - val_loss: 417.4442 - val_acc: 0.7540\n",
            "Epoch 4849/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 417.1586 - acc: 0.9239 - val_loss: 417.4020 - val_acc: 0.7540\n",
            "Epoch 4850/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 417.1144 - acc: 0.9219 - val_loss: 417.3596 - val_acc: 0.7540\n",
            "Epoch 4851/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 417.0768 - acc: 0.9087 - val_loss: 417.3172 - val_acc: 0.7540\n",
            "Epoch 4852/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 417.0285 - acc: 0.9270 - val_loss: 417.2749 - val_acc: 0.7540\n",
            "Epoch 4853/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 416.9892 - acc: 0.9158 - val_loss: 417.2325 - val_acc: 0.7540\n",
            "Epoch 4854/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 416.9472 - acc: 0.9219 - val_loss: 417.1900 - val_acc: 0.7540\n",
            "Epoch 4855/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 416.9092 - acc: 0.9209 - val_loss: 417.1476 - val_acc: 0.7540\n",
            "Epoch 4856/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 416.8636 - acc: 0.9148 - val_loss: 417.1054 - val_acc: 0.7540\n",
            "Epoch 4857/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 416.8200 - acc: 0.9138 - val_loss: 417.0629 - val_acc: 0.7540\n",
            "Epoch 4858/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 416.7794 - acc: 0.9260 - val_loss: 417.0206 - val_acc: 0.7540\n",
            "Epoch 4859/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 416.7344 - acc: 0.9189 - val_loss: 416.9782 - val_acc: 0.7540\n",
            "Epoch 4860/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 416.6903 - acc: 0.9290 - val_loss: 416.9358 - val_acc: 0.7540\n",
            "Epoch 4861/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 416.6543 - acc: 0.9229 - val_loss: 416.8935 - val_acc: 0.7540\n",
            "Epoch 4862/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 416.6005 - acc: 0.9239 - val_loss: 416.8512 - val_acc: 0.7540\n",
            "Epoch 4863/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 416.5571 - acc: 0.9320 - val_loss: 416.8090 - val_acc: 0.7540\n",
            "Epoch 4864/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 416.5234 - acc: 0.9219 - val_loss: 416.7665 - val_acc: 0.7540\n",
            "Epoch 4865/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 416.4879 - acc: 0.9270 - val_loss: 416.7242 - val_acc: 0.7540\n",
            "Epoch 4866/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 416.4400 - acc: 0.9148 - val_loss: 416.6820 - val_acc: 0.7540\n",
            "Epoch 4867/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 416.3970 - acc: 0.9189 - val_loss: 416.6397 - val_acc: 0.7540\n",
            "Epoch 4868/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 416.3463 - acc: 0.9310 - val_loss: 416.5975 - val_acc: 0.7540\n",
            "Epoch 4869/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 416.3139 - acc: 0.9209 - val_loss: 416.5551 - val_acc: 0.7540\n",
            "Epoch 4870/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 416.2678 - acc: 0.9270 - val_loss: 416.5128 - val_acc: 0.7540\n",
            "Epoch 4871/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 416.2321 - acc: 0.9229 - val_loss: 416.4705 - val_acc: 0.7540\n",
            "Epoch 4872/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 416.1810 - acc: 0.9280 - val_loss: 416.4283 - val_acc: 0.7540\n",
            "Epoch 4873/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 416.1403 - acc: 0.9239 - val_loss: 416.3861 - val_acc: 0.7540\n",
            "Epoch 4874/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 416.0928 - acc: 0.9239 - val_loss: 416.3436 - val_acc: 0.7540\n",
            "Epoch 4875/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 416.0542 - acc: 0.9239 - val_loss: 416.3012 - val_acc: 0.7540\n",
            "Epoch 4876/5000\n",
            "986/986 [==============================] - 0s 263us/step - loss: 416.0115 - acc: 0.9280 - val_loss: 416.2588 - val_acc: 0.7540\n",
            "Epoch 4877/5000\n",
            "986/986 [==============================] - 0s 285us/step - loss: 415.9754 - acc: 0.9249 - val_loss: 416.2163 - val_acc: 0.7540\n",
            "Epoch 4878/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 415.9255 - acc: 0.9209 - val_loss: 416.1743 - val_acc: 0.7540\n",
            "Epoch 4879/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 415.8814 - acc: 0.9199 - val_loss: 416.1321 - val_acc: 0.7540\n",
            "Epoch 4880/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 415.8387 - acc: 0.9219 - val_loss: 416.0897 - val_acc: 0.7540\n",
            "Epoch 4881/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 415.8037 - acc: 0.9209 - val_loss: 416.0473 - val_acc: 0.7540\n",
            "Epoch 4882/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 415.7668 - acc: 0.9168 - val_loss: 416.0051 - val_acc: 0.7540\n",
            "Epoch 4883/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 415.7200 - acc: 0.9209 - val_loss: 415.9628 - val_acc: 0.7540\n",
            "Epoch 4884/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 415.6777 - acc: 0.9209 - val_loss: 415.9205 - val_acc: 0.7540\n",
            "Epoch 4885/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 415.6375 - acc: 0.9219 - val_loss: 415.8782 - val_acc: 0.7540\n",
            "Epoch 4886/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 415.5988 - acc: 0.9118 - val_loss: 415.8360 - val_acc: 0.7540\n",
            "Epoch 4887/5000\n",
            "986/986 [==============================] - 0s 284us/step - loss: 415.5465 - acc: 0.9239 - val_loss: 415.7937 - val_acc: 0.7540\n",
            "Epoch 4888/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 415.4984 - acc: 0.9280 - val_loss: 415.7514 - val_acc: 0.7540\n",
            "Epoch 4889/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 415.4663 - acc: 0.9199 - val_loss: 415.7091 - val_acc: 0.7540\n",
            "Epoch 4890/5000\n",
            "986/986 [==============================] - 0s 287us/step - loss: 415.4252 - acc: 0.9178 - val_loss: 415.6669 - val_acc: 0.7540\n",
            "Epoch 4891/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 415.3808 - acc: 0.9270 - val_loss: 415.6247 - val_acc: 0.7540\n",
            "Epoch 4892/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 415.3399 - acc: 0.9280 - val_loss: 415.5825 - val_acc: 0.7540\n",
            "Epoch 4893/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 415.2941 - acc: 0.9280 - val_loss: 415.5403 - val_acc: 0.7540\n",
            "Epoch 4894/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 415.2576 - acc: 0.9138 - val_loss: 415.4981 - val_acc: 0.7540\n",
            "Epoch 4895/5000\n",
            "986/986 [==============================] - 0s 261us/step - loss: 415.2116 - acc: 0.9249 - val_loss: 415.4559 - val_acc: 0.7540\n",
            "Epoch 4896/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 415.1649 - acc: 0.9310 - val_loss: 415.4136 - val_acc: 0.7540\n",
            "Epoch 4897/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 415.1236 - acc: 0.9239 - val_loss: 415.3714 - val_acc: 0.7540\n",
            "Epoch 4898/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 415.0816 - acc: 0.9199 - val_loss: 415.3292 - val_acc: 0.7540\n",
            "Epoch 4899/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 415.0394 - acc: 0.9270 - val_loss: 415.2871 - val_acc: 0.7540\n",
            "Epoch 4900/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 414.9972 - acc: 0.9178 - val_loss: 415.2448 - val_acc: 0.7540\n",
            "Epoch 4901/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 414.9589 - acc: 0.9310 - val_loss: 415.2026 - val_acc: 0.7540\n",
            "Epoch 4902/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 414.9094 - acc: 0.9249 - val_loss: 415.1605 - val_acc: 0.7540\n",
            "Epoch 4903/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 414.8740 - acc: 0.9178 - val_loss: 415.1182 - val_acc: 0.7540\n",
            "Epoch 4904/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 414.8309 - acc: 0.9320 - val_loss: 415.0760 - val_acc: 0.7540\n",
            "Epoch 4905/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 414.7916 - acc: 0.9300 - val_loss: 415.0339 - val_acc: 0.7540\n",
            "Epoch 4906/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 414.7470 - acc: 0.9229 - val_loss: 414.9916 - val_acc: 0.7540\n",
            "Epoch 4907/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 414.7046 - acc: 0.9199 - val_loss: 414.9495 - val_acc: 0.7540\n",
            "Epoch 4908/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 414.6650 - acc: 0.9118 - val_loss: 414.9072 - val_acc: 0.7540\n",
            "Epoch 4909/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 414.6197 - acc: 0.9219 - val_loss: 414.8650 - val_acc: 0.7540\n",
            "Epoch 4910/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 414.5786 - acc: 0.9290 - val_loss: 414.8228 - val_acc: 0.7540\n",
            "Epoch 4911/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 414.5382 - acc: 0.9189 - val_loss: 414.7807 - val_acc: 0.7540\n",
            "Epoch 4912/5000\n",
            "986/986 [==============================] - 0s 280us/step - loss: 414.4891 - acc: 0.9290 - val_loss: 414.7384 - val_acc: 0.7540\n",
            "Epoch 4913/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 414.4541 - acc: 0.9148 - val_loss: 414.6962 - val_acc: 0.7540\n",
            "Epoch 4914/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 414.4062 - acc: 0.9239 - val_loss: 414.6541 - val_acc: 0.7540\n",
            "Epoch 4915/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 414.3655 - acc: 0.9168 - val_loss: 414.6120 - val_acc: 0.7540\n",
            "Epoch 4916/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 414.3204 - acc: 0.9229 - val_loss: 414.5699 - val_acc: 0.7540\n",
            "Epoch 4917/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 414.2864 - acc: 0.9199 - val_loss: 414.5279 - val_acc: 0.7540\n",
            "Epoch 4918/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 414.2371 - acc: 0.9260 - val_loss: 414.4859 - val_acc: 0.7540\n",
            "Epoch 4919/5000\n",
            "986/986 [==============================] - 0s 286us/step - loss: 414.1883 - acc: 0.9341 - val_loss: 414.4436 - val_acc: 0.7540\n",
            "Epoch 4920/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 414.1550 - acc: 0.9209 - val_loss: 414.4015 - val_acc: 0.7540\n",
            "Epoch 4921/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 414.1085 - acc: 0.9300 - val_loss: 414.3594 - val_acc: 0.7540\n",
            "Epoch 4922/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 414.0703 - acc: 0.9260 - val_loss: 414.3172 - val_acc: 0.7540\n",
            "Epoch 4923/5000\n",
            "986/986 [==============================] - 0s 262us/step - loss: 414.0284 - acc: 0.9260 - val_loss: 414.2751 - val_acc: 0.7540\n",
            "Epoch 4924/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 413.9844 - acc: 0.9219 - val_loss: 414.2331 - val_acc: 0.7540\n",
            "Epoch 4925/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 413.9455 - acc: 0.9138 - val_loss: 414.1910 - val_acc: 0.7540\n",
            "Epoch 4926/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 413.8981 - acc: 0.9290 - val_loss: 414.1488 - val_acc: 0.7540\n",
            "Epoch 4927/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 413.8613 - acc: 0.9189 - val_loss: 414.1068 - val_acc: 0.7540\n",
            "Epoch 4928/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 413.8198 - acc: 0.9209 - val_loss: 414.0647 - val_acc: 0.7540\n",
            "Epoch 4929/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 413.7790 - acc: 0.9260 - val_loss: 414.0226 - val_acc: 0.7540\n",
            "Epoch 4930/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 413.7357 - acc: 0.9219 - val_loss: 413.9807 - val_acc: 0.7540\n",
            "Epoch 4931/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 413.6865 - acc: 0.9219 - val_loss: 413.9387 - val_acc: 0.7540\n",
            "Epoch 4932/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 413.6566 - acc: 0.9189 - val_loss: 413.8967 - val_acc: 0.7540\n",
            "Epoch 4933/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 413.6043 - acc: 0.9310 - val_loss: 413.8546 - val_acc: 0.7540\n",
            "Epoch 4934/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 413.5692 - acc: 0.9128 - val_loss: 413.8124 - val_acc: 0.7540\n",
            "Epoch 4935/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 413.5205 - acc: 0.9341 - val_loss: 413.7705 - val_acc: 0.7540\n",
            "Epoch 4936/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 413.4818 - acc: 0.9158 - val_loss: 413.7284 - val_acc: 0.7540\n",
            "Epoch 4937/5000\n",
            "986/986 [==============================] - 0s 283us/step - loss: 413.4369 - acc: 0.9249 - val_loss: 413.6864 - val_acc: 0.7540\n",
            "Epoch 4938/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 413.3977 - acc: 0.9249 - val_loss: 413.6444 - val_acc: 0.7540\n",
            "Epoch 4939/5000\n",
            "986/986 [==============================] - 0s 261us/step - loss: 413.3567 - acc: 0.9138 - val_loss: 413.6023 - val_acc: 0.7540\n",
            "Epoch 4940/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 413.3112 - acc: 0.9280 - val_loss: 413.5604 - val_acc: 0.7540\n",
            "Epoch 4941/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 413.2694 - acc: 0.9199 - val_loss: 413.5182 - val_acc: 0.7540\n",
            "Epoch 4942/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 413.2282 - acc: 0.9260 - val_loss: 413.4761 - val_acc: 0.7540\n",
            "Epoch 4943/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 413.1888 - acc: 0.9178 - val_loss: 413.4342 - val_acc: 0.7540\n",
            "Epoch 4944/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 413.1435 - acc: 0.9270 - val_loss: 413.3920 - val_acc: 0.7540\n",
            "Epoch 4945/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 413.1097 - acc: 0.9077 - val_loss: 413.3500 - val_acc: 0.7540\n",
            "Epoch 4946/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 413.0623 - acc: 0.9209 - val_loss: 413.3081 - val_acc: 0.7540\n",
            "Epoch 4947/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 413.0182 - acc: 0.9168 - val_loss: 413.2663 - val_acc: 0.7540\n",
            "Epoch 4948/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 412.9781 - acc: 0.9239 - val_loss: 413.2243 - val_acc: 0.7540\n",
            "Epoch 4949/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 412.9321 - acc: 0.9290 - val_loss: 413.1823 - val_acc: 0.7540\n",
            "Epoch 4950/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 412.8936 - acc: 0.9199 - val_loss: 413.1404 - val_acc: 0.7540\n",
            "Epoch 4951/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 412.8548 - acc: 0.9260 - val_loss: 413.0983 - val_acc: 0.7540\n",
            "Epoch 4952/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 412.8113 - acc: 0.9128 - val_loss: 413.0562 - val_acc: 0.7540\n",
            "Epoch 4953/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 412.7650 - acc: 0.9280 - val_loss: 413.0142 - val_acc: 0.7540\n",
            "Epoch 4954/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 412.7256 - acc: 0.9158 - val_loss: 412.9723 - val_acc: 0.7540\n",
            "Epoch 4955/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 412.6812 - acc: 0.9189 - val_loss: 412.9304 - val_acc: 0.7540\n",
            "Epoch 4956/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 412.6432 - acc: 0.9199 - val_loss: 412.8882 - val_acc: 0.7540\n",
            "Epoch 4957/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 412.6003 - acc: 0.9199 - val_loss: 412.8463 - val_acc: 0.7540\n",
            "Epoch 4958/5000\n",
            "986/986 [==============================] - 0s 291us/step - loss: 412.5511 - acc: 0.9341 - val_loss: 412.8044 - val_acc: 0.7540\n",
            "Epoch 4959/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 412.5143 - acc: 0.9138 - val_loss: 412.7625 - val_acc: 0.7540\n",
            "Epoch 4960/5000\n",
            "986/986 [==============================] - 0s 279us/step - loss: 412.4724 - acc: 0.9290 - val_loss: 412.7205 - val_acc: 0.7566\n",
            "Epoch 4961/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 412.4263 - acc: 0.9239 - val_loss: 412.6788 - val_acc: 0.7540\n",
            "Epoch 4962/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 412.3899 - acc: 0.9178 - val_loss: 412.6368 - val_acc: 0.7540\n",
            "Epoch 4963/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 412.3460 - acc: 0.9260 - val_loss: 412.5947 - val_acc: 0.7540\n",
            "Epoch 4964/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 412.3085 - acc: 0.9249 - val_loss: 412.5529 - val_acc: 0.7540\n",
            "Epoch 4965/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 412.2615 - acc: 0.9270 - val_loss: 412.5109 - val_acc: 0.7566\n",
            "Epoch 4966/5000\n",
            "986/986 [==============================] - 0s 278us/step - loss: 412.2198 - acc: 0.9219 - val_loss: 412.4691 - val_acc: 0.7566\n",
            "Epoch 4967/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 412.1818 - acc: 0.9178 - val_loss: 412.4272 - val_acc: 0.7566\n",
            "Epoch 4968/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 412.1380 - acc: 0.9249 - val_loss: 412.3854 - val_acc: 0.7540\n",
            "Epoch 4969/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 412.0912 - acc: 0.9199 - val_loss: 412.3435 - val_acc: 0.7566\n",
            "Epoch 4970/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 412.0510 - acc: 0.9199 - val_loss: 412.3017 - val_acc: 0.7566\n",
            "Epoch 4971/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 412.0155 - acc: 0.9189 - val_loss: 412.2597 - val_acc: 0.7566\n",
            "Epoch 4972/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 411.9748 - acc: 0.9199 - val_loss: 412.2179 - val_acc: 0.7566\n",
            "Epoch 4973/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 411.9256 - acc: 0.9249 - val_loss: 412.1760 - val_acc: 0.7566\n",
            "Epoch 4974/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 411.8895 - acc: 0.9118 - val_loss: 412.1343 - val_acc: 0.7540\n",
            "Epoch 4975/5000\n",
            "986/986 [==============================] - 0s 268us/step - loss: 411.8380 - acc: 0.9300 - val_loss: 412.0921 - val_acc: 0.7566\n",
            "Epoch 4976/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 411.8031 - acc: 0.9239 - val_loss: 412.0501 - val_acc: 0.7566\n",
            "Epoch 4977/5000\n",
            "986/986 [==============================] - 0s 277us/step - loss: 411.7627 - acc: 0.9199 - val_loss: 412.0083 - val_acc: 0.7566\n",
            "Epoch 4978/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 411.7215 - acc: 0.9270 - val_loss: 411.9662 - val_acc: 0.7566\n",
            "Epoch 4979/5000\n",
            "986/986 [==============================] - 0s 267us/step - loss: 411.6839 - acc: 0.9138 - val_loss: 411.9243 - val_acc: 0.7566\n",
            "Epoch 4980/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 411.6312 - acc: 0.9178 - val_loss: 411.8825 - val_acc: 0.7566\n",
            "Epoch 4981/5000\n",
            "986/986 [==============================] - 0s 282us/step - loss: 411.5958 - acc: 0.9168 - val_loss: 411.8406 - val_acc: 0.7566\n",
            "Epoch 4982/5000\n",
            "986/986 [==============================] - 0s 264us/step - loss: 411.5510 - acc: 0.9209 - val_loss: 411.7988 - val_acc: 0.7566\n",
            "Epoch 4983/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 411.5021 - acc: 0.9280 - val_loss: 411.7570 - val_acc: 0.7566\n",
            "Epoch 4984/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 411.4645 - acc: 0.9209 - val_loss: 411.7151 - val_acc: 0.7566\n",
            "Epoch 4985/5000\n",
            "986/986 [==============================] - 0s 276us/step - loss: 411.4259 - acc: 0.9290 - val_loss: 411.6733 - val_acc: 0.7566\n",
            "Epoch 4986/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 411.3814 - acc: 0.9249 - val_loss: 411.6314 - val_acc: 0.7566\n",
            "Epoch 4987/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 411.3413 - acc: 0.9219 - val_loss: 411.5895 - val_acc: 0.7566\n",
            "Epoch 4988/5000\n",
            "986/986 [==============================] - 0s 265us/step - loss: 411.2983 - acc: 0.9209 - val_loss: 411.5477 - val_acc: 0.7566\n",
            "Epoch 4989/5000\n",
            "986/986 [==============================] - 0s 281us/step - loss: 411.2599 - acc: 0.9290 - val_loss: 411.5059 - val_acc: 0.7566\n",
            "Epoch 4990/5000\n",
            "986/986 [==============================] - 0s 274us/step - loss: 411.2084 - acc: 0.9280 - val_loss: 411.4641 - val_acc: 0.7566\n",
            "Epoch 4991/5000\n",
            "986/986 [==============================] - 0s 266us/step - loss: 411.1721 - acc: 0.9249 - val_loss: 411.4222 - val_acc: 0.7566\n",
            "Epoch 4992/5000\n",
            "986/986 [==============================] - 0s 270us/step - loss: 411.1285 - acc: 0.9341 - val_loss: 411.3804 - val_acc: 0.7566\n",
            "Epoch 4993/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 411.0921 - acc: 0.9128 - val_loss: 411.3386 - val_acc: 0.7566\n",
            "Epoch 4994/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 411.0443 - acc: 0.9341 - val_loss: 411.2968 - val_acc: 0.7566\n",
            "Epoch 4995/5000\n",
            "986/986 [==============================] - 0s 275us/step - loss: 411.0103 - acc: 0.9209 - val_loss: 411.2551 - val_acc: 0.7566\n",
            "Epoch 4996/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 410.9673 - acc: 0.9249 - val_loss: 411.2134 - val_acc: 0.7566\n",
            "Epoch 4997/5000\n",
            "986/986 [==============================] - 0s 271us/step - loss: 410.9186 - acc: 0.9300 - val_loss: 411.1716 - val_acc: 0.7566\n",
            "Epoch 4998/5000\n",
            "986/986 [==============================] - 0s 269us/step - loss: 410.8899 - acc: 0.9189 - val_loss: 411.1297 - val_acc: 0.7566\n",
            "Epoch 4999/5000\n",
            "986/986 [==============================] - 0s 273us/step - loss: 410.8400 - acc: 0.9239 - val_loss: 411.0880 - val_acc: 0.7566\n",
            "Epoch 5000/5000\n",
            "986/986 [==============================] - 0s 272us/step - loss: 410.7965 - acc: 0.9199 - val_loss: 411.0463 - val_acc: 0.7566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xzY9WuImH_S",
        "colab_type": "code",
        "outputId": "3abbf0cd-b29e-4765-ba88-776a5990fcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model.evaluate(x=test_x, y=test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1016/1016 [==============================] - 0s 94us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[410.98900454063113, 0.7667322839338948]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKbaVwG1kvvO",
        "colab_type": "code",
        "outputId": "66ba7ea3-ea15-4e2a-e63c-0989dc0205e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd512aba2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9aj3wAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O78cnQXCMid",
        "colab_type": "code",
        "outputId": "fd02584b-3761-4bdc-ab96-671daca03e0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"acc\"], label=\"train accuracy\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_acc\"], label=\"Validation accuracy\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd5128fb588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSXAXQggPJMFdCCE8kAR3IYTwQP8Pfc5W8uI7f/8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zImYx10zLvbV",
        "colab_type": "code",
        "outputId": "111400d7-14b2-47db-cca6-976d28023a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "visualize_data(test_x, test_y, 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Same class:  508 Different class:  508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAitCAYAAAAwzm7kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACAwMGkBAEBg/H9ykJ1fZoF+iwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x2880 with 40 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z4ffKqYKtbH",
        "colab_type": "text"
      },
      "source": [
        "## Tests\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTdTLfaxTeYE",
        "colab_type": "text"
      },
      "source": [
        "### Test 1: All Classes, Single example per class, 10 way training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDqtDUaoRdl8",
        "colab_type": "code",
        "outputId": "44d2eda4-b057-45f2-9b99-2342147d4602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "train_x, train_y, valid_x, valid_y, test_x, test_y = get_training_data(sample_per_class=1, n_ways=10, batch_size = 32, valid_sample_per_class=20, test_sample_per_class=50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "(10, 28, 28, 1) (10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBxqitoGVuH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ip_shape = (28,28,1)\n",
        "model = get_classifier_model(ip_shape)\n",
        "\n",
        "optimizer = Adam(lr = 0.00000005)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcquag_XR-XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize_data(train_x, train_y, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iI9mWyJSHmM",
        "colab_type": "code",
        "outputId": "f70a185d-13f0-4af5-c558-a03f48e7f7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_x, train_y, epochs=500, validation_data=(valid_x, valid_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20 samples, validate on 372 samples\n",
            "Epoch 1/500\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 669.2401 - acc: 0.0500 - val_loss: 669.1600 - val_acc: 0.3952\n",
            "Epoch 2/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1450 - acc: 0.3000 - val_loss: 669.1600 - val_acc: 0.3952\n",
            "Epoch 3/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2450 - acc: 0.2000 - val_loss: 669.1599 - val_acc: 0.3952\n",
            "Epoch 4/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2770 - acc: 0.1500 - val_loss: 669.1596 - val_acc: 0.3952\n",
            "Epoch 5/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1420 - acc: 0.3500 - val_loss: 669.1595 - val_acc: 0.3952\n",
            "Epoch 6/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1663 - acc: 0.2500 - val_loss: 669.1595 - val_acc: 0.3952\n",
            "Epoch 7/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1736 - acc: 0.2500 - val_loss: 669.1594 - val_acc: 0.3952\n",
            "Epoch 8/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2293 - acc: 0.2500 - val_loss: 669.1594 - val_acc: 0.3952\n",
            "Epoch 9/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1364 - acc: 0.3500 - val_loss: 669.1591 - val_acc: 0.3952\n",
            "Epoch 10/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1293 - acc: 0.4500 - val_loss: 669.1591 - val_acc: 0.3952\n",
            "Epoch 11/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1215 - acc: 0.3500 - val_loss: 669.1589 - val_acc: 0.3952\n",
            "Epoch 12/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1960 - acc: 0.3000 - val_loss: 669.1589 - val_acc: 0.3952\n",
            "Epoch 13/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1868 - acc: 0.1000 - val_loss: 669.1589 - val_acc: 0.3952\n",
            "Epoch 14/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1853 - acc: 0.3000 - val_loss: 669.1587 - val_acc: 0.3952\n",
            "Epoch 15/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1400 - acc: 0.4000 - val_loss: 669.1586 - val_acc: 0.3952\n",
            "Epoch 16/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1308 - acc: 0.4500 - val_loss: 669.1585 - val_acc: 0.3952\n",
            "Epoch 17/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1737 - acc: 0.2500 - val_loss: 669.1583 - val_acc: 0.3952\n",
            "Epoch 18/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1982 - acc: 0.2500 - val_loss: 669.1583 - val_acc: 0.3952\n",
            "Epoch 19/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1219 - acc: 0.4000 - val_loss: 669.1582 - val_acc: 0.3952\n",
            "Epoch 20/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2042 - acc: 0.3500 - val_loss: 669.1581 - val_acc: 0.3952\n",
            "Epoch 21/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1488 - acc: 0.4000 - val_loss: 669.1579 - val_acc: 0.3952\n",
            "Epoch 22/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1483 - acc: 0.3500 - val_loss: 669.1579 - val_acc: 0.3952\n",
            "Epoch 23/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1849 - acc: 0.3500 - val_loss: 669.1577 - val_acc: 0.3952\n",
            "Epoch 24/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2081 - acc: 0.3500 - val_loss: 669.1576 - val_acc: 0.3952\n",
            "Epoch 25/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2231 - acc: 0.2500 - val_loss: 669.1576 - val_acc: 0.3952\n",
            "Epoch 26/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1390 - acc: 0.4000 - val_loss: 669.1575 - val_acc: 0.3952\n",
            "Epoch 27/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1951 - acc: 0.2500 - val_loss: 669.1573 - val_acc: 0.3952\n",
            "Epoch 28/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1140 - acc: 0.4500 - val_loss: 669.1573 - val_acc: 0.3952\n",
            "Epoch 29/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1466 - acc: 0.4000 - val_loss: 669.1571 - val_acc: 0.3952\n",
            "Epoch 30/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1320 - acc: 0.4000 - val_loss: 669.1570 - val_acc: 0.3952\n",
            "Epoch 31/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2125 - acc: 0.4000 - val_loss: 669.1569 - val_acc: 0.3952\n",
            "Epoch 32/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2313 - acc: 0.4000 - val_loss: 669.1568 - val_acc: 0.3952\n",
            "Epoch 33/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1539 - acc: 0.3500 - val_loss: 669.1567 - val_acc: 0.3952\n",
            "Epoch 34/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1477 - acc: 0.3500 - val_loss: 669.1566 - val_acc: 0.3952\n",
            "Epoch 35/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1885 - acc: 0.3000 - val_loss: 669.1565 - val_acc: 0.3952\n",
            "Epoch 36/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2422 - acc: 0.2500 - val_loss: 669.1563 - val_acc: 0.3952\n",
            "Epoch 37/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1032 - acc: 0.4500 - val_loss: 669.1563 - val_acc: 0.3952\n",
            "Epoch 38/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0658 - acc: 0.6000 - val_loss: 669.1563 - val_acc: 0.3952\n",
            "Epoch 39/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1262 - acc: 0.4000 - val_loss: 669.1562 - val_acc: 0.3952\n",
            "Epoch 40/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1378 - acc: 0.3500 - val_loss: 669.1560 - val_acc: 0.3952\n",
            "Epoch 41/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1230 - acc: 0.5500 - val_loss: 669.1558 - val_acc: 0.3952\n",
            "Epoch 42/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1958 - acc: 0.3000 - val_loss: 669.1558 - val_acc: 0.3952\n",
            "Epoch 43/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1503 - acc: 0.3500 - val_loss: 669.1557 - val_acc: 0.3952\n",
            "Epoch 44/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2026 - acc: 0.3000 - val_loss: 669.1556 - val_acc: 0.3952\n",
            "Epoch 45/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1580 - acc: 0.4500 - val_loss: 669.1555 - val_acc: 0.3952\n",
            "Epoch 46/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1807 - acc: 0.2500 - val_loss: 669.1554 - val_acc: 0.3952\n",
            "Epoch 47/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1292 - acc: 0.4000 - val_loss: 669.1552 - val_acc: 0.3952\n",
            "Epoch 48/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2032 - acc: 0.3500 - val_loss: 669.1552 - val_acc: 0.3952\n",
            "Epoch 49/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2017 - acc: 0.3000 - val_loss: 669.1551 - val_acc: 0.3952\n",
            "Epoch 50/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.2051 - acc: 0.2500 - val_loss: 669.1550 - val_acc: 0.3952\n",
            "Epoch 51/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1850 - acc: 0.3000 - val_loss: 669.1549 - val_acc: 0.3952\n",
            "Epoch 52/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1752 - acc: 0.3500 - val_loss: 669.1547 - val_acc: 0.3952\n",
            "Epoch 53/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2386 - acc: 0.1500 - val_loss: 669.1547 - val_acc: 0.3952\n",
            "Epoch 54/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1519 - acc: 0.3500 - val_loss: 669.1546 - val_acc: 0.3952\n",
            "Epoch 55/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1760 - acc: 0.3000 - val_loss: 669.1545 - val_acc: 0.3952\n",
            "Epoch 56/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2249 - acc: 0.2000 - val_loss: 669.1543 - val_acc: 0.3952\n",
            "Epoch 57/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1868 - acc: 0.3000 - val_loss: 669.1543 - val_acc: 0.3952\n",
            "Epoch 58/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1617 - acc: 0.2500 - val_loss: 669.1542 - val_acc: 0.3952\n",
            "Epoch 59/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2064 - acc: 0.2500 - val_loss: 669.1540 - val_acc: 0.3952\n",
            "Epoch 60/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2084 - acc: 0.2000 - val_loss: 669.1539 - val_acc: 0.3952\n",
            "Epoch 61/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1643 - acc: 0.2500 - val_loss: 669.1539 - val_acc: 0.3952\n",
            "Epoch 62/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1157 - acc: 0.4500 - val_loss: 669.1537 - val_acc: 0.3952\n",
            "Epoch 63/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1627 - acc: 0.3500 - val_loss: 669.1536 - val_acc: 0.3952\n",
            "Epoch 64/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1766 - acc: 0.4000 - val_loss: 669.1536 - val_acc: 0.3952\n",
            "Epoch 65/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1561 - acc: 0.2500 - val_loss: 669.1534 - val_acc: 0.3952\n",
            "Epoch 66/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1324 - acc: 0.4500 - val_loss: 669.1533 - val_acc: 0.3952\n",
            "Epoch 67/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1413 - acc: 0.3000 - val_loss: 669.1532 - val_acc: 0.3952\n",
            "Epoch 68/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2288 - acc: 0.2500 - val_loss: 669.1532 - val_acc: 0.3952\n",
            "Epoch 69/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1920 - acc: 0.2500 - val_loss: 669.1530 - val_acc: 0.3952\n",
            "Epoch 70/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2133 - acc: 0.1500 - val_loss: 669.1529 - val_acc: 0.3952\n",
            "Epoch 71/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1677 - acc: 0.4500 - val_loss: 669.1528 - val_acc: 0.3952\n",
            "Epoch 72/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2037 - acc: 0.1500 - val_loss: 669.1527 - val_acc: 0.3952\n",
            "Epoch 73/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1381 - acc: 0.3500 - val_loss: 669.1526 - val_acc: 0.3952\n",
            "Epoch 74/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1015 - acc: 0.4000 - val_loss: 669.1525 - val_acc: 0.3952\n",
            "Epoch 75/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1701 - acc: 0.2000 - val_loss: 669.1524 - val_acc: 0.3952\n",
            "Epoch 76/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0884 - acc: 0.3500 - val_loss: 669.1523 - val_acc: 0.3952\n",
            "Epoch 77/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1844 - acc: 0.2000 - val_loss: 669.1522 - val_acc: 0.3952\n",
            "Epoch 78/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2269 - acc: 0.2500 - val_loss: 669.1521 - val_acc: 0.3952\n",
            "Epoch 79/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1904 - acc: 0.3500 - val_loss: 669.1520 - val_acc: 0.3952\n",
            "Epoch 80/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1364 - acc: 0.3000 - val_loss: 669.1519 - val_acc: 0.3952\n",
            "Epoch 81/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2105 - acc: 0.2000 - val_loss: 669.1517 - val_acc: 0.3952\n",
            "Epoch 82/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1711 - acc: 0.4500 - val_loss: 669.1516 - val_acc: 0.3952\n",
            "Epoch 83/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2068 - acc: 0.1000 - val_loss: 669.1515 - val_acc: 0.3952\n",
            "Epoch 84/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1763 - acc: 0.3000 - val_loss: 669.1514 - val_acc: 0.3952\n",
            "Epoch 85/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2034 - acc: 0.2000 - val_loss: 669.1514 - val_acc: 0.3952\n",
            "Epoch 86/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2137 - acc: 0.3000 - val_loss: 669.1513 - val_acc: 0.3952\n",
            "Epoch 87/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1934 - acc: 0.3000 - val_loss: 669.1512 - val_acc: 0.3952\n",
            "Epoch 88/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1033 - acc: 0.6000 - val_loss: 669.1510 - val_acc: 0.3952\n",
            "Epoch 89/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2385 - acc: 0.1000 - val_loss: 669.1509 - val_acc: 0.3952\n",
            "Epoch 90/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1647 - acc: 0.3000 - val_loss: 669.1508 - val_acc: 0.3952\n",
            "Epoch 91/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1716 - acc: 0.2500 - val_loss: 669.1508 - val_acc: 0.3952\n",
            "Epoch 92/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1948 - acc: 0.3000 - val_loss: 669.1507 - val_acc: 0.3952\n",
            "Epoch 93/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2124 - acc: 0.3500 - val_loss: 669.1505 - val_acc: 0.3952\n",
            "Epoch 94/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1918 - acc: 0.2500 - val_loss: 669.1504 - val_acc: 0.3952\n",
            "Epoch 95/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1107 - acc: 0.5000 - val_loss: 669.1503 - val_acc: 0.3952\n",
            "Epoch 96/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1819 - acc: 0.3500 - val_loss: 669.1502 - val_acc: 0.3952\n",
            "Epoch 97/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1828 - acc: 0.4000 - val_loss: 669.1502 - val_acc: 0.3952\n",
            "Epoch 98/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1393 - acc: 0.4000 - val_loss: 669.1501 - val_acc: 0.3952\n",
            "Epoch 99/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1176 - acc: 0.4500 - val_loss: 669.1499 - val_acc: 0.3952\n",
            "Epoch 100/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1265 - acc: 0.5000 - val_loss: 669.1498 - val_acc: 0.3952\n",
            "Epoch 101/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1069 - acc: 0.5000 - val_loss: 669.1497 - val_acc: 0.3952\n",
            "Epoch 102/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1317 - acc: 0.3000 - val_loss: 669.1496 - val_acc: 0.3952\n",
            "Epoch 103/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1340 - acc: 0.4000 - val_loss: 669.1496 - val_acc: 0.3952\n",
            "Epoch 104/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2020 - acc: 0.3000 - val_loss: 669.1494 - val_acc: 0.3952\n",
            "Epoch 105/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1855 - acc: 0.2500 - val_loss: 669.1494 - val_acc: 0.3952\n",
            "Epoch 106/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1686 - acc: 0.1500 - val_loss: 669.1491 - val_acc: 0.3952\n",
            "Epoch 107/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1482 - acc: 0.5000 - val_loss: 669.1491 - val_acc: 0.3952\n",
            "Epoch 108/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1611 - acc: 0.3000 - val_loss: 669.1489 - val_acc: 0.3952\n",
            "Epoch 109/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2013 - acc: 0.2000 - val_loss: 669.1489 - val_acc: 0.3952\n",
            "Epoch 110/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1548 - acc: 0.3000 - val_loss: 669.1488 - val_acc: 0.3952\n",
            "Epoch 111/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1363 - acc: 0.4500 - val_loss: 669.1487 - val_acc: 0.3952\n",
            "Epoch 112/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1108 - acc: 0.2500 - val_loss: 669.1485 - val_acc: 0.3952\n",
            "Epoch 113/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1497 - acc: 0.2500 - val_loss: 669.1484 - val_acc: 0.3952\n",
            "Epoch 114/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1768 - acc: 0.2500 - val_loss: 669.1484 - val_acc: 0.3952\n",
            "Epoch 115/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1858 - acc: 0.2000 - val_loss: 669.1483 - val_acc: 0.3952\n",
            "Epoch 116/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1299 - acc: 0.4500 - val_loss: 669.1481 - val_acc: 0.3952\n",
            "Epoch 117/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1469 - acc: 0.3500 - val_loss: 669.1481 - val_acc: 0.3952\n",
            "Epoch 118/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1694 - acc: 0.4000 - val_loss: 669.1480 - val_acc: 0.3952\n",
            "Epoch 119/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1673 - acc: 0.2500 - val_loss: 669.1478 - val_acc: 0.3952\n",
            "Epoch 120/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1632 - acc: 0.3500 - val_loss: 669.1478 - val_acc: 0.3952\n",
            "Epoch 121/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1223 - acc: 0.4000 - val_loss: 669.1477 - val_acc: 0.3952\n",
            "Epoch 122/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1635 - acc: 0.3000 - val_loss: 669.1476 - val_acc: 0.3952\n",
            "Epoch 123/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1423 - acc: 0.3000 - val_loss: 669.1475 - val_acc: 0.3952\n",
            "Epoch 124/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1664 - acc: 0.2500 - val_loss: 669.1474 - val_acc: 0.3952\n",
            "Epoch 125/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2018 - acc: 0.2000 - val_loss: 669.1472 - val_acc: 0.3952\n",
            "Epoch 126/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1897 - acc: 0.3500 - val_loss: 669.1471 - val_acc: 0.3952\n",
            "Epoch 127/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1871 - acc: 0.3500 - val_loss: 669.1471 - val_acc: 0.3952\n",
            "Epoch 128/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2090 - acc: 0.2000 - val_loss: 669.1470 - val_acc: 0.3952\n",
            "Epoch 129/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2108 - acc: 0.2500 - val_loss: 669.1468 - val_acc: 0.3952\n",
            "Epoch 130/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2397 - acc: 0.2500 - val_loss: 669.1468 - val_acc: 0.3952\n",
            "Epoch 131/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1614 - acc: 0.2500 - val_loss: 669.1467 - val_acc: 0.3952\n",
            "Epoch 132/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1528 - acc: 0.3000 - val_loss: 669.1465 - val_acc: 0.3952\n",
            "Epoch 133/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1694 - acc: 0.3000 - val_loss: 669.1464 - val_acc: 0.3952\n",
            "Epoch 134/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1465 - acc: 0.4000 - val_loss: 669.1463 - val_acc: 0.3952\n",
            "Epoch 135/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1866 - acc: 0.2000 - val_loss: 669.1462 - val_acc: 0.3952\n",
            "Epoch 136/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1415 - acc: 0.3500 - val_loss: 669.1461 - val_acc: 0.3952\n",
            "Epoch 137/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1752 - acc: 0.3000 - val_loss: 669.1460 - val_acc: 0.3952\n",
            "Epoch 138/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1547 - acc: 0.3500 - val_loss: 669.1459 - val_acc: 0.3952\n",
            "Epoch 139/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1005 - acc: 0.5000 - val_loss: 669.1459 - val_acc: 0.3952\n",
            "Epoch 140/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1709 - acc: 0.3500 - val_loss: 669.1457 - val_acc: 0.3952\n",
            "Epoch 141/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1685 - acc: 0.2500 - val_loss: 669.1456 - val_acc: 0.3952\n",
            "Epoch 142/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1223 - acc: 0.4500 - val_loss: 669.1455 - val_acc: 0.3952\n",
            "Epoch 143/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1923 - acc: 0.3000 - val_loss: 669.1454 - val_acc: 0.3952\n",
            "Epoch 144/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.0909 - acc: 0.4000 - val_loss: 669.1453 - val_acc: 0.3952\n",
            "Epoch 145/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2218 - acc: 0.1500 - val_loss: 669.1452 - val_acc: 0.3952\n",
            "Epoch 146/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1439 - acc: 0.4500 - val_loss: 669.1452 - val_acc: 0.3952\n",
            "Epoch 147/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1754 - acc: 0.4000 - val_loss: 669.1450 - val_acc: 0.3952\n",
            "Epoch 148/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2393 - acc: 0.2500 - val_loss: 669.1449 - val_acc: 0.3952\n",
            "Epoch 149/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1114 - acc: 0.3500 - val_loss: 669.1448 - val_acc: 0.3952\n",
            "Epoch 150/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1343 - acc: 0.3500 - val_loss: 669.1448 - val_acc: 0.3952\n",
            "Epoch 151/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1771 - acc: 0.3500 - val_loss: 669.1446 - val_acc: 0.3952\n",
            "Epoch 152/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1308 - acc: 0.4000 - val_loss: 669.1445 - val_acc: 0.3952\n",
            "Epoch 153/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1598 - acc: 0.3000 - val_loss: 669.1444 - val_acc: 0.3952\n",
            "Epoch 154/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1324 - acc: 0.3500 - val_loss: 669.1443 - val_acc: 0.3952\n",
            "Epoch 155/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1577 - acc: 0.3000 - val_loss: 669.1441 - val_acc: 0.3952\n",
            "Epoch 156/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1597 - acc: 0.3000 - val_loss: 669.1441 - val_acc: 0.3952\n",
            "Epoch 157/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1909 - acc: 0.3000 - val_loss: 669.1440 - val_acc: 0.3952\n",
            "Epoch 158/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0845 - acc: 0.4500 - val_loss: 669.1439 - val_acc: 0.3952\n",
            "Epoch 159/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1907 - acc: 0.2000 - val_loss: 669.1438 - val_acc: 0.3952\n",
            "Epoch 160/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2015 - acc: 0.2000 - val_loss: 669.1436 - val_acc: 0.3952\n",
            "Epoch 161/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1950 - acc: 0.3000 - val_loss: 669.1435 - val_acc: 0.3952\n",
            "Epoch 162/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1280 - acc: 0.3500 - val_loss: 669.1435 - val_acc: 0.3952\n",
            "Epoch 163/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1956 - acc: 0.3000 - val_loss: 669.1434 - val_acc: 0.3952\n",
            "Epoch 164/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1684 - acc: 0.3500 - val_loss: 669.1432 - val_acc: 0.3952\n",
            "Epoch 165/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1939 - acc: 0.3000 - val_loss: 669.1431 - val_acc: 0.3952\n",
            "Epoch 166/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1924 - acc: 0.2500 - val_loss: 669.1430 - val_acc: 0.3952\n",
            "Epoch 167/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1281 - acc: 0.4000 - val_loss: 669.1429 - val_acc: 0.3952\n",
            "Epoch 168/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1678 - acc: 0.2000 - val_loss: 669.1428 - val_acc: 0.3952\n",
            "Epoch 169/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1908 - acc: 0.2500 - val_loss: 669.1427 - val_acc: 0.3952\n",
            "Epoch 170/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1869 - acc: 0.3000 - val_loss: 669.1427 - val_acc: 0.3952\n",
            "Epoch 171/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1214 - acc: 0.3500 - val_loss: 669.1425 - val_acc: 0.3952\n",
            "Epoch 172/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1985 - acc: 0.3500 - val_loss: 669.1425 - val_acc: 0.3952\n",
            "Epoch 173/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1431 - acc: 0.3000 - val_loss: 669.1423 - val_acc: 0.3952\n",
            "Epoch 174/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1450 - acc: 0.4000 - val_loss: 669.1422 - val_acc: 0.3952\n",
            "Epoch 175/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1173 - acc: 0.3500 - val_loss: 669.1422 - val_acc: 0.3952\n",
            "Epoch 176/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1335 - acc: 0.3500 - val_loss: 669.1421 - val_acc: 0.3952\n",
            "Epoch 177/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0962 - acc: 0.5000 - val_loss: 669.1420 - val_acc: 0.3952\n",
            "Epoch 178/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0761 - acc: 0.4500 - val_loss: 669.1419 - val_acc: 0.3952\n",
            "Epoch 179/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1451 - acc: 0.4500 - val_loss: 669.1417 - val_acc: 0.3952\n",
            "Epoch 180/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1723 - acc: 0.3000 - val_loss: 669.1416 - val_acc: 0.3952\n",
            "Epoch 181/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1645 - acc: 0.3500 - val_loss: 669.1416 - val_acc: 0.3952\n",
            "Epoch 182/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1423 - acc: 0.3500 - val_loss: 669.1414 - val_acc: 0.3952\n",
            "Epoch 183/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1928 - acc: 0.2000 - val_loss: 669.1412 - val_acc: 0.3952\n",
            "Epoch 184/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1957 - acc: 0.2500 - val_loss: 669.1412 - val_acc: 0.3952\n",
            "Epoch 185/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1035 - acc: 0.4000 - val_loss: 669.1410 - val_acc: 0.3952\n",
            "Epoch 186/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2201 - acc: 0.2500 - val_loss: 669.1410 - val_acc: 0.3952\n",
            "Epoch 187/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1434 - acc: 0.3500 - val_loss: 669.1409 - val_acc: 0.3952\n",
            "Epoch 188/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1405 - acc: 0.4000 - val_loss: 669.1408 - val_acc: 0.3952\n",
            "Epoch 189/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2961 - acc: 0.1500 - val_loss: 669.1407 - val_acc: 0.3952\n",
            "Epoch 190/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2049 - acc: 0.1500 - val_loss: 669.1406 - val_acc: 0.3952\n",
            "Epoch 191/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1495 - acc: 0.3000 - val_loss: 669.1404 - val_acc: 0.3952\n",
            "Epoch 192/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1887 - acc: 0.3000 - val_loss: 669.1403 - val_acc: 0.3952\n",
            "Epoch 193/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1252 - acc: 0.3000 - val_loss: 669.1403 - val_acc: 0.3952\n",
            "Epoch 194/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2009 - acc: 0.3500 - val_loss: 669.1401 - val_acc: 0.3952\n",
            "Epoch 195/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1740 - acc: 0.2000 - val_loss: 669.1400 - val_acc: 0.3952\n",
            "Epoch 196/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1297 - acc: 0.4000 - val_loss: 669.1399 - val_acc: 0.3952\n",
            "Epoch 197/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1461 - acc: 0.5000 - val_loss: 669.1398 - val_acc: 0.3952\n",
            "Epoch 198/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1974 - acc: 0.3000 - val_loss: 669.1398 - val_acc: 0.3952\n",
            "Epoch 199/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1732 - acc: 0.3000 - val_loss: 669.1396 - val_acc: 0.3952\n",
            "Epoch 200/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1135 - acc: 0.4500 - val_loss: 669.1396 - val_acc: 0.3952\n",
            "Epoch 201/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1630 - acc: 0.3000 - val_loss: 669.1394 - val_acc: 0.3952\n",
            "Epoch 202/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1826 - acc: 0.2000 - val_loss: 669.1393 - val_acc: 0.3952\n",
            "Epoch 203/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1505 - acc: 0.3000 - val_loss: 669.1392 - val_acc: 0.3952\n",
            "Epoch 204/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1953 - acc: 0.3000 - val_loss: 669.1392 - val_acc: 0.3952\n",
            "Epoch 205/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1612 - acc: 0.2000 - val_loss: 669.1391 - val_acc: 0.3952\n",
            "Epoch 206/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0941 - acc: 0.3500 - val_loss: 669.1390 - val_acc: 0.3952\n",
            "Epoch 207/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1588 - acc: 0.3000 - val_loss: 669.1389 - val_acc: 0.3952\n",
            "Epoch 208/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1151 - acc: 0.3500 - val_loss: 669.1387 - val_acc: 0.3952\n",
            "Epoch 209/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1137 - acc: 0.4000 - val_loss: 669.1385 - val_acc: 0.3952\n",
            "Epoch 210/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1395 - acc: 0.3500 - val_loss: 669.1385 - val_acc: 0.3952\n",
            "Epoch 211/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1372 - acc: 0.3500 - val_loss: 669.1384 - val_acc: 0.3952\n",
            "Epoch 212/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1464 - acc: 0.3500 - val_loss: 669.1383 - val_acc: 0.3952\n",
            "Epoch 213/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1909 - acc: 0.3000 - val_loss: 669.1382 - val_acc: 0.3952\n",
            "Epoch 214/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.2333 - acc: 0.2000 - val_loss: 669.1380 - val_acc: 0.3952\n",
            "Epoch 215/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1942 - acc: 0.2000 - val_loss: 669.1380 - val_acc: 0.3952\n",
            "Epoch 216/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1628 - acc: 0.3000 - val_loss: 669.1379 - val_acc: 0.3952\n",
            "Epoch 217/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1567 - acc: 0.3500 - val_loss: 669.1378 - val_acc: 0.3952\n",
            "Epoch 218/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0748 - acc: 0.4500 - val_loss: 669.1377 - val_acc: 0.3952\n",
            "Epoch 219/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1863 - acc: 0.3000 - val_loss: 669.1376 - val_acc: 0.3952\n",
            "Epoch 220/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1959 - acc: 0.3000 - val_loss: 669.1374 - val_acc: 0.3952\n",
            "Epoch 221/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0964 - acc: 0.5000 - val_loss: 669.1374 - val_acc: 0.3952\n",
            "Epoch 222/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1586 - acc: 0.4000 - val_loss: 669.1373 - val_acc: 0.3952\n",
            "Epoch 223/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1663 - acc: 0.2500 - val_loss: 669.1372 - val_acc: 0.3952\n",
            "Epoch 224/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1898 - acc: 0.2500 - val_loss: 669.1372 - val_acc: 0.3952\n",
            "Epoch 225/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1021 - acc: 0.5500 - val_loss: 669.1370 - val_acc: 0.3952\n",
            "Epoch 226/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1419 - acc: 0.3500 - val_loss: 669.1369 - val_acc: 0.3952\n",
            "Epoch 227/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1293 - acc: 0.3000 - val_loss: 669.1368 - val_acc: 0.3952\n",
            "Epoch 228/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1711 - acc: 0.3500 - val_loss: 669.1367 - val_acc: 0.3952\n",
            "Epoch 229/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1550 - acc: 0.2500 - val_loss: 669.1366 - val_acc: 0.3952\n",
            "Epoch 230/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2292 - acc: 0.2500 - val_loss: 669.1365 - val_acc: 0.3952\n",
            "Epoch 231/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1542 - acc: 0.4000 - val_loss: 669.1364 - val_acc: 0.3952\n",
            "Epoch 232/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1580 - acc: 0.3500 - val_loss: 669.1363 - val_acc: 0.3952\n",
            "Epoch 233/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1385 - acc: 0.4500 - val_loss: 669.1362 - val_acc: 0.3952\n",
            "Epoch 234/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1729 - acc: 0.3500 - val_loss: 669.1360 - val_acc: 0.3952\n",
            "Epoch 235/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1451 - acc: 0.3000 - val_loss: 669.1359 - val_acc: 0.3952\n",
            "Epoch 236/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1453 - acc: 0.3500 - val_loss: 669.1359 - val_acc: 0.3952\n",
            "Epoch 237/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1359 - acc: 0.3500 - val_loss: 669.1357 - val_acc: 0.3952\n",
            "Epoch 238/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1666 - acc: 0.2000 - val_loss: 669.1356 - val_acc: 0.3952\n",
            "Epoch 239/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1468 - acc: 0.3000 - val_loss: 669.1355 - val_acc: 0.3952\n",
            "Epoch 240/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1453 - acc: 0.3500 - val_loss: 669.1354 - val_acc: 0.3952\n",
            "Epoch 241/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1526 - acc: 0.3000 - val_loss: 669.1353 - val_acc: 0.3952\n",
            "Epoch 242/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1596 - acc: 0.2500 - val_loss: 669.1352 - val_acc: 0.3952\n",
            "Epoch 243/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1259 - acc: 0.3000 - val_loss: 669.1351 - val_acc: 0.3952\n",
            "Epoch 244/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2051 - acc: 0.2500 - val_loss: 669.1350 - val_acc: 0.3952\n",
            "Epoch 245/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1794 - acc: 0.2000 - val_loss: 669.1349 - val_acc: 0.3952\n",
            "Epoch 246/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1269 - acc: 0.4000 - val_loss: 669.1348 - val_acc: 0.3952\n",
            "Epoch 247/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1716 - acc: 0.2000 - val_loss: 669.1347 - val_acc: 0.3952\n",
            "Epoch 248/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1583 - acc: 0.3000 - val_loss: 669.1347 - val_acc: 0.3952\n",
            "Epoch 249/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1760 - acc: 0.1500 - val_loss: 669.1345 - val_acc: 0.3952\n",
            "Epoch 250/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1039 - acc: 0.4500 - val_loss: 669.1344 - val_acc: 0.3952\n",
            "Epoch 251/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0901 - acc: 0.4000 - val_loss: 669.1343 - val_acc: 0.3952\n",
            "Epoch 252/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1210 - acc: 0.3000 - val_loss: 669.1342 - val_acc: 0.3952\n",
            "Epoch 253/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0635 - acc: 0.6000 - val_loss: 669.1341 - val_acc: 0.3952\n",
            "Epoch 254/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1694 - acc: 0.3000 - val_loss: 669.1341 - val_acc: 0.3952\n",
            "Epoch 255/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1640 - acc: 0.2500 - val_loss: 669.1339 - val_acc: 0.3952\n",
            "Epoch 256/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1652 - acc: 0.4000 - val_loss: 669.1338 - val_acc: 0.3952\n",
            "Epoch 257/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1640 - acc: 0.3000 - val_loss: 669.1337 - val_acc: 0.3952\n",
            "Epoch 258/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1537 - acc: 0.2500 - val_loss: 669.1336 - val_acc: 0.3952\n",
            "Epoch 259/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1099 - acc: 0.4000 - val_loss: 669.1335 - val_acc: 0.3952\n",
            "Epoch 260/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0940 - acc: 0.3500 - val_loss: 669.1334 - val_acc: 0.3952\n",
            "Epoch 261/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1518 - acc: 0.2500 - val_loss: 669.1332 - val_acc: 0.3952\n",
            "Epoch 262/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1142 - acc: 0.4000 - val_loss: 669.1331 - val_acc: 0.3952\n",
            "Epoch 263/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1633 - acc: 0.4000 - val_loss: 669.1330 - val_acc: 0.3952\n",
            "Epoch 264/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1420 - acc: 0.4000 - val_loss: 669.1329 - val_acc: 0.3952\n",
            "Epoch 265/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1542 - acc: 0.4000 - val_loss: 669.1329 - val_acc: 0.3952\n",
            "Epoch 266/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2172 - acc: 0.3000 - val_loss: 669.1327 - val_acc: 0.3952\n",
            "Epoch 267/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2093 - acc: 0.1000 - val_loss: 669.1327 - val_acc: 0.3952\n",
            "Epoch 268/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1816 - acc: 0.2500 - val_loss: 669.1325 - val_acc: 0.3952\n",
            "Epoch 269/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1981 - acc: 0.4000 - val_loss: 669.1324 - val_acc: 0.3952\n",
            "Epoch 270/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1621 - acc: 0.3500 - val_loss: 669.1324 - val_acc: 0.3952\n",
            "Epoch 271/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0873 - acc: 0.4000 - val_loss: 669.1322 - val_acc: 0.3952\n",
            "Epoch 272/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1861 - acc: 0.3000 - val_loss: 669.1321 - val_acc: 0.3952\n",
            "Epoch 273/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1103 - acc: 0.3500 - val_loss: 669.1321 - val_acc: 0.3952\n",
            "Epoch 274/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2389 - acc: 0.1500 - val_loss: 669.1319 - val_acc: 0.3952\n",
            "Epoch 275/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1581 - acc: 0.2500 - val_loss: 669.1319 - val_acc: 0.3952\n",
            "Epoch 276/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1792 - acc: 0.2500 - val_loss: 669.1318 - val_acc: 0.3952\n",
            "Epoch 277/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1265 - acc: 0.3000 - val_loss: 669.1316 - val_acc: 0.3952\n",
            "Epoch 278/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1368 - acc: 0.3500 - val_loss: 669.1316 - val_acc: 0.3952\n",
            "Epoch 279/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1653 - acc: 0.2500 - val_loss: 669.1314 - val_acc: 0.3952\n",
            "Epoch 280/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1691 - acc: 0.3500 - val_loss: 669.1314 - val_acc: 0.3952\n",
            "Epoch 281/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1478 - acc: 0.3000 - val_loss: 669.1313 - val_acc: 0.3952\n",
            "Epoch 282/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1418 - acc: 0.4500 - val_loss: 669.1311 - val_acc: 0.3952\n",
            "Epoch 283/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1591 - acc: 0.4000 - val_loss: 669.1311 - val_acc: 0.3952\n",
            "Epoch 284/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1551 - acc: 0.2500 - val_loss: 669.1310 - val_acc: 0.3952\n",
            "Epoch 285/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1476 - acc: 0.3500 - val_loss: 669.1308 - val_acc: 0.3952\n",
            "Epoch 286/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1733 - acc: 0.2000 - val_loss: 669.1307 - val_acc: 0.3952\n",
            "Epoch 287/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1760 - acc: 0.2500 - val_loss: 669.1306 - val_acc: 0.3952\n",
            "Epoch 288/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1589 - acc: 0.1500 - val_loss: 669.1305 - val_acc: 0.3952\n",
            "Epoch 289/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1434 - acc: 0.2500 - val_loss: 669.1304 - val_acc: 0.3952\n",
            "Epoch 290/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1686 - acc: 0.2500 - val_loss: 669.1303 - val_acc: 0.3952\n",
            "Epoch 291/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1706 - acc: 0.1500 - val_loss: 669.1302 - val_acc: 0.3952\n",
            "Epoch 292/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1462 - acc: 0.3000 - val_loss: 669.1301 - val_acc: 0.3952\n",
            "Epoch 293/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1382 - acc: 0.4000 - val_loss: 669.1300 - val_acc: 0.3952\n",
            "Epoch 294/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1064 - acc: 0.4000 - val_loss: 669.1299 - val_acc: 0.3952\n",
            "Epoch 295/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1635 - acc: 0.4000 - val_loss: 669.1298 - val_acc: 0.3952\n",
            "Epoch 296/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1811 - acc: 0.2500 - val_loss: 669.1297 - val_acc: 0.3952\n",
            "Epoch 297/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2064 - acc: 0.2000 - val_loss: 669.1296 - val_acc: 0.3952\n",
            "Epoch 298/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1321 - acc: 0.3500 - val_loss: 669.1295 - val_acc: 0.3952\n",
            "Epoch 299/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0812 - acc: 0.5500 - val_loss: 669.1294 - val_acc: 0.3952\n",
            "Epoch 300/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1869 - acc: 0.0500 - val_loss: 669.1293 - val_acc: 0.3952\n",
            "Epoch 301/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1899 - acc: 0.1500 - val_loss: 669.1292 - val_acc: 0.3952\n",
            "Epoch 302/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0826 - acc: 0.3500 - val_loss: 669.1290 - val_acc: 0.3952\n",
            "Epoch 303/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2264 - acc: 0.2000 - val_loss: 669.1289 - val_acc: 0.3952\n",
            "Epoch 304/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1210 - acc: 0.3500 - val_loss: 669.1289 - val_acc: 0.3952\n",
            "Epoch 305/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1301 - acc: 0.4000 - val_loss: 669.1287 - val_acc: 0.3952\n",
            "Epoch 306/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2076 - acc: 0.1500 - val_loss: 669.1286 - val_acc: 0.3952\n",
            "Epoch 307/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1624 - acc: 0.3500 - val_loss: 669.1286 - val_acc: 0.3952\n",
            "Epoch 308/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1415 - acc: 0.3000 - val_loss: 669.1285 - val_acc: 0.3952\n",
            "Epoch 309/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1204 - acc: 0.2500 - val_loss: 669.1284 - val_acc: 0.3952\n",
            "Epoch 310/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1923 - acc: 0.2500 - val_loss: 669.1282 - val_acc: 0.3952\n",
            "Epoch 311/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1576 - acc: 0.3000 - val_loss: 669.1281 - val_acc: 0.3952\n",
            "Epoch 312/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1865 - acc: 0.2000 - val_loss: 669.1280 - val_acc: 0.3952\n",
            "Epoch 313/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1195 - acc: 0.3500 - val_loss: 669.1279 - val_acc: 0.3952\n",
            "Epoch 314/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0547 - acc: 0.5500 - val_loss: 669.1278 - val_acc: 0.3952\n",
            "Epoch 315/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1075 - acc: 0.3000 - val_loss: 669.1278 - val_acc: 0.3952\n",
            "Epoch 316/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1107 - acc: 0.3500 - val_loss: 669.1276 - val_acc: 0.3952\n",
            "Epoch 317/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2087 - acc: 0.2500 - val_loss: 669.1275 - val_acc: 0.3952\n",
            "Epoch 318/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1993 - acc: 0.2500 - val_loss: 669.1274 - val_acc: 0.3952\n",
            "Epoch 319/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1442 - acc: 0.2500 - val_loss: 669.1273 - val_acc: 0.3952\n",
            "Epoch 320/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1381 - acc: 0.3500 - val_loss: 669.1272 - val_acc: 0.3952\n",
            "Epoch 321/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1106 - acc: 0.3500 - val_loss: 669.1271 - val_acc: 0.3952\n",
            "Epoch 322/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1501 - acc: 0.3000 - val_loss: 669.1270 - val_acc: 0.3952\n",
            "Epoch 323/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1041 - acc: 0.3500 - val_loss: 669.1268 - val_acc: 0.3952\n",
            "Epoch 324/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1128 - acc: 0.3500 - val_loss: 669.1268 - val_acc: 0.3952\n",
            "Epoch 325/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1437 - acc: 0.2500 - val_loss: 669.1267 - val_acc: 0.3952\n",
            "Epoch 326/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1025 - acc: 0.4500 - val_loss: 669.1267 - val_acc: 0.3952\n",
            "Epoch 327/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1049 - acc: 0.3000 - val_loss: 669.1265 - val_acc: 0.3952\n",
            "Epoch 328/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1395 - acc: 0.3000 - val_loss: 669.1265 - val_acc: 0.3952\n",
            "Epoch 329/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1613 - acc: 0.3000 - val_loss: 669.1263 - val_acc: 0.3952\n",
            "Epoch 330/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0966 - acc: 0.3500 - val_loss: 669.1262 - val_acc: 0.3952\n",
            "Epoch 331/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1458 - acc: 0.4000 - val_loss: 669.1262 - val_acc: 0.3952\n",
            "Epoch 332/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1031 - acc: 0.4500 - val_loss: 669.1261 - val_acc: 0.3952\n",
            "Epoch 333/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1307 - acc: 0.3000 - val_loss: 669.1259 - val_acc: 0.3952\n",
            "Epoch 334/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1079 - acc: 0.4000 - val_loss: 669.1257 - val_acc: 0.3952\n",
            "Epoch 335/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1467 - acc: 0.3000 - val_loss: 669.1257 - val_acc: 0.3952\n",
            "Epoch 336/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1733 - acc: 0.2500 - val_loss: 669.1256 - val_acc: 0.3952\n",
            "Epoch 337/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1522 - acc: 0.3000 - val_loss: 669.1255 - val_acc: 0.3952\n",
            "Epoch 338/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1266 - acc: 0.3000 - val_loss: 669.1254 - val_acc: 0.3952\n",
            "Epoch 339/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1613 - acc: 0.2500 - val_loss: 669.1252 - val_acc: 0.3952\n",
            "Epoch 340/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1132 - acc: 0.5000 - val_loss: 669.1252 - val_acc: 0.3952\n",
            "Epoch 341/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1213 - acc: 0.2000 - val_loss: 669.1251 - val_acc: 0.3952\n",
            "Epoch 342/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1129 - acc: 0.4000 - val_loss: 669.1249 - val_acc: 0.3952\n",
            "Epoch 343/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1245 - acc: 0.3500 - val_loss: 669.1249 - val_acc: 0.3952\n",
            "Epoch 344/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1443 - acc: 0.3500 - val_loss: 669.1248 - val_acc: 0.3952\n",
            "Epoch 345/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1148 - acc: 0.4000 - val_loss: 669.1246 - val_acc: 0.3952\n",
            "Epoch 346/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1273 - acc: 0.3000 - val_loss: 669.1246 - val_acc: 0.3952\n",
            "Epoch 347/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1000 - acc: 0.4000 - val_loss: 669.1244 - val_acc: 0.3952\n",
            "Epoch 348/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1903 - acc: 0.2500 - val_loss: 669.1243 - val_acc: 0.3952\n",
            "Epoch 349/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1468 - acc: 0.2000 - val_loss: 669.1243 - val_acc: 0.3952\n",
            "Epoch 350/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1168 - acc: 0.3500 - val_loss: 669.1241 - val_acc: 0.3952\n",
            "Epoch 351/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1020 - acc: 0.3500 - val_loss: 669.1240 - val_acc: 0.3952\n",
            "Epoch 352/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1017 - acc: 0.3500 - val_loss: 669.1239 - val_acc: 0.3952\n",
            "Epoch 353/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1143 - acc: 0.2500 - val_loss: 669.1238 - val_acc: 0.3952\n",
            "Epoch 354/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1393 - acc: 0.4500 - val_loss: 669.1237 - val_acc: 0.3952\n",
            "Epoch 355/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1519 - acc: 0.3500 - val_loss: 669.1236 - val_acc: 0.3952\n",
            "Epoch 356/500\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 669.0655 - acc: 0.5500 - val_loss: 669.1236 - val_acc: 0.3952\n",
            "Epoch 357/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1878 - acc: 0.2500 - val_loss: 669.1234 - val_acc: 0.3952\n",
            "Epoch 358/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1662 - acc: 0.3000 - val_loss: 669.1233 - val_acc: 0.3952\n",
            "Epoch 359/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1341 - acc: 0.4000 - val_loss: 669.1232 - val_acc: 0.3952\n",
            "Epoch 360/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2178 - acc: 0.2000 - val_loss: 669.1231 - val_acc: 0.3952\n",
            "Epoch 361/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1747 - acc: 0.3000 - val_loss: 669.1231 - val_acc: 0.3952\n",
            "Epoch 362/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1104 - acc: 0.4000 - val_loss: 669.1228 - val_acc: 0.3952\n",
            "Epoch 363/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1061 - acc: 0.5000 - val_loss: 669.1228 - val_acc: 0.3952\n",
            "Epoch 364/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0777 - acc: 0.4000 - val_loss: 669.1226 - val_acc: 0.3952\n",
            "Epoch 365/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1998 - acc: 0.1000 - val_loss: 669.1225 - val_acc: 0.3952\n",
            "Epoch 366/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1881 - acc: 0.1500 - val_loss: 669.1225 - val_acc: 0.3952\n",
            "Epoch 367/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1436 - acc: 0.2500 - val_loss: 669.1224 - val_acc: 0.3952\n",
            "Epoch 368/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1216 - acc: 0.2500 - val_loss: 669.1223 - val_acc: 0.3952\n",
            "Epoch 369/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1340 - acc: 0.4500 - val_loss: 669.1221 - val_acc: 0.3952\n",
            "Epoch 370/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1227 - acc: 0.3500 - val_loss: 669.1220 - val_acc: 0.3952\n",
            "Epoch 371/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1837 - acc: 0.2500 - val_loss: 669.1219 - val_acc: 0.3952\n",
            "Epoch 372/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1353 - acc: 0.4000 - val_loss: 669.1219 - val_acc: 0.3952\n",
            "Epoch 373/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0897 - acc: 0.4500 - val_loss: 669.1218 - val_acc: 0.3952\n",
            "Epoch 374/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1623 - acc: 0.2000 - val_loss: 669.1217 - val_acc: 0.3952\n",
            "Epoch 375/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0948 - acc: 0.4000 - val_loss: 669.1216 - val_acc: 0.3952\n",
            "Epoch 376/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1598 - acc: 0.2000 - val_loss: 669.1214 - val_acc: 0.3952\n",
            "Epoch 377/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1273 - acc: 0.4000 - val_loss: 669.1214 - val_acc: 0.3952\n",
            "Epoch 378/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1041 - acc: 0.4500 - val_loss: 669.1212 - val_acc: 0.3952\n",
            "Epoch 379/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1792 - acc: 0.1500 - val_loss: 669.1212 - val_acc: 0.3952\n",
            "Epoch 380/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1184 - acc: 0.3000 - val_loss: 669.1212 - val_acc: 0.3952\n",
            "Epoch 381/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1016 - acc: 0.4000 - val_loss: 669.1210 - val_acc: 0.3952\n",
            "Epoch 382/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1520 - acc: 0.3500 - val_loss: 669.1208 - val_acc: 0.3952\n",
            "Epoch 383/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1213 - acc: 0.3500 - val_loss: 669.1207 - val_acc: 0.3952\n",
            "Epoch 384/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1111 - acc: 0.5000 - val_loss: 669.1207 - val_acc: 0.3952\n",
            "Epoch 385/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0868 - acc: 0.4000 - val_loss: 669.1206 - val_acc: 0.3952\n",
            "Epoch 386/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1417 - acc: 0.4000 - val_loss: 669.1205 - val_acc: 0.3952\n",
            "Epoch 387/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0919 - acc: 0.5000 - val_loss: 669.1204 - val_acc: 0.3952\n",
            "Epoch 388/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1415 - acc: 0.3000 - val_loss: 669.1202 - val_acc: 0.3952\n",
            "Epoch 389/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0839 - acc: 0.4000 - val_loss: 669.1200 - val_acc: 0.3952\n",
            "Epoch 390/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1297 - acc: 0.4000 - val_loss: 669.1200 - val_acc: 0.3952\n",
            "Epoch 391/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1224 - acc: 0.4000 - val_loss: 669.1199 - val_acc: 0.3952\n",
            "Epoch 392/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1643 - acc: 0.2000 - val_loss: 669.1199 - val_acc: 0.3952\n",
            "Epoch 393/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1073 - acc: 0.4500 - val_loss: 669.1197 - val_acc: 0.3952\n",
            "Epoch 394/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1316 - acc: 0.3500 - val_loss: 669.1196 - val_acc: 0.3952\n",
            "Epoch 395/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1441 - acc: 0.2500 - val_loss: 669.1194 - val_acc: 0.3952\n",
            "Epoch 396/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1691 - acc: 0.2500 - val_loss: 669.1194 - val_acc: 0.3952\n",
            "Epoch 397/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1552 - acc: 0.2000 - val_loss: 669.1193 - val_acc: 0.3952\n",
            "Epoch 398/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1365 - acc: 0.2000 - val_loss: 669.1192 - val_acc: 0.3952\n",
            "Epoch 399/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1594 - acc: 0.4000 - val_loss: 669.1191 - val_acc: 0.3952\n",
            "Epoch 400/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1938 - acc: 0.2000 - val_loss: 669.1190 - val_acc: 0.3952\n",
            "Epoch 401/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1284 - acc: 0.3000 - val_loss: 669.1189 - val_acc: 0.3952\n",
            "Epoch 402/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1095 - acc: 0.4000 - val_loss: 669.1188 - val_acc: 0.3952\n",
            "Epoch 403/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1068 - acc: 0.4000 - val_loss: 669.1187 - val_acc: 0.3952\n",
            "Epoch 404/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1049 - acc: 0.3500 - val_loss: 669.1186 - val_acc: 0.3952\n",
            "Epoch 405/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.0652 - acc: 0.5000 - val_loss: 669.1184 - val_acc: 0.3952\n",
            "Epoch 406/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1689 - acc: 0.4000 - val_loss: 669.1184 - val_acc: 0.3952\n",
            "Epoch 407/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1863 - acc: 0.2500 - val_loss: 669.1183 - val_acc: 0.3952\n",
            "Epoch 408/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1217 - acc: 0.3000 - val_loss: 669.1182 - val_acc: 0.3952\n",
            "Epoch 409/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2167 - acc: 0.2500 - val_loss: 669.1181 - val_acc: 0.3952\n",
            "Epoch 410/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1426 - acc: 0.3000 - val_loss: 669.1180 - val_acc: 0.3952\n",
            "Epoch 411/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1026 - acc: 0.3500 - val_loss: 669.1179 - val_acc: 0.3952\n",
            "Epoch 412/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1843 - acc: 0.3000 - val_loss: 669.1178 - val_acc: 0.3952\n",
            "Epoch 413/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1766 - acc: 0.2500 - val_loss: 669.1176 - val_acc: 0.3952\n",
            "Epoch 414/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1665 - acc: 0.3000 - val_loss: 669.1175 - val_acc: 0.3952\n",
            "Epoch 415/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0878 - acc: 0.4000 - val_loss: 669.1175 - val_acc: 0.3952\n",
            "Epoch 416/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1653 - acc: 0.1500 - val_loss: 669.1173 - val_acc: 0.3952\n",
            "Epoch 417/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1652 - acc: 0.2500 - val_loss: 669.1173 - val_acc: 0.3952\n",
            "Epoch 418/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1412 - acc: 0.2000 - val_loss: 669.1172 - val_acc: 0.3952\n",
            "Epoch 419/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0988 - acc: 0.4000 - val_loss: 669.1170 - val_acc: 0.3952\n",
            "Epoch 420/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0414 - acc: 0.4000 - val_loss: 669.1170 - val_acc: 0.3952\n",
            "Epoch 421/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1436 - acc: 0.4000 - val_loss: 669.1168 - val_acc: 0.3952\n",
            "Epoch 422/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1574 - acc: 0.3000 - val_loss: 669.1167 - val_acc: 0.3952\n",
            "Epoch 423/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1537 - acc: 0.3000 - val_loss: 669.1166 - val_acc: 0.3952\n",
            "Epoch 424/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0944 - acc: 0.4000 - val_loss: 669.1165 - val_acc: 0.3952\n",
            "Epoch 425/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1313 - acc: 0.2500 - val_loss: 669.1164 - val_acc: 0.3952\n",
            "Epoch 426/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0743 - acc: 0.4500 - val_loss: 669.1163 - val_acc: 0.3952\n",
            "Epoch 427/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1667 - acc: 0.3500 - val_loss: 669.1162 - val_acc: 0.3952\n",
            "Epoch 428/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0922 - acc: 0.2500 - val_loss: 669.1162 - val_acc: 0.3952\n",
            "Epoch 429/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1278 - acc: 0.3500 - val_loss: 669.1161 - val_acc: 0.3952\n",
            "Epoch 430/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1912 - acc: 0.2000 - val_loss: 669.1159 - val_acc: 0.3925\n",
            "Epoch 431/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1440 - acc: 0.2500 - val_loss: 669.1158 - val_acc: 0.3925\n",
            "Epoch 432/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1383 - acc: 0.3000 - val_loss: 669.1157 - val_acc: 0.3925\n",
            "Epoch 433/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0657 - acc: 0.5000 - val_loss: 669.1156 - val_acc: 0.3925\n",
            "Epoch 434/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1030 - acc: 0.3500 - val_loss: 669.1155 - val_acc: 0.3925\n",
            "Epoch 435/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1261 - acc: 0.2500 - val_loss: 669.1154 - val_acc: 0.3925\n",
            "Epoch 436/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1331 - acc: 0.3500 - val_loss: 669.1154 - val_acc: 0.3925\n",
            "Epoch 437/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1269 - acc: 0.3000 - val_loss: 669.1152 - val_acc: 0.3925\n",
            "Epoch 438/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1081 - acc: 0.4500 - val_loss: 669.1151 - val_acc: 0.3925\n",
            "Epoch 439/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1464 - acc: 0.3500 - val_loss: 669.1151 - val_acc: 0.3925\n",
            "Epoch 440/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1416 - acc: 0.3000 - val_loss: 669.1149 - val_acc: 0.3925\n",
            "Epoch 441/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1841 - acc: 0.1000 - val_loss: 669.1148 - val_acc: 0.3925\n",
            "Epoch 442/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0980 - acc: 0.3000 - val_loss: 669.1147 - val_acc: 0.3925\n",
            "Epoch 443/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1379 - acc: 0.3500 - val_loss: 669.1145 - val_acc: 0.3925\n",
            "Epoch 444/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1691 - acc: 0.2000 - val_loss: 669.1144 - val_acc: 0.3925\n",
            "Epoch 445/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1433 - acc: 0.2000 - val_loss: 669.1144 - val_acc: 0.3925\n",
            "Epoch 446/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1208 - acc: 0.3500 - val_loss: 669.1143 - val_acc: 0.3925\n",
            "Epoch 447/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1387 - acc: 0.3000 - val_loss: 669.1141 - val_acc: 0.3925\n",
            "Epoch 448/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1492 - acc: 0.3500 - val_loss: 669.1141 - val_acc: 0.3925\n",
            "Epoch 449/500\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 669.1275 - acc: 0.3000 - val_loss: 669.1139 - val_acc: 0.3925\n",
            "Epoch 450/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1534 - acc: 0.3000 - val_loss: 669.1138 - val_acc: 0.3925\n",
            "Epoch 451/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1110 - acc: 0.3000 - val_loss: 669.1138 - val_acc: 0.3925\n",
            "Epoch 452/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1647 - acc: 0.3500 - val_loss: 669.1137 - val_acc: 0.3925\n",
            "Epoch 453/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0618 - acc: 0.4500 - val_loss: 669.1136 - val_acc: 0.3925\n",
            "Epoch 454/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1379 - acc: 0.3000 - val_loss: 669.1134 - val_acc: 0.3925\n",
            "Epoch 455/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0787 - acc: 0.3500 - val_loss: 669.1133 - val_acc: 0.3925\n",
            "Epoch 456/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.2377 - acc: 0.3000 - val_loss: 669.1133 - val_acc: 0.3925\n",
            "Epoch 457/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1324 - acc: 0.3500 - val_loss: 669.1131 - val_acc: 0.3925\n",
            "Epoch 458/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1209 - acc: 0.3000 - val_loss: 669.1131 - val_acc: 0.3925\n",
            "Epoch 459/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1129 - acc: 0.4000 - val_loss: 669.1129 - val_acc: 0.3925\n",
            "Epoch 460/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1536 - acc: 0.3500 - val_loss: 669.1128 - val_acc: 0.3925\n",
            "Epoch 461/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1916 - acc: 0.1500 - val_loss: 669.1127 - val_acc: 0.3925\n",
            "Epoch 462/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1293 - acc: 0.2000 - val_loss: 669.1126 - val_acc: 0.3925\n",
            "Epoch 463/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1697 - acc: 0.2000 - val_loss: 669.1126 - val_acc: 0.3925\n",
            "Epoch 464/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0993 - acc: 0.4000 - val_loss: 669.1125 - val_acc: 0.3925\n",
            "Epoch 465/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1584 - acc: 0.2500 - val_loss: 669.1123 - val_acc: 0.3925\n",
            "Epoch 466/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0945 - acc: 0.5000 - val_loss: 669.1121 - val_acc: 0.3925\n",
            "Epoch 467/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0895 - acc: 0.3500 - val_loss: 669.1120 - val_acc: 0.3925\n",
            "Epoch 468/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1492 - acc: 0.4000 - val_loss: 669.1120 - val_acc: 0.3925\n",
            "Epoch 469/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1282 - acc: 0.2000 - val_loss: 669.1119 - val_acc: 0.3925\n",
            "Epoch 470/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1297 - acc: 0.3000 - val_loss: 669.1118 - val_acc: 0.3925\n",
            "Epoch 471/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1779 - acc: 0.2500 - val_loss: 669.1116 - val_acc: 0.3925\n",
            "Epoch 472/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1586 - acc: 0.2500 - val_loss: 669.1115 - val_acc: 0.3925\n",
            "Epoch 473/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0822 - acc: 0.4500 - val_loss: 669.1114 - val_acc: 0.3925\n",
            "Epoch 474/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0797 - acc: 0.4500 - val_loss: 669.1113 - val_acc: 0.3925\n",
            "Epoch 475/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1524 - acc: 0.2500 - val_loss: 669.1113 - val_acc: 0.3925\n",
            "Epoch 476/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1439 - acc: 0.3500 - val_loss: 669.1112 - val_acc: 0.3925\n",
            "Epoch 477/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0936 - acc: 0.4500 - val_loss: 669.1110 - val_acc: 0.3925\n",
            "Epoch 478/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1028 - acc: 0.3500 - val_loss: 669.1109 - val_acc: 0.3925\n",
            "Epoch 479/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0865 - acc: 0.4000 - val_loss: 669.1108 - val_acc: 0.3925\n",
            "Epoch 480/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0654 - acc: 0.3000 - val_loss: 669.1107 - val_acc: 0.3925\n",
            "Epoch 481/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1465 - acc: 0.4000 - val_loss: 669.1107 - val_acc: 0.3925\n",
            "Epoch 482/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1097 - acc: 0.2000 - val_loss: 669.1106 - val_acc: 0.3925\n",
            "Epoch 483/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1053 - acc: 0.5000 - val_loss: 669.1105 - val_acc: 0.3925\n",
            "Epoch 484/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0739 - acc: 0.4000 - val_loss: 669.1104 - val_acc: 0.3925\n",
            "Epoch 485/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1056 - acc: 0.4000 - val_loss: 669.1103 - val_acc: 0.3925\n",
            "Epoch 486/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1230 - acc: 0.2500 - val_loss: 669.1102 - val_acc: 0.3925\n",
            "Epoch 487/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1685 - acc: 0.3000 - val_loss: 669.1101 - val_acc: 0.3925\n",
            "Epoch 488/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1447 - acc: 0.2000 - val_loss: 669.1099 - val_acc: 0.3925\n",
            "Epoch 489/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0988 - acc: 0.2500 - val_loss: 669.1099 - val_acc: 0.3925\n",
            "Epoch 490/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1348 - acc: 0.2500 - val_loss: 669.1096 - val_acc: 0.3925\n",
            "Epoch 491/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1343 - acc: 0.3500 - val_loss: 669.1096 - val_acc: 0.3925\n",
            "Epoch 492/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1594 - acc: 0.2000 - val_loss: 669.1095 - val_acc: 0.3925\n",
            "Epoch 493/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1617 - acc: 0.3500 - val_loss: 669.1094 - val_acc: 0.3925\n",
            "Epoch 494/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1353 - acc: 0.3500 - val_loss: 669.1094 - val_acc: 0.3925\n",
            "Epoch 495/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1691 - acc: 0.3500 - val_loss: 669.1092 - val_acc: 0.3925\n",
            "Epoch 496/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1538 - acc: 0.1500 - val_loss: 669.1091 - val_acc: 0.3925\n",
            "Epoch 497/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1281 - acc: 0.2500 - val_loss: 669.1090 - val_acc: 0.3925\n",
            "Epoch 498/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1238 - acc: 0.4000 - val_loss: 669.1089 - val_acc: 0.3925\n",
            "Epoch 499/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.0882 - acc: 0.3000 - val_loss: 669.1088 - val_acc: 0.3925\n",
            "Epoch 500/500\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 669.1422 - acc: 0.3500 - val_loss: 669.1087 - val_acc: 0.3925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUr2MarSUX4",
        "colab_type": "code",
        "outputId": "8b5f68ea-c8bf-46a2-f037-2e1e8f505c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd512a4db00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwmAwGAyBGEFhMBgMhkCMoDAYDAZDIP8fMqSBzTcpk/8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLkOntCsS4zM",
        "colab_type": "code",
        "outputId": "8a43270a-08ba-43b7-815d-626add9bf5ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"acc\"], label=\"train accuracy\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_acc\"], label=\"Validation accuracy\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd512c37ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNLkrpZQDaXJXSikH+n9qJlhDJNTBngAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9iiPu_xSkbQ",
        "colab_type": "code",
        "outputId": "4d8ead13-c5a0-4d0a-9b5b-00d985f45caa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model.evaluate(x=test_x, y=test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1016/1016 [==============================] - 0s 99us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[669.1108951118049, 0.4153543308259934]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI9L_gS6TUq8",
        "colab_type": "text"
      },
      "source": [
        "### Test 2: All Classes, Single example per class, 5 way training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpmqk-DXSk8h",
        "colab_type": "code",
        "outputId": "5ee6741d-87da-437d-e7d4-8b3a601a1831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "train_x, train_y, valid_x, valid_y, test_x, test_y = get_training_data(sample_per_class=1, n_ways=5, batch_size = 32, valid_sample_per_class=20, test_sample_per_class=50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "(10, 28, 28, 1) (10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVmFiJhuYI2u",
        "colab_type": "code",
        "outputId": "176a19c4-0242-4fc5-b819-6ba95d489071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0WWhe3-WHfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ip_shape = (28,28,1)\n",
        "model = get_classifier_model(ip_shape)\n",
        "\n",
        "optimizer = Adam(lr = 1e-9)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPsNPcSpTmjS",
        "colab_type": "code",
        "outputId": "27040f1f-3f24-4c1a-fb17-af6d123f0c53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_x, train_y, epochs=1000, validation_data=(valid_x, valid_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10 samples, validate on 190 samples\n",
            "Epoch 1/1000\n",
            "10/10 [==============================] - 1s 82ms/step - loss: 669.0804 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 2/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2031 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 3/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2781 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 4/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1482 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 5/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0839 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 6/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2301 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 7/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0874 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 8/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1783 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 9/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1981 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 10/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1998 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 11/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1721 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 12/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2416 - acc: 0.1000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 13/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1201 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 14/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0837 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 15/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1697 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 16/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2000 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 17/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1581 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 18/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1824 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 19/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2511 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 20/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0601 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 21/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2068 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 22/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1652 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 23/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1771 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 24/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2277 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 25/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0873 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 26/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1978 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 27/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0758 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 28/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1581 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 29/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1335 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 30/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1840 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 31/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1396 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 32/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1042 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 33/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1334 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 34/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1568 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 35/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1915 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 36/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0868 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 37/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1865 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 38/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1116 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 39/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1632 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 40/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1448 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 41/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1703 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 42/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1686 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 43/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1459 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 44/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1332 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 45/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1398 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 46/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1279 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 47/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1116 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 48/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1393 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 49/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1094 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 50/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1605 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 51/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1229 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 52/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1259 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 53/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2572 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 54/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1139 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 55/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1030 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 56/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1031 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 57/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1701 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 58/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2097 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 59/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2784 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 60/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0993 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 61/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2509 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 62/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0980 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 63/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2046 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 64/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1429 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 65/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2111 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 66/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1369 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 67/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1459 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 68/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1871 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 69/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2019 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 70/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1203 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 71/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2008 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 72/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2203 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 73/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1561 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 74/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1327 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 75/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1823 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 76/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1925 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 77/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1467 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 78/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1495 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 79/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2103 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 80/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1304 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 81/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1239 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 82/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2152 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 83/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1799 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 84/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1310 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 85/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2610 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 86/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1638 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 87/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1168 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 88/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1904 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 89/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2015 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 90/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1851 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 91/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2162 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 92/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1661 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 93/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1012 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 94/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1707 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 95/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1267 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 96/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1342 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 97/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1833 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 98/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1543 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 99/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1132 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 100/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0908 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 101/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1390 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 102/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1688 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 103/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1172 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 104/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1257 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 105/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.3007 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 106/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1621 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 107/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1421 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 108/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1665 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 109/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1096 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 110/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2093 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 111/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1657 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 112/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1927 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 113/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1256 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 114/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0789 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 115/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1663 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 116/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2053 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 117/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1927 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 118/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1354 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 119/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1944 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 120/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1983 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 121/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1735 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 122/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1946 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 123/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1674 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 124/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1543 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 125/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0965 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 126/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1119 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 127/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1826 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 128/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1084 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 129/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1257 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 130/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1940 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 131/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2261 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 132/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1915 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 133/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0871 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 134/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2174 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 135/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2065 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 136/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1548 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 137/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1233 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 138/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1364 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 139/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1065 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 140/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1790 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 141/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1898 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 142/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1282 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 143/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1765 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 144/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1606 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 145/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2224 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 146/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2206 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 147/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2155 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 148/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2155 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 149/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1442 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 150/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2000 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 151/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1766 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 152/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1849 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 153/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1415 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 154/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1724 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 155/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2394 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 156/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2052 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 157/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1512 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 158/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1077 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 159/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1230 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 160/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1428 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 161/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1925 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 162/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1445 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 163/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0818 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 164/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1872 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 165/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1841 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 166/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1134 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 167/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1442 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 168/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1544 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 169/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1380 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 170/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1848 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 171/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2067 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 172/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0903 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 173/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1396 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 174/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1954 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 175/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1133 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 176/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1852 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 177/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2060 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 178/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1518 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 179/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1403 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 180/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1829 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 181/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1669 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 182/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1565 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 183/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2333 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 184/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1680 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 185/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1519 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 186/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1785 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 187/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1287 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 188/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1155 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 189/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1476 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 190/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1796 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 191/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2057 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 192/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0899 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 193/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2285 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 194/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2411 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 195/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1756 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 196/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0880 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 197/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1082 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 198/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1913 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 199/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1938 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 200/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1292 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 201/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2745 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 202/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1398 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 203/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1987 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 204/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2075 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 205/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1530 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 206/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1490 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 207/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.3232 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 208/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2346 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 209/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2162 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 210/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1703 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 211/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2086 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 212/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1284 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 213/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1516 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 214/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2203 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 215/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1565 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 216/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2028 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 217/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1833 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 218/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1042 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 219/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2209 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 220/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1558 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 221/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1428 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 222/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1942 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 223/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0954 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 224/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1791 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 225/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2171 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 226/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.3018 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 227/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0823 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 228/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1885 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 229/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1130 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 230/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0463 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 231/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1797 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 232/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2142 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 233/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0898 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 234/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0665 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 235/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2325 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 236/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2675 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 237/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0908 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 238/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2087 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 239/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1657 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 240/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2086 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 241/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0924 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 242/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1553 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 243/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1149 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 244/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1121 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 245/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1678 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 246/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1204 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 247/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2083 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 248/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1522 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 249/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0957 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 250/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1581 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 251/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1823 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 252/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1542 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 253/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1263 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 254/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2102 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 255/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1525 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 256/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1077 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 257/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0576 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 258/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2164 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 259/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1846 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 260/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1733 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 261/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0877 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 262/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1218 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 263/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1473 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 264/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1435 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 265/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2025 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 266/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1343 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 267/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1370 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 268/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2004 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 269/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2516 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 270/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1855 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 271/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1967 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 272/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1404 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 273/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1719 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 274/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1770 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 275/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1445 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 276/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2393 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 277/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1443 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 278/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0731 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 279/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1150 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 280/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1552 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 281/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2076 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 282/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1503 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 283/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1723 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 284/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1406 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 285/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1430 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 286/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2876 - acc: 0.1000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 287/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2100 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 288/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1840 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 289/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1607 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 290/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1489 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 291/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1708 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 292/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1058 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 293/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1571 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 294/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1432 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 295/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1407 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 296/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1802 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 297/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1270 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 298/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1952 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 299/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1542 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 300/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2209 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 301/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1460 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 302/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1532 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 303/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2444 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 304/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0881 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 305/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.3328 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 306/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1583 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 307/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1276 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 308/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1995 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 309/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1356 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 310/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2155 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 311/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1809 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 312/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1985 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 313/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1448 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 314/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0709 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 315/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2301 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 316/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2028 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 317/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2486 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 318/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1347 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 319/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1514 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 320/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1572 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 321/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0699 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 322/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2028 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 323/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1234 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 324/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2335 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 325/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2197 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 326/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1655 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 327/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1119 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 328/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2114 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 329/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1494 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 330/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1821 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 331/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0873 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 332/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1657 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 333/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2197 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 334/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1459 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 335/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2235 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 336/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1131 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 337/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1702 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 338/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2342 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 339/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1772 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 340/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1682 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 341/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2016 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 342/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2521 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 343/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2133 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 344/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1111 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 345/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1406 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 346/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0854 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 347/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1570 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 348/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1819 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 349/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1749 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 350/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1929 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 351/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1939 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 352/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1126 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 353/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1249 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 354/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1869 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 355/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0672 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 356/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1583 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 357/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2138 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 358/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1192 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 359/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2379 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 360/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2606 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 361/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2061 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 362/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1171 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 363/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1411 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 364/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2054 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 365/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1860 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 366/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1617 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 367/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1489 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 368/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1241 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 369/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1614 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 370/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2074 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 371/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2645 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 372/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1011 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 373/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1185 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 374/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1724 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 375/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1547 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 376/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1343 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 377/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0695 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 378/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1669 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 379/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1863 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 380/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1912 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 381/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1572 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 382/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0988 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 383/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.3228 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 384/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1100 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 385/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1840 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 386/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2052 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 387/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1561 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 388/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1057 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 389/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1733 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 390/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1162 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 391/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1401 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 392/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1770 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 393/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2336 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 394/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1356 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 395/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1558 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 396/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1568 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 397/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1534 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 398/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2437 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 399/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0374 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 400/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2671 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 401/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1894 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 402/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2049 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 403/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1080 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 404/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1125 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 405/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2028 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 406/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1837 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 407/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1756 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 408/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2216 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 409/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2350 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 410/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2224 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 411/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1186 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 412/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2193 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 413/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1037 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 414/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1564 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 415/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1957 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 416/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1204 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 417/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2232 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 418/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1514 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 419/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0992 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 420/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2234 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 421/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2296 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 422/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2579 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 423/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0899 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 424/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1791 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 425/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1426 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 426/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1307 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 427/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1167 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 428/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1667 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 429/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1749 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 430/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1088 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 431/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2005 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 432/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1749 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 433/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1696 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 434/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1511 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 435/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1691 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 436/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2465 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 437/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2147 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 438/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1898 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 439/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1467 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 440/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1246 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 441/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1226 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 442/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2007 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 443/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1531 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 444/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2936 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 445/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2515 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 446/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1882 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 447/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0754 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 448/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1783 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 449/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1560 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 450/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1193 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 451/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1282 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 452/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1847 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 453/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1807 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 454/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0817 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 455/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1321 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 456/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1469 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 457/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2352 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 458/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0966 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 459/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1765 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 460/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1804 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 461/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2057 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 462/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1006 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 463/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1454 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 464/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2152 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 465/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2184 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 466/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1298 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 467/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1546 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 468/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1976 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 469/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1826 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 470/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2140 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 471/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1083 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 472/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1635 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 473/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1993 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 474/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2021 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 475/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1556 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 476/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.0422 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 477/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1430 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 478/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2638 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 479/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1406 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 480/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1924 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 481/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1028 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 482/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1612 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 483/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2526 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 484/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1347 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 485/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1525 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 486/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1204 - acc: 0.8000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 487/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2900 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 488/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1465 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 489/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1101 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 490/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2015 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 491/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1413 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 492/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1290 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 493/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1298 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 494/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0900 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 495/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2228 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 496/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2111 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 497/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1158 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 498/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1822 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 499/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1882 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 500/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2645 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 501/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2492 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 502/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1642 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 503/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1772 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 504/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1706 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 505/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2632 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 506/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2454 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 507/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1431 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 508/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2773 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 509/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2261 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 510/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2017 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 511/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0460 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 512/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0517 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 513/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2268 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 514/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0870 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 515/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1052 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 516/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2198 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 517/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1620 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 518/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2088 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 519/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1454 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 520/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1278 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 521/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1231 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 522/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2038 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 523/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1219 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 524/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1578 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 525/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1068 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 526/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1022 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 527/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1492 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 528/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1328 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 529/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1431 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 530/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1622 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 531/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0405 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 532/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1602 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 533/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0940 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 534/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1798 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 535/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1674 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 536/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1301 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 537/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0790 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 538/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2238 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 539/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1196 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 540/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1976 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 541/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1838 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 542/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1706 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 543/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1447 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 544/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1454 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 545/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1653 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 546/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0973 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 547/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0972 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 548/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0984 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 549/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0874 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 550/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1426 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 551/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0975 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 552/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2601 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 553/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2458 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 554/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2241 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 555/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2745 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 556/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1667 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 557/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1674 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 558/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1400 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 559/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1794 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 560/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1469 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 561/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0902 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 562/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1406 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 563/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1625 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 564/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1597 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 565/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1855 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 566/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2000 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 567/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1255 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 568/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1177 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 569/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1630 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 570/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1963 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 571/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1897 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 572/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1429 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 573/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1471 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 574/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1748 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 575/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1807 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 576/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2330 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 577/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0893 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 578/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1687 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 579/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1719 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 580/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.3127 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 581/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1511 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 582/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1058 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 583/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1501 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 584/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0638 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 585/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1356 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 586/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1317 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 587/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1080 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 588/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2360 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 589/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1641 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 590/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0972 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 591/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1906 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 592/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1888 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 593/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1464 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 594/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0536 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 595/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1662 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 596/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1217 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 597/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1083 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 598/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1286 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 599/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1432 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 600/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2583 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 601/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1493 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 602/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1920 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 603/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1470 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 604/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1816 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 605/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1204 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 606/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0783 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 607/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1803 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 608/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2434 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 609/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1370 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 610/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2478 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 611/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1280 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 612/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1330 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 613/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1206 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 614/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0550 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 615/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1995 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 616/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2336 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 617/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1164 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 618/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1658 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 619/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1663 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 620/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1240 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 621/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1544 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 622/1000\n",
            "10/10 [==============================] - 0s 5ms/step - loss: 669.2579 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 623/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2603 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 624/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2023 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 625/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0959 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 626/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1233 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 627/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1538 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 628/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2223 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 629/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2787 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 630/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1327 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 631/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1548 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 632/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1525 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 633/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2465 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 634/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2007 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 635/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2675 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 636/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1418 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 637/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0978 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 638/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1874 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 639/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1854 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 640/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1508 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 641/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2766 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 642/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1706 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 643/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1667 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 644/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0981 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 645/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1757 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 646/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2789 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 647/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1552 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 648/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1445 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 649/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0942 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 650/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1647 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 651/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1953 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 652/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1544 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 653/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1375 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 654/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1443 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 655/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1019 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 656/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2112 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 657/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1103 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 658/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1736 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 659/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1162 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 660/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1702 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 661/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2094 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 662/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1992 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 663/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1142 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 664/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0978 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 665/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0850 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 666/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1234 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 667/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0978 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 668/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2625 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 669/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1407 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 670/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1575 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 671/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2089 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 672/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1883 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 673/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1468 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 674/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0925 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 675/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2603 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 676/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1346 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 677/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1638 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 678/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1575 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 679/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2150 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 680/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1583 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 681/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2811 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 682/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1590 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 683/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1481 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 684/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1627 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 685/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1222 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 686/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1345 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 687/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1860 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 688/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1704 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 689/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1390 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 690/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2050 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 691/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0898 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 692/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1412 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 693/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1678 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 694/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1697 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 695/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2197 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 696/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2748 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 697/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1473 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 698/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2073 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 699/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2043 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 700/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1578 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 701/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1486 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 702/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2509 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 703/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1323 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 704/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1793 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 705/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1572 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 706/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1335 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 707/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0801 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 708/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2066 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 709/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2581 - acc: 0.1000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 710/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2164 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 711/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1771 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 712/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1168 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 713/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1260 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 714/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1302 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 715/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2045 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 716/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1715 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 717/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2612 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 718/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1643 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 719/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1793 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 720/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1539 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 721/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2495 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 722/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0997 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 723/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1359 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 724/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1554 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 725/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1712 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 726/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1470 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 727/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2013 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 728/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1920 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 729/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1808 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 730/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1770 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 731/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2681 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 732/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1868 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 733/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1492 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 734/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2318 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 735/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1943 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 736/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1259 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 737/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2112 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 738/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1313 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 739/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1074 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 740/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1717 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 741/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2127 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 742/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1438 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 743/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2508 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 744/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1575 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 745/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1473 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 746/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0701 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 747/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0924 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 748/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1461 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 749/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1749 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 750/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2432 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 751/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1537 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 752/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2654 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 753/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2043 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 754/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1853 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 755/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1949 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 756/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1478 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 757/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1956 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 758/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1477 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 759/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1762 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 760/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1700 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 761/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1989 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 762/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1478 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 763/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1509 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 764/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1301 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 765/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2258 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 766/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1365 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 767/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1424 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 768/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2273 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 769/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1738 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 770/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0721 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 771/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1627 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 772/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2035 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 773/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1147 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 774/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1839 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 775/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1545 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 776/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1885 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 777/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2282 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 778/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1539 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 779/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1791 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 780/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2782 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 781/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1493 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 782/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1545 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 783/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1862 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 784/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1630 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 785/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1565 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 786/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1249 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 787/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1255 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 788/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1805 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 789/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1420 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 790/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1132 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 791/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2195 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 792/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1983 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 793/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1183 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 794/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1414 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 795/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1512 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 796/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2239 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 797/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1978 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 798/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1308 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 799/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2390 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 800/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1213 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 801/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2346 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 802/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0408 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 803/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1479 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 804/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1021 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 805/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1959 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 806/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1882 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 807/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1977 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 808/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1948 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 809/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2551 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 810/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1348 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 811/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1587 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 812/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1766 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 813/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2491 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 814/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1726 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 815/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1616 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 816/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1529 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 817/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1594 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 818/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1823 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 819/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1905 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 820/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0942 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 821/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1022 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 822/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1754 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 823/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2075 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 824/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1872 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 825/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1338 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 826/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1665 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 827/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2205 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 828/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1081 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 829/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1560 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 830/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1339 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 831/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2458 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 832/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2075 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 833/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1996 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 834/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2169 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 835/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1685 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 836/1000\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 669.1848 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 837/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1619 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 838/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1566 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 839/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1398 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 840/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1168 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 841/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1754 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 842/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1616 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 843/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1450 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 844/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1184 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 845/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2175 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 846/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1707 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 847/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1275 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 848/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1179 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 849/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1850 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 850/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1890 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 851/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1333 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 852/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0793 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 853/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1763 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 854/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2460 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 855/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2606 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 856/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1552 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 857/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2205 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 858/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1751 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 859/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1022 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 860/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2444 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 861/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0974 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 862/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1433 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 863/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1860 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 864/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1683 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 865/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1511 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 866/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2358 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 867/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1735 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 868/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1235 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 869/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1410 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 870/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2459 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 871/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1133 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 872/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1392 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 873/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1577 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 874/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1220 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 875/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1642 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 876/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1652 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 877/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1711 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 878/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1405 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 879/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2464 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 880/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2442 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 881/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1782 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 882/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2025 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 883/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0964 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 884/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1626 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 885/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2112 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 886/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1782 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 887/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0979 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 888/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2352 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 889/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2161 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 890/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1845 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 891/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0724 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 892/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1913 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 893/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2042 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 894/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2837 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 895/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1199 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 896/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2376 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 897/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1277 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 898/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1804 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 899/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1077 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 900/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1666 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 901/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1066 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 902/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2932 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 903/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1202 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 904/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2223 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 905/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2061 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 906/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1621 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 907/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1281 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 908/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1198 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 909/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1479 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 910/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2356 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 911/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1755 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 912/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1942 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 913/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1154 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 914/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1870 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 915/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2448 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 916/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1847 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 917/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2256 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 918/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1978 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 919/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2029 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 920/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1022 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 921/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1484 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 922/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1371 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 923/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1573 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 924/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1726 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 925/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1829 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 926/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1842 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 927/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2592 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 928/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2359 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 929/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1240 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 930/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1393 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 931/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2214 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 932/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1578 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 933/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1800 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 934/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1832 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 935/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2488 - acc: 0.1000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 936/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1088 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 937/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1680 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 938/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1720 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 939/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1149 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 940/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1680 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 941/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0532 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 942/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1214 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 943/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0742 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 944/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1106 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 945/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0757 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 946/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2128 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 947/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1906 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 948/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2173 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 949/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0609 - acc: 0.7000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 950/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1899 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 951/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1530 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 952/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1476 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 953/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1837 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 954/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1771 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 955/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1271 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 956/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2047 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 957/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1706 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 958/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1935 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 959/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1665 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 960/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1061 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 961/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1969 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 962/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1974 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 963/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2400 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 964/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0502 - acc: 0.8000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 965/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2592 - acc: 0.2000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 966/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1366 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 967/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2330 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 968/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.3163 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 969/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1737 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 970/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0714 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 971/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1754 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 972/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1467 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 973/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1059 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 974/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1254 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 975/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1343 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 976/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1362 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 977/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1382 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 978/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1663 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 979/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1910 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 980/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1196 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 981/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1504 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 982/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1442 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 983/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2112 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 984/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0648 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 985/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1267 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 986/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2382 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 987/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1473 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 988/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1138 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 989/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.1164 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 990/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.0851 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 991/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1301 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 992/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1241 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 993/1000\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 669.2161 - acc: 0.3000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 994/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2396 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 995/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1646 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 996/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.2400 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 997/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1360 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 998/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1185 - acc: 0.5000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 999/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.0287 - acc: 0.6000 - val_loss: 669.0942 - val_acc: 0.4632\n",
            "Epoch 1000/1000\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 669.1077 - acc: 0.4000 - val_loss: 669.0942 - val_acc: 0.4632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCZdDexKTtFU",
        "colab_type": "code",
        "outputId": "4e615a20-a6fb-4d2c-d10b-6987c5d125c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd5001680f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD8CAYAAABpcuN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlsuFwszMcrlQmJlZrv8Cxz2J7UZHxuMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhs7m2_TTx9D",
        "colab_type": "code",
        "outputId": "e11a8cc6-0387-4466-be84-511243716b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"acc\"], label=\"train accuracy\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_acc\"], label=\"Validation accuracy\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd5000c1978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dy3ZG6ETxHq",
        "colab_type": "code",
        "outputId": "83706997-49d8-4756-8477-c01955d5d50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model.evaluate(x=test_x, y=test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "492/492 [==============================] - 0s 105us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[669.0975917412983, 0.5182926830479769]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwZoYhXOT6KK",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WQ40s_bsT8x4"
      },
      "source": [
        "### Test 3: First 5 classes for training, another 5 for testing, 1 example, 5 way training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nYnH8KTUJJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def get_training_data(sample_per_class=5, n_ways=5, batch_size = 32, valid_sample_per_class=2, test_sample_per_class=2):\n",
        "  train_x, train_y, valid_x, valid_y, test_x, test_y = None, None, None, None, None, None\n",
        "  \n",
        "  for i in range(5):\n",
        "    indices = np.argwhere(y_train_master == i)\n",
        "    choice = np.random.choice(indices.shape[0], sample_per_class + valid_sample_per_class, replace=False)\n",
        "    choice = indices[choice]\n",
        "\n",
        "    sub_train_x = x_train_master[choice[:sample_per_class]]\n",
        "    sub_train_y = y_train_master[choice[:sample_per_class]]\n",
        "    \n",
        "    sub_valid_x = x_train_master[choice[sample_per_class:]]\n",
        "    sub_valid_y = y_train_master[choice[sample_per_class:]]\n",
        "    \n",
        "    sub_train_x, sub_train_y, sub_valid_x, sub_valid_y = np.squeeze(sub_train_x, axis=1), (sub_train_y), np.squeeze(sub_valid_x, axis=1), (sub_valid_y)\n",
        "    \n",
        "    if(train_x is None):\n",
        "      train_x, train_y, valid_x, valid_y = sub_train_x, sub_train_y, sub_valid_x, sub_valid_y\n",
        "    else:\n",
        "      train_x = np.vstack((train_x, sub_train_x))\n",
        "      train_y = np.vstack((train_y, sub_train_y))\n",
        "\n",
        "      valid_x = np.vstack((valid_x, sub_valid_x))\n",
        "      valid_y = np.vstack((valid_y, sub_valid_y))\n",
        "    \n",
        "  for i in range(5,10):\n",
        "    t_indices = np.argwhere(y_test_master == i)\n",
        "    t_choice = np.random.choice(t_indices.shape[0], test_sample_per_class, replace=False)\n",
        "    t_choice = t_indices[t_choice]\n",
        "    sub_test_x = x_test_master[t_choice]\n",
        "    sub_test_y = y_test_master[t_choice]\n",
        "    \n",
        "    sub_test_x, sub_test_y =  np.squeeze(sub_test_x, axis=1), (sub_test_y)\n",
        "\n",
        "    if(test_x is None):\n",
        "      test_x, test_y = sub_test_x, sub_test_y\n",
        "    else:\n",
        "      test_x = np.vstack((test_x, sub_test_x))\n",
        "      test_y = np.vstack((test_y, sub_test_y))\n",
        "\n",
        "      \n",
        "  train_x, train_y = create_tuples(train_x, train_y, n_ways)\n",
        "  train_x, train_y = equalize_class_examples(train_x, train_y)\n",
        "  \n",
        "  valid_x, valid_y = create_tuples(valid_x, valid_y, n_ways)\n",
        "  valid_x, valid_y = equalize_class_examples(valid_x, valid_y)\n",
        "  test_x, test_y = create_tuples(test_x, test_y, n_ways)\n",
        "  test_x, test_y = equalize_class_examples(test_x, test_y)\n",
        "  \n",
        "  return train_x, train_y, valid_x, valid_y, test_x, test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kPXfPj-mT8yE",
        "outputId": "52fa6069-c102-499d-f0d5-23249f8c63eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_x, train_y, valid_x, valid_y, test_x, test_y = get_training_data(sample_per_class=1, n_ways=2, batch_size = 32, valid_sample_per_class=20, test_sample_per_class=50)\n",
        "print(train_y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hZ3Cz2KW6I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize_data(train_x, train_y, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrSarQCBXBGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize_data(test_x, test_y, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnD0UIPfYKpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ip_shape = (28,28,1)\n",
        "model = get_classifier_model(ip_shape)\n",
        "\n",
        "optimizer = Adam(lr = 1e-14)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f6uO49zIT8yQ",
        "outputId": "2d23149e-8b88-4e39-bc60-4af1a4355f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_x, train_y, epochs=500, validation_data=(valid_x, valid_y), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6 samples, validate on 82 samples\n",
            "Epoch 1/500\n",
            "6/6 [==============================] - 1s 197ms/step - loss: 668.9790 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 2/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8344 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 3/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0840 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 4/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9556 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 5/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9476 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 6/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9666 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 7/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9948 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 8/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9137 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 9/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9667 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 10/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0227 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 11/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8943 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 12/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9291 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 13/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9977 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 14/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9073 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 15/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 669.1320 - acc: 0.3333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 16/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9709 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 17/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9041 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 18/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9158 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 19/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9183 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 20/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8842 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 21/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9378 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 22/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8624 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 23/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9705 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 24/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9713 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 25/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9529 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 26/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0046 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 27/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9062 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 28/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9351 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 29/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9744 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 30/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9872 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 31/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8898 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 32/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9504 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 33/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9857 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 34/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9465 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 35/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9160 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 36/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9273 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 37/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9877 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 38/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8975 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 39/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9088 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 40/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9590 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 41/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9442 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 42/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9436 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 43/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9679 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 44/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8896 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 45/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0340 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 46/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0289 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 47/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9352 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 48/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8930 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 49/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9842 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 50/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9841 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 51/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9696 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 52/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8964 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 53/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9416 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 54/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9153 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 55/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9269 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 56/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0699 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 57/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9300 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 58/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9207 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 59/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9396 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 60/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9930 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 61/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8760 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 62/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9446 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 63/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9391 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 64/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9296 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 65/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9590 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 66/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9506 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 67/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8875 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 68/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8934 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 69/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0369 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 70/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9086 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 71/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9402 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 72/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9280 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 73/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8795 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 74/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9483 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 75/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9465 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 76/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9818 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 77/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.8838 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 78/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9153 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 79/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9720 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 80/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9232 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 81/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9323 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 82/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9031 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 83/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0322 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 84/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9531 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 85/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9818 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 86/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9337 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 87/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9731 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 88/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9666 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 89/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8992 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 90/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9938 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 91/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9949 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 92/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9357 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 93/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9117 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 94/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9479 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 95/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9660 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 96/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8806 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 97/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9844 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 98/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9333 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 99/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9299 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 100/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8080 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 101/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0018 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 102/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0024 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 103/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8665 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 104/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9523 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 105/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8693 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 106/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9437 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 107/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9842 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 108/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9065 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 109/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9257 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 110/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9000 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 111/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9236 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 112/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9294 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 113/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9864 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 114/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9086 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 115/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9405 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 116/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9349 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 117/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9561 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 118/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9464 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 119/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9479 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 120/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0447 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 121/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9893 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 122/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8985 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 123/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9283 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 124/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9641 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 125/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9958 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 126/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8918 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 127/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9241 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 128/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8853 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 129/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9410 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 130/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9789 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 131/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9767 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 132/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9601 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 133/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9005 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 134/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8893 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 135/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9751 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 136/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0150 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 137/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9291 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 138/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8982 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 139/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9437 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 140/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0211 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 141/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9916 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 142/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9615 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 143/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0071 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 144/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9733 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 145/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9462 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 146/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9312 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 147/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8932 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 148/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8987 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 149/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9341 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 150/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9061 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 151/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9595 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 152/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8809 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 153/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9662 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 154/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8892 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 155/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9000 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 156/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9527 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 157/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9460 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 158/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8099 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 159/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8907 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 160/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9945 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 161/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0345 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 162/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9607 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 163/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8951 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 164/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9523 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 165/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9335 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 166/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9447 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 167/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9525 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 168/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9842 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 169/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8625 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 170/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9887 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 171/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8682 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 172/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9938 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 173/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9312 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 174/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9399 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 175/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8997 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 176/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9666 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 177/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9245 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 178/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0720 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 179/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8981 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 180/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9501 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 181/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0073 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 182/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9548 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 183/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9299 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 184/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8463 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 185/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8359 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 186/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9990 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 187/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9519 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 188/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9271 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 189/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9007 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 190/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9491 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 191/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9395 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 192/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9570 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 193/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9419 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 194/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8494 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 195/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8784 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 196/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9666 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 197/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0118 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 198/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9054 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 199/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9016 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 200/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9859 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 201/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9537 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 202/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9445 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 203/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0124 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 204/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0537 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 205/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9485 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 206/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9312 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 207/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8938 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 208/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8674 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 209/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9350 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 210/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0042 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 211/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0439 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 212/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8647 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 213/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8801 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 214/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8907 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 215/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0247 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 216/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9524 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 217/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9449 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 218/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9089 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 219/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0414 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 220/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8947 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 221/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9681 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 222/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9473 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 223/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9865 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 224/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8893 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 225/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9645 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 226/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9476 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 227/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0140 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 228/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8806 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 229/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0476 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 230/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9553 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 231/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9517 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 232/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9282 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 233/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 669.0702 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 234/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9785 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 235/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8442 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 236/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9607 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 237/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0129 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 238/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0052 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 239/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9186 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 240/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8889 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 241/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8887 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 242/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9676 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 243/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9318 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 244/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9223 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 245/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8486 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 246/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9515 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 247/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8613 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 248/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0264 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 249/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9792 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 250/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0366 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 251/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0187 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 252/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0287 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 253/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9926 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 254/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0112 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 255/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9153 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 256/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9099 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 257/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9029 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 258/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8932 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 259/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9331 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 260/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9395 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 261/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9692 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 262/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8641 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 263/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8966 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 264/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9775 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 265/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9777 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 266/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9768 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 267/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9319 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 268/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9394 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 269/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9268 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 270/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9413 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 271/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9509 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 272/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9268 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 273/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8740 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 274/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9913 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 275/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8692 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 276/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8951 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 277/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0176 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 278/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9923 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 279/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9632 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 280/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0060 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 281/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9473 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 282/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9067 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 283/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9803 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 284/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9880 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 285/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8704 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 286/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9489 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 287/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9706 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 288/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9195 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 289/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0021 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 290/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9174 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 291/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8970 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 292/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9286 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 293/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9445 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 294/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9423 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 295/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0734 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 296/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0043 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 297/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8716 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 298/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0104 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 299/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9870 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 300/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0069 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 301/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9715 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 302/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8461 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 303/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0444 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 304/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9871 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 305/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9431 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 306/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9763 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 307/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9332 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 308/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9333 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 309/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8520 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 310/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0092 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 311/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.8712 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 312/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8738 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 313/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9695 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 314/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9841 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 315/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9695 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 316/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9226 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 317/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0525 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 318/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9205 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 319/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9978 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 320/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9681 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 321/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8568 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 322/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9337 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 323/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9144 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 324/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9457 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 325/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9440 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 326/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0123 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 327/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9668 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 328/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9772 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 329/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0508 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 330/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9922 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 331/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8978 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 332/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9684 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 333/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8713 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 334/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9253 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 335/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.1132 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 336/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8923 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 337/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.8934 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 338/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8276 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 339/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0261 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 340/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8388 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 341/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9828 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 342/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0731 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 343/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 669.0226 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 344/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9557 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 345/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9199 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 346/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0048 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 347/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9390 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 348/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8578 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 349/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0157 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 350/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0359 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 351/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9033 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 352/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9545 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 353/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0261 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 354/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9895 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 355/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0886 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 356/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9092 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 357/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8697 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 358/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9453 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 359/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9371 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 360/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9705 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 361/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9913 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 362/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9393 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 363/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8439 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 364/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.8917 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 365/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9460 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 366/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9598 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 367/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9865 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 368/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8271 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 369/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9161 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 370/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9758 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 371/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0015 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 372/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9338 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 373/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9308 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 374/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8665 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 375/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9474 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 376/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8320 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 377/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9520 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 378/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9498 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 379/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9849 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 380/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8761 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 381/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8928 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 382/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0098 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 383/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0475 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 384/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9706 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 385/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9058 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 386/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0134 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 387/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9830 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 388/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9818 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 389/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9558 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 390/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.1175 - acc: 0.3333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 391/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9404 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 392/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9863 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 393/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8762 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 394/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9101 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 395/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9741 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 396/500\n",
            "6/6 [==============================] - 0s 5ms/step - loss: 668.8965 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 397/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9119 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 398/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0007 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 399/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0009 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 400/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9026 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 401/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8987 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 402/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9530 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 403/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9092 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 404/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0139 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 405/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9622 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 406/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9498 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 407/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9008 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 408/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9352 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 409/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8848 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 410/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9754 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 411/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9243 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 412/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9377 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 413/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9439 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 414/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8681 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 415/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9302 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 416/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9143 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 417/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9396 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 418/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9539 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 419/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9382 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 420/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9432 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 421/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0112 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 422/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9306 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 423/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0046 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 424/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9594 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 425/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9704 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 426/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9371 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 427/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9334 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 428/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9979 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 429/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9040 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 430/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8751 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 431/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8997 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 432/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9472 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 433/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9055 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 434/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9969 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 435/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8429 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 436/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9103 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 437/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0029 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 438/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8981 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 439/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8774 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 440/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9054 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 441/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9408 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 442/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8823 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 443/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9595 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 444/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0146 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 445/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9011 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 446/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9467 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 447/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9858 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 448/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9458 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 449/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 669.0541 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 450/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9012 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 451/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0592 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 452/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9382 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 453/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9936 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 454/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9722 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 455/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9268 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 456/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8594 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 457/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9944 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 458/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9590 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 459/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0187 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 460/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9448 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 461/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8748 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 462/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9872 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 463/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0253 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 464/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8664 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 465/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9409 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 466/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9209 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 467/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9879 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 468/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9908 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 469/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9078 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 470/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8770 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 471/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9277 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 472/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9319 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 473/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9326 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 474/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8763 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 475/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9243 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 476/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9399 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 477/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9637 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 478/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.8962 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 479/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9785 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 480/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 669.0790 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 481/500\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 668.9935 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 482/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9534 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 483/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9066 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 484/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8804 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 485/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9645 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 486/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0078 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 487/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 669.0207 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 488/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.7850 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 489/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9229 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 490/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9000 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 491/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9431 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 492/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9370 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 493/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8797 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 494/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8583 - acc: 1.0000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 495/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9736 - acc: 0.5000 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 496/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8594 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 497/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9861 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 498/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.9784 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 499/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8990 - acc: 0.6667 - val_loss: 669.0788 - val_acc: 0.5244\n",
            "Epoch 500/500\n",
            "6/6 [==============================] - 0s 3ms/step - loss: 668.8590 - acc: 0.8333 - val_loss: 669.0788 - val_acc: 0.5244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PFHvk20vT8yY",
        "outputId": "9142988c-3bac-485c-b600-5210ef24228a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd4e8ddad30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD9CAYAAACiLjDdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsfXmYFcW5/vv1OWdmmGFHxCgqYEQ2\n2SEYg4hbTEwkJurVq9GYGG7MvSaam9wQf9fENSHGGDVxiRqISYzES1wScYlRFOOCorKDKIswIDAs\nwwzMcpau3x/d1V1VXdXd58wZZoB+n4eHM71UV3dVffv3FTHGkCBBggQJEphgdXQHEiRIkCBB50bC\nKBIkSJAgQSgSRpEgQYIECUKRMIoECRIkSBCKhFEkSJAgQYJQJIwiQYIECRKEIhajIKKeRDSXiFYT\n0SoiOsk9frV7bAUR3eYeqyCi2US0jIiWENGphjYvcO+ziWi8cHwAETUT0WL33/1leM8ECRIkSFAi\n0jGvuwvAc4yx84moAkA1EU0FMA3AKMZYKxEd7l77TQBgjJ3oHnuWiCYwxmylzeUAvgzgt5rnrWWM\njS76bRIkSJAgQdkRqVEQUQ8ApwD4HQAwxrKMsXoAVwGYyRhrdY9vd28ZBuAl4Vg9gPFqu4yxVYyx\n98vxEgkSJEiQoP0Qx/Q0EEAdgNlE9B4RPURENQAGA5hMRAuJ6BUimuBevwTAuUSUJqKBAMYBOLrI\nfg10n/UKEU0u8t4ECRIkSFBGxDE9pQGMBXA1Y2whEd0FYIZ7vDeASQAmAHiMiAYBmAVgKIBFAD4C\n8DqAQhF9+hjAMYyxnUQ0DsCTRDScMdYgXkRE0wFMB4CamppxQ4YMKeIRCRIkSJDgnXfe2cEY6xt1\nXRxGUQugljG20P17LhxGUQvgceYUi3qLiGwAhzHG6gBcy28motcBrInbcdeUxc1Z7xDRWjjayyLl\nugcAPAAA48ePZ4sWLVKbSpAgQYIEISCij+JcF2l6YoxtBbCJiE5wD50OYCWAJwFMdR82GEAFgB1E\nVO2apkBEZwLIM8ZWFtHxvkSUcn8PAnA8gHVx70+QIEGCBOVF3KinqwE84kY8rQNwBYB9AGYR0XIA\nWQCXM8aYG+n0vKthbAbwVd4IET0E4H7G2CIiOg/ArwH0BTCPiBYzxj4Lx3F+ExHlANgAvsUY21WW\nt02QIEGCBEWDDoYy44npKUGCBAmKBxG9wxgLRKWqiKtRJEiQIIGHXC6H2tpatLS0dHRXEsRAVVUV\n+vfvj0wmU9L9CaNIkCBB0aitrUW3bt0wYMAAEFFHdydBCBhj2LlzJ2prazFw4MCS2khqPSVIkKBo\ntLS0oE+fPgmTOABAROjTp0+btL+EUSRIkKAkJEziwEFbxyphFBFYs60Rb29Igq4SJEhw6CJhFBE4\n61cLcMH9b3R0NxIkSCBg586dGD16NEaPHo0jjjgCRx11lPd3NpuN1cYVV1yB99+PX27uoYcewjXX\nXFNqlw9oJM7sBAkSHHDo06cPFi9eDAC44YYb0LVrV3z/+9+XrmGMgTEGy9LLw7Nnz273fh4sSDSK\nBAkSHDT48MMPMWzYMFxyySUYPnw4Pv74Y0yfPh3jx4/H8OHDcdNNN3nXfuYzn8HixYuRz+fRs2dP\nzJgxA6NGjcJJJ52E7du3hzwFWL9+PaZOnYqRI0fizDPPRG1tLQBgzpw5GDFiBEaNGoWpU6cCAJYt\nW4YJEyZg9OjRGDlyJNatO/AKTSQaRYIECdqEG/++Aiu3NERfWASGHdkdP/ni8JLuXb16Nf7whz9g\n/Hgnj2zmzJno3bs38vk8pk6divPPPx/Dhg2T7tmzZw+mTJmCmTNn4nvf+x5mzZqFGTNmGJ/x7W9/\nG1deeSUuueQSPPDAA7jmmmswd+5c3HjjjXj55ZfRr18/1NfXAwDuvfdefP/738e//du/obW1FQdi\nknOiUSRIkOCgwnHHHecxCQB49NFHMXbsWIwdOxarVq3CypXB0nNdunTB5z73OQDAuHHjsGHDhtBn\nLFy4EBdddBEA4LLLLsOrr74KADj55JNx2WWX4aGHHoJtO3u1ffrTn8Ytt9yC2267DZs2bUJVVVU5\nXnO/ItEoEiRI0CaUKvm3F2pqarzfH3zwAe666y689dZb6NmzJy699FJtPkFFRYX3O5VKIZ/Pl/Ts\nBx98EAsXLsTTTz+NsWPH4r333sNXv/pVnHTSSZg3bx7OPvtszJo1C6ecckpJ7XcUEo0iQYIEBy0a\nGhrQrVs3dO/eHR9//DGef/75srQ7adIkPPbYYwCAP/3pTx7hX7duHSZNmoSbb74ZvXr1wubNm7Fu\n3Tp88pOfxHe/+1184QtfwNKlS8vSh/2JRKNIkCDBAYV8wYZFBMuKTiIbO3Yshg0bhiFDhuDYY4/F\nySefXJY+3HPPPfj617+On/3sZ+jXr58XQXXttddi/fr1YIzhrLPOwogRI3DLLbfg0UcfRSaTwZFH\nHokbbrihLH3Yn0iqx0ZgwIx5AIANM89pl/YTJDgQsWrVKgwdOrRDnr20th41lWkc17drhzz/QIVu\nzOJWj01MTwkSJDjgsK+1NB9CgtKQMIoECRLEwpJN9bjx7ysOyPDOBG1DwigSJEgQCxfc/wZmv7YB\n2YLd0V1JsJ+RMIoECRIkSBCKhFEkSJAgFhgSk9OhioRRJEiQoCgkLopDD7EYBRH1JKK5RLSaiFYR\n0Unu8avdYyuI6Db3WAURzSaiZUS0hIhONbR5gXufTUTjlXM/IqIPieh9IvpsG98xQYIEBxm+ceEX\nA8lzd955J6666qrQ+7p2dUJqt2zZgvPPP197zamnnoqocPs777wTTU1N3t+f//znvdpObcENN9yA\n22+/vc3tlBtxNYq7ADzHGBsCYBSAVUQ0FcA0AKMYY8MB8Lf7JgAwxk4EcCaAXxKR7jnLAXwZwALx\nIBENA3ARgOEAzgZwLxGlinqrBAkStBvsTqBSfG7aVzBnzhzp2Jw5c3DxxRfHuv/II4/E3LlzS36+\nyiieeeYZ9OzZs+T2OjsiGQUR9QBwCoDfAQBjLMsYqwdwFYCZjLFW9zivyzsMwEvCsXoAgYQOxtgq\nxphu15BpAOYwxloZY+sBfAhgYrEvliBBgvKC84eO5BM8NPfMz0/DvHnzvE2KNmzYgC1btmDy5MnY\nu3cvTj/9dIwdOxYnnnginnrqqUA7GzZswIgRIwAAzc3NuOiiizB06FCcd955aG5u9q676qqrvBLl\nP/nJTwAAd999N7Zs2YKpU6d6pcQHDBiAHTt2AADuuOMOjBgxAiNGjMCdd97pPW/o0KH45je/ieHD\nh+Oss86SnqPD4sWLMWnSJIwcORLnnXcedu/e7T1/2LBhGDlypFeY8JVXXvE2bhozZgwaGxtL+8AG\nxCnhMRBAHYDZRDQKwDsAvgtgMIDJRHQrgBYA32eMvQ1gCYBziehRAEcDGOf+/1bMPh0F4E3h71r3\nmAQimg5gOgAcc8wxMZtOkCBBWxHgE8/OALYuK+9DjjgR+NxM4+kevXph4sSJePbZZzFt2jTMmTMH\nF154IYgIVVVVeOKJJ9C9e3fs2LEDkyZNwrnnnmvcN/q+++5DdXU1Vq1ahaVLl2Ls2LHeuVtvvRW9\ne/dGoVDA6aefjqVLl+I73/kO7rjjDsyfPx+HHXaY1NY777yD2bNnY+HChWCM4VOf+hSmTJmCXr16\n4YMPPsCjjz6KBx98EBdeeCH++te/4tJLLzW+42WXXYZf//rXmDJlCn784x/jxhtvxJ133omZM2di\n/fr1qKys9Mxdt99+O+655x6cfPLJ2Lt3b9kr1MYxPaUBjAVwH2NsDIB9AGa4x3sDmATgBwAeI2ck\nZsEh7osA3AngdQCFsvYaAGPsAcbYeMbY+L59+5a7+QQJEhjQWRLuLr74Ys/8JJqdGGO47rrrMHLk\nSJxxxhnYvHkztm3bZmxnwYIFHsEeOXIkRo4c6Z177LHHMHbsWIwZMwYrVqzQligX8a9//QvnnXce\nampq0LVrV3z5y1/2SpAPHDgQo0ePBhBdynzPnj2or6/HlClTAACXX345FixY4PXxkksuwZ/+9Cek\n046sf/LJJ+N73/se7r77btTX13vHy4U4rdUCqGWMLXT/nguHUdQCeJw5s+YtIrIBHMYYqwNwLb+Z\niF4HsKaIPm2Go4Fw9HePJUiQoBPAVvlEiORfboiPnjZtGq699lq8++67aGpqwrhx4wAAjzzyCOrq\n6vDOO+8gk8lgwIAB2tLiUVi/fj1uv/12vP322+jVqxe+9rWvldQOR2Vlpfc7lUpFmp5MmDdvHhYs\nWIC///3vuPXWW7Fs2TLMmDED55xzDp555hmcfPLJeP755zFkyJCS+6oiUqNgjG0FsImITnAPnQ5g\nJYAnAUwFACIaDKACwA4iqiaiGvf4mQDyjLFwNizjbwAuIqJKIhoI4HjEN1slSJCgncACPzoWXbt2\nxdSpU/H1r39dcmLv2bMHhx9+ODKZDObPn4+PPvootJ1TTjkFf/7znwEAy5cv98qANzQ0oKamBj16\n9MC2bdvw7LPPevd069ZN6weYPHkynnzySTQ1NWHfvn144oknMHny5KLfrUePHujVq5enjfzxj3/E\nlClTYNs2Nm3ahKlTp+LnP/859uzZg71792Lt2rU48cQT8cMf/hATJkzA6tWri35mGOLqJ1cDeISI\nKgCsA3AFHBPULCJaDiAL4HLGGCOiwwE872oYmwF8lTdCRA8BuJ8xtoiIzgPwawB9AcwjosWMsc8y\nxlYQ0WNwmFEewH8yxspuukqQIEFp6NDEO+XRF198Mc477zwpAuqSSy7BF7/4RZx44okYP358pGR9\n1VVX4YorrsDQoUMxdOhQTzMZNWoUxowZgyFDhuDoo4+WSpRPnz4dZ599No488kjMnz/fOz527Fh8\n7Wtfw8SJTvzNlVdeiTFjxkTumKfDww8/jG9961toamrCoEGDMHv2bBQKBVx66aXYs2cPGGP4zne+\ng549e+L666/H/PnzYVkWhg8f7u3WVy4kZcYjkJQZT5DAwXHXPYOCzfDe9Wdi68a1HVJm3LYZlm/Z\nAwAY2f/gDUdtDyRlxhMkSLDfEDePIlewsWHHPhTs8hURPPDF2gMTCaNIkCBBUYhLrLc3tqKhJYfd\nTbl27U+C9kfCKAC05guwA6EcMg4GE12CBG0BXwN+4l3EmmiXJZOsw1LQVvqVMAoAJ/zvc7juifCE\noQg+kiDBIQMGhqqqKuzcuTOCADnnone2LuLZyTosGowx7Ny5s01JeOXNyjiAMeftTZj5lZHG8zZj\nSJV1yidIcGCCMaB///6ora1FXV2d8brdTVnsay2gdUcG2yvLQ2psm2HbHieXYVVjl7K0eSigqqoK\n/fv3L/n+hFHERCLJJEjggDEgk8lg4MCBodf96PFlePStWtzypRG4dPSxZXn2rn1ZnHPzCwAOzkjE\nNdsacUzvalRlOlcd1EM+PJY9+0MsfOMVAMCkgX0C599cvxMAMHFAb1iGWjEJEhwK4GthzDE9UZmK\nJmTrduzF9sZWDDysBv26laf2UK5g452NTnE83Xo9kJG3bSz6aDd611Rg8OHd4t8YURcrDEl4bJlx\n4LPTBAnKhM6Tb3dQgftBG5o7X5TYIW96sj87Exe98gwAYMMVQVX2IjfhbuWln0V1xSH/uRIcwrj4\nR/PAGPCv86eif6/qyOvvn7sEjy2qxcxJJ+KiieWp8Fzf0IKLfvoiAP16PZDRsLcVF93yT/SuqcC7\nV5zZ0d2RcMhrFHFNb0nUU+fAmm3lrbN/MGHAjHn4xu/fbvfn8CWzaVcTBsyYhxVuprTpunJabJN1\n2DE45BlF3InXnrt65Qs2svnyZa8erHh22cc461cLMG/pxx3aj/qmLOoaWzu0Dya8uHp79EUlQl0C\nL65ySnc/9vYm/fXu/1TGaMHOsLtee6Ezv9ohzyjiFjhj7UjHv/ib1zD4f5+NvvAQx+qtjjbxfgdr\nFaNvegETbv1nh/ahI8EJmmU5DKBgoHDtoVF0YlraZnRmJpgwiphj054VM1d93NBubSdIUC5wgs8J\nGt8xzqSVM+W6ciCqgsKBDNvLfO9873jIM4q4OIjn5wGHJEi5Y+CV7nD/TrkMwETYfNNT+ftwMKLg\nEpnO+IqHPKOIO/E6s1qYIMH+BGcMruXJI3Cm68prejp412EZi+yWHQmjiOujOHjn5wGDZAg6B/g4\ncB+F0fTEryun6ekgngQmX09nQMIo4vooOvEgHmpIEuQ7Fp4zm/soDNSbHy5veOzBuw59H0UHd0SD\nhFHEvO5glmQOGHTGFXQIQjU9mYh3ewhXnVVgu/C3b+ALv361TW1whtsZ3/GQTzWOOygHs200QYJi\n4DmzvfDY8OvKGfXUCWkoAOCt9bva3MYBb3oiop5ENJeIVhPRKiI6yT1+tXtsBRHd5h6rIKLZRLSM\niJYQ0amGNnsT0QtE9IH7fy/3+KlEtIeIFrv/flymd9Ui0SgOICQ2p04BPz+C+yjCOUU5R60zrsNy\naQDcmd0JXzG26ekuAM8xxoYAGAVgFRFNBTANwCjG2HAAt7vXfhMAGGMnAjgTwC+JSPecGQBeZIwd\nD+BF92+OVxljo91/NxX9VkUgdtRTZ5yhhxp4FE2JpGfhup34zM9fQlM2X85eHdDIF2zUN2WLukfV\nrs3hseWPeuqMPoqBP3qmLO10xnfjiGQURNQDwCkAfgcAjLEsY6wewFUAZjLGWt3jvHbAMAAvCcfq\nAejK2E4D8LD7+2EAXyr9NUpHbNNT5x3DBDHx02dWoXZ3M97fmtSL4vjJ31Zg9E0voCVX0J7fuLMp\nICRxybfg/jCFx7ZHuOfBvA5N37EzII5GMRBAHYDZRPQeET1ERDUABgOYTEQLiegVIprgXr8EwLlE\nlCaigQDGATha024/xhgv2rMVQD/h3Emu2epZIhpe0pvFRGfIzE4QD20dgby7ENPWIR/D4eFvS7YA\nAFpzQaq+ZlsjTvnFfNy/YK10nK+FgnuLOTyWhZ4vBZ1Z6m4rPB9FJ3zFOCsmDWAsgPsYY2MA7INj\nJkoD6A1gEoAfAHiMHKPlLAC1ABYBuBPA6wD04ooL5oj1/PO8C+BYxtgoAL8G8KTuHiKaTkSLiGhR\n2HaMUUh8FAceSjVlcImNO2GLxYI1dWjNh07lgwqbdzcDABaukx21nJ5FRel4dK+NxP31D3cIz2pT\nU50a/Dt1xleMwyhqAdQyxha6f8+FwzhqATzOHLwFwAZwGGMszxi71vUvTAPQE8AaTbvbiOgTAOD+\nvx0AGGMNjLG97u9nAGSI6DD1ZsbYA4yx8Yyx8X379i3qpZV2Yl13MEsyBwraOgSeRpEqnlEs2VSP\ny2a9hZ89s7ptnWgnlOpDa4v7gEvAUQl3bRm3F1Zuw78/tBC/f32D2+bBuw4LB3JmNmNsK4BNRHSC\ne+h0ACvhSPpTAYCIBgOoALCDiKpd0xSI6EwAecbYSk3TfwNwufv7cgBPufcc4WomIKKJbh93lvZ6\n0Yg77Q5VPjH//e14QDE9HKjgGoWoUHywrRH5GCt0t+vwXVu3t1361laYBJk/L9yIDTv2Rd6vJcDE\nz+mfxb9nVB5FW4j7pl1NAICN7v+dTbMvZ85DZ/ZRxM2juBrAI0RUAWAdgCvgmKBmEdFyAFkAlzPG\nGBEdDuB5IrIBbAbwVd4IET0E4H7G2CIAM+GYq74B4CMAF7qXnQ/gKiLKA2gGcBFrxwyUJDM7HFfM\ndjbCmX7KcR3ck7ZHz+Q956vz97q6vTjzVwvw7VOPw/+cPSTi2Z07NFdHY2yb4bonlqFXdQbv/fis\n0Pt105u8c2qUE3+mzDBMbbbFqc2fYUWF4nYQ2sP/0hlpTSxGwRhbDH3k0qWaazcAOCF4KcAYu1L4\nvROOdqJe8xsAv4nTr3IgrrTTiZn9IQMvfr/E+wsFWQL+eE8LAOC9jfXeNZt2NeGcu1/F3/7rMxhw\nWI133CeaJT68naEjoPzY7qboPZh195uYI7+yEOE3YMr/pUA1F3a2719OxuUxirK1WD4k4R9J1NMh\nA050OIHLuapFJu0vg6cWb0ZDSx7/9468axuXaMV50JkkP11XiumdThAyMUemaBJR1WPbQkx9c2F4\nSfOOQjnNRZ3Z9HTIM4rYUU8d6GjasbcV2xtbOq4DnQRtZdaqBJxzNYxMjCgob9MeYR7EWdd7W/dP\ncp+OGBdDU3UEmDwfhZJHoZieTIzA+z5tGDY+ZumISrUdhXLyrc5mVhORMIqYY9ORgzj+ln9i4q0v\ndtjzOxtKdRd4GoU7ltyJnUn5y8A0zDqiGSUBPvneZoz4yfP7JcEvzPQU7/7gMZ4BH2yGaxLB52xr\naMGMvy5FNm97X6osGoXVOX0U5azP5JXw6FyvCCBhFLEnXlsHb3tDC257brVn7kjg4+anV+K0X74c\neV1bx0A1lWTdsdCFy6plQvjfIkGNmjsLPnDye5bW1odeVw6UKmmHEXNOBIOmJ/ke8dn/74llmPP2\nJixYU1eWvABVo+hsRLScjOuALwp4MCN2eGwbzR53v/QB7n15LZ5dvrVN7eiQL9i4658ftKuZoz1t\nw7/713qsq4sO4WwreNQT8zQK1/SUil4GnhYjfIYojaK6IgUAxvIY5YRufIrTKDSMgn8vtbaTdz5o\netrX6rxrF/fdi+1HoA/uvSmrc/ooylkDzksq7IT+0IRRxE64a9tzjuzZBQCweKNZuix1ETy99GP8\n6p9r8Ivn2i8ZLN8JjMNtjVBVNQqu3aVj+Ch0zuwoCbC6wgkqbN4PjEI3PMX5KILHOCNVz9nKdxSJ\nJS+4WF2RKstGPLbizO4E01BCOfuTaBSdGPvLR9GnpgJAuBmi1EnHTSh7W9uPIOVNmw7sR6jlrYtF\n3pOAnb9zXuhlfI1CMj1FDFhVxpGqb3vufWxvaN9gBHF+lmLyCfNxqGdUc1VOmBv7ss4czKSsspTw\n8Muu8Gd3/DwUUc5IJdv7XmVrsmw45BlFXLR18Pgk2LnPXNK5VGa0P1LBcp155/eYUG3r3JldEaOk\nhy75LIxI7GnKodbNJs7bDNc/tbyEHseHyLSiMqa19+s0Co9yyccZc8xp/HxemBvNLqMo2MwzubVl\n6ajMvbNpFOU0hXXmrQySHe5ijk1bJ0ScRVsyo9CYRcqNTqFRlKkdTkj5O6VTFrY3tmgrqHL4UU9C\nOyHjdf79r+OD7X65j1L30IgLkcbkbYZ0qjjhRsf0vHBi5cs35/IYcv1z3t+yRuGYnv73yeVYtnmP\n07c2EEDVXHggRj1t2tWER9/aiB989oRQbTjJo+jE0BHXxpYc9jTL2axtHUNfrTQ3VOoa2B9Zw3Hq\nIR0o8Ewmth8eO/HWFzH5tvmRzEg2PZmvE5kEAPSqyZTS1diwNZpOHOHGVKYDMPsoGlvkoAkxkq/J\n1Sg4k3D6FtkNI/xQZr1209GI827/8cd3cO/La/Hh9vA6YUlmdieGbi39vyeW45o57ynXtW344tiN\nS5WW+PYK7RkRkou52rN5G+vaqXBeuV7PYxR55/+MYHry/SDqPcFOFON87FldUXxHi4A4d/K2nsCH\n3x88VohJuERGkc0HuWebTE9u236l2s5FRuNoS9zBH1Xe3ns3ocnOEuWVMArNsbrGVuxSfAlt1ihi\nLN5Sn6GL8S834moUf1uyBWff+SoaW6LrC6nYX4uCawJ5O5hwx6EuaT90MXgsDkrcAiM2xE9X0PQ1\nCvrwWH0L6rVRU6Mt48oZDw/V7WzWmTiMKxczDFv3HSf+9EWcc/erJfWtnEh8FDqV27YDE7LtPgq3\nnZDlW7qPgrfdfsjF9FHUN2WRLdhobMmjW1Vx5paCzUL3iihXAVcunfJosTgbGfG315l44iDM/xGG\nzfXNqKlIRWokskZhB46ZoHsvvx29CUu9NB8R6NCWpcPHyNSXjkbYHDjn7lcx9phensYVNc90Gmpd\nYyvqGlvb1sky4JBnFLpxzhVYYOG03Ufhqs4ha4qV6Aag/VCCOYoYcPCFU0qSGXfCmmAyCxULrgkU\n46DX5QQUY3rKlujjOXnmS+hWmcayGz8b0T+hXyWYnnTX8u8UFJrkvwsR37Et89LTKAr6vnQ0wt5t\nxZYGrNjSgF7VjsAU1fVy7N/RXjjkGYVu+LQaxX4YvJJ9FJqs4XIjiqj+cO5SjDiquyf5tZQgQe+v\nqA8vjyIm8bZt5jkii8mj4OjXvbJkjQIAGmNk3EsaRaF4ghOmUahQj0YlY7ZlVFvzxWsUts3Q0JJr\nV7/QnxduxKeP6xOLcfHxiJovwaKVnSeAJPFRaMYuX2CBCVkujSJsopeeR9H+4bFRk/Yvizbh+qdW\neIuhpYS9paMJTnnej2sCuZgL+K4XP8CPn1rh9EEsV5Et4Jf/eF+rPYlWhsp0qt332tbld7TFmb1p\nVxP2utFNajPqHI5i8G3RKDijWLCmDp+87hnUN0f7vu6Z/yFG3/QCtrVTkmPBdjaE+sp9r8cSbrg2\nGfUZ1LZ2h+Rc7W8kjEJzLFewNaancvkooq8pFp6Poj01ipid8zWK4gljXI0ibk7CNXPew9TbXwYg\nEze/1hOvZRSOdz7arT3+6MKN+PVLH+Lel4NbxYr26Iq0hScXb8E7H+2K1e9SIH667Y2tKNisSEYh\nXzz5tvn41T+dre471EfhMop1O/YhbzMp7NaEF1ZtA+BvTBWG+19ZW3SUHhea6ptzge+mEwRz3jyL\nx1D5VWHJufsbCaPQaRQ2cxeacLJsGkVYX0rVKORntAdim2lYW3wUUeEz/L947/nk4i1Y7+4XLTIh\nL+FOk8Sla1s8L/6urnQcKjpCIzKKSndjpK/c90asfpcCsV8X/vYN3Pz0Su18eOej3Xh7Q5BhFTP3\ndP67MK2sHFFPHHHmFa8LFSV4NLbkMPPZ1bj4wTeL6hOfNymigM9R98i4WeXqeTXysiORMAqdj6Lg\nSGPi/G4rEWYxiFzpGkX7l2CO6/jli2hbQys27IiuCKsj4FFoaM7jhZXbYl2r9kt8TqhJQPCYy3WU\n/Eu6uLWcttQ3B27PWP7Sqki3/zJTv93vX9/gxe+L+Mp9r+OC+4MMK8z3EnBe60Jp2yGRFAi+V6sm\nT0MF59FRa5Y33VRkjTSuiVqjrPLaAAAgAElEQVSWLlS4dNOyei/XKHrXtG8OThwkjEIzdtz0JJ7a\nP3kU5TFvtQfCpH1RYuTv+aPHl+FU1+wTBlFijGRGLgG4/5W1+OYfFmGTW0spDsRFyLvLnx0l8Yrf\nVbyS379ZwyhSMepHFYOfPrMKK7c0GM/rXuGMOxbEbl9kDqqZUf1bm5wXSiBjd0Nzr8IoYgQFcG0u\nyvfkjXuRQ8V9WxZRgEGGreHIeaY47LmPqKYyJBRwPyEWoyCinkQ0l4hWE9EqIjrJPX61e2wFEd3m\nHqsgotlEtIyIlhDRqYY2exPRC0T0gft/L/c4EdHdRPQhES0lorFlelctTKYnm+nt2iKm/GI+Zv1r\nfaznxFkspTMKpvxffoTlURRCiEwUREYR+f7K6WLMW5JGwfMo8vF8FBIjFH5z6Xa7Js5dLF3elogn\njgcWrMMlD5lNJOUUMlSiX1CEBB0BPu32l/Hle1/Ttt2WIASVEMcJCohbkpy/Z2NLHj97ZlXsPnGh\nKUUU6b+Jew4I+jFzmh0YOwpxe3AXgOcYY0MAjAKwioimApgGYBRjbDiA291rvwkAjLETAZwJ4JdE\npHvODAAvMsaOB/Ci+zcAfA7A8e6/6QDuK/qtioBuEsfVKD7a2YSbnl4Z6zm63cACfSlxPcW1gTrP\nYPjzwo147O1NRT0jTNoXcwSKDXFtLfgLvz33vBD75VWP1SSm6cagYFApwhiV6KMoNYdChdq12t1N\neGm1Y4IrV1QeENQe1bHXjfGWPS1417DXSlTf8gVbayYDguMRx/SUirltqvgev12wznjdqo8bMGDG\nPM/fxb8HUTCbmjM23TeKZFyKH5MLMqIZs6MQ2QMi6gHgFAC/AwDGWJYxVg/gKgAzGWOt7vHt7i3D\nALwkHKsHMF7T9DQAD7u/HwbwJeH4H5iDNwH0JKJPlPBusWAOj5XPBSWH4lYm8xhF6TbMqPvi9GnF\nlgZc98Qy/M9flxb1jDDTk6gVqAskSv0Pu1dFW2ih2H814S5snAGz6YkTLUuTAZgWFjd/x7b6KtTn\nfO7OV/H13y9y+9hWjULQuJRxUEvMF/2siOu/99gSDPvx85H9AvSMgjGGH85diu/OeQ+2zTxGETWf\n4gomT7y3GQDwjxXO7pQ5z0dBxuhI3XqJ9JmIwozNiqoc0N6IM3MHAqgDMJuI3iOih4ioBsBgAJOJ\naCERvUJEE9zrlwA4l4jSRDQQwDgAR2va7ccY+9j9vRVAP/f3UQBEcbfWPSaBiKYT0SIiWlRXVxfj\nNfTQm54cjUKOdpGvKVZy5pe3hy23mNo+pUq3YaYnidgrHzTqecX4KALMGsDWPS246e8rI8dD5zT3\nS1f71+mIh2kPCm4G0a1jSaNw37FrZdvyW9XniIl45SoxAwS/gZp5XSyjiJrXf1uyxXgu4MzWaHEN\nzXn8ZdEmPLV4C/Zl87ErFcRdw+q35d/HIgo6/nkdMc1cjuyPOM8YE0xPBwajSAMYC+A+xtgYAPvg\nmInSAHoDmATgBwAeI2eEZsEh7osA3AngdQChhkXmjERRs48x9gBjbDxjbHzfvn2LuVVuR90PmDHk\nCk547GmCM1a9rlgziV/Co/wahRdRFdP0VArCigKKUp5KVKLs88WYrdTTuYKNHz2+FLNeW4831u4M\nvVdcuLwdXbavblxN9nv+3ro9BsSaVfwdqyva6pQ0E4y2mp4un/UWFm9yTEdBjUJlFMW1HddHoZub\ncUxPIoG17fhRT7qxnnzbSzjtly97fz/+bi0amh2GzIfZ0yiIAt/C3xQr+l1UqFqdJ0SVq8hZGxCH\nUdQCqGWMLXT/nguHcdQCeNw1Eb0FwAZwGGMszxi7ljE2mjE2DUBPAGs07W7jJiX3f2662gxZA+nv\nHmsXiIO3tzXv2SFt5thdOdQJUaxkzm8PDyNso+lJONaSK2DBmrqinG1hyNkMj79bi//687uBcxKx\nL9L5KGkUEXkU6sLPF5hfjiPiXp2PQqeJcSIgLk2TWYb7KHQahezMdq5rK6MwWSAYY5EmvjjlRp5b\n7phWAlFPylwvVZuOQtjmSRw6jUKcNwXGkHIJa5SGqhN+Nu1qxro6hwa8v7UR33tsCf6yaJNyn9Nu\nygrOdy/rXzMfIxmFovV6uUvtGfceE5GMgjG2FcAmIjrBPXQ6gJUAngQwFQCIaDCACgA7iKjaNU2B\niM4EkGeM6Ty+fwNwufv7cgBPCccvc6OfJgHYI5ioyg5xCC564A2c9stXnOMRPolid3xTiZP+GmDJ\npvqiGYYXLSHc9/m7X8Vls94KbJZSquSZL9h4e8NuLFgTNPOF+RminI+5CI3i3pc/xKsfOM9UP0uu\nYHsEOZIoaJ3ZQS2P/xaFOCnHQBP1lNJIfCnRR+FpFG01Pek5hc1iOIxjDDwfC1UrDITHFjmJ4k5n\nvTYXPZ9EmlywGSw+J9roozA52CXTk9K/2a+tR2NLDuNv+Wewn5GmMOG3a9kA2jOWMT7ieteuBvAI\nES0FMBrAT+GYmAYR0XIAcwBc7pqQDgfwLhGtAvBDAF/ljbj+De7YngngTCL6AMAZ7t8A8AyAdQA+\nBPAggG+34f0iIRLX5Zv9OPVgar58X7E7vvH7wybLCyu3Ydo9r+EvRUYk6bK+uVTEdxzz+1Gq6Ymh\nNV8w5p141wUYRbhGIX5G3cK97bn38dXfvQUg2PdcQXRcxtco+DP5PWKrUT4KiVHkQkxPgvg/4qge\nAPwEvTioa2wNJPKZLBCBKgIaxKn+y+d0VNRT0aYnt291ja1YG1IuI8rsB+gjzQrK+HDGHVVNoNjg\nCV46Ji+anpQ27pm/1phRza80jZVa2DFujaj9gVgiDmNsMfSRS5dqrt0A4ITgpQBj7Erh90442ol6\nDQPwn3H6VQ6YxsBke+Qo1vTEJ1TY5OTSf9hi0iEsokpd9KXOuZxtozVva01nUi6E8n5RVWSLycxW\nT+dt2/MFREuPwXwNXQkPfVijsIBF05PLBHULX3Rmz/7aBIy9+YWiTM0Tbg1KpGaNgkUS7zj7iWQL\n+jka+Ltojde5/jM/fwmteRsbZp6jvU5Xrlyd0y1ajUJmFFyZi3rnyCKUhvf0NApLzzRNDMpmDP+3\naBN+MHcplvz4LPSozgTOe88o2EKeT8dzio4P0O1gmOZ81H4UUaYONYtWzHUwJfL5E7A455Vvegqe\nC0qDxU06TpvyBYbWXEFLSMP8DFEahYkIN2Xz+OyvFhiv5X3iJp5iop5Upi02W5QzOyeXwBYhahQ9\nqyswfkDvNoewEjnlQtTiggU7uH+KijgacK6gf59A9nGJpqcoM2SckFI5wz7I2Ao28xhq1DtHaaHq\nJ1Wd2Ski7bxTtXixvz+Y64Sl1+0NJmmqgRL8OTG3gmlXHPKMwiRjq5MkGCLH1U+nrPGQ659FNm+j\nJVfA00u34PN3v4p5S33XiinUVgpLFFRafZ/CVdZYewpErPH5q7fjw+2NgeMFm6E1b2uZUavkZ1DO\nRWgUsjTvX7t4Uz3e39aoXCvfm43po9ixtxW7m/zy1Jzw+XkUeo3ixVXb8Oa6nVofBuATPt13V3fq\ns6jtC94iwqdnvhQoLpi3WaSUX5SPokwaxakn9EVl2jJqO08t3uwFj+ieA4QzpXc37g70x7Z9bS5q\nn/fIcOyI+ywibfmWxha9b0PuTrj2lCvYQtXZILbuaWm3vel1OOQ3LjLNeXXSBh2p/mT5xfPvAwDO\nuOMVbNzVhKtP+yQASI5kkRitrduL1pyNE/v3UCaH81vnHAWciaYLqdbt5+y36Uy2Tbua0NCSCyza\nfa157NqXxdG9qwEAV/z+bQDwzAOib6U1F216UqW0KClSKv8h7A+xdntwEegCCtKG5Crxb9WxyE/p\nop7E+77xsJPMNqBPtf9MiVEUtM8GgklSKYvavBGNSdG0Y/godM/euqdFenn+/Yvx01SkrUCFV47u\nVRlUpC2j6eS7cxZLuSXic19ZU4fNu5tDTWpfue8NbJh5jhwtxHyNIlfE3NPB9Em5kLhuxz7crKnM\nYNovXmxPNxXEpZMrsNBaZJNvewm5AjOa8cqNhFEYjgcyjAOOVD87k8/mjW6ROn6tuLDF5s5yTSov\n/fcUHNWri/BMoU0NbMaQ0sTSe21rXoa/x+Tb5gMA/viNidL5f3/wTSyp3aOdcGq4Xmu+oJWe5YQ7\n+VyU6UlePM4f972y1mO+Un80Wp3JRxFGlL3MbI2PghMB8VFRUU/cnCg6tVNK2QXLYKYoBjqnOeAQ\nR1VbGXtMT6mkhmqvf2rxZnx3zmLpWNbTKDS5CpLJxz9emTIzipTluH91BJffs1dIGhSfcfksJ4Ah\nTkixlKgmmp4iVLhSfRRRvg+TRiGF8WqeLb5HNm97PqOwPuQLNtL7oRbUIW96MkkNqrNaHdcw6Z+3\nKRJ8HYHduqdFax83So7uxU8t3oxxN7/gmar48bc27MIf3/xImuDqpFa7saTWvBGMminKTU/qAiqb\nRuH+Xrhev8GPTqtLWXqiEMYo/Ho8QaagC0k0ObvlgobyM9QhTGnKPRSLfYYtUW2Nj6JKibBSifmb\n64Lf2It60hAolRhzZELKkqQsgmUFC+cBQLMbvSQunzjhsTqofotUTGd2scETHFEMyMQoxP5Emdmy\nBdvTiMI+QZzNmcqBQ5pR1Ddl8feQ8gEiApnZnj8heK1ugumO7W7KBeySgNn0xC/93yeWY+e+LPa5\nTjOx7eufXG50vjrXxidWqgOYE/2CzbBPSE70ktRIlyAVwSg0BChr0EKCAQW2V1NJJW5hRCIY9STe\nF1T3TdqFiKjvbHJ8FgNdlVrAr3YsQg3FjWP2yhminoBgrgJHRYg0m3Y1ioCbjDEvzFWc6zpNJo5f\nR+obE53Z4d874LRXzc3Kmicv7LY0jSInad7hzDhXsP3wWI2poMbVtD7aGb/UfltwSDOKj3Y24Y9v\nfhTrWpNGoXM884EVz+mkqvrmrOLMdu8LMT0BftanvwWqecKHhcdG78MgEnE/K9ZmwK3PrMLU21/G\nlvpmb0KnrSAxjDY9BTUK00IM5lHYxkqhcUxPfl+DzFquKBtN4E15N9NPGQTAGVMdAX57wy7c8cIa\n1O4ufcHr8iiqFJNNnHBufo1OsldzFTjCCh2mLIJFBAYWGGceGVTQjL+IWBqFQduJzqOI0EJNGkUk\no9D7KKKSS8VDWTHqSfM47lP8aFf05mDlwCHNKEzRRToEiJQmWcu/1vlfbF433+tVjcKdEaZqkXzC\neNE6Nj9ulmaDpidxwQv3aIiNWvZCjPLZ6Yb3PbLwI8+skdIyiqjFKv623T6b49BF5ARntvqeJru5\n+Ew/M1tu03mW+Fxz/zlUIsfAMP7YXrju80MB8L0LgvfdOm8V7n7xA3zdDSIoBbo8ClWjuPShhdJG\nTzrml/d8FBpGocwFjrCCdWmLQK4LTwwZzRVsNGd5Dor4/HBGUWlgSrLm6//N1+iufVlvvooIJofG\nyzkq1fQUVdfMVphcmDOb+26aDaG45cYh7cwuJgFKHSs+qfdq7MZ8YGVndnCwd+/LesTeaTPc9KSa\nTLgkpc45U8ip+h7SdYyBAu34v3l4LP/dt1slAGDz7mavNIXOvBLJKBStBTAT+YDpSXBmR0qHUjvu\nd9So9jqNolg7OQC3OJ0/jpamLhAg7JK3OxhmGRe6PIqqjExUm7IFKUBAR8xyIVFPclKbfzxsU52U\nZYFcBin6V3IF5vko1PcIPFc41LUyjda8nPX84qptkp2+IDBNvkbH3vwCAAQCNgLFDwvmtSJfFz4f\nGowahV7z4RAZkGx6krFzb6uXyNqee7iIOKQZRTEaRTGmDX6KIDqzg9epPgo+eUTTk26bUY4de1vR\n2JIL9O0//viO0E9V2xB/h09cNeywxTM9+aF7DS15v9+arSGjdqFjGqZmMpPoNAqvAFxg0ZsXkM2Y\n6wDmf4v3ucxQqjYbw/Sk81EI00tX7kFsO07mtPHZLMgodOVCxOm+uylYZsLPo9BEPRnmYRijSKd4\n1BOTBKp8wfZ9FIIWGkX01NwUwA9h9vomfIso01PQr6UGsOj7E5XIpxMe1ft0bRdsP9xYTLhTOcU4\nIdy7rX6vuDi0TU9FvL04Hs8t34pvPxKsosqRLQQjOrQ+iqas1pltGUxW6pw461cLMOUXLwckn1c/\n2OH9Vie1yUkrVavkx8S+5W3JVMMZxZ7mnJSPoC6+KEahi3oyaRQ6rc6UmR2lURQM34E/Oyst6pAX\ncKEySAZ5HFNWkIkCwjsX7KLqcC3a4Ect5W0W+DZq1BMgC0b1TUGpV+ejOKyrozmqodIcYaYni1wf\nBQP2tco7GXJTlGhmjSJ6Jk1bhBgBVmzU05vr5FL16t386VEMzRz15M8pkz+GM/hcgSGX99eVCfuL\nURzSGgUVsau6uFCue2JZ6LW6SB+dBFHfnNMWpEsZwmpNEk7YZAmTtFXTU0AqFv5uEgi+zfyCZQ3N\nvkZTsFmgL6ZyBrq+e7ZlA5FXoz9yBdszsajvGea8Ldjyc3XhxCJhK0WjYIzBEiQR1Sz38vvb8eH2\nvdJ9eZvF3qTm/Pv97Gyd6Ukn6cuMIqhR5DVRT4d1rcCOva1YttkPo45revJ9FAz7hEqs2bztmZ4y\nFoH3JMr2b8ojESHOwSjJX50z1/5lifS3adyjNBWToCPmRei0y7ztMIo9zTnZ9BQy/xKNYj+gmJJK\n4qSqitjSUt0i88a/r8CTi4NhuNm8LSd7eXvx6k1WpokbRsiCBFQkgP7xQoEFJDBRAhadZgXGPGYo\nahQFFiwlobNFi5CcmSFRT7mCHYgZz9l+SZGARhHiG7Ftpi07zp8DAH99t1a6Pgo79mZx3r2veQ5j\nm8kapWXJpqevzX4bt8xbJfWj1Mxt2w6+vy4gQuyPzneky6PgvijRxFN81JP8vLzN0OLOJyuGRsH7\nHWdL0EIRpqeoWk+mcY+KejJF+kVGPdnME3xyQh5F2PRLGMV+QBwJhUOcVF0iskVblGSi2a9t0F6n\nmgz8rGA9ETPxg6iJ9G+/9aVPOUlM1ihUiU6chGJtfnE/34aWnO9Ud6U50T6ump5uf/59DJgxzy/o\npvGT6CSy6x5fhveETGMA8sZFClEwmR0yKSfxTfRBiFfqiEuctfjU4s14b2M9frtgrdMmY5IEn9L4\nbwAlnDNf2qIvsKDpSaeZiHQ2rF6T2Kexx/QKPR+lUQBuxJwwD/IF25tPYvFEk0mHPyOOYMeYH8UW\nWesp4rwpNyZqkyxTAEc+wvRUsJlnMnQys815FN49RZgr24JDmlGUqlGkI5wbqkZhQsHWaxSiJGOK\nUhIRpprmCraU6Syqv2LEVcFmAUlJbFbUKGzmv2NLzvakwwJzGEWNUL9HDd/7zfwPveep78Sfr2MU\nL6zaFjiWL9jeIlIXtcmMUZGyAkxRdmYHv6XJOSnBHWr/vWRBxMmjCN4mvn+p+5mr8wgIlhAB5PkY\nx1z52ozTcKxQ58q7N2Z4bCpFjh+QycQzW7DRnOMh1X4/OfNW5zNP6otTVblg+1ptdPXYcCJrCjuP\n1ihimJ6Uth9/txZvb9jlCaFZyfRkflaiUewHFBP1JNnSI7i4qlGYoGbUelEnBuZgmhOhpqeQ/AI1\n6ingzJY0Ctn0JLbDK7My5jyva6WvUZhMT6IW4j/PHPWkG6uspFHoF7WKirQFxkJ8FBHhvCb4m9r4\nxE7ssUWG3AWbeQKLyVTSu6Yi9NkFOzg30lrTk2DS1Em0nq/J6UdV2tKae5jEKKIys0nKwQGcb9Ss\n2UY2bxh/zozirFfR19bW/SjU4eCXRzEg3XatTn/MpqfvPbYEeZuhKp1y2/DNqgmj6GAUwyhMNm0d\nxHyDMBRsFnBmAmbTk8lmGjZvVWk4jFGElZMWGYVYzgOQQy2zBRtH965G2iL0rqnwJEdAji/3/Bqa\n99dBN1Z5IVIoyOT07VSkLUd7MmhtxUr1nCCTolEwxIt6sgVTnckJOuNzQ3DRhKO15/gzgxpFuOlJ\n96k9Quj+SFuW9ruLylpYCY+UZTkMErIJMm/baHZNT3nN+KsSOWdGxUY97TVEH3FErk/lm/qJfKVp\nFFIJD5OZzWXOLREVDaLaKTcOaUZRTMKdWvcoDHxRvLR6O8a5yT6mNiUfhZcVzLBrXxZjbvqHVAHU\nxKDCspDV6BaRoErObDvozBbfs1mNesoXPEdmQ7PPAHJ5G8f0rsayGz6LCQN6eWYpABh14z+k5+n6\nYILO6iD6eFQpz9RWRdryTGQcog242AQm/g1493K2P4aS6clQ6ykv2KVNGkVl2pLKcauwWTCrXqdR\nSKYnA9MC/G+XSpGW4Yj3hjqzie8rLRPPXIFhrxtVpiOeatQgZxRx1qtjenJ+79inr43FEWVC0kWy\nAdDuxCfCyCgMGoU4dilymK+4bjpD1NMhHR5bzE5yecm+GH4tJ9xiPoMOqiTIVe+CDSzZVI/dTTn8\n8h9+Nq3puWIkk4pdSry8yFTU+jui3b41X5D20xCd2QXXmd21Mo1d+ay0MFrd+ktdKlKorkhLDEbN\n23DeKR6R1hGsXMG3zXMmN/edWjQ053BEjyptO5mUBcaUqCfFV1MMHCJWEDQK22tT1Si0CXc2Q5XL\nBEzajEWEVIgvQFcUUPe9orRTNfM/7UYt6frMEZqZnbIANzxWjATKFWxPgBHfma8xNWqIl+6IE/Vk\nM39e79yr37uaQwxQ6d+rC2qF7HjGNBq2e3lcYaJrZVrS6EUfhVRSXPgGKYuQSZG0hXBosEpncmYT\nUU8imktEq4loFRGd5B6/2j22gohuc49liOhhIlrmXvsjQ5unEdG7RLTcvT7tHj+ViPYQ0WL334/L\n9bIqinFmi5MqiphEJZkBziLMK4yCN2sz3yG8vaFVOK9/bti+1LuVjd5NiWQ2kzWK659cjquEpEJ5\n4jo+Cp3JRCzUV5VJmX0UivSaSYVXV9WbnnwCyd/r+/+3BDc9vdIcQWNZrplNYJht2JPYk3ZVHwVk\nn4DJ9FRgzHNgmmzqKYuQCQmg0JUZ1xFwSaINicDyNAorWqPQZUt75zijYbKWkC8wz1ypq39kMj1Z\nRNImUjrYAoHf05wL1bbFOTJpUB/pnOgU944pPpwofHnsUdLfJo2iScjZSVmEirQlC1hhUU9tyOgv\nBnE1irsAPMcYO5+IKgBUE9FUANMAjGKMtRLR4e61FwCoZIydSETVAFYS0aOMsQ28MSKyADwM4HTG\n2BoiugnA5QB+517yKmPsC21/vXAUk3An27RLUz1FZFKWq1EEzzFBAhP31jU9N+x5u1RGIVwrl1Vg\nkvnmLcOeEIDDYLJ5Gz26ZQLPZ8w3e3TJpIxFyz7e04K9rXnv/StSFlpyBaP5RUcnW/MFbxEFyi8Y\nGIVlOaYQcTxLDDYC4DtafSLiMoqAM5u0JbMd05MfO6/tM4VL07p5pNfAZL+MsOeWdwzwmV2KCDqF\nQbwnzM/HNy5Sndm5go3d+/wACA7PR6GantKuM9siPPmfJ+OavyzGy+/XaZ+5ZlsjXl/rZ1iL/rOC\n7e9fMv6WF7BD0DhqlJB3m4WYnmLSZpVZ502MIiczikzKMmrigcKdnUWjIKIeAE6BS8QZY1nGWD2A\nqwDMZIy1use3u7cwADWuhtAFQBZAg9JsHwBZxtga9+8XAHylje9SNIrTKPRqow5xNAruVNUR/4Lt\nh5XKVTv1bZn2bwCCNX3EBXvGHa8IbcuqdreqjLFNXiCQFwNUGRU36XWpcCa87h2/8Ot/YcovXvYk\n4Uzawpy3N2GKuxOfCh1BasoWvEWkSo4m7cTZu5oppsS2axTc1p63GZ5eugWrtzbKeRSGooCiMzuX\nd/ILxH2kAUczCQtDLcT0UajfSC3zIUrMFjnjGGV6CltDmZRfwkN2ZjPs0mSGc0n96kfl8jgVnjMb\n6FldgcH9uhmfeec/P5D+3iEIWiIj3qGYpaoVH5Ba5sXpX3EahcooTFURmgSBzSJHo2iRNAof6hTq\nTM7sgQDqAMwmoveI6CEiqgEwGMBkIlpIRK8Q0QT3+rkA9gH4GMBGALczxlTxdAeANBGNd/8+H4AY\n1nESES0homeJaHiJ7xaJYhLucoqP4uKJxwRUS46WGBoFZxT66BOmbUPntAQcQt2rOoOzhx8ROKeW\n0DBJrXmbSdEc3arCnafZvI0aNwxWtSmLGoXOSS73z1kknBhsKWLHruZcwSNarXlbKqNtyn3gJqC4\n0VZR4KYXv06Sjf/683sAZC3Iceq6fhnl2V6SVcHG12a9jam3vyw9wyIK3e5SZ3rSaRSqD0RlFKKP\ngucK6RhFQWIU5jWUSVleCQ8pjyJvB0yi/LkAsLZOZpTplNyXKCc04FfPFRmCX/QweL+qUagRiYAv\nqMUlzhUKc9f5YwB5jaZdjaLF5NszMK/2RhxGkQYwFsB9jLExcJjADPd4bwCTAPwAwGPkUN6JAAoA\njoTDZP6biAaJDTKH2l0E4FdE9BaARvceAHgXwLGMsVEAfg3gSV2niGg6ES0iokV1dXo1NAql+ihs\n24l9N4XrxRm8ipSFvG1r7Y/iDmAibBuYpcnybs3ZEiEKg8lm6yTc+edqQqJs8gXHGWzSKPh34YQo\nrIzH2u0OUQiLngH0Un9TtuB9vaZswdsXHIB2/wHAjz6Sndlt0Cgs2WwkEgDRtMkla6YQTQCoTPs+\nirc2BE1+KUuvIXDkbRYwn+l8B+rYq/s78CkummiinOJhwlaalxmHLEzsbsoaM5OXbKoPHFcT7uJI\n9D27OLknYogsHxsxSo+Dz2WvL4wFhBaxplkcqMxdjjj02xDrYFmK6cn5/mbttzMxiloAtYyxhe7f\nc+EwjloAjzMHbwGwARwG4N/h+DNyrjnqNQDj1UYZY28wxiYzxiYCWABgjXu8gTG21/39DIAMER2m\nuf8Bxth4xtj4vn37FvnaDlRpSK3hL0LNozA5+uLC0yg0c75gYhSM4eanVwaOZwu2tyCjEMYoRMlf\nvK5/ry7StbxvXApT2zi5VNkAACAASURBVOTZttxJG2aKW7OtEUB49AwA7Xdqzha8haM+Y6dGYgWc\nDV+asgVFoyjdScHt57rcGXF6pTwiFxxb35mt7wcRhTIKXZlxXfUAtf0Ao3A11jeEKqpa05PwrCjT\nE8Fhjs3Zgjdf6gxbum7c1YRp97wWOM6FCC8xMQZxrPSKRcq+EUBfYr2mUtYoHl24Efe9vFY6psv9\nCUPQRyH6xfzfoh8v5ZmenL6mXZ8aR6c1PTHGtgLYREQnuIdOB7ASjqQ/FQCIaDCACjgmpY0ATnOP\n18DROFar7XLnNxFVAvghgPvdv49wNRMQ0US3jzvV+8sBcRE8f80peOLbJxuvVaucWkRFhdeqqPCc\n2cGBvmf+WryxNvjKNmOYfHyAZ6I1X3Azf6OfawrBVBPu+EYzP//KiRh4WI10LZd2ulTotQ6+Pnjs\nv25rSG5z37KnORbTVb9TRcpCUzbvvbMYvgsEnfgc3asyaGzJySU8bL9NETwjeugnuhv7xYlBVvBR\ncMg+Ct/prSZT8SKTJibu2K3N9cXyBSZtSgToNRCVUehMT+9va8TS2j2KRCujGNOTZQH/XLUdzy7f\n6l2rI9SAmbnzueJ9wximp0rNN12zbS9278t6lQREqBrFvz4Mhrar4cNRUP1KkulJdGaLpqcUoSJF\nnjCRtkjZs6WTOrNdXA3gESJaCmA0gJ8CmAVgEBEtBzAHwOWuSekeAF2JaAWAtwHMZowtBQAieoaI\njnTb/AERrQKwFMDfGWMvucfPB7CciJYAuBvARayYQv1FgIS3p4jIEjWT1yKKlSlqQkXags3MEsGz\ny7cGjpnmJzc9hVeudxBqehKI597WPM4Y2g//NuGYwHdRNQoVXKPo0cVxiO/RqPrebmoFx4wXJjED\nQSbYvUvadWY7J1RfjEp0PjWwNy476Vh075JGQ3Neq1GopTKO6O7kYogLXt0QKGB6Er6hVD3W/cO2\ng+HMXKMwVR21KNw0t2FncN9krY9CNT0FGAXQ0Cwz3OioJ2O33I2L/AsaXeFDLOEuwsQAMoqPIqoo\nH+BrVOK6vfR3C3Hxg29qS6yrGoWO4gT3WgcmH38YzhjaT9sHdcwk05PEKGRntuijSFmOpWDDjn04\n/77XAwJQp0q4Y4wthsZ8BOBSzbV74YTI6tr5vPD7B3B8G+o1vwHwmzj9aitEaYgQnoCnRj1ZFM/H\n0a97JU45vi/+751a6TifRDpV24Qdja0o2MzLweBozduBUEcTTBpF3rYl09PqrY0YcoQTXaIyRC5t\nqpEiHJyw9qp2CK9ukxz/uQyVSk2hipQV6KeqUXSvymDH3qxnhlCd16qz9OKJx+BLY47CLU+vRENL\nzvt+FvnhjmpVYD5GogmhKiOHLnLTk6dRCN9QjXoCnLmjhgxz5rOlXu/IT0VEPem0J9FHce0Zg3Hv\nyx9KCV+Avlw+J1qPXPmpwDtwqD4KMmizmZSlXSMmn5XIZMX57JuefPNdFLjlTdWiVm9t1GoUXTKK\nj0LnQ2EML7+/HYsFP0pl2jJmjKumJ7H6sak8jpdH4R7LpCzkbYaH/rUOiz7ajbkKHek0pqeDGeIk\nphDnNKD3UcQxPVVXpDFhYO/A8bgb1Ii48g+LkCvYAUmlNV+ApaioQDypksNmLFAGgzMC9T25RKzb\nbhPwF3bPakej0C1MtZ9iX6srg+1uV+za3V1thYcWqoQq4GB32+9WlUFTtuAVbkunLM85qtrsuSkq\nTKNIexpFUNrUaRQFO2h64pL9XS/KoZ1+OxTomwjOKB795iTvmFiV9bMj+uGMof0CYdS6XfA4geru\nhkdHObOtEP9JJmVp627ofFZpiyRGJjqCPWe229TpBgleBF/LasTdgD7VWo1CFRJ0JmGbOfuIiIQ9\nTpl1HWyD6YnnUfCox5S7rgf0ccy/H9btldpJGMV+gJxwF24nL9hOAboL7n8dLTnHeRzH9JS29Asp\nzOYchp17sxpGYXsbxEjP0ExiE6PIF1jAScgns/qevI1qg+mJLx4eeaJbmCIskr99jcH3IYIzin2G\nhD5VkuTtd++SdvvkMK+KlOVpASaNQnQMVynXcCaS1TmzlagnwHHsiqUiADPD9e8NNz1xRtGnq286\nE+ecRcGyEECQMQI+0eKBHVFFAcNMtk5mdvC4qlGlLUI6RVI0k9h/ryige+zcUUfi2jMGa5/p94sz\nCvmdj+vbVbtVqRrIEmZ6EsGrEesQNmai4ClGYaWIPP8l4Poo4PtQ1m5PGMV+h7gGIn0U7g5wb2/Y\nDcAxJfDrxXZUu30mpS/VHFZ1MwzbGloC92Zd05M6YSs1UVwm05OzmY987qOdTl6C2n9uSzdFifGF\n3a0qDYv0PgoRqo/CxIBEdHfzPPYpJqf/PtMhICqB4ASPS8o84YtvZATAK/Hsv4dzj6hRBa9xnaaa\n/abVWk+As7A/cCO9vDZDou348ytS5m/CGUXPLn6SpGh6IrefgdIYOkaR42ObkvotQo16MpUXyaQs\nSRQ7d5TjnuT94GOesghpy5Kkfx2jEENxo+YIv31prRxuW1Pp1B+rSFt44KvjMKivI6mrwpBeowge\nEzWKSycdgx99boj2nArR9CSaZrnpiSOdchgR1wY3K0JGwij2A1QfRZQzW7Shpoi8iStKhF2VRLWM\noQJnmCkhDPuyBaNGoU5knaRq1CiU3AIAGNm/B4Cg6YmXWNCZLgCZwPbokkF9Uw53G8wqQND0FJbD\nweFpFAqjqDKU7PY1Ctcc5hLXtGsDBswahWglVK/hJhKdRiHOLy8HgDEvJFjtswlcIzCBM4ruIqMQ\nvie5GskOJbdEJ6z836JNAMIL8cnmNXPBQp6ZDThz8e6Lx4DINz1VCj6glEWS6VMkstwPlNJ8TxN4\nD59ZJgeFFBhDUzaPmooUzhp+hDFqS18LS/eOvo9i8vF9pfUfxihE7UQUpHhRQI50yjE9cSYayKPo\nZFFPByVkH0W46Smv5BmQUAdHDK1TiVwmZWlNVFEJZmFQF3jBdtL2vn7yQOm4LrvaFMOu5lEM6luD\n/z7LiYhW6QCXno0+CqF/PasrsLspizteWKO9FuB2bv+eiyea917g4JqBanry6yapeQXcR+F8k137\nfNMTX7SBiCaNJBu8xjU9ud9E1GSkPAoh6mnDDj+DXOybCZGmp6YsKtOWxHBEHwURef4iETpGsbR2\nDwDfb6LrWiHgowjRKIj3xyf2XKPwTHspwhHdq7DyY7/Sj6gR+Ql3fttR7kHbZlrmWigwNGUL3poV\n/Xr/e85Q77fW9KQ5qAp8EoMzMNCKtCOcPPz6Bpx372vYKFQU4D4KDm568rdFldGWqgLF4BBnFIpG\nEeJzKNi2JPGkLN9HIYbWdVMYRVrQKFKSOl28M5tDRzQ21zdj6pDDsWHmOd4x3R4GYdVcxfcbdFhX\nP9rE7Tcnwnyhi4RJXLjiRO/eJRNterLIIyjXnjHYGG4ogvsaVI3Cz3LW15/iDIb7TdIpP4JMNdVx\nAiWOlGomqlA0CvH7ytVjnf8LjKEpp4agEo7uLSc1Sn0nCmUUjCHACCSNAsC3T/1k4L6wNsN8FCK9\nDAttTqd8rZvPfcsiz3Tp+4AI54z8hORDEZmP78wWv6f8TPVvk6T93qbdeHHVdk8zFNu8cvIg3H7B\nKOcdNffqzDzqOq6QGIX++1amLOxrzeMnf1uB9zbWSwxSHeuUxU1PzrcJbM+aMIr2h+qjCFNnuY+C\nwxKuF6VM1TQh+ijEBdUmjSLmvWGF/VTwQn8c4gJIkfyeWUUidK4XTQX+7y4ZK0DMVaTc8haAQ1zi\n7DzIGXKAUXgZuUoEGMnMjofTZkI0Cm9TIqE7qpmIS76cMYmOUnE6+XkUTGsWe/V/TtO/qHtvlKmy\nRxd5rEXCSUSoqUwHNIiweeQV4ouRcGfSxEUfRahGYVkYpiQ1ihpFRmMGU0uHqO+WLwT3kwCAbQ2t\n2NOc83wcv/3qOHzt0wNwXN+uAOBt46sjwLo8F/UbZmIwikza8nydKtIWSe/iaBT+NsWqppxoFPsB\n6g5k6raWItQ9pS1BoxCZg0pIREYhTpww5yQA/PDsIZIqLCJuNJPqLwkr9FewmbRRkRieyPvv13bi\nMd6+PVV0LopMpiKd0kaZiJC1kSCjGH10z8A93DSiLpRKQzQZF1D5wm4WMl/zEYxCxJdGy4Ug1cxs\nEaYSHtm8LWkmKqFVn0sUXeJE1R5FQsubV+d1GKPgayOqKKDTNwOjELZSFbVq7uMSmZFam0rnzJY0\nCjJfz/sYRkP5Oh3UtytuOHe4J/R5DF2jkTRr9n1Rx0X8FqbvkrIoUCGYg9d64nB8FHofGJBoFB0C\nPpl10ptaSE4s4SESGF3kjKdRSAQ0/NMf3bsLjuypN0fE1ig0/hITZjy+DK9+4BdXzIjhlZbMDPyo\nFSvARACZkVWmLWMlV7F9XhwxZVmBvSd0UUF8oauVRE2SNzdlcEbCNREnQ55JbQbfw/8WZwzrh/U/\n+zwOc0NR1agn6b0UQQTwK++Kz+JEbvopg/CNzwwMvIMaCaNDQNMRfRTQC0BRzIc/WwVTfRSGdtIp\n8sYy7RFieHkkPEQ8kwr6OSShKh1kFGq3LCUUN8rJa4qaEhm6ihZNKHZAozBo2dIzQjTmQNST66Mw\n1QFLnNn7GUT+RNRJ7HnFhp8iX/KVnYiKpCNoFOIEUUsQqzDlXwDxFjgQlDKj6intyxaEshWCxOv2\nmyfgPf7uZqePKX/nNVmjkBd5tEbhm550GoUuKogT05xtS4y6Iq3PBuZd8jSKrEajUM2GGtOT87d/\ngI+RTqPQRT3ZzNlGVhQo+HXXfX4orv/CsMA4WUSR4dTqNxLnGv+pfteoOQgYTE9KeGzYPOVMij/b\nSQx1ny+YlMJ8DhmByXjPDXwj+Z6osFETsebt6kw6qm8JkGkFY/HMy7pvyrvDy4xzpC0L2bwt+TFE\nJOGx+xlihU5dMpwaFeQwFue3KPE2KtJzhRD1JA6pOolUQpCyLOM2k6X6KOLkbhzlVooVn+1pDRqz\nGr9OLOchTvTKVLRGkRKIR1owV3Bwojr+2F6YMKCX0677DRiTgwnSGqID+ISK38fzBfhOg+I59T10\no+AzNl+jCBsXPgd49JzIlNRxVt8/KupJ13e1jpmu3cO6Voa2qbsHkMNELcP3BtzqsZwA8sJ+QnuV\nXua7FTDT6Gz94nN0/RKPqQT0/HH9MXGAXyHBlE/E+6fzR+h2a6xIy7kicsBKfEbB57gaCs2/m8mn\nkTCK/QyCL00EFh05dWikPAqLPHVQlA7VSqk86xSQVXZ14atVYR2NQj884r3XnHG88Z1UH0XY/sYc\nfJKKk5wvQLVwmkOUnevERMOKdHwTm9O+vy9wWiAuHJwRV1emvYq1oi9CNHtl0kFGA/iLkzNLvk+x\nyChUjaLSoFGIqBAYVnVFKpAR7T/f+Z9H9ohzLMgY5L+dHe6K0yikiD73t/oa/bpX4aX/nhLabpTp\niUJMTyTkGnECbGkkbjEykCOjMdPqoshMfVUJ6Lhje0lRbSZTDm9Dtw+9jlFkUpYkAIpr1qRp6b6p\n7n3D2uBIGMV+hqj6qcTNIkJdYyuumbNYOsbNDWJZB3VTFJFwiWOqSvd3XzwGf5ku1uoxm54qhXvD\nIptU53Wc/TO4NiU+W5cvAsiSYI1Jo4jFKGRTjtpPTgTFfahFLa4ybfnaoCETXgzPzKTIkygzKfIZ\nhcFHEba3uvSdlAggXdQTJzYiUwqaXYJ9jzY9qXNW81t5DcsiDOrbFXOmT8JVpx6nbVdnohFNTwTg\nxnOHG0ux82er5lciQcuwrIBQpJPMxU+gMlOmHNNFvYltmnZd5G3o6lHpQsvFcSHF/KUTkuZ+6yR9\npQZ33TGlzZRBWORIGMV+hmdL1SxKixwiv06IVLAsQqsm8Yzb4z1bv6AZiJEUavmEmso0PjWoj/e3\nyYQCyIRYdViLUH0UplILIrjdWpQSebcDGkXK15ZqQnwUUZBMTymN6cn9vs4+IM4xUaMg8glvOqWv\nwaVWp/WP+xpFMNTRzCD4SIoE31KYuy7qiRMgUQsNMAqN6UkXur1h5jleKRM12kvOEaLAMfE5kwb1\nweB+XQPtO+8UPGYrQR2jj+6JeVd/Rns/f6KaSyRGRGU0c12eQ8H+6/M7/H6p0UBilCJgrlDA+6GW\nOwH0jEJcx6qPQrd+P9Gzi8QsOSoEq4OacBeGJDx2P4MPGt9hSj6ns4cCuTyPlvGv/48pjmTWr7tj\n/+WbtwByolKUhOiEDOqvyQimHdW8JEKVMuOYnvzS2qL0xYsAKqYsQRIUTR/FMgqxoGHGChaS4yYD\nvmGUeIzfz5m1871logQotvGMrzVZ5C82lUBzKS/U9KTkyYgEXZcgxv01Unisyhg0zmwTuCanJguq\nOUJOO/K96p7eOujLjIvn9X1W7/c1Oue4E7QA75zKlCWncCqYHBfQKJjcr4BGYcnzwcwozOd15qiK\nlNlHkaKgVUCcI2oQBofIfKLWbJztj8uBhFG44MPRtSodTF7SSahEyBaC0uE3PjMQG2ae4xOjlE9M\nTT6K12acFmg/nTKbnsQcDLWvput4X6LAibxoCuBmmmDBQ7+Pku25iHwR5154Ino6ZQUYM/++NmMe\n0RMZExF5UVcq4fafEexfyiKFoKqMwuyj4IcsIm9fB7Xkttge3yWQR6+EObNVSTSMUfH3VsOyZR9F\nsD+AUjtJ+H3Jp44x9gWIv8Od+GyvACCJGqvPRHTRghxe7bAIH4VINFWTjJoYOP2UQdr+xkn2BPz3\nqkjLVZvl/BV9NFc6glHEye7mULcGaC8kjILDHc8/fmMivjVFtteaYuS51KE6QQF/QVSk/JpQ4tzl\ntvujenbBUZp8iZSlt7UD8qTqHuKjUMs6xHBReJNUnPBZrjlpCuJFZZ3rKtiqSJFf0FAnQYmmJ07s\nRN8Hwd+WVczsVjNc1f45GoW4sOXnegmYGh8FH0rV7KUj0ABwTO9qdKtK492PnOgVXXis6e8w3xLX\netTvrDc9yfdK0q/w+9bzTjT2BVA3LjJ2zT2vaBTc3JSyJI1C9VFICXdeGG2wXRGicK0yCpEZfXnM\nUfjKuP7a/sZlFJyAi4IQkVI6xQqOnaRRaMy1jMnBIFF+xf1keUoYBQefIEOO6B7YElPnMLIswZkd\nUv3TIabuJBBkD7/gnOE+yxzpIhLisBLVx/XtivsvHYvz3UURZwnoTE882kuVWsU+igtMiuCIocWI\n0r5Oi+LvaDP/HcRvYFm+ZO1kwrttKdIdh1gZVedHCPQr5MOlLPL8A05FYf11RIThR3b3djmrCnFm\nB0xEIcSLf+ugRhH8rRJXCmGSpr4BwY2LwsDPisEEvN9ebgUFK9BqNy4ymBIBZ/wLIRqFaAYKI75x\nAj7EPolznTHZ+aw60Pk78Ft0GgVDsT6KRKPYrxCHI4bPFynLj5zQVVHldv3KtJ9HIUU9hZRx5sdN\n5yolRmFmUpYFnD3iE170UxxpiZfLOOEIP4rFCwMOyaMwSafF+ih0IcFi1BM376hhqKKPIiVIrWF9\nUs1cFuk32gn7ahaRx8icZ8iEQ8Tgft20lXejCtyFDZtXTSDgo6DA7zCNwuyjCB6TTE8Rw6s6sX3T\nk5+1nbJIqgQAyJUBdMKI+txuVWmpX7d8aYT8HoIkH2b3j6tR8DYySh6FOi91Pgr+LXQaBaBowiF9\nTVmE/cQnEkbBoRIMEaptnl+jCw3l4BvjHNa10peWNM5s08RUieHtF4zCGUMP985xhJl2okwaOpw/\nrj9euPYUTBnc1zuW1TjtAdneairYFic81ikK6DxDF2nEF1HB9VEETDzwF10mRYLUGs4oUlbQVLTu\nZ371XXVrWRH8HJG8yY/4iZlSg5QXngMQWuupmHHjkWyqRiGCvP8VhhQRRQToTTyyMzt8TqkCkTg2\nYjRW0EcR1FDD+itG+P3yglG4cIJcql7UKEz5SWI/o+BptoFEWXk+qeGtKUELFwWvCsH0JDmzQ/qa\nSVHnKuFBRD2JaC4RrSaiVUR0knv8avfYCiK6zT2WIaKHiWiZe+2PDG2eRkTvEtFy9/q0e5yI6G4i\n+pCIlhLR2HK9bOg7Cr/5WPeqzmDDzHPQR5PBahHhJ18cju+cfjxOdwm4CL6ZTN9ulYJGEXRmm9aZ\nWijt+MO7epNGVMtNRfAAIWbdUOuH4w9fn+j9JiIc36+bdD7raUfBZ/E+mQiGqFGYMoGJZJu/Ck4o\nCrbLoBX/jejMFoswVhvMO5WCj0Ie9/jEUmxXZBQinVHXMN9NDZAJuyp1msJl//XDqfjzNz8lneNz\nJFSzJP34izQoLoFUEfZtADFwQPYzZCxxr4ogQVSLUn557FH41CA/szrAKKr0u/uJbfh9CBvP0Nfx\nwN+7Iq0m3IVrFCny54ve9BSubYrIuKHdYQJNuRBXo7gLwHOMsSEARgFYRURTAUwDMIoxNhzA7e61\nFwCoZIydCGAcgP8gogFiY0RkAXgYwEWMsREAPgJwuXv6cwCOd/9NB3Bfaa9WHHTRL5zz33nRaJw5\nTN4jwSJCr5oKfO/MwVo7It/esG+3Sm+wxeHUqdMixNBTfp0aQQKE+yjihll+5pOHaY9z5DxfTPBZ\nfFEakwMFRnHreSO014h5FDpDj5eHYvsahSxd+osrbemr+kp1ttJ6wq4LudT3SL6ni6hRSKYneQEP\nEjQKKf8iwnnNz/fvVY1+bn4OByeooZobRT8npsUlgCj+IgYOAIrpSXB0qz6KjEJw77hwNCYf72u5\n6jfqFrGznFNqhJ83d1pccxeM629cG/zxqkYhMjiL9GZFvo66SCHl/nXi8TDTE9c89odDO5JREFEP\nAKcA+B0AMMayjLF6AFcBmMkYa3WPb3dvYQBqXA2hC4AsALWiVR8AWcYY3/bsBQBfcX9PA/AH5uBN\nAD2J6BOlvmBciAvcUhjF2GN64cHLxkvXR0VgcIiMQhfzbCpOllLKGhAFY9IBZ6L+49pT8MoPTjX2\ngT/C1M2obSVNPgpAqApqaEPUKEw74kVJs3yxOOGxTn6JuNkREaFLRcqtLeSbniwib59mnZqftkga\nO7Ub3HSk+258JC2C4qMQrlGG+wiByFeGSI0Bp7Mw11Rir9r2dbAM4x/H9BQFXUTY3/7rZCy87nQA\nwcqvkulJYGBBrUqIatN0TT3WrTLtaaw6RuGYnmTtRgfJb5C2jGtGzB/q79ZH69ElE/D76MaWa5O6\nSrOMKX2IMD0B+8ehHUejGAigDsBsInqPiB4iohoAgwFMJqKFRPQKEU1wr58LYB+AjwFsBHA7Y2yX\n0uYOAGki4tT3fADcqHgUgE3CtbXuMQlENJ2IFhHRorq6OvV08ZBMBmZ7ufD8WM32qq7wNQoN5zdP\nRDkJiSgYk877MbhfNxzbxzdrBCan+38YMQjbq8JnFGZHs4nhieYqk3REgjNb10yvaicKbcRRPWCR\nTxz/f3tnHmVHcR763zd3Vo00IyTNaF8YhCRAQitCLBKSQIDwAsbCDwwYsyaAsXFYjJ6XnNixTWwf\nCEkcvUdsZPsdO8Zh8cozIZiXQxzCFstYIIgxISAWSywW4FjbTL0/unq93X37LjPdmvl+50hzb3V1\ndVV33/76W+orL3+QwClHTOKi4w8OlZeahJv/xyIevH5N6MfXFniwx2mSZf2LLcUeW7yHfiniDI++\n6QWvS5p5IfrQTA3hzfCw8HI9RfYNRRHVaHqK2613TLun+SRpFMGop7hMyaGFs2IOEr3fxrQ3s8Ka\nptzlfu+8/Bi/n02EjpdE0CTYGjOnx+XkIyby5Q1HMqm7nRvWz2PTuUs45pDxZTPz447l3i9BIR+M\nekpL7xLEFVZDkcYji6BoBpYAm4wxi3GEwA22fBywArgO+J44Z3U50A9MwREy14hIX7BB4zyJzwZu\nFpFHgLftPpkxxtxqjFlmjFnW09NTeYcKBO8Hd1Zn2mSXtJzyoXpNydFLkPzwju7nmJ7KNYo47r16\nFV96/5Fl5WldfuR/nsRTnz0ldtuFdi3uoDPWxQ0lLjWFbfAuoWUdIx3wwlQFT4rGdXFydzvfv/I4\nvnjmAsZ2tDDOrgXhPuhEhBV949m4/rBQuRueOH3cqPBxA5MKw5pk+Lhppl93W1PgDbEU0VCizuwg\nac7suBQe/ueIULFjScpdBMkvCklzPtKI1kvKWuDSVgprW/7DOqBRxPxGkkKbvbJI/dFtLfzJujlM\nH9fBqjmOuWjpzHEsmeFE8QWFeJo5p701HFGY9FMb097MWcucd9u25hLrF0wOjROccxN9GXHadX2N\nYcuAS8j0ZNsb39nKpnPD7lpXmKZd+0aR/Brpsx3Ybox52H6/A0dQbAfusg/9R0RkAJgAfBDHn7EP\n2CEiPweWAc8FGzXGPASsBBCRk3E0FICX8LULgGm2bFAJXst9XsK4NPt/ent3XXEsL735ByD+rcJV\nk1fPjRdy0eyxwdDNSoJidu9oZvf6D/WgiSaJuEmDLmcsnsoZi8uUOgDP0b+333DPR1eWZeVsLSU/\nECd2tfPCG//t+Ci8vpb3UcQP2732lLmh1en2Ui5c3EMmXaNw1JNfHn34eD6KlPPW1AQdrdlMT0HS\ncj2VvfkHNYpI3StXz2bLC7/juBQ/U3C+QpCgQEp78bnz8mN4/6aHAOd6BvMgxd6KgbLgm7LTf6c8\nmCyzJFJ2jpsrmJ6iYzn+0An09YwuW1K2FHiZ8NtOHmvwfk1a3AiSr21cYEKThKOT2gOh3C7zp3bD\noy/SN2F0OFovEKwQjXBMW12x0VTUKIwxrwIvishcW3Qi8BTwfWANgIjMAVpxTEovAGtteSeOxvF0\ntF0R6bV/24BPAP/Lbvoh8CEb/bQC2GWMeaXWAWYleCO5En3epDFJ1SvadJfMOIj3WPt4XN1J3e38\n6w1ruebkuWXboDzqqUlqtyMH22g0461G8cbv99DeUirLZuv6ZQ7tHZ34ZiUi/Mm6OXS0lEICziX4\nIB3T3kLvGMesPTRsQwAAG69JREFUUarwAKw0DyWa0tytfsQUZw6JJ7xiW3H3iWoU/ra4aJSWmCil\nskWHUkxR0SEdPqWLn9+wtmySaBDvRSHa96DvI+XmWDrTjzaKJsuLfdsPBQ44B3FXInTH1tLk63Jx\n1yluzfbwMfzPT3/uVJbOPCi27+7vun/A1+/SUtkEnwOjWkuJjuKkd4A453W0/8G1J1w2LJ3GTz56\nPKfOnxSqG1yeOepbcc9tUsr0RpI16ukq4Nsi8gSwCPgCcBvQJyJbge8CF1jt4qvAaBF5EngU2GyM\neQJARO4RkSm2zetEZBvwBPAjY8zPbPk9ONrHs8DfAVfUO8gsBK9vX89ovnXR8lAqg/L62Z+6wR/6\nu4/0/fJTxnYkPsyam5pCN1hnW7P3w6rWJCkJD9RGcJAnKPbGbnfNPp84dR7RR1XwjXL13F62fe7U\nsoy3wXpl5Z4jPaE8Yb9wOGvYVABw9xXH8eSfneI/6GOacbc5E+6S5lGU45yHsE8obunTIKFQ0Tqu\nYZkzO2LarIU4rS0kKEpuZI4JbWsJZAmOE1Jxa7YH8U1Z6aHB3qJhxgQWx8o21o7WkpeFNhrdlJSM\nr1wzKjc9eUkug+HypSaOmNJd1l4o40Gk366gGAqNIovpCWPMFhzzUZTzYuq+gxMiG9fOaYHP1+H4\nNqJ1DHBlln41kmj0xqo56X6Papx/wR/CLWcv5itnLay4T6kpnIl0cne7dxNWGzctkb/1cNbSafzD\n49u97xOsoHj9nXhBMWF0G8/f6Exie/y/wqt0eQKswutK0jMsavf2yitoFO6DJRhyHGyntbkpFB+f\nth5FMNzRSTHi1417mFyyso9LVvbxzKtve2Xlae2Tj5c1iCKuvfSkgNnaOvuo6Xz3UT/WpJKPwg1F\n9jL02o2hmdkxbQQf5mkvD5USXbrHqGViWmdrs3cNuzpaeO2dPf7GjM1FX0YAL4lkf8C3UCnyUKR8\nrO59k7RaXyPJqlEMe6r9/UXrj+9s5WIbdZPEZav6QhO00oi+9UjAR1Hr/JpaHjJRvhwRcu7cgGWz\n4lX/JEa3NQd8LhV+7CkO/9j67sMoYbtre97fPxA6JwdFkij22Yyvi6aXv+m5iPgJCd3vLmmaX1p4\ndXrwQ3KbSXhO48i+tUQ9RZPpxV2boLD0J0uGU7m3RLLHRgleuzhhEJf4MQ633oBJDy6Io6O15AmK\naILNrC01xyT39JJcZvghexFrSJkTvnAahVJO9C3o8U+vS63vvlVnJfgjPtyuHuYestYc9IPho5jU\n3c5DG9fSk2H9ZZfujhYeuHY1F9z2CAApQShAiqCo4KNI2s913O/ZPxDSFVzfh8uyWeO4/5oT6JvQ\nyad/8GRomz+PQnjPkZPZ+fYe1s7r5fo7funXSblMqQ7ylG21hLFmcWZnznFUFrpbXic476O1OWJ6\ncn0UgfUo4o6dVVNIC2EPtj0wYDIFJwQJ+iii6fyzavVxUU/u2KKLK8XhWQOk/Ny7QjJukaVGo4LC\nUu3LdiVVsVE8eP0axrvhoK7pqdpGbFcHwUUBwOTu8jTpsd2wxz94QifjOlur0Cjiy4N26rjySqan\nff0D3jntam+OjfyKCwmOHqu3y4mld/riHzPtYZLma0i7tWrRCpP2aKpJUFQ2k8Vl6nU1Cj9ENRD1\nFHP5KwmAuMSPcbjHC6VGT93DZ1RrybuG5YIiWxtx4fHuAz+bRmH/Un7uhzLqSQWFJc0O7bJ05kGe\nnX0wHMNxBOcAuA+J2jWKxvT5riuOrdn8BZSFwlbKr5MklONCH53v6fu5pqe9/QNe3WhqjKyUHSLw\nPe0c1Zrquj7TU0QbaIr/nEbSZM6kMk+jGAjvH0rREWt6Su9QNM1OEp9YP4/X3tnL0X3jefDXr9l9\nU3fx6GjxM9KWCYpsTcT6KNxzkGUZ06B/KSo8hzLqSQWFJcvN893LVrDqSw/wyq7dNc9krQff9AQ/\nvXpl6qJFof0qJAWsliUzqvNH+P0Ik3VeSCUfRaJGkWR6cjWK/f7SqlULigxmjDSbeNq1SNNWa7nv\nkoIGgm1ljaaKPqzindl+mbuwT39M1FPSErSQPinOaSe+P1HmTeriR3Y9by/jb+oePmmmp/lTu2L2\nKCe6OBb4570/w0S54K5Rc1yh5lGMFLLcPC2lJu8hk4Oc8I9pDPMmdTElZmW8OBZOc5yxceF3Q8kM\nqx2dY1NAx+WuiiNps1ueFM2T1K5rYgq+ifWOye5jiTuW3yf/e7ozO0UY1Oi/qERUa44LDa5EecLC\nmOMEyrx5FGVRT8HssdVrFG57WZb3dalWCQ6anrqsoJg6toP7rzmBMxbFT0CNEp3UCf55TzM9Texy\n7kfPR0GMj8J1ZqtGMXRktv0mqPFDwR+tOoRfbd/Fu46cUrlygPULJvPg9Wv4/d79fPneZwapd5UZ\nHwiVhfA8ijQqaRRlmkqFeRSusN+73/dRpE1YA/jlZ06OfZuIHiP4Lc30VKvDup7bLknzituWRJmP\nImbHuAl3A2VRT8Le/cnXqZJG4b5FVzI9BanWmd0RWDXPXZOmf8BU9FsFaS7Fr5vttpXEP159Arv+\nsI9//rXNYyfla8+0DWHUk2oUlqy/P7deHoJi+rhR/OAjx1d8qCXtm0ef06jkS4jWi+I6wZOieZIW\ndRplw1n39g/Qb43n0fDHKN2jWkLmh7Qkhn6d5AdB2rUIbotbz6BmUqKespq0ounAK2kUbv9dDSCY\nPdb1tcUd2z0HSWk09nsT4WoxxWWrN6q12fOt9No3/EtX9aXsUY6Teid8QDfb7MJpYxP36x7Vwozx\no0IaRXQ+iWt2U2f2EJL15vFnkw5OPz7/vvk8/Fw02W5jyMNclkaWbJ6Q/HBMyunkPniS0pp7GkX/\nAG/t3g9A96jqhW/wWC7hqKfs+4Xb8D9HZ23XI+zT5lHUGh5byUfhpdiPaBTNJX9Vw7hzMaa9mXOW\nT+cDy6aXbQNYMLWbDx49g8tWZn9we2njK9SbP7WLrS+9RalJPGE2uq2l6hB3cDSqqHZ02OQu7r16\nFbN7R7NgWhdv23swDi/qSaTsfna1qT1qeho6sqqjfrja4Dx1zz16JucePXNQ2m7EhLtG4jmja3Vm\nByYjherb9pIWdXJ9FMbAW39wFpga25EtMKBS34Jf08Jj04Z87oqZ3uz36OTM+kxPKcLJiyBLbyNN\nMPpl/udoKKinUTT5M9+TUnR88czyDMjB7V9ISbETh8miBgLfuXQFv921G/AjDCuZwpK4dGUf71m4\nmws3Pxoqn2vzyK2dNzFuNw8vEIXy368X9aSmp+LhXrhqZ3kWgbrMFoOA251KGkVSt5Mebu4DOlGj\nCJgzdllBEY1qyUpa19PukDThuGj6WB68fg1QrlHUI+yDe06NBEJ4gQEV2ogupBNvegpoKhF7fDBa\nyX1wxyf9a/y9mvUX29Xe4i0H7LoRsuaHinLY5C7WzC1fKjkrfsBG+TZ1ZhcYL0R18K9NwymajyKr\nMzvp4eitrhfZvtdbujXd9ATw1m6rUVTwUSQRfeAH+5oW9VTpWrgRWZ0xeY4Arj7p0Njyxz91kpeG\nPemYq+b08JWzwm/rnnaW0K//+7GV7N7XH+OjSB+Ha1c/xK5VEox68lOPp2sljSLL0rZR0vwoQ0Ha\n6W3VCXfF50DUKOJuum9ceFR54RBT6yx3fzJSuDxt6VYI/+h3eaan6nwUJjIvwEVi6sT2ocID9uAJ\nnVy68uBYM2SarXx8SioV95B/fEJfWboSP6dQPIfZNDK7I0Ko0rvHxK52vnXRchbbBYSCOZoGPI2i\nfL/BeKmZO9GJVoouZJWGn3E2H+OLb3qKN881iQqKQuJncM25IzUQfSCfMKeH1XWoxfXi2X9TBMX/\nu3Z14rak7LF7KwgKgOtOmcuKvnGc9zUn31R3rRpFpOshH0XafhUz5gqffNfhNfUpuU37IaZjSecy\nqZ7XZob382Am5mD2WM/0VCHEtlFccOwsFk4fy+IqJoz2D+SrUVRKv9Pa3KSmpyJSLONNdQTfYh+8\nfg09NU4yazRpGsWsCeXLq7qUEnwU+/Y7P+4kZzbAlWtmA3Dc7PH807YdjEkw8VSidme2U3Eor0Fa\nrjDvElRyZkcnGFb5oh00PRExPW35zDoWffa+mtrNgohUJSQg28tMFv5t44k1OcQr+oxKTapRFBE3\nQV+tURB5ErzXq1G/B4tqF5KJ4k+4C+/vhgsmObOD/PU5S9j59p6qzV9e9tiUN+w0rbOl1MQXz1zA\ncYckL2HaaPyU2+Ud8wIDKrWRIeopSx9aS+L5+dyysaNaaWt2llotij8tTeuphkndteUS80yCMefD\nGCfYQTWKAnLL2Yv5yRMvM3di8jKpRaVo4bHRVc+C/Piq41NnrkIwnXi4fF8FZ3aQjtYSM8bXLjSj\nb9jBr8sPHkca5yyfUfNxa8ELxIg5rUn+nkpU+/x05740NzWlzmsoiqCoNzy2XipFo7WqRlFMxnW2\ncv4xs/LuRk0UbcJd2tva/KmV81IlperI4qNoFGU+Cvv3b89dwvrI+sd5k7ZCom95qu4mqfblIzzh\nzm0jpl5BAvfdl5V6TU+1knZ6Z/eOdnwUKiiURpKbQy4B942y1h9hc5KPwhMUg/e0ScwbZL9PCixd\nWxQ8X3aMRpFxLlpimwBr5vbwwDM70+sHop6i6eaDxy/KufMis6pwmjz+qZMaZg6KZn6+6QML+bsH\n/5O/Pmcxs3tHc96KmfTWmCK/GjIJChEZC3wNmI9zT11kjHlIRK7CWd+6H/iJMeZ6EWmxdZfY9r9l\njPliTJsnAl/GmfT3DvBhY8yzIvJhW/6Srfo3xpiv1TFGxVKUH5+L+yOsOTzWExQRjaIK01O9JK3P\nUMSoOLercT6KztYS562YwYal8Skzkgie+1s/tKwsfDZK0Jnt9iMcUuz2tRj3qqnBmZ0WolwtvuB0\n/p65ZBpnLvGXo72kihQm9ZBVo7gF+KkxZoOItAKjRGQNcDqw0BizR0TcOMuzgDZjzAIRGQU8JSJ/\nb4x5PtLmJuB0Y8w2EbkC+BTwYbvtdmPMR+oYlxJDwRQK7zW2Zmd2ko+iCmd2vSSHxxZPUgTXj44i\nIvz5GdWlxIjSUmqqmM01GB6btqZHUe7VvCfcuQxWyqCsVBQUItINrMI+xI0xe4G9InI5cKMxZo8t\n32F3MUCniDQDHcBe4K2Ypg3grv7RDbxc+zCULBTlLc0lzZmdhSTT054h0Cg8R2zChLsiahSuiaIz\nISNrLVR75cIT7sJpPSCQUbgg92q9KTzqxY96yuXwHlk0ioOBncBmEVkIPA58DJgDrBSRzwO7gWuN\nMY8Cd+BoGq8Ao4CPG2Pi0qFeAtwjIn/AESQrAtveLyKrgP+w+79Y0+iUEHm/FUVJSwqXhSRn9lD4\nKFzKTE8pcxXy5tPvPozF08dyzCHjc+uDF/VUEj/EOE6jKMi9OpDzhLusObgGmyy/pGYcf8MmY8xi\n4PfADbZ8HM4D/jrge+L8Spbj+Cym4AiZa0QkzpD2ceA0Y8w0YDNwky3/ETDLGHMkcB/wzbhOichl\nIvKYiDy2c2e6A01xyPutJEpamuks+Kan8P7XnjwXgPbmoTc9uTmNxrQXL05kVGszHzhqeq6+quBS\nqAOe6Smu3hB2KoV+z0eRbwqPvMky+u3AdmPMw/b7HTiCYztwl3F4BBgAJgAfxPFn7LPmqJ8Dy4IN\nikgPjm/DbfN24FgAY8zrrjkLxym+NK5TxphbjTHLjDHLenp64qooEYqizrv4ESX1aRRRLlnZx/M3\nvmtQ30r9BHPhY3z29PlsvvAo5k3KtqbySMO91i1NTXXlwhoqPB9FTvMovNOQ8/moKCiMMa8CL4rI\nXFt0IvAU8H1gDYCIzAFagdeAF4C1trwTR+N4OtLsm0C33Q9gHbDN7jM5UO+9brlSP0UTFJ7pqcZ+\nuW/teQ4reuz2llJdaaUbyefOmM97F1a3bG61VD9BLzCPIlIWbrcY92ruPorI37zIqh9fBXzbRjw9\nB1yIY4K6TUS24jisLzDGGBH5Ko4/40mc8W02xjwBICL3AJcYY14WkUuBO0VkAEdwXGSP9VEReS+w\nH3gDPxJKqZOiqPMu9Zqeem2epHdSVggbLD7/vgV84Z5tXqrnInL+ipmcv2JwFsGqlcnd7Yxua6ar\no8W7/kGZ4GpoRblXW5qEveT3MhINj82LTILCGLOFiPnIcl5M3XdwQmTj2jkt8Plu4O6YOhuBjVn6\npVRHUd7SXOrNozPRRvHseHtPhZqNZ8PSaWxYOq1yxWFOtTb0dYdP5NFPnkRHaynWfOetUVGQe/XO\nK47l3q2/pW0I/F3xSOD//Ciex00ZMbgPhVoFhZtoLQ9BodSGiISWonXKyusVJepp3qSuXP1N/gp3\nBfdRKMpg4WYPrVmjsIvv7FRBkRsTxlS34FMQX3vwy4pmesqbvAWEi2oUSm7U68zu7XJ8FJXSRiiD\nQ9pKe1nwZ4gX1/SUNweaM1tRGk69zuz2lhLXnjyHE+YUI8pIqY4005PKCYcDypk9nLn/mhN44fX/\nzrsbI5JGLArzkbWHNqg3ytBTrj0UZYJZUfAERc7nZcT7KA7pGc2aefpGmgf1OrOVAxtvZnag7KYP\nLGTuxDFDMqv+QMATEKpRKCOVemdmKwc2XlLAwOvq+gWTWb9gcsIeIw8phpxQjULJD1Nn9ljlwMbP\n/qvXP4minBvVKJTccINe8lqPWKmNhzau5e0GzIb/wvsWMH3cb1g5e0IDejU8KUiqJxUUSn54zuy8\nfwVKVUzu7mBy5SXNK9Lb1c6fvueI+hsaxqgzWxnx1BseqyjDneia2XmhgkLJDW/N7Lx/BYpSUPyV\nFPPthwoKJTeK8iNQlKKjpidlxGJUo1CUVIqy9roKCiU3ivIjUJSi4v5E8n6XUkGh5EbcwjWKovik\nLRc7lKigUHJDndmKko6vUaiPQhmhTD2oA4CWAi8nqii5EpMPKw90wp2SG7eev5R/e+4Neuza14qi\nhClKZKAKihHGbR9eRndH7auSNZLxo9t415GaAE5RKpG3RpFJ5xeRsSJyh4g8LSLbROQYW36VLXtS\nRL5ky1pE5Jsi8itbd2NCmyeKyL+LyBYR+RcRmW3L20TkdhF5VkQeFpFZjRmqArB23kSWzjwo724o\nipKBgviyM/sobgF+aoyZBywEtonIGuB0YKEx5gjgK7buWUCbMWYBsBT4o4SH/SbgXGPMIuA7wKds\n+cXAm8aY2cDNwF9UPSpFUZRhgL8KYMGd2SLSDawCvg5gjNlrjPkdcDlwozFmjy3fYXcxQKeINAMd\nwF7grZimDdBlP3cDL9vPpwPftJ/vAE6UvM+SoihKDnhRT7n2IptGcTCwE9gsIr8Qka+JSCcwB1hp\nzUP/LCJH2fp3AL8HXgFeAL5ijHkjpt1LgHtEZDtwPnCjLZ8KvAhgjNkP7ALGR3cWkctE5DEReWzn\nzp1Zx6soinLAUJS5RlkERTOwBNhkjFmMIwRusOXjgBXAdcD37Jv/cqAfmIIjZK4Rkb6Ydj8OnGaM\nmQZsBm6qpuPGmFuNMcuMMct6enqq2VVRFOWAwHdRFNz0BGwHthtjHrbf78ARHNuBu4zDI8AAMAH4\nII4/Y581R/0cWBZsUER6cHwbbpu3A8fazy8B0229Zhyz1Os1jk9RFOWAp/AahTHmVeBFEZlri04E\nngK+D6wBEJE5QCvwGo65aa0t78TROJ6ONPsm0G33A1gHbLOffwhcYD9vAH5mijKPXVEUZQgpypMv\n6zyKq4Bvi0gr8BxwIY4J6jYR2YrjsL7AGGNE5Ks4/owncfSlzcaYJwBE5B7gEmPMyyJyKXCniAzg\nCI6L7LG+DvwfEXkWeAM4uyEjVRRFOeCwPoqce5FJUBhjthAxH1nOi6n7Dk6IbFw7pwU+3w3cHVNn\nd9L+iqIoIwk/PDbffmiSHUVRlILih8cW35mtKIqi5IBqFIqiKEoqpSZHQrQ15/uo1qSAiqIoBWXd\n4RO5fPUhXLYybira0KGCQlEUpaCUmoRPnDov726o6UlRFEVJRwWFoiiKkooKCkVRFCUVFRSKoihK\nKiooFEVRlFRUUCiKoiipqKBQFEVRUlFBoSiKoqQiw2GpBxHZCfxXHU1MwFlLYyShYx4Z6JhHBrWO\neaYxpuISocNCUNSLiDxmjIlLoz5s0TGPDHTMI4PBHrOanhRFUZRUVFAoiqIoqaigcLg17w7kgI55\nZKBjHhkM6pjVR6EoiqKkohqFoiiKksqIFhQicqqIPCMiz4rIDXn3p1GIyG0iskNEtgbKxonIfSLy\na/v3IFsuIvJX9hw8ISJL8ut57YjIdBF5QESeEpEnReRjtnzYjltE2kXkERH5pR3zn9nyg0XkYTu2\n20Wk1Za32e/P2u2z8ux/PYhISUR+ISI/tt+H9ZhF5HkR+ZWIbBGRx2zZkN3bI1ZQiEgJ+CqwHjgc\nOEdEDs+3Vw3jG8CpkbIbgPuNMYcC99vv4Iz/UPvvMmDTEPWx0ewHrjHGHA6sAK6013M4j3sPsNYY\nsxBYBJwqIiuAvwBuNsbMBt4ELrb1LwbetOU323oHKh8DtgW+j4QxrzHGLAqEwQ7dvW2MGZH/gGOA\newPfNwIb8+5XA8c3C9ga+P4MMNl+ngw8Yz//b+CcuHoH8j/gB8C6kTJuYBTw78DROBOvmm25d58D\n9wLH2M/Ntp7k3fcaxjrNPhjXAj8GZASM+XlgQqRsyO7tEatRAFOBFwPft9uy4cpEY8wr9vOrwET7\nedidB2teWAw8zDAftzXBbAF2APcBvwF+Z4zZb6sEx+WN2W7fBYwf2h43hL8ErgcG7PfxDP8xG+Af\nReRxEbnMlg3Zva1rZo9AjDFGRIZluJuIjAbuBK42xrwlIt624ThuY0w/sEhExgJ3A/kvsDyIiMi7\ngR3GmMdFZHXe/RlCjjfGvCQivcB9IvJ0cONg39sjWaN4CZge+D7Nlg1XfisikwHs3x22fNicBxFp\nwRES3zbG3GWLh/24AYwxvwMewDG7jBUR9yUwOC5vzHZ7N/D6EHe1Xo4D3isizwPfxTE/3cLwHjPG\nmJfs3x04LwTLGcJ7eyQLikeBQ220RCtwNvDDnPs0mPwQuMB+vgDHhu+Wf8hGSqwAdgXU2QMGcVSH\nrwPbjDE3BTYN23GLSI/VJBCRDhyfzDYcgbHBVouO2T0XG4CfGWvEPlAwxmw0xkwzxszC+c3+zBhz\nLsN4zCLSKSJj3M/AycBWhvLezttJk7OD6DTgP3Dsup/Muz8NHNffA68A+3Dskxfj2GXvB34N/BMw\nztYVnOiv3wC/Apbl3f8ax3w8jh33CWCL/XfacB43cCTwCzvmrcBnbHkf8AjwLPAPQJstb7ffn7Xb\n+/IeQ53jXw38eLiP2Y7tl/bfk+6zaijvbZ2ZrSiKoqQykk1PiqIoSgZUUCiKoiipqKBQFEVRUlFB\noSiKoqSigkJRFEVJRQWFoiiKkooKCkVRFCUVFRSKoihKKv8fRtx3D5BWRoYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i4-YYgEoT8ye",
        "outputId": "6ebb3f2f-95ee-45b6-fcd7-bf3e30d0e634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"acc\"], label=\"train accuracy\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_acc\"], label=\"Validation accuracy\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd4e8dbbb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3nqQmtqAT8yi",
        "outputId": "9d59946c-ff95-4dd3-acbb-ce189c4d931f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model.evaluate(x=test_x, y=test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "210/210 [==============================] - 0s 156us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[669.0746058872768, 0.5380952363922482]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sTCSonkLZtY6"
      },
      "source": [
        "### Test 4: First 5 classes for training, another 5 for testing, 50 examples for each class, 5 way training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lqH3tAPUZtZL",
        "colab": {}
      },
      "source": [
        "train_x, train_y, valid_x, valid_y, test_x, test_y = get_training_data(sample_per_class=50, n_ways=5, batch_size = 32, valid_sample_per_class=50, test_sample_per_class=50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7PZpW79MZtZS",
        "colab": {}
      },
      "source": [
        "# visualize_data(train_x, train_y, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-s50062hZtZY",
        "colab": {}
      },
      "source": [
        "# visualize_data(test_x, test_y, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7md1AVgVZtZd",
        "colab": {}
      },
      "source": [
        "ip_shape = (28,28,1)\n",
        "model = get_classifier_model(ip_shape)\n",
        "\n",
        "optimizer = Adam(lr = 1E-6)\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g-o8hfIsZtZi",
        "outputId": "2bca115f-111f-4c27-df44-9357a49c57dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(train_x, train_y, epochs=3000, validation_data=(valid_x, valid_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 488 samples, validate on 478 samples\n",
            "Epoch 1/3000\n",
            "488/488 [==============================] - 1s 3ms/step - loss: 669.0606 - acc: 0.5574 - val_loss: 669.0118 - val_acc: 0.6004\n",
            "Epoch 2/3000\n",
            "488/488 [==============================] - 0s 395us/step - loss: 669.0317 - acc: 0.5389 - val_loss: 668.9802 - val_acc: 0.6046\n",
            "Epoch 3/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 669.0011 - acc: 0.5471 - val_loss: 668.9486 - val_acc: 0.6025\n",
            "Epoch 4/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 668.9520 - acc: 0.5697 - val_loss: 668.9169 - val_acc: 0.6046\n",
            "Epoch 5/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 668.9277 - acc: 0.5533 - val_loss: 668.8853 - val_acc: 0.6046\n",
            "Epoch 6/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 668.8869 - acc: 0.5574 - val_loss: 668.8536 - val_acc: 0.6046\n",
            "Epoch 7/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 668.8528 - acc: 0.6025 - val_loss: 668.8221 - val_acc: 0.6046\n",
            "Epoch 8/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 668.8301 - acc: 0.5676 - val_loss: 668.7904 - val_acc: 0.6046\n",
            "Epoch 9/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 668.7881 - acc: 0.5963 - val_loss: 668.7588 - val_acc: 0.6046\n",
            "Epoch 10/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 668.7693 - acc: 0.5533 - val_loss: 668.7271 - val_acc: 0.6025\n",
            "Epoch 11/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 668.7339 - acc: 0.5635 - val_loss: 668.6955 - val_acc: 0.6025\n",
            "Epoch 12/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 668.6876 - acc: 0.5799 - val_loss: 668.6639 - val_acc: 0.6004\n",
            "Epoch 13/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 668.6732 - acc: 0.5758 - val_loss: 668.6322 - val_acc: 0.6025\n",
            "Epoch 14/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 668.6330 - acc: 0.5758 - val_loss: 668.6007 - val_acc: 0.6004\n",
            "Epoch 15/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 668.5938 - acc: 0.5717 - val_loss: 668.5690 - val_acc: 0.6004\n",
            "Epoch 16/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 668.5710 - acc: 0.5676 - val_loss: 668.5374 - val_acc: 0.6004\n",
            "Epoch 17/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 668.5372 - acc: 0.5656 - val_loss: 668.5057 - val_acc: 0.6004\n",
            "Epoch 18/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 668.5075 - acc: 0.5451 - val_loss: 668.4742 - val_acc: 0.5983\n",
            "Epoch 19/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 668.4643 - acc: 0.5717 - val_loss: 668.4425 - val_acc: 0.6004\n",
            "Epoch 20/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 668.4335 - acc: 0.5922 - val_loss: 668.4108 - val_acc: 0.6004\n",
            "Epoch 21/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 668.4024 - acc: 0.5943 - val_loss: 668.3793 - val_acc: 0.6025\n",
            "Epoch 22/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 668.3733 - acc: 0.5861 - val_loss: 668.3477 - val_acc: 0.6025\n",
            "Epoch 23/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 668.3434 - acc: 0.5799 - val_loss: 668.3160 - val_acc: 0.6004\n",
            "Epoch 24/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 668.3123 - acc: 0.5820 - val_loss: 668.2844 - val_acc: 0.6004\n",
            "Epoch 25/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 668.2956 - acc: 0.5533 - val_loss: 668.2529 - val_acc: 0.6004\n",
            "Epoch 26/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 668.2463 - acc: 0.5984 - val_loss: 668.2212 - val_acc: 0.5983\n",
            "Epoch 27/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 668.2162 - acc: 0.5533 - val_loss: 668.1896 - val_acc: 0.5983\n",
            "Epoch 28/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 668.1867 - acc: 0.5820 - val_loss: 668.1579 - val_acc: 0.5983\n",
            "Epoch 29/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 668.1491 - acc: 0.5820 - val_loss: 668.1263 - val_acc: 0.5983\n",
            "Epoch 30/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 668.1158 - acc: 0.5758 - val_loss: 668.0945 - val_acc: 0.5983\n",
            "Epoch 31/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 668.0802 - acc: 0.6230 - val_loss: 668.0629 - val_acc: 0.5962\n",
            "Epoch 32/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 668.0480 - acc: 0.6045 - val_loss: 668.0313 - val_acc: 0.5962\n",
            "Epoch 33/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 668.0115 - acc: 0.6066 - val_loss: 667.9997 - val_acc: 0.5983\n",
            "Epoch 34/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 668.0007 - acc: 0.5984 - val_loss: 667.9682 - val_acc: 0.5983\n",
            "Epoch 35/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 667.9421 - acc: 0.6168 - val_loss: 667.9365 - val_acc: 0.5983\n",
            "Epoch 36/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 667.9159 - acc: 0.6066 - val_loss: 667.9047 - val_acc: 0.5983\n",
            "Epoch 37/3000\n",
            "488/488 [==============================] - 0s 373us/step - loss: 667.8857 - acc: 0.6148 - val_loss: 667.8731 - val_acc: 0.6025\n",
            "Epoch 38/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 667.8682 - acc: 0.5922 - val_loss: 667.8416 - val_acc: 0.6046\n",
            "Epoch 39/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 667.8283 - acc: 0.5799 - val_loss: 667.8101 - val_acc: 0.6046\n",
            "Epoch 40/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 667.8029 - acc: 0.5676 - val_loss: 667.7785 - val_acc: 0.6046\n",
            "Epoch 41/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 667.7708 - acc: 0.5881 - val_loss: 667.7470 - val_acc: 0.6046\n",
            "Epoch 42/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 667.7343 - acc: 0.6107 - val_loss: 667.7155 - val_acc: 0.6025\n",
            "Epoch 43/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 667.7096 - acc: 0.5594 - val_loss: 667.6839 - val_acc: 0.6046\n",
            "Epoch 44/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 667.6697 - acc: 0.6127 - val_loss: 667.6522 - val_acc: 0.6046\n",
            "Epoch 45/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 667.6367 - acc: 0.6025 - val_loss: 667.6206 - val_acc: 0.6025\n",
            "Epoch 46/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 667.6013 - acc: 0.5902 - val_loss: 667.5892 - val_acc: 0.6025\n",
            "Epoch 47/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 667.5743 - acc: 0.5820 - val_loss: 667.5576 - val_acc: 0.6025\n",
            "Epoch 48/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 667.5405 - acc: 0.5963 - val_loss: 667.5260 - val_acc: 0.6067\n",
            "Epoch 49/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 667.5044 - acc: 0.6127 - val_loss: 667.4944 - val_acc: 0.6067\n",
            "Epoch 50/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 667.4748 - acc: 0.6352 - val_loss: 667.4628 - val_acc: 0.6067\n",
            "Epoch 51/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 667.4526 - acc: 0.5922 - val_loss: 667.4311 - val_acc: 0.6067\n",
            "Epoch 52/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 667.4197 - acc: 0.6148 - val_loss: 667.3997 - val_acc: 0.6067\n",
            "Epoch 53/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 667.3766 - acc: 0.6168 - val_loss: 667.3680 - val_acc: 0.6067\n",
            "Epoch 54/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 667.3584 - acc: 0.5902 - val_loss: 667.3364 - val_acc: 0.6067\n",
            "Epoch 55/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 667.3281 - acc: 0.5881 - val_loss: 667.3051 - val_acc: 0.6088\n",
            "Epoch 56/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 667.2906 - acc: 0.6066 - val_loss: 667.2735 - val_acc: 0.6088\n",
            "Epoch 57/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 667.2534 - acc: 0.6311 - val_loss: 667.2420 - val_acc: 0.6088\n",
            "Epoch 58/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 667.2163 - acc: 0.5881 - val_loss: 667.2104 - val_acc: 0.6088\n",
            "Epoch 59/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 667.1867 - acc: 0.6373 - val_loss: 667.1788 - val_acc: 0.6088\n",
            "Epoch 60/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 667.1646 - acc: 0.6250 - val_loss: 667.1473 - val_acc: 0.6067\n",
            "Epoch 61/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 667.1335 - acc: 0.6168 - val_loss: 667.1157 - val_acc: 0.6067\n",
            "Epoch 62/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 667.0900 - acc: 0.6311 - val_loss: 667.0842 - val_acc: 0.6067\n",
            "Epoch 63/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 667.0715 - acc: 0.5881 - val_loss: 667.0527 - val_acc: 0.6067\n",
            "Epoch 64/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 667.0337 - acc: 0.6148 - val_loss: 667.0211 - val_acc: 0.6067\n",
            "Epoch 65/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 666.9914 - acc: 0.6434 - val_loss: 666.9895 - val_acc: 0.6046\n",
            "Epoch 66/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 666.9624 - acc: 0.6496 - val_loss: 666.9580 - val_acc: 0.6046\n",
            "Epoch 67/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 666.9429 - acc: 0.6127 - val_loss: 666.9263 - val_acc: 0.6067\n",
            "Epoch 68/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 666.9158 - acc: 0.6086 - val_loss: 666.8947 - val_acc: 0.6067\n",
            "Epoch 69/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 666.8695 - acc: 0.6434 - val_loss: 666.8632 - val_acc: 0.6067\n",
            "Epoch 70/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 666.8372 - acc: 0.6066 - val_loss: 666.8316 - val_acc: 0.6088\n",
            "Epoch 71/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 666.8048 - acc: 0.6311 - val_loss: 666.8000 - val_acc: 0.6088\n",
            "Epoch 72/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 666.7723 - acc: 0.6230 - val_loss: 666.7685 - val_acc: 0.6067\n",
            "Epoch 73/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 666.7590 - acc: 0.5963 - val_loss: 666.7368 - val_acc: 0.6067\n",
            "Epoch 74/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 666.7084 - acc: 0.6291 - val_loss: 666.7051 - val_acc: 0.6067\n",
            "Epoch 75/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 666.6800 - acc: 0.6230 - val_loss: 666.6735 - val_acc: 0.6067\n",
            "Epoch 76/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 666.6467 - acc: 0.6025 - val_loss: 666.6420 - val_acc: 0.6067\n",
            "Epoch 77/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 666.6126 - acc: 0.6373 - val_loss: 666.6105 - val_acc: 0.6067\n",
            "Epoch 78/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 666.5851 - acc: 0.6168 - val_loss: 666.5788 - val_acc: 0.6067\n",
            "Epoch 79/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 666.5565 - acc: 0.6250 - val_loss: 666.5472 - val_acc: 0.6067\n",
            "Epoch 80/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 666.5301 - acc: 0.6086 - val_loss: 666.5157 - val_acc: 0.6067\n",
            "Epoch 81/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 666.5041 - acc: 0.6127 - val_loss: 666.4841 - val_acc: 0.6067\n",
            "Epoch 82/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 666.4495 - acc: 0.6291 - val_loss: 666.4526 - val_acc: 0.6088\n",
            "Epoch 83/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 666.4168 - acc: 0.6373 - val_loss: 666.4210 - val_acc: 0.6088\n",
            "Epoch 84/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 666.3951 - acc: 0.6332 - val_loss: 666.3893 - val_acc: 0.6088\n",
            "Epoch 85/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 666.3645 - acc: 0.5984 - val_loss: 666.3577 - val_acc: 0.6088\n",
            "Epoch 86/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 666.3386 - acc: 0.5963 - val_loss: 666.3261 - val_acc: 0.6088\n",
            "Epoch 87/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 666.2982 - acc: 0.6168 - val_loss: 666.2944 - val_acc: 0.6088\n",
            "Epoch 88/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 666.2643 - acc: 0.6332 - val_loss: 666.2629 - val_acc: 0.6088\n",
            "Epoch 89/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 666.2322 - acc: 0.6107 - val_loss: 666.2312 - val_acc: 0.6088\n",
            "Epoch 90/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 666.2034 - acc: 0.6373 - val_loss: 666.1997 - val_acc: 0.6088\n",
            "Epoch 91/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 666.1752 - acc: 0.6189 - val_loss: 666.1680 - val_acc: 0.6088\n",
            "Epoch 92/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 666.1414 - acc: 0.6025 - val_loss: 666.1365 - val_acc: 0.6088\n",
            "Epoch 93/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 666.1106 - acc: 0.6352 - val_loss: 666.1050 - val_acc: 0.6088\n",
            "Epoch 94/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 666.0785 - acc: 0.5984 - val_loss: 666.0733 - val_acc: 0.6088\n",
            "Epoch 95/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 666.0389 - acc: 0.6475 - val_loss: 666.0417 - val_acc: 0.6088\n",
            "Epoch 96/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 666.0208 - acc: 0.6189 - val_loss: 666.0101 - val_acc: 0.6088\n",
            "Epoch 97/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 665.9864 - acc: 0.6107 - val_loss: 665.9786 - val_acc: 0.6088\n",
            "Epoch 98/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 665.9551 - acc: 0.6127 - val_loss: 665.9471 - val_acc: 0.6088\n",
            "Epoch 99/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 665.9366 - acc: 0.6066 - val_loss: 665.9155 - val_acc: 0.6088\n",
            "Epoch 100/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 665.9074 - acc: 0.5881 - val_loss: 665.8840 - val_acc: 0.6088\n",
            "Epoch 101/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 665.8597 - acc: 0.6189 - val_loss: 665.8523 - val_acc: 0.6088\n",
            "Epoch 102/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 665.8280 - acc: 0.6127 - val_loss: 665.8208 - val_acc: 0.6088\n",
            "Epoch 103/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 665.8095 - acc: 0.6066 - val_loss: 665.7893 - val_acc: 0.6088\n",
            "Epoch 104/3000\n",
            "488/488 [==============================] - 0s 366us/step - loss: 665.7626 - acc: 0.6270 - val_loss: 665.7576 - val_acc: 0.6109\n",
            "Epoch 105/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 665.7292 - acc: 0.6189 - val_loss: 665.7259 - val_acc: 0.6109\n",
            "Epoch 106/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 665.7001 - acc: 0.6352 - val_loss: 665.6945 - val_acc: 0.6109\n",
            "Epoch 107/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 665.6636 - acc: 0.6189 - val_loss: 665.6630 - val_acc: 0.6109\n",
            "Epoch 108/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 665.6366 - acc: 0.6311 - val_loss: 665.6314 - val_acc: 0.6109\n",
            "Epoch 109/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 665.6182 - acc: 0.6045 - val_loss: 665.5998 - val_acc: 0.6109\n",
            "Epoch 110/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 665.5764 - acc: 0.6373 - val_loss: 665.5682 - val_acc: 0.6109\n",
            "Epoch 111/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 665.5413 - acc: 0.6209 - val_loss: 665.5366 - val_acc: 0.6109\n",
            "Epoch 112/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 665.5104 - acc: 0.6414 - val_loss: 665.5051 - val_acc: 0.6109\n",
            "Epoch 113/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 665.4740 - acc: 0.6373 - val_loss: 665.4735 - val_acc: 0.6109\n",
            "Epoch 114/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 665.4451 - acc: 0.6373 - val_loss: 665.4420 - val_acc: 0.6109\n",
            "Epoch 115/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 665.4048 - acc: 0.6434 - val_loss: 665.4104 - val_acc: 0.6109\n",
            "Epoch 116/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 665.3810 - acc: 0.6066 - val_loss: 665.3789 - val_acc: 0.6109\n",
            "Epoch 117/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 665.3580 - acc: 0.6291 - val_loss: 665.3473 - val_acc: 0.6130\n",
            "Epoch 118/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 665.3146 - acc: 0.6475 - val_loss: 665.3157 - val_acc: 0.6151\n",
            "Epoch 119/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 665.3021 - acc: 0.5922 - val_loss: 665.2841 - val_acc: 0.6130\n",
            "Epoch 120/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 665.2488 - acc: 0.6250 - val_loss: 665.2527 - val_acc: 0.6130\n",
            "Epoch 121/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 665.2067 - acc: 0.6660 - val_loss: 665.2210 - val_acc: 0.6130\n",
            "Epoch 122/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 665.1875 - acc: 0.6311 - val_loss: 665.1895 - val_acc: 0.6130\n",
            "Epoch 123/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 665.1644 - acc: 0.6189 - val_loss: 665.1580 - val_acc: 0.6130\n",
            "Epoch 124/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 665.1313 - acc: 0.6086 - val_loss: 665.1265 - val_acc: 0.6130\n",
            "Epoch 125/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 665.1080 - acc: 0.6025 - val_loss: 665.0949 - val_acc: 0.6130\n",
            "Epoch 126/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 665.0678 - acc: 0.6373 - val_loss: 665.0633 - val_acc: 0.6130\n",
            "Epoch 127/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 665.0374 - acc: 0.6332 - val_loss: 665.0318 - val_acc: 0.6130\n",
            "Epoch 128/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 665.0008 - acc: 0.6230 - val_loss: 665.0003 - val_acc: 0.6109\n",
            "Epoch 129/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 664.9809 - acc: 0.6127 - val_loss: 664.9686 - val_acc: 0.6109\n",
            "Epoch 130/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 664.9398 - acc: 0.6209 - val_loss: 664.9371 - val_acc: 0.6088\n",
            "Epoch 131/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 664.9263 - acc: 0.5963 - val_loss: 664.9056 - val_acc: 0.6109\n",
            "Epoch 132/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 664.8676 - acc: 0.6537 - val_loss: 664.8741 - val_acc: 0.6109\n",
            "Epoch 133/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 664.8449 - acc: 0.6250 - val_loss: 664.8425 - val_acc: 0.6109\n",
            "Epoch 134/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 664.8252 - acc: 0.6332 - val_loss: 664.8111 - val_acc: 0.6088\n",
            "Epoch 135/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 664.7992 - acc: 0.5738 - val_loss: 664.7797 - val_acc: 0.6088\n",
            "Epoch 136/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 664.7419 - acc: 0.6578 - val_loss: 664.7482 - val_acc: 0.6088\n",
            "Epoch 137/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 664.7104 - acc: 0.6516 - val_loss: 664.7167 - val_acc: 0.6088\n",
            "Epoch 138/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 664.6882 - acc: 0.6434 - val_loss: 664.6852 - val_acc: 0.6088\n",
            "Epoch 139/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 664.6542 - acc: 0.6434 - val_loss: 664.6538 - val_acc: 0.6088\n",
            "Epoch 140/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 664.6262 - acc: 0.6311 - val_loss: 664.6223 - val_acc: 0.6088\n",
            "Epoch 141/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 664.5933 - acc: 0.6066 - val_loss: 664.5907 - val_acc: 0.6088\n",
            "Epoch 142/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 664.5632 - acc: 0.6311 - val_loss: 664.5593 - val_acc: 0.6088\n",
            "Epoch 143/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 664.5269 - acc: 0.6189 - val_loss: 664.5278 - val_acc: 0.6088\n",
            "Epoch 144/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 664.5000 - acc: 0.6209 - val_loss: 664.4963 - val_acc: 0.6088\n",
            "Epoch 145/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 664.4544 - acc: 0.6414 - val_loss: 664.4648 - val_acc: 0.6109\n",
            "Epoch 146/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 664.4379 - acc: 0.6393 - val_loss: 664.4333 - val_acc: 0.6130\n",
            "Epoch 147/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 664.4122 - acc: 0.6127 - val_loss: 664.4019 - val_acc: 0.6109\n",
            "Epoch 148/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 664.3729 - acc: 0.6311 - val_loss: 664.3704 - val_acc: 0.6130\n",
            "Epoch 149/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 664.3410 - acc: 0.6496 - val_loss: 664.3390 - val_acc: 0.6109\n",
            "Epoch 150/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 664.3157 - acc: 0.6352 - val_loss: 664.3074 - val_acc: 0.6109\n",
            "Epoch 151/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 664.2708 - acc: 0.6373 - val_loss: 664.2759 - val_acc: 0.6109\n",
            "Epoch 152/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 664.2387 - acc: 0.6270 - val_loss: 664.2444 - val_acc: 0.6130\n",
            "Epoch 153/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 664.2130 - acc: 0.6373 - val_loss: 664.2129 - val_acc: 0.6130\n",
            "Epoch 154/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 664.1879 - acc: 0.6209 - val_loss: 664.1815 - val_acc: 0.6109\n",
            "Epoch 155/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 664.1472 - acc: 0.6352 - val_loss: 664.1500 - val_acc: 0.6109\n",
            "Epoch 156/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 664.1067 - acc: 0.6639 - val_loss: 664.1186 - val_acc: 0.6088\n",
            "Epoch 157/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 664.0843 - acc: 0.6455 - val_loss: 664.0869 - val_acc: 0.6088\n",
            "Epoch 158/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 664.0455 - acc: 0.6660 - val_loss: 664.0555 - val_acc: 0.6088\n",
            "Epoch 159/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 664.0151 - acc: 0.6455 - val_loss: 664.0241 - val_acc: 0.6088\n",
            "Epoch 160/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 663.9888 - acc: 0.6516 - val_loss: 663.9925 - val_acc: 0.6088\n",
            "Epoch 161/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 663.9704 - acc: 0.6291 - val_loss: 663.9610 - val_acc: 0.6130\n",
            "Epoch 162/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 663.9233 - acc: 0.6352 - val_loss: 663.9295 - val_acc: 0.6130\n",
            "Epoch 163/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 663.8946 - acc: 0.6189 - val_loss: 663.8979 - val_acc: 0.6130\n",
            "Epoch 164/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 663.8697 - acc: 0.6455 - val_loss: 663.8664 - val_acc: 0.6130\n",
            "Epoch 165/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 663.8436 - acc: 0.6393 - val_loss: 663.8349 - val_acc: 0.6130\n",
            "Epoch 166/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 663.7974 - acc: 0.6455 - val_loss: 663.8035 - val_acc: 0.6130\n",
            "Epoch 167/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 663.7777 - acc: 0.6270 - val_loss: 663.7720 - val_acc: 0.6130\n",
            "Epoch 168/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 663.7458 - acc: 0.6189 - val_loss: 663.7405 - val_acc: 0.6109\n",
            "Epoch 169/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 663.7298 - acc: 0.6209 - val_loss: 663.7089 - val_acc: 0.6088\n",
            "Epoch 170/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 663.6739 - acc: 0.6434 - val_loss: 663.6773 - val_acc: 0.6109\n",
            "Epoch 171/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 663.6406 - acc: 0.6291 - val_loss: 663.6458 - val_acc: 0.6088\n",
            "Epoch 172/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 663.6116 - acc: 0.6168 - val_loss: 663.6143 - val_acc: 0.6109\n",
            "Epoch 173/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 663.5819 - acc: 0.6414 - val_loss: 663.5828 - val_acc: 0.6109\n",
            "Epoch 174/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 663.5461 - acc: 0.6434 - val_loss: 663.5513 - val_acc: 0.6109\n",
            "Epoch 175/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 663.5229 - acc: 0.6373 - val_loss: 663.5196 - val_acc: 0.6130\n",
            "Epoch 176/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 663.4918 - acc: 0.6168 - val_loss: 663.4882 - val_acc: 0.6130\n",
            "Epoch 177/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 663.4541 - acc: 0.6332 - val_loss: 663.4565 - val_acc: 0.6109\n",
            "Epoch 178/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 663.4261 - acc: 0.6086 - val_loss: 663.4251 - val_acc: 0.6151\n",
            "Epoch 179/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 663.3951 - acc: 0.6127 - val_loss: 663.3936 - val_acc: 0.6151\n",
            "Epoch 180/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 663.3571 - acc: 0.6434 - val_loss: 663.3622 - val_acc: 0.6130\n",
            "Epoch 181/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 663.3254 - acc: 0.6516 - val_loss: 663.3306 - val_acc: 0.6151\n",
            "Epoch 182/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 663.2977 - acc: 0.6434 - val_loss: 663.2989 - val_acc: 0.6151\n",
            "Epoch 183/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 663.2762 - acc: 0.6270 - val_loss: 663.2674 - val_acc: 0.6172\n",
            "Epoch 184/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 663.2240 - acc: 0.6475 - val_loss: 663.2359 - val_acc: 0.6151\n",
            "Epoch 185/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 663.2030 - acc: 0.6332 - val_loss: 663.2044 - val_acc: 0.6151\n",
            "Epoch 186/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 663.1670 - acc: 0.6516 - val_loss: 663.1729 - val_acc: 0.6172\n",
            "Epoch 187/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 663.1372 - acc: 0.6393 - val_loss: 663.1414 - val_acc: 0.6151\n",
            "Epoch 188/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 663.1052 - acc: 0.6373 - val_loss: 663.1098 - val_acc: 0.6151\n",
            "Epoch 189/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 663.0778 - acc: 0.6352 - val_loss: 663.0783 - val_acc: 0.6151\n",
            "Epoch 190/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 663.0503 - acc: 0.6557 - val_loss: 663.0468 - val_acc: 0.6151\n",
            "Epoch 191/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 663.0236 - acc: 0.6189 - val_loss: 663.0152 - val_acc: 0.6151\n",
            "Epoch 192/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 662.9856 - acc: 0.6537 - val_loss: 662.9837 - val_acc: 0.6151\n",
            "Epoch 193/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 662.9466 - acc: 0.6557 - val_loss: 662.9521 - val_acc: 0.6151\n",
            "Epoch 194/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 662.9126 - acc: 0.6414 - val_loss: 662.9204 - val_acc: 0.6151\n",
            "Epoch 195/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 662.8698 - acc: 0.6967 - val_loss: 662.8889 - val_acc: 0.6172\n",
            "Epoch 196/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 662.8366 - acc: 0.6578 - val_loss: 662.8574 - val_acc: 0.6172\n",
            "Epoch 197/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 662.8273 - acc: 0.5943 - val_loss: 662.8259 - val_acc: 0.6151\n",
            "Epoch 198/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 662.7871 - acc: 0.6660 - val_loss: 662.7942 - val_acc: 0.6130\n",
            "Epoch 199/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 662.7477 - acc: 0.6475 - val_loss: 662.7627 - val_acc: 0.6130\n",
            "Epoch 200/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 662.7225 - acc: 0.6496 - val_loss: 662.7312 - val_acc: 0.6130\n",
            "Epoch 201/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 662.6923 - acc: 0.6414 - val_loss: 662.6996 - val_acc: 0.6109\n",
            "Epoch 202/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 662.6744 - acc: 0.6189 - val_loss: 662.6680 - val_acc: 0.6109\n",
            "Epoch 203/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 662.6400 - acc: 0.6086 - val_loss: 662.6365 - val_acc: 0.6109\n",
            "Epoch 204/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 662.5954 - acc: 0.6660 - val_loss: 662.6050 - val_acc: 0.6109\n",
            "Epoch 205/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 662.5800 - acc: 0.6168 - val_loss: 662.5734 - val_acc: 0.6109\n",
            "Epoch 206/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 662.5358 - acc: 0.6352 - val_loss: 662.5418 - val_acc: 0.6109\n",
            "Epoch 207/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 662.5041 - acc: 0.6516 - val_loss: 662.5103 - val_acc: 0.6109\n",
            "Epoch 208/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 662.4723 - acc: 0.6680 - val_loss: 662.4786 - val_acc: 0.6130\n",
            "Epoch 209/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 662.4450 - acc: 0.6393 - val_loss: 662.4472 - val_acc: 0.6130\n",
            "Epoch 210/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 662.4124 - acc: 0.6352 - val_loss: 662.4157 - val_acc: 0.6130\n",
            "Epoch 211/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 662.3794 - acc: 0.6414 - val_loss: 662.3840 - val_acc: 0.6130\n",
            "Epoch 212/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 662.3360 - acc: 0.6619 - val_loss: 662.3525 - val_acc: 0.6130\n",
            "Epoch 213/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 662.3234 - acc: 0.6393 - val_loss: 662.3210 - val_acc: 0.6109\n",
            "Epoch 214/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 662.2788 - acc: 0.6434 - val_loss: 662.2894 - val_acc: 0.6109\n",
            "Epoch 215/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 662.2555 - acc: 0.6496 - val_loss: 662.2580 - val_acc: 0.6109\n",
            "Epoch 216/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 662.2284 - acc: 0.6496 - val_loss: 662.2265 - val_acc: 0.6130\n",
            "Epoch 217/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 662.1993 - acc: 0.6414 - val_loss: 662.1948 - val_acc: 0.6130\n",
            "Epoch 218/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 662.1528 - acc: 0.6496 - val_loss: 662.1633 - val_acc: 0.6130\n",
            "Epoch 219/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 662.1313 - acc: 0.6270 - val_loss: 662.1318 - val_acc: 0.6130\n",
            "Epoch 220/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 662.0842 - acc: 0.6660 - val_loss: 662.1002 - val_acc: 0.6130\n",
            "Epoch 221/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 662.0743 - acc: 0.6393 - val_loss: 662.0687 - val_acc: 0.6130\n",
            "Epoch 222/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 662.0221 - acc: 0.6619 - val_loss: 662.0371 - val_acc: 0.6172\n",
            "Epoch 223/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 662.0058 - acc: 0.6373 - val_loss: 662.0057 - val_acc: 0.6151\n",
            "Epoch 224/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 661.9706 - acc: 0.6496 - val_loss: 661.9741 - val_acc: 0.6151\n",
            "Epoch 225/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 661.9296 - acc: 0.6660 - val_loss: 661.9425 - val_acc: 0.6130\n",
            "Epoch 226/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 661.8921 - acc: 0.6619 - val_loss: 661.9110 - val_acc: 0.6130\n",
            "Epoch 227/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 661.8754 - acc: 0.6455 - val_loss: 661.8794 - val_acc: 0.6130\n",
            "Epoch 228/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 661.8439 - acc: 0.6434 - val_loss: 661.8478 - val_acc: 0.6130\n",
            "Epoch 229/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 661.8182 - acc: 0.6311 - val_loss: 661.8162 - val_acc: 0.6130\n",
            "Epoch 230/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 661.7837 - acc: 0.6352 - val_loss: 661.7846 - val_acc: 0.6130\n",
            "Epoch 231/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 661.7532 - acc: 0.6025 - val_loss: 661.7530 - val_acc: 0.6130\n",
            "Epoch 232/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 661.7177 - acc: 0.6434 - val_loss: 661.7214 - val_acc: 0.6130\n",
            "Epoch 233/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 661.6787 - acc: 0.6619 - val_loss: 661.6900 - val_acc: 0.6130\n",
            "Epoch 234/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 661.6397 - acc: 0.6824 - val_loss: 661.6584 - val_acc: 0.6109\n",
            "Epoch 235/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 661.6221 - acc: 0.6537 - val_loss: 661.6267 - val_acc: 0.6109\n",
            "Epoch 236/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 661.5840 - acc: 0.6619 - val_loss: 661.5952 - val_acc: 0.6088\n",
            "Epoch 237/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 661.5546 - acc: 0.6598 - val_loss: 661.5636 - val_acc: 0.6109\n",
            "Epoch 238/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 661.5230 - acc: 0.6455 - val_loss: 661.5319 - val_acc: 0.6130\n",
            "Epoch 239/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 661.4934 - acc: 0.6352 - val_loss: 661.5004 - val_acc: 0.6130\n",
            "Epoch 240/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 661.4542 - acc: 0.6311 - val_loss: 661.4687 - val_acc: 0.6130\n",
            "Epoch 241/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 661.4248 - acc: 0.6598 - val_loss: 661.4371 - val_acc: 0.6151\n",
            "Epoch 242/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 661.3910 - acc: 0.6598 - val_loss: 661.4055 - val_acc: 0.6151\n",
            "Epoch 243/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 661.3632 - acc: 0.6557 - val_loss: 661.3740 - val_acc: 0.6172\n",
            "Epoch 244/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 661.3215 - acc: 0.6824 - val_loss: 661.3426 - val_acc: 0.6172\n",
            "Epoch 245/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 661.3005 - acc: 0.6578 - val_loss: 661.3111 - val_acc: 0.6192\n",
            "Epoch 246/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 661.2662 - acc: 0.6660 - val_loss: 661.2795 - val_acc: 0.6172\n",
            "Epoch 247/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 661.2322 - acc: 0.6516 - val_loss: 661.2478 - val_acc: 0.6192\n",
            "Epoch 248/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 661.2008 - acc: 0.6660 - val_loss: 661.2162 - val_acc: 0.6192\n",
            "Epoch 249/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 661.1762 - acc: 0.6701 - val_loss: 661.1847 - val_acc: 0.6192\n",
            "Epoch 250/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 661.1389 - acc: 0.6619 - val_loss: 661.1531 - val_acc: 0.6213\n",
            "Epoch 251/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 661.1129 - acc: 0.6434 - val_loss: 661.1215 - val_acc: 0.6213\n",
            "Epoch 252/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 661.0764 - acc: 0.6557 - val_loss: 661.0899 - val_acc: 0.6255\n",
            "Epoch 253/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 661.0580 - acc: 0.6516 - val_loss: 661.0584 - val_acc: 0.6234\n",
            "Epoch 254/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 661.0283 - acc: 0.6250 - val_loss: 661.0268 - val_acc: 0.6255\n",
            "Epoch 255/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 660.9981 - acc: 0.6516 - val_loss: 660.9953 - val_acc: 0.6255\n",
            "Epoch 256/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 660.9634 - acc: 0.6311 - val_loss: 660.9636 - val_acc: 0.6255\n",
            "Epoch 257/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 660.9263 - acc: 0.6434 - val_loss: 660.9321 - val_acc: 0.6234\n",
            "Epoch 258/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 660.8850 - acc: 0.6680 - val_loss: 660.9004 - val_acc: 0.6234\n",
            "Epoch 259/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 660.8743 - acc: 0.6086 - val_loss: 660.8689 - val_acc: 0.6192\n",
            "Epoch 260/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 660.8247 - acc: 0.6557 - val_loss: 660.8373 - val_acc: 0.6234\n",
            "Epoch 261/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 660.8027 - acc: 0.6352 - val_loss: 660.8057 - val_acc: 0.6234\n",
            "Epoch 262/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 660.7709 - acc: 0.6373 - val_loss: 660.7741 - val_acc: 0.6234\n",
            "Epoch 263/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 660.7210 - acc: 0.6926 - val_loss: 660.7425 - val_acc: 0.6213\n",
            "Epoch 264/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 660.7084 - acc: 0.6311 - val_loss: 660.7109 - val_acc: 0.6213\n",
            "Epoch 265/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 660.6742 - acc: 0.6270 - val_loss: 660.6794 - val_acc: 0.6192\n",
            "Epoch 266/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 660.6470 - acc: 0.6434 - val_loss: 660.6479 - val_acc: 0.6192\n",
            "Epoch 267/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 660.6002 - acc: 0.6865 - val_loss: 660.6163 - val_acc: 0.6213\n",
            "Epoch 268/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 660.5798 - acc: 0.6434 - val_loss: 660.5847 - val_acc: 0.6255\n",
            "Epoch 269/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 660.5448 - acc: 0.6393 - val_loss: 660.5531 - val_acc: 0.6234\n",
            "Epoch 270/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 660.5029 - acc: 0.6660 - val_loss: 660.5215 - val_acc: 0.6234\n",
            "Epoch 271/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 660.4794 - acc: 0.6557 - val_loss: 660.4899 - val_acc: 0.6234\n",
            "Epoch 272/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 660.4501 - acc: 0.6557 - val_loss: 660.4584 - val_acc: 0.6255\n",
            "Epoch 273/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 660.4114 - acc: 0.6762 - val_loss: 660.4270 - val_acc: 0.6255\n",
            "Epoch 274/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 660.3828 - acc: 0.6639 - val_loss: 660.3954 - val_acc: 0.6276\n",
            "Epoch 275/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 660.3430 - acc: 0.6619 - val_loss: 660.3638 - val_acc: 0.6276\n",
            "Epoch 276/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 660.3289 - acc: 0.6455 - val_loss: 660.3321 - val_acc: 0.6255\n",
            "Epoch 277/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 660.2871 - acc: 0.6762 - val_loss: 660.3006 - val_acc: 0.6255\n",
            "Epoch 278/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 660.2540 - acc: 0.6742 - val_loss: 660.2690 - val_acc: 0.6255\n",
            "Epoch 279/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 660.2413 - acc: 0.6332 - val_loss: 660.2374 - val_acc: 0.6255\n",
            "Epoch 280/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 660.1885 - acc: 0.6680 - val_loss: 660.2059 - val_acc: 0.6234\n",
            "Epoch 281/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 660.1688 - acc: 0.6721 - val_loss: 660.1743 - val_acc: 0.6234\n",
            "Epoch 282/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 660.1264 - acc: 0.6701 - val_loss: 660.1426 - val_acc: 0.6234\n",
            "Epoch 283/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 660.1075 - acc: 0.6373 - val_loss: 660.1113 - val_acc: 0.6213\n",
            "Epoch 284/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 660.0655 - acc: 0.6537 - val_loss: 660.0797 - val_acc: 0.6213\n",
            "Epoch 285/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 660.0510 - acc: 0.6393 - val_loss: 660.0481 - val_acc: 0.6213\n",
            "Epoch 286/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 660.0130 - acc: 0.6475 - val_loss: 660.0167 - val_acc: 0.6213\n",
            "Epoch 287/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 659.9810 - acc: 0.6496 - val_loss: 659.9851 - val_acc: 0.6213\n",
            "Epoch 288/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 659.9448 - acc: 0.6475 - val_loss: 659.9536 - val_acc: 0.6213\n",
            "Epoch 289/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 659.9193 - acc: 0.6332 - val_loss: 659.9220 - val_acc: 0.6213\n",
            "Epoch 290/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 659.8736 - acc: 0.6639 - val_loss: 659.8905 - val_acc: 0.6234\n",
            "Epoch 291/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 659.8452 - acc: 0.6783 - val_loss: 659.8589 - val_acc: 0.6234\n",
            "Epoch 292/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 659.8065 - acc: 0.6967 - val_loss: 659.8273 - val_acc: 0.6213\n",
            "Epoch 293/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 659.7867 - acc: 0.6578 - val_loss: 659.7956 - val_acc: 0.6213\n",
            "Epoch 294/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 659.7528 - acc: 0.6639 - val_loss: 659.7640 - val_acc: 0.6213\n",
            "Epoch 295/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 659.7214 - acc: 0.6578 - val_loss: 659.7325 - val_acc: 0.6213\n",
            "Epoch 296/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 659.6741 - acc: 0.6844 - val_loss: 659.7008 - val_acc: 0.6234\n",
            "Epoch 297/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 659.6525 - acc: 0.6639 - val_loss: 659.6693 - val_acc: 0.6234\n",
            "Epoch 298/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 659.6153 - acc: 0.6598 - val_loss: 659.6376 - val_acc: 0.6234\n",
            "Epoch 299/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 659.5901 - acc: 0.6701 - val_loss: 659.6061 - val_acc: 0.6234\n",
            "Epoch 300/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 659.5691 - acc: 0.6537 - val_loss: 659.5745 - val_acc: 0.6234\n",
            "Epoch 301/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 659.5294 - acc: 0.6639 - val_loss: 659.5429 - val_acc: 0.6234\n",
            "Epoch 302/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 659.5037 - acc: 0.6352 - val_loss: 659.5114 - val_acc: 0.6234\n",
            "Epoch 303/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 659.4713 - acc: 0.6516 - val_loss: 659.4797 - val_acc: 0.6234\n",
            "Epoch 304/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 659.4335 - acc: 0.6783 - val_loss: 659.4482 - val_acc: 0.6213\n",
            "Epoch 305/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 659.3947 - acc: 0.6844 - val_loss: 659.4166 - val_acc: 0.6234\n",
            "Epoch 306/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 659.3824 - acc: 0.6516 - val_loss: 659.3851 - val_acc: 0.6234\n",
            "Epoch 307/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 659.3561 - acc: 0.6209 - val_loss: 659.3534 - val_acc: 0.6234\n",
            "Epoch 308/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 659.3148 - acc: 0.6332 - val_loss: 659.3219 - val_acc: 0.6234\n",
            "Epoch 309/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 659.2866 - acc: 0.6332 - val_loss: 659.2902 - val_acc: 0.6213\n",
            "Epoch 310/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 659.2579 - acc: 0.6496 - val_loss: 659.2587 - val_acc: 0.6213\n",
            "Epoch 311/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 659.2270 - acc: 0.6475 - val_loss: 659.2271 - val_acc: 0.6234\n",
            "Epoch 312/3000\n",
            "488/488 [==============================] - 0s 308us/step - loss: 659.1885 - acc: 0.6619 - val_loss: 659.1956 - val_acc: 0.6234\n",
            "Epoch 313/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 659.1666 - acc: 0.6434 - val_loss: 659.1641 - val_acc: 0.6234\n",
            "Epoch 314/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 659.1334 - acc: 0.6127 - val_loss: 659.1325 - val_acc: 0.6255\n",
            "Epoch 315/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 659.0839 - acc: 0.6578 - val_loss: 659.1009 - val_acc: 0.6234\n",
            "Epoch 316/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 659.0633 - acc: 0.6598 - val_loss: 659.0693 - val_acc: 0.6255\n",
            "Epoch 317/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 659.0289 - acc: 0.6414 - val_loss: 659.0378 - val_acc: 0.6234\n",
            "Epoch 318/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 658.9995 - acc: 0.6475 - val_loss: 659.0064 - val_acc: 0.6234\n",
            "Epoch 319/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 658.9527 - acc: 0.6783 - val_loss: 658.9748 - val_acc: 0.6255\n",
            "Epoch 320/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 658.9275 - acc: 0.6660 - val_loss: 658.9432 - val_acc: 0.6255\n",
            "Epoch 321/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 658.9026 - acc: 0.6721 - val_loss: 658.9116 - val_acc: 0.6255\n",
            "Epoch 322/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 658.8587 - acc: 0.6926 - val_loss: 658.8800 - val_acc: 0.6255\n",
            "Epoch 323/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 658.8306 - acc: 0.6537 - val_loss: 658.8484 - val_acc: 0.6255\n",
            "Epoch 324/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 658.8019 - acc: 0.6598 - val_loss: 658.8168 - val_acc: 0.6255\n",
            "Epoch 325/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 658.7649 - acc: 0.6885 - val_loss: 658.7852 - val_acc: 0.6276\n",
            "Epoch 326/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 658.7445 - acc: 0.6619 - val_loss: 658.7537 - val_acc: 0.6276\n",
            "Epoch 327/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 658.7123 - acc: 0.6516 - val_loss: 658.7221 - val_acc: 0.6255\n",
            "Epoch 328/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 658.6888 - acc: 0.6516 - val_loss: 658.6906 - val_acc: 0.6234\n",
            "Epoch 329/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 658.6511 - acc: 0.6537 - val_loss: 658.6591 - val_acc: 0.6255\n",
            "Epoch 330/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 658.6201 - acc: 0.6475 - val_loss: 658.6275 - val_acc: 0.6255\n",
            "Epoch 331/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 658.5833 - acc: 0.6578 - val_loss: 658.5958 - val_acc: 0.6255\n",
            "Epoch 332/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 658.5544 - acc: 0.6619 - val_loss: 658.5642 - val_acc: 0.6255\n",
            "Epoch 333/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 658.5178 - acc: 0.6721 - val_loss: 658.5326 - val_acc: 0.6276\n",
            "Epoch 334/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 658.4862 - acc: 0.6496 - val_loss: 658.5010 - val_acc: 0.6255\n",
            "Epoch 335/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 658.4527 - acc: 0.6783 - val_loss: 658.4695 - val_acc: 0.6276\n",
            "Epoch 336/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 658.4231 - acc: 0.6803 - val_loss: 658.4379 - val_acc: 0.6276\n",
            "Epoch 337/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 658.3970 - acc: 0.6557 - val_loss: 658.4062 - val_acc: 0.6276\n",
            "Epoch 338/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 658.3613 - acc: 0.6598 - val_loss: 658.3747 - val_acc: 0.6297\n",
            "Epoch 339/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 658.3197 - acc: 0.6885 - val_loss: 658.3432 - val_acc: 0.6318\n",
            "Epoch 340/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 658.3006 - acc: 0.6475 - val_loss: 658.3116 - val_acc: 0.6318\n",
            "Epoch 341/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 658.2625 - acc: 0.6762 - val_loss: 658.2802 - val_acc: 0.6318\n",
            "Epoch 342/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 658.2373 - acc: 0.6414 - val_loss: 658.2486 - val_acc: 0.6318\n",
            "Epoch 343/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 658.2039 - acc: 0.6455 - val_loss: 658.2171 - val_acc: 0.6318\n",
            "Epoch 344/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 658.1766 - acc: 0.6455 - val_loss: 658.1855 - val_acc: 0.6297\n",
            "Epoch 345/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 658.1342 - acc: 0.6865 - val_loss: 658.1538 - val_acc: 0.6318\n",
            "Epoch 346/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 658.1114 - acc: 0.6557 - val_loss: 658.1222 - val_acc: 0.6297\n",
            "Epoch 347/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 658.0693 - acc: 0.6762 - val_loss: 658.0905 - val_acc: 0.6297\n",
            "Epoch 348/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 658.0351 - acc: 0.6639 - val_loss: 658.0591 - val_acc: 0.6318\n",
            "Epoch 349/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 658.0119 - acc: 0.6639 - val_loss: 658.0275 - val_acc: 0.6297\n",
            "Epoch 350/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 657.9878 - acc: 0.6475 - val_loss: 657.9959 - val_acc: 0.6318\n",
            "Epoch 351/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 657.9525 - acc: 0.6906 - val_loss: 657.9643 - val_acc: 0.6297\n",
            "Epoch 352/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 657.9272 - acc: 0.6660 - val_loss: 657.9327 - val_acc: 0.6297\n",
            "Epoch 353/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 657.8846 - acc: 0.6824 - val_loss: 657.9011 - val_acc: 0.6297\n",
            "Epoch 354/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 657.8529 - acc: 0.6762 - val_loss: 657.8695 - val_acc: 0.6297\n",
            "Epoch 355/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 657.8289 - acc: 0.6598 - val_loss: 657.8378 - val_acc: 0.6297\n",
            "Epoch 356/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 657.8019 - acc: 0.6414 - val_loss: 657.8063 - val_acc: 0.6318\n",
            "Epoch 357/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 657.7599 - acc: 0.6639 - val_loss: 657.7747 - val_acc: 0.6318\n",
            "Epoch 358/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 657.7189 - acc: 0.6742 - val_loss: 657.7431 - val_acc: 0.6318\n",
            "Epoch 359/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 657.6938 - acc: 0.6885 - val_loss: 657.7115 - val_acc: 0.6318\n",
            "Epoch 360/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 657.6697 - acc: 0.6598 - val_loss: 657.6798 - val_acc: 0.6339\n",
            "Epoch 361/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 657.6360 - acc: 0.6639 - val_loss: 657.6483 - val_acc: 0.6339\n",
            "Epoch 362/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 657.6103 - acc: 0.6434 - val_loss: 657.6168 - val_acc: 0.6339\n",
            "Epoch 363/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 657.5806 - acc: 0.6701 - val_loss: 657.5852 - val_acc: 0.6318\n",
            "Epoch 364/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 657.5424 - acc: 0.6680 - val_loss: 657.5536 - val_acc: 0.6318\n",
            "Epoch 365/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 657.4992 - acc: 0.6742 - val_loss: 657.5222 - val_acc: 0.6318\n",
            "Epoch 366/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 657.4827 - acc: 0.6660 - val_loss: 657.4906 - val_acc: 0.6339\n",
            "Epoch 367/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 657.4481 - acc: 0.6660 - val_loss: 657.4590 - val_acc: 0.6339\n",
            "Epoch 368/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 657.4226 - acc: 0.6496 - val_loss: 657.4273 - val_acc: 0.6339\n",
            "Epoch 369/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 657.3813 - acc: 0.6885 - val_loss: 657.3957 - val_acc: 0.6339\n",
            "Epoch 370/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 657.3525 - acc: 0.6619 - val_loss: 657.3641 - val_acc: 0.6339\n",
            "Epoch 371/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 657.3265 - acc: 0.6598 - val_loss: 657.3324 - val_acc: 0.6339\n",
            "Epoch 372/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 657.2881 - acc: 0.6578 - val_loss: 657.3008 - val_acc: 0.6339\n",
            "Epoch 373/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 657.2473 - acc: 0.6885 - val_loss: 657.2692 - val_acc: 0.6339\n",
            "Epoch 374/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 657.2165 - acc: 0.6844 - val_loss: 657.2376 - val_acc: 0.6506\n",
            "Epoch 375/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 657.1836 - acc: 0.6926 - val_loss: 657.2059 - val_acc: 0.6506\n",
            "Epoch 376/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 657.1628 - acc: 0.6537 - val_loss: 657.1744 - val_acc: 0.6506\n",
            "Epoch 377/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 657.1290 - acc: 0.6906 - val_loss: 657.1427 - val_acc: 0.6506\n",
            "Epoch 378/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 657.1078 - acc: 0.6455 - val_loss: 657.1110 - val_acc: 0.6485\n",
            "Epoch 379/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 657.0651 - acc: 0.6537 - val_loss: 657.0794 - val_acc: 0.6485\n",
            "Epoch 380/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 657.0352 - acc: 0.6537 - val_loss: 657.0479 - val_acc: 0.6485\n",
            "Epoch 381/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 657.0045 - acc: 0.6332 - val_loss: 657.0164 - val_acc: 0.6485\n",
            "Epoch 382/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 656.9673 - acc: 0.6598 - val_loss: 656.9846 - val_acc: 0.6485\n",
            "Epoch 383/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 656.9379 - acc: 0.6988 - val_loss: 656.9531 - val_acc: 0.6506\n",
            "Epoch 384/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 656.9106 - acc: 0.6598 - val_loss: 656.9214 - val_acc: 0.6506\n",
            "Epoch 385/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 656.8808 - acc: 0.6762 - val_loss: 656.8899 - val_acc: 0.6506\n",
            "Epoch 386/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 656.8553 - acc: 0.6455 - val_loss: 656.8583 - val_acc: 0.6506\n",
            "Epoch 387/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 656.8134 - acc: 0.6824 - val_loss: 656.8267 - val_acc: 0.6527\n",
            "Epoch 388/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 656.7608 - acc: 0.7070 - val_loss: 656.7951 - val_acc: 0.6527\n",
            "Epoch 389/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 656.7512 - acc: 0.6660 - val_loss: 656.7635 - val_acc: 0.6527\n",
            "Epoch 390/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 656.7103 - acc: 0.6598 - val_loss: 656.7320 - val_acc: 0.6548\n",
            "Epoch 391/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 656.6834 - acc: 0.6803 - val_loss: 656.7004 - val_acc: 0.6548\n",
            "Epoch 392/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 656.6679 - acc: 0.6660 - val_loss: 656.6688 - val_acc: 0.6548\n",
            "Epoch 393/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 656.6287 - acc: 0.6701 - val_loss: 656.6371 - val_acc: 0.6548\n",
            "Epoch 394/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 656.6001 - acc: 0.6578 - val_loss: 656.6055 - val_acc: 0.6548\n",
            "Epoch 395/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 656.5536 - acc: 0.6762 - val_loss: 656.5740 - val_acc: 0.6548\n",
            "Epoch 396/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 656.5290 - acc: 0.6619 - val_loss: 656.5424 - val_acc: 0.6527\n",
            "Epoch 397/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 656.4964 - acc: 0.6742 - val_loss: 656.5110 - val_acc: 0.6527\n",
            "Epoch 398/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 656.4579 - acc: 0.6926 - val_loss: 656.4793 - val_acc: 0.6527\n",
            "Epoch 399/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 656.4394 - acc: 0.6578 - val_loss: 656.4478 - val_acc: 0.6527\n",
            "Epoch 400/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 656.3911 - acc: 0.6824 - val_loss: 656.4163 - val_acc: 0.6527\n",
            "Epoch 401/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 656.3668 - acc: 0.6906 - val_loss: 656.3847 - val_acc: 0.6548\n",
            "Epoch 402/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 656.3394 - acc: 0.6516 - val_loss: 656.3532 - val_acc: 0.6527\n",
            "Epoch 403/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 656.3067 - acc: 0.6660 - val_loss: 656.3215 - val_acc: 0.6548\n",
            "Epoch 404/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 656.2773 - acc: 0.6660 - val_loss: 656.2899 - val_acc: 0.6548\n",
            "Epoch 405/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 656.2524 - acc: 0.6578 - val_loss: 656.2584 - val_acc: 0.6527\n",
            "Epoch 406/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 656.2134 - acc: 0.6680 - val_loss: 656.2269 - val_acc: 0.6506\n",
            "Epoch 407/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 656.1941 - acc: 0.6455 - val_loss: 656.1952 - val_acc: 0.6506\n",
            "Epoch 408/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 656.1361 - acc: 0.6742 - val_loss: 656.1638 - val_acc: 0.6527\n",
            "Epoch 409/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 656.1238 - acc: 0.6557 - val_loss: 656.1322 - val_acc: 0.6527\n",
            "Epoch 410/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 656.0803 - acc: 0.6742 - val_loss: 656.1007 - val_acc: 0.6527\n",
            "Epoch 411/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 656.0583 - acc: 0.6844 - val_loss: 656.0690 - val_acc: 0.6527\n",
            "Epoch 412/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 656.0193 - acc: 0.6701 - val_loss: 656.0375 - val_acc: 0.6506\n",
            "Epoch 413/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 655.9929 - acc: 0.6578 - val_loss: 656.0058 - val_acc: 0.6527\n",
            "Epoch 414/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 655.9625 - acc: 0.6598 - val_loss: 655.9740 - val_acc: 0.6527\n",
            "Epoch 415/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 655.9385 - acc: 0.6660 - val_loss: 655.9425 - val_acc: 0.6506\n",
            "Epoch 416/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 655.9095 - acc: 0.6393 - val_loss: 655.9109 - val_acc: 0.6506\n",
            "Epoch 417/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 655.8811 - acc: 0.6455 - val_loss: 655.8793 - val_acc: 0.6506\n",
            "Epoch 418/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 655.8339 - acc: 0.6824 - val_loss: 655.8477 - val_acc: 0.6548\n",
            "Epoch 419/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 655.7952 - acc: 0.6701 - val_loss: 655.8162 - val_acc: 0.6548\n",
            "Epoch 420/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 655.7673 - acc: 0.6803 - val_loss: 655.7845 - val_acc: 0.6548\n",
            "Epoch 421/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 655.7404 - acc: 0.6824 - val_loss: 655.7530 - val_acc: 0.6548\n",
            "Epoch 422/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 655.7031 - acc: 0.6701 - val_loss: 655.7214 - val_acc: 0.6569\n",
            "Epoch 423/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 655.6789 - acc: 0.6475 - val_loss: 655.6897 - val_acc: 0.6569\n",
            "Epoch 424/3000\n",
            "488/488 [==============================] - 0s 371us/step - loss: 655.6339 - acc: 0.6844 - val_loss: 655.6582 - val_acc: 0.6569\n",
            "Epoch 425/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 655.6104 - acc: 0.6742 - val_loss: 655.6266 - val_acc: 0.6611\n",
            "Epoch 426/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 655.5816 - acc: 0.6598 - val_loss: 655.5951 - val_acc: 0.6611\n",
            "Epoch 427/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 655.5519 - acc: 0.6455 - val_loss: 655.5636 - val_acc: 0.6611\n",
            "Epoch 428/3000\n",
            "488/488 [==============================] - 0s 319us/step - loss: 655.5108 - acc: 0.7008 - val_loss: 655.5319 - val_acc: 0.6611\n",
            "Epoch 429/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 655.4729 - acc: 0.7090 - val_loss: 655.5004 - val_acc: 0.6611\n",
            "Epoch 430/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 655.4412 - acc: 0.7008 - val_loss: 655.4688 - val_acc: 0.6590\n",
            "Epoch 431/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 655.4198 - acc: 0.6844 - val_loss: 655.4372 - val_acc: 0.6590\n",
            "Epoch 432/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 655.3874 - acc: 0.6701 - val_loss: 655.4056 - val_acc: 0.6590\n",
            "Epoch 433/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 655.3532 - acc: 0.6742 - val_loss: 655.3741 - val_acc: 0.6590\n",
            "Epoch 434/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 655.3152 - acc: 0.6824 - val_loss: 655.3426 - val_acc: 0.6590\n",
            "Epoch 435/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 655.2953 - acc: 0.6865 - val_loss: 655.3111 - val_acc: 0.6590\n",
            "Epoch 436/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 655.2710 - acc: 0.6783 - val_loss: 655.2795 - val_acc: 0.6569\n",
            "Epoch 437/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 655.2178 - acc: 0.6947 - val_loss: 655.2479 - val_acc: 0.6590\n",
            "Epoch 438/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 655.2011 - acc: 0.6721 - val_loss: 655.2164 - val_acc: 0.6527\n",
            "Epoch 439/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 655.1666 - acc: 0.6598 - val_loss: 655.1849 - val_acc: 0.6590\n",
            "Epoch 440/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 655.1335 - acc: 0.6721 - val_loss: 655.1533 - val_acc: 0.6611\n",
            "Epoch 441/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 655.1083 - acc: 0.6639 - val_loss: 655.1218 - val_acc: 0.6569\n",
            "Epoch 442/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 655.0797 - acc: 0.6475 - val_loss: 655.0902 - val_acc: 0.6569\n",
            "Epoch 443/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 655.0591 - acc: 0.6639 - val_loss: 655.0587 - val_acc: 0.6590\n",
            "Epoch 444/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 655.0082 - acc: 0.6844 - val_loss: 655.0270 - val_acc: 0.6590\n",
            "Epoch 445/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 654.9795 - acc: 0.6598 - val_loss: 654.9954 - val_acc: 0.6590\n",
            "Epoch 446/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 654.9558 - acc: 0.6496 - val_loss: 654.9639 - val_acc: 0.6611\n",
            "Epoch 447/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 654.9091 - acc: 0.6967 - val_loss: 654.9323 - val_acc: 0.6611\n",
            "Epoch 448/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 654.8808 - acc: 0.6844 - val_loss: 654.9010 - val_acc: 0.6590\n",
            "Epoch 449/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 654.8572 - acc: 0.6680 - val_loss: 654.8694 - val_acc: 0.6611\n",
            "Epoch 450/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 654.8157 - acc: 0.7049 - val_loss: 654.8378 - val_acc: 0.6569\n",
            "Epoch 451/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 654.7857 - acc: 0.6803 - val_loss: 654.8063 - val_acc: 0.6548\n",
            "Epoch 452/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 654.7725 - acc: 0.6537 - val_loss: 654.7747 - val_acc: 0.6611\n",
            "Epoch 453/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 654.7213 - acc: 0.6824 - val_loss: 654.7431 - val_acc: 0.6611\n",
            "Epoch 454/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 654.6998 - acc: 0.6701 - val_loss: 654.7114 - val_acc: 0.6611\n",
            "Epoch 455/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 654.6731 - acc: 0.6680 - val_loss: 654.6799 - val_acc: 0.6590\n",
            "Epoch 456/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 654.6299 - acc: 0.6639 - val_loss: 654.6484 - val_acc: 0.6590\n",
            "Epoch 457/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 654.6060 - acc: 0.6434 - val_loss: 654.6168 - val_acc: 0.6611\n",
            "Epoch 458/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 654.5623 - acc: 0.6742 - val_loss: 654.5852 - val_acc: 0.6611\n",
            "Epoch 459/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 654.5462 - acc: 0.6557 - val_loss: 654.5538 - val_acc: 0.6590\n",
            "Epoch 460/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 654.5048 - acc: 0.6660 - val_loss: 654.5222 - val_acc: 0.6590\n",
            "Epoch 461/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 654.4807 - acc: 0.6578 - val_loss: 654.4907 - val_acc: 0.6590\n",
            "Epoch 462/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 654.4428 - acc: 0.6701 - val_loss: 654.4593 - val_acc: 0.6590\n",
            "Epoch 463/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 654.4164 - acc: 0.6803 - val_loss: 654.4277 - val_acc: 0.6611\n",
            "Epoch 464/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 654.3823 - acc: 0.6660 - val_loss: 654.3961 - val_acc: 0.6590\n",
            "Epoch 465/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 654.3486 - acc: 0.6844 - val_loss: 654.3647 - val_acc: 0.6569\n",
            "Epoch 466/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 654.3091 - acc: 0.6824 - val_loss: 654.3331 - val_acc: 0.6611\n",
            "Epoch 467/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 654.2700 - acc: 0.7172 - val_loss: 654.3016 - val_acc: 0.6590\n",
            "Epoch 468/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 654.2574 - acc: 0.6988 - val_loss: 654.2701 - val_acc: 0.6590\n",
            "Epoch 469/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 654.2276 - acc: 0.6660 - val_loss: 654.2385 - val_acc: 0.6590\n",
            "Epoch 470/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 654.1860 - acc: 0.6947 - val_loss: 654.2069 - val_acc: 0.6590\n",
            "Epoch 471/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 654.1516 - acc: 0.6701 - val_loss: 654.1754 - val_acc: 0.6611\n",
            "Epoch 472/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 654.1023 - acc: 0.7193 - val_loss: 654.1437 - val_acc: 0.6611\n",
            "Epoch 473/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 654.0891 - acc: 0.6639 - val_loss: 654.1122 - val_acc: 0.6611\n",
            "Epoch 474/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 654.0586 - acc: 0.6701 - val_loss: 654.0806 - val_acc: 0.6590\n",
            "Epoch 475/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 654.0344 - acc: 0.6885 - val_loss: 654.0491 - val_acc: 0.6590\n",
            "Epoch 476/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 654.0018 - acc: 0.6783 - val_loss: 654.0176 - val_acc: 0.6569\n",
            "Epoch 477/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 653.9596 - acc: 0.6988 - val_loss: 653.9860 - val_acc: 0.6590\n",
            "Epoch 478/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 653.9372 - acc: 0.6783 - val_loss: 653.9543 - val_acc: 0.6611\n",
            "Epoch 479/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 653.9090 - acc: 0.6742 - val_loss: 653.9228 - val_acc: 0.6611\n",
            "Epoch 480/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 653.8686 - acc: 0.6865 - val_loss: 653.8912 - val_acc: 0.6611\n",
            "Epoch 481/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 653.8457 - acc: 0.6598 - val_loss: 653.8598 - val_acc: 0.6611\n",
            "Epoch 482/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 653.8080 - acc: 0.6762 - val_loss: 653.8283 - val_acc: 0.6590\n",
            "Epoch 483/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 653.7723 - acc: 0.6885 - val_loss: 653.7967 - val_acc: 0.6590\n",
            "Epoch 484/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 653.7544 - acc: 0.6639 - val_loss: 653.7653 - val_acc: 0.6548\n",
            "Epoch 485/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 653.7138 - acc: 0.6865 - val_loss: 653.7338 - val_acc: 0.6590\n",
            "Epoch 486/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 653.6826 - acc: 0.6967 - val_loss: 653.7022 - val_acc: 0.6548\n",
            "Epoch 487/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 653.6469 - acc: 0.6967 - val_loss: 653.6705 - val_acc: 0.6548\n",
            "Epoch 488/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 653.6160 - acc: 0.6865 - val_loss: 653.6390 - val_acc: 0.6548\n",
            "Epoch 489/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 653.5886 - acc: 0.6742 - val_loss: 653.6076 - val_acc: 0.6569\n",
            "Epoch 490/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 653.5459 - acc: 0.7131 - val_loss: 653.5760 - val_acc: 0.6569\n",
            "Epoch 491/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 653.5211 - acc: 0.7008 - val_loss: 653.5443 - val_acc: 0.6569\n",
            "Epoch 492/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 653.4968 - acc: 0.6742 - val_loss: 653.5128 - val_acc: 0.6569\n",
            "Epoch 493/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 653.4638 - acc: 0.6906 - val_loss: 653.4810 - val_acc: 0.6548\n",
            "Epoch 494/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 653.4312 - acc: 0.6947 - val_loss: 653.4495 - val_acc: 0.6548\n",
            "Epoch 495/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 653.4044 - acc: 0.6762 - val_loss: 653.4179 - val_acc: 0.6548\n",
            "Epoch 496/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 653.3670 - acc: 0.6844 - val_loss: 653.3865 - val_acc: 0.6590\n",
            "Epoch 497/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 653.3390 - acc: 0.6742 - val_loss: 653.3549 - val_acc: 0.6590\n",
            "Epoch 498/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 653.3115 - acc: 0.6578 - val_loss: 653.3235 - val_acc: 0.6590\n",
            "Epoch 499/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 653.2622 - acc: 0.6824 - val_loss: 653.2919 - val_acc: 0.6590\n",
            "Epoch 500/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 653.2334 - acc: 0.7131 - val_loss: 653.2604 - val_acc: 0.6590\n",
            "Epoch 501/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 653.2106 - acc: 0.6783 - val_loss: 653.2289 - val_acc: 0.6590\n",
            "Epoch 502/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 653.1772 - acc: 0.6660 - val_loss: 653.1972 - val_acc: 0.6590\n",
            "Epoch 503/3000\n",
            "488/488 [==============================] - 0s 376us/step - loss: 653.1590 - acc: 0.6537 - val_loss: 653.1658 - val_acc: 0.6611\n",
            "Epoch 504/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 653.1206 - acc: 0.6803 - val_loss: 653.1344 - val_acc: 0.6527\n",
            "Epoch 505/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 653.0867 - acc: 0.6742 - val_loss: 653.1028 - val_acc: 0.6527\n",
            "Epoch 506/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 653.0492 - acc: 0.6844 - val_loss: 653.0712 - val_acc: 0.6548\n",
            "Epoch 507/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 653.0281 - acc: 0.6660 - val_loss: 653.0398 - val_acc: 0.6569\n",
            "Epoch 508/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 652.9922 - acc: 0.6721 - val_loss: 653.0081 - val_acc: 0.6569\n",
            "Epoch 509/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 652.9504 - acc: 0.6742 - val_loss: 652.9766 - val_acc: 0.6569\n",
            "Epoch 510/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 652.9212 - acc: 0.6988 - val_loss: 652.9451 - val_acc: 0.6548\n",
            "Epoch 511/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 652.8975 - acc: 0.6762 - val_loss: 652.9135 - val_acc: 0.6548\n",
            "Epoch 512/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 652.8616 - acc: 0.7008 - val_loss: 652.8821 - val_acc: 0.6548\n",
            "Epoch 513/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 652.8247 - acc: 0.6988 - val_loss: 652.8505 - val_acc: 0.6569\n",
            "Epoch 514/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 652.7976 - acc: 0.6926 - val_loss: 652.8190 - val_acc: 0.6590\n",
            "Epoch 515/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 652.7670 - acc: 0.6906 - val_loss: 652.7875 - val_acc: 0.6590\n",
            "Epoch 516/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 652.7248 - acc: 0.6967 - val_loss: 652.7560 - val_acc: 0.6569\n",
            "Epoch 517/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 652.7039 - acc: 0.6783 - val_loss: 652.7244 - val_acc: 0.6590\n",
            "Epoch 518/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 652.6713 - acc: 0.7029 - val_loss: 652.6929 - val_acc: 0.6611\n",
            "Epoch 519/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 652.6246 - acc: 0.7234 - val_loss: 652.6614 - val_acc: 0.6569\n",
            "Epoch 520/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 652.6001 - acc: 0.6988 - val_loss: 652.6298 - val_acc: 0.6590\n",
            "Epoch 521/3000\n",
            "488/488 [==============================] - 0s 373us/step - loss: 652.5680 - acc: 0.6926 - val_loss: 652.5982 - val_acc: 0.6569\n",
            "Epoch 522/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 652.5504 - acc: 0.6742 - val_loss: 652.5666 - val_acc: 0.6569\n",
            "Epoch 523/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 652.5173 - acc: 0.6742 - val_loss: 652.5351 - val_acc: 0.6569\n",
            "Epoch 524/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 652.4828 - acc: 0.6783 - val_loss: 652.5035 - val_acc: 0.6569\n",
            "Epoch 525/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 652.4527 - acc: 0.6967 - val_loss: 652.4719 - val_acc: 0.6590\n",
            "Epoch 526/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 652.4170 - acc: 0.6988 - val_loss: 652.4405 - val_acc: 0.6632\n",
            "Epoch 527/3000\n",
            "488/488 [==============================] - 0s 376us/step - loss: 652.3842 - acc: 0.6926 - val_loss: 652.4090 - val_acc: 0.6632\n",
            "Epoch 528/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 652.3565 - acc: 0.6639 - val_loss: 652.3775 - val_acc: 0.6632\n",
            "Epoch 529/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 652.3328 - acc: 0.6803 - val_loss: 652.3460 - val_acc: 0.6653\n",
            "Epoch 530/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 652.2844 - acc: 0.7234 - val_loss: 652.3144 - val_acc: 0.6653\n",
            "Epoch 531/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 652.2511 - acc: 0.7049 - val_loss: 652.2829 - val_acc: 0.6653\n",
            "Epoch 532/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 652.2304 - acc: 0.6947 - val_loss: 652.2515 - val_acc: 0.6653\n",
            "Epoch 533/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 652.2007 - acc: 0.6926 - val_loss: 652.2200 - val_acc: 0.6653\n",
            "Epoch 534/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 652.1695 - acc: 0.6885 - val_loss: 652.1885 - val_acc: 0.6632\n",
            "Epoch 535/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 652.1355 - acc: 0.6926 - val_loss: 652.1570 - val_acc: 0.6632\n",
            "Epoch 536/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 652.0996 - acc: 0.6906 - val_loss: 652.1255 - val_acc: 0.6653\n",
            "Epoch 537/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 652.0765 - acc: 0.6865 - val_loss: 652.0940 - val_acc: 0.6611\n",
            "Epoch 538/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 652.0385 - acc: 0.6844 - val_loss: 652.0625 - val_acc: 0.6611\n",
            "Epoch 539/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 652.0086 - acc: 0.6865 - val_loss: 652.0309 - val_acc: 0.6632\n",
            "Epoch 540/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 651.9870 - acc: 0.6783 - val_loss: 651.9994 - val_acc: 0.6632\n",
            "Epoch 541/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 651.9486 - acc: 0.6947 - val_loss: 651.9680 - val_acc: 0.6632\n",
            "Epoch 542/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 651.9133 - acc: 0.7070 - val_loss: 651.9366 - val_acc: 0.6632\n",
            "Epoch 543/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 651.8833 - acc: 0.6906 - val_loss: 651.9051 - val_acc: 0.6632\n",
            "Epoch 544/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 651.8467 - acc: 0.6906 - val_loss: 651.8736 - val_acc: 0.6611\n",
            "Epoch 545/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 651.8229 - acc: 0.6680 - val_loss: 651.8420 - val_acc: 0.6611\n",
            "Epoch 546/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 651.7991 - acc: 0.6824 - val_loss: 651.8106 - val_acc: 0.6632\n",
            "Epoch 547/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 651.7607 - acc: 0.6885 - val_loss: 651.7790 - val_acc: 0.6632\n",
            "Epoch 548/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 651.7302 - acc: 0.6742 - val_loss: 651.7476 - val_acc: 0.6632\n",
            "Epoch 549/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 651.6907 - acc: 0.6680 - val_loss: 651.7161 - val_acc: 0.6632\n",
            "Epoch 550/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 651.6660 - acc: 0.6742 - val_loss: 651.6846 - val_acc: 0.6611\n",
            "Epoch 551/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 651.6219 - acc: 0.6967 - val_loss: 651.6530 - val_acc: 0.6611\n",
            "Epoch 552/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 651.5959 - acc: 0.7090 - val_loss: 651.6217 - val_acc: 0.6611\n",
            "Epoch 553/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 651.5706 - acc: 0.6926 - val_loss: 651.5901 - val_acc: 0.6611\n",
            "Epoch 554/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 651.5393 - acc: 0.6947 - val_loss: 651.5586 - val_acc: 0.6632\n",
            "Epoch 555/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 651.5012 - acc: 0.7029 - val_loss: 651.5271 - val_acc: 0.6632\n",
            "Epoch 556/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 651.4774 - acc: 0.6844 - val_loss: 651.4957 - val_acc: 0.6611\n",
            "Epoch 557/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 651.4446 - acc: 0.6844 - val_loss: 651.4642 - val_acc: 0.6632\n",
            "Epoch 558/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 651.4098 - acc: 0.6865 - val_loss: 651.4328 - val_acc: 0.6632\n",
            "Epoch 559/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 651.3867 - acc: 0.6967 - val_loss: 651.4013 - val_acc: 0.6632\n",
            "Epoch 560/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 651.3463 - acc: 0.6783 - val_loss: 651.3697 - val_acc: 0.6632\n",
            "Epoch 561/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 651.3179 - acc: 0.6783 - val_loss: 651.3383 - val_acc: 0.6632\n",
            "Epoch 562/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 651.2926 - acc: 0.6824 - val_loss: 651.3068 - val_acc: 0.6632\n",
            "Epoch 563/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 651.2371 - acc: 0.7090 - val_loss: 651.2752 - val_acc: 0.6632\n",
            "Epoch 564/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 651.2224 - acc: 0.6906 - val_loss: 651.2438 - val_acc: 0.6653\n",
            "Epoch 565/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 651.1887 - acc: 0.6967 - val_loss: 651.2124 - val_acc: 0.6632\n",
            "Epoch 566/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 651.1535 - acc: 0.6783 - val_loss: 651.1808 - val_acc: 0.6653\n",
            "Epoch 567/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 651.1283 - acc: 0.6844 - val_loss: 651.1494 - val_acc: 0.6653\n",
            "Epoch 568/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 651.0920 - acc: 0.6988 - val_loss: 651.1178 - val_acc: 0.6653\n",
            "Epoch 569/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 651.0660 - acc: 0.6783 - val_loss: 651.0864 - val_acc: 0.6653\n",
            "Epoch 570/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 651.0263 - acc: 0.6947 - val_loss: 651.0548 - val_acc: 0.6632\n",
            "Epoch 571/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 651.0061 - acc: 0.6865 - val_loss: 651.0234 - val_acc: 0.6632\n",
            "Epoch 572/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 650.9584 - acc: 0.6803 - val_loss: 650.9919 - val_acc: 0.6632\n",
            "Epoch 573/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 650.9441 - acc: 0.6680 - val_loss: 650.9603 - val_acc: 0.6632\n",
            "Epoch 574/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 650.9011 - acc: 0.6803 - val_loss: 650.9289 - val_acc: 0.6632\n",
            "Epoch 575/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 650.8754 - acc: 0.6885 - val_loss: 650.8973 - val_acc: 0.6653\n",
            "Epoch 576/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 650.8391 - acc: 0.6947 - val_loss: 650.8659 - val_acc: 0.6653\n",
            "Epoch 577/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 650.8119 - acc: 0.6926 - val_loss: 650.8345 - val_acc: 0.6653\n",
            "Epoch 578/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 650.7923 - acc: 0.6844 - val_loss: 650.8031 - val_acc: 0.6653\n",
            "Epoch 579/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 650.7441 - acc: 0.7131 - val_loss: 650.7717 - val_acc: 0.6632\n",
            "Epoch 580/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 650.7217 - acc: 0.6844 - val_loss: 650.7402 - val_acc: 0.6632\n",
            "Epoch 581/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 650.6835 - acc: 0.7049 - val_loss: 650.7087 - val_acc: 0.6632\n",
            "Epoch 582/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 650.6515 - acc: 0.7193 - val_loss: 650.6772 - val_acc: 0.6632\n",
            "Epoch 583/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 650.6304 - acc: 0.6803 - val_loss: 650.6458 - val_acc: 0.6632\n",
            "Epoch 584/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 650.5859 - acc: 0.7111 - val_loss: 650.6143 - val_acc: 0.6653\n",
            "Epoch 585/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 650.5535 - acc: 0.6967 - val_loss: 650.5828 - val_acc: 0.6653\n",
            "Epoch 586/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 650.5299 - acc: 0.6803 - val_loss: 650.5512 - val_acc: 0.6653\n",
            "Epoch 587/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 650.4882 - acc: 0.6762 - val_loss: 650.5198 - val_acc: 0.6632\n",
            "Epoch 588/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 650.4652 - acc: 0.6967 - val_loss: 650.4884 - val_acc: 0.6653\n",
            "Epoch 589/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 650.4232 - acc: 0.7254 - val_loss: 650.4569 - val_acc: 0.6653\n",
            "Epoch 590/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 650.4135 - acc: 0.6721 - val_loss: 650.4253 - val_acc: 0.6653\n",
            "Epoch 591/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 650.3628 - acc: 0.7008 - val_loss: 650.3939 - val_acc: 0.6653\n",
            "Epoch 592/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 650.3499 - acc: 0.6742 - val_loss: 650.3624 - val_acc: 0.6653\n",
            "Epoch 593/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 650.3064 - acc: 0.7070 - val_loss: 650.3309 - val_acc: 0.6653\n",
            "Epoch 594/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 650.2809 - acc: 0.6885 - val_loss: 650.2993 - val_acc: 0.6653\n",
            "Epoch 595/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 650.2526 - acc: 0.6803 - val_loss: 650.2678 - val_acc: 0.6653\n",
            "Epoch 596/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 650.2033 - acc: 0.6824 - val_loss: 650.2365 - val_acc: 0.6653\n",
            "Epoch 597/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 650.1778 - acc: 0.6783 - val_loss: 650.2051 - val_acc: 0.6632\n",
            "Epoch 598/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 650.1399 - acc: 0.7111 - val_loss: 650.1735 - val_acc: 0.6653\n",
            "Epoch 599/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 650.1207 - acc: 0.6988 - val_loss: 650.1420 - val_acc: 0.6653\n",
            "Epoch 600/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 650.0831 - acc: 0.6885 - val_loss: 650.1107 - val_acc: 0.6653\n",
            "Epoch 601/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 650.0630 - acc: 0.6639 - val_loss: 650.0792 - val_acc: 0.6653\n",
            "Epoch 602/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 650.0189 - acc: 0.7193 - val_loss: 650.0477 - val_acc: 0.6653\n",
            "Epoch 603/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 649.9903 - acc: 0.6906 - val_loss: 650.0161 - val_acc: 0.6653\n",
            "Epoch 604/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 649.9635 - acc: 0.6803 - val_loss: 649.9845 - val_acc: 0.6653\n",
            "Epoch 605/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 649.9217 - acc: 0.7152 - val_loss: 649.9531 - val_acc: 0.6653\n",
            "Epoch 606/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 649.8953 - acc: 0.7029 - val_loss: 649.9217 - val_acc: 0.6653\n",
            "Epoch 607/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 649.8760 - acc: 0.6865 - val_loss: 649.8902 - val_acc: 0.6653\n",
            "Epoch 608/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 649.8306 - acc: 0.6926 - val_loss: 649.8587 - val_acc: 0.6653\n",
            "Epoch 609/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 649.7966 - acc: 0.7111 - val_loss: 649.8273 - val_acc: 0.6653\n",
            "Epoch 610/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 649.7731 - acc: 0.6947 - val_loss: 649.7959 - val_acc: 0.6632\n",
            "Epoch 611/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 649.7518 - acc: 0.6926 - val_loss: 649.7645 - val_acc: 0.6632\n",
            "Epoch 612/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 649.7102 - acc: 0.7008 - val_loss: 649.7331 - val_acc: 0.6611\n",
            "Epoch 613/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 649.6774 - acc: 0.7090 - val_loss: 649.7016 - val_acc: 0.6611\n",
            "Epoch 614/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 649.6467 - acc: 0.7111 - val_loss: 649.6701 - val_acc: 0.6611\n",
            "Epoch 615/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 649.6139 - acc: 0.6885 - val_loss: 649.6387 - val_acc: 0.6611\n",
            "Epoch 616/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 649.5831 - acc: 0.6844 - val_loss: 649.6072 - val_acc: 0.6611\n",
            "Epoch 617/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 649.5492 - acc: 0.6988 - val_loss: 649.5758 - val_acc: 0.6632\n",
            "Epoch 618/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 649.5121 - acc: 0.7070 - val_loss: 649.5441 - val_acc: 0.6632\n",
            "Epoch 619/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 649.4902 - acc: 0.6865 - val_loss: 649.5126 - val_acc: 0.6632\n",
            "Epoch 620/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 649.4574 - acc: 0.7029 - val_loss: 649.4811 - val_acc: 0.6632\n",
            "Epoch 621/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 649.4145 - acc: 0.7008 - val_loss: 649.4496 - val_acc: 0.6632\n",
            "Epoch 622/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 649.3905 - acc: 0.6844 - val_loss: 649.4181 - val_acc: 0.6632\n",
            "Epoch 623/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 649.3576 - acc: 0.7234 - val_loss: 649.3868 - val_acc: 0.6632\n",
            "Epoch 624/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 649.3380 - acc: 0.6844 - val_loss: 649.3552 - val_acc: 0.6632\n",
            "Epoch 625/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 649.3011 - acc: 0.6865 - val_loss: 649.3238 - val_acc: 0.6632\n",
            "Epoch 626/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 649.2688 - acc: 0.7029 - val_loss: 649.2924 - val_acc: 0.6632\n",
            "Epoch 627/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 649.2296 - acc: 0.7172 - val_loss: 649.2609 - val_acc: 0.6632\n",
            "Epoch 628/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 649.2055 - acc: 0.7008 - val_loss: 649.2295 - val_acc: 0.6632\n",
            "Epoch 629/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 649.1679 - acc: 0.6947 - val_loss: 649.1980 - val_acc: 0.6632\n",
            "Epoch 630/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 649.1440 - acc: 0.6947 - val_loss: 649.1666 - val_acc: 0.6632\n",
            "Epoch 631/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 649.1056 - acc: 0.7049 - val_loss: 649.1353 - val_acc: 0.6632\n",
            "Epoch 632/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 649.0832 - acc: 0.6721 - val_loss: 649.1037 - val_acc: 0.6632\n",
            "Epoch 633/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 649.0282 - acc: 0.7254 - val_loss: 649.0723 - val_acc: 0.6632\n",
            "Epoch 634/3000\n",
            "488/488 [==============================] - 0s 319us/step - loss: 649.0209 - acc: 0.7152 - val_loss: 649.0409 - val_acc: 0.6632\n",
            "Epoch 635/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 648.9780 - acc: 0.7090 - val_loss: 649.0096 - val_acc: 0.6632\n",
            "Epoch 636/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 648.9474 - acc: 0.6967 - val_loss: 648.9782 - val_acc: 0.6632\n",
            "Epoch 637/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 648.9183 - acc: 0.7049 - val_loss: 648.9469 - val_acc: 0.6632\n",
            "Epoch 638/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 648.8826 - acc: 0.7213 - val_loss: 648.9155 - val_acc: 0.6632\n",
            "Epoch 639/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 648.8559 - acc: 0.7234 - val_loss: 648.8841 - val_acc: 0.6632\n",
            "Epoch 640/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 648.8213 - acc: 0.6906 - val_loss: 648.8526 - val_acc: 0.6632\n",
            "Epoch 641/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 648.7905 - acc: 0.7336 - val_loss: 648.8213 - val_acc: 0.6632\n",
            "Epoch 642/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 648.7523 - acc: 0.7131 - val_loss: 648.7900 - val_acc: 0.6632\n",
            "Epoch 643/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 648.7195 - acc: 0.7254 - val_loss: 648.7585 - val_acc: 0.6632\n",
            "Epoch 644/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 648.7071 - acc: 0.7193 - val_loss: 648.7272 - val_acc: 0.6632\n",
            "Epoch 645/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 648.6608 - acc: 0.7029 - val_loss: 648.6957 - val_acc: 0.6632\n",
            "Epoch 646/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 648.6350 - acc: 0.6967 - val_loss: 648.6643 - val_acc: 0.6632\n",
            "Epoch 647/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 648.5981 - acc: 0.7111 - val_loss: 648.6328 - val_acc: 0.6632\n",
            "Epoch 648/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 648.5747 - acc: 0.6844 - val_loss: 648.6014 - val_acc: 0.6632\n",
            "Epoch 649/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 648.5294 - acc: 0.7193 - val_loss: 648.5700 - val_acc: 0.6632\n",
            "Epoch 650/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 648.5193 - acc: 0.6844 - val_loss: 648.5386 - val_acc: 0.6632\n",
            "Epoch 651/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 648.4800 - acc: 0.7254 - val_loss: 648.5071 - val_acc: 0.6632\n",
            "Epoch 652/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 648.4410 - acc: 0.7398 - val_loss: 648.4758 - val_acc: 0.6632\n",
            "Epoch 653/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 648.4256 - acc: 0.6926 - val_loss: 648.4443 - val_acc: 0.6632\n",
            "Epoch 654/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 648.3818 - acc: 0.6926 - val_loss: 648.4130 - val_acc: 0.6653\n",
            "Epoch 655/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 648.3497 - acc: 0.7172 - val_loss: 648.3816 - val_acc: 0.6653\n",
            "Epoch 656/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 648.3213 - acc: 0.6906 - val_loss: 648.3501 - val_acc: 0.6653\n",
            "Epoch 657/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 648.2835 - acc: 0.7316 - val_loss: 648.3188 - val_acc: 0.6653\n",
            "Epoch 658/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 648.2591 - acc: 0.6988 - val_loss: 648.2874 - val_acc: 0.6653\n",
            "Epoch 659/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 648.2313 - acc: 0.6885 - val_loss: 648.2559 - val_acc: 0.6653\n",
            "Epoch 660/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 648.1809 - acc: 0.7582 - val_loss: 648.2245 - val_acc: 0.6653\n",
            "Epoch 661/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 648.1660 - acc: 0.7049 - val_loss: 648.1930 - val_acc: 0.6653\n",
            "Epoch 662/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 648.1342 - acc: 0.6988 - val_loss: 648.1617 - val_acc: 0.6653\n",
            "Epoch 663/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 648.1069 - acc: 0.7008 - val_loss: 648.1302 - val_acc: 0.6653\n",
            "Epoch 664/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 648.0571 - acc: 0.7152 - val_loss: 648.0988 - val_acc: 0.6653\n",
            "Epoch 665/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 648.0462 - acc: 0.6906 - val_loss: 648.0674 - val_acc: 0.6653\n",
            "Epoch 666/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 648.0011 - acc: 0.7152 - val_loss: 648.0359 - val_acc: 0.6653\n",
            "Epoch 667/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 647.9777 - acc: 0.7111 - val_loss: 648.0046 - val_acc: 0.6653\n",
            "Epoch 668/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 647.9487 - acc: 0.7049 - val_loss: 647.9733 - val_acc: 0.6653\n",
            "Epoch 669/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 647.9082 - acc: 0.7234 - val_loss: 647.9419 - val_acc: 0.6653\n",
            "Epoch 670/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 647.8752 - acc: 0.7131 - val_loss: 647.9104 - val_acc: 0.6653\n",
            "Epoch 671/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 647.8539 - acc: 0.6701 - val_loss: 647.8791 - val_acc: 0.6653\n",
            "Epoch 672/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 647.8253 - acc: 0.7049 - val_loss: 647.8477 - val_acc: 0.6653\n",
            "Epoch 673/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 647.7885 - acc: 0.7254 - val_loss: 647.8164 - val_acc: 0.6653\n",
            "Epoch 674/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 647.7577 - acc: 0.6967 - val_loss: 647.7850 - val_acc: 0.6653\n",
            "Epoch 675/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 647.7270 - acc: 0.7111 - val_loss: 647.7537 - val_acc: 0.6653\n",
            "Epoch 676/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 647.6965 - acc: 0.7008 - val_loss: 647.7222 - val_acc: 0.6653\n",
            "Epoch 677/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 647.6623 - acc: 0.7131 - val_loss: 647.6908 - val_acc: 0.6653\n",
            "Epoch 678/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 647.6315 - acc: 0.6721 - val_loss: 647.6595 - val_acc: 0.6653\n",
            "Epoch 679/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 647.6087 - acc: 0.6967 - val_loss: 647.6281 - val_acc: 0.6653\n",
            "Epoch 680/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 647.5670 - acc: 0.6906 - val_loss: 647.5968 - val_acc: 0.6653\n",
            "Epoch 681/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 647.5423 - acc: 0.6988 - val_loss: 647.5655 - val_acc: 0.6653\n",
            "Epoch 682/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 647.5064 - acc: 0.7008 - val_loss: 647.5339 - val_acc: 0.6653\n",
            "Epoch 683/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 647.4763 - acc: 0.7316 - val_loss: 647.5027 - val_acc: 0.6653\n",
            "Epoch 684/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 647.4465 - acc: 0.6947 - val_loss: 647.4713 - val_acc: 0.6653\n",
            "Epoch 685/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 647.4048 - acc: 0.6988 - val_loss: 647.4399 - val_acc: 0.6653\n",
            "Epoch 686/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 647.3717 - acc: 0.7152 - val_loss: 647.4086 - val_acc: 0.6653\n",
            "Epoch 687/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 647.3598 - acc: 0.6844 - val_loss: 647.3771 - val_acc: 0.6653\n",
            "Epoch 688/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 647.3184 - acc: 0.6906 - val_loss: 647.3457 - val_acc: 0.6653\n",
            "Epoch 689/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 647.2897 - acc: 0.7090 - val_loss: 647.3144 - val_acc: 0.6653\n",
            "Epoch 690/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 647.2629 - acc: 0.6967 - val_loss: 647.2830 - val_acc: 0.6632\n",
            "Epoch 691/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 647.2199 - acc: 0.7131 - val_loss: 647.2516 - val_acc: 0.6632\n",
            "Epoch 692/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 647.1978 - acc: 0.6967 - val_loss: 647.2203 - val_acc: 0.6632\n",
            "Epoch 693/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 647.1654 - acc: 0.6701 - val_loss: 647.1889 - val_acc: 0.6632\n",
            "Epoch 694/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 647.1345 - acc: 0.6947 - val_loss: 647.1575 - val_acc: 0.6632\n",
            "Epoch 695/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 647.0911 - acc: 0.7152 - val_loss: 647.1262 - val_acc: 0.6632\n",
            "Epoch 696/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 647.0736 - acc: 0.6824 - val_loss: 647.0949 - val_acc: 0.6632\n",
            "Epoch 697/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 647.0379 - acc: 0.6967 - val_loss: 647.0636 - val_acc: 0.6632\n",
            "Epoch 698/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 647.0085 - acc: 0.6926 - val_loss: 647.0321 - val_acc: 0.6653\n",
            "Epoch 699/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 646.9702 - acc: 0.7131 - val_loss: 647.0008 - val_acc: 0.6632\n",
            "Epoch 700/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 646.9364 - acc: 0.7131 - val_loss: 646.9693 - val_acc: 0.6632\n",
            "Epoch 701/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 646.9148 - acc: 0.6865 - val_loss: 646.9380 - val_acc: 0.6632\n",
            "Epoch 702/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 646.8783 - acc: 0.6988 - val_loss: 646.9066 - val_acc: 0.6632\n",
            "Epoch 703/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 646.8483 - acc: 0.7049 - val_loss: 646.8751 - val_acc: 0.6632\n",
            "Epoch 704/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 646.8184 - acc: 0.7090 - val_loss: 646.8439 - val_acc: 0.6632\n",
            "Epoch 705/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 646.7918 - acc: 0.6926 - val_loss: 646.8125 - val_acc: 0.6632\n",
            "Epoch 706/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 646.7523 - acc: 0.6926 - val_loss: 646.7812 - val_acc: 0.6632\n",
            "Epoch 707/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 646.7226 - acc: 0.7049 - val_loss: 646.7499 - val_acc: 0.6632\n",
            "Epoch 708/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 646.6918 - acc: 0.7213 - val_loss: 646.7185 - val_acc: 0.6632\n",
            "Epoch 709/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 646.6570 - acc: 0.7008 - val_loss: 646.6872 - val_acc: 0.6632\n",
            "Epoch 710/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 646.6262 - acc: 0.6967 - val_loss: 646.6558 - val_acc: 0.6632\n",
            "Epoch 711/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 646.5984 - acc: 0.7111 - val_loss: 646.6245 - val_acc: 0.6632\n",
            "Epoch 712/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 646.5613 - acc: 0.7152 - val_loss: 646.5931 - val_acc: 0.6653\n",
            "Epoch 713/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 646.5327 - acc: 0.7090 - val_loss: 646.5618 - val_acc: 0.6632\n",
            "Epoch 714/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 646.5032 - acc: 0.6844 - val_loss: 646.5304 - val_acc: 0.6632\n",
            "Epoch 715/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 646.4704 - acc: 0.7213 - val_loss: 646.4991 - val_acc: 0.6632\n",
            "Epoch 716/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 646.4295 - acc: 0.7234 - val_loss: 646.4678 - val_acc: 0.6632\n",
            "Epoch 717/3000\n",
            "488/488 [==============================] - 0s 380us/step - loss: 646.4054 - acc: 0.7049 - val_loss: 646.4365 - val_acc: 0.6632\n",
            "Epoch 718/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 646.3665 - acc: 0.7152 - val_loss: 646.4051 - val_acc: 0.6632\n",
            "Epoch 719/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 646.3517 - acc: 0.6783 - val_loss: 646.3736 - val_acc: 0.6632\n",
            "Epoch 720/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 646.3127 - acc: 0.6988 - val_loss: 646.3423 - val_acc: 0.6632\n",
            "Epoch 721/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 646.2768 - acc: 0.7357 - val_loss: 646.3110 - val_acc: 0.6611\n",
            "Epoch 722/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 646.2559 - acc: 0.6885 - val_loss: 646.2796 - val_acc: 0.6611\n",
            "Epoch 723/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 646.2235 - acc: 0.7049 - val_loss: 646.2483 - val_acc: 0.6653\n",
            "Epoch 724/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 646.1978 - acc: 0.6865 - val_loss: 646.2170 - val_acc: 0.6653\n",
            "Epoch 725/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 646.1520 - acc: 0.7049 - val_loss: 646.1855 - val_acc: 0.6653\n",
            "Epoch 726/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 646.1276 - acc: 0.7070 - val_loss: 646.1542 - val_acc: 0.6632\n",
            "Epoch 727/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 646.0945 - acc: 0.7029 - val_loss: 646.1229 - val_acc: 0.6632\n",
            "Epoch 728/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 646.0596 - acc: 0.6988 - val_loss: 646.0916 - val_acc: 0.6632\n",
            "Epoch 729/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 646.0301 - acc: 0.6926 - val_loss: 646.0602 - val_acc: 0.6632\n",
            "Epoch 730/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 646.0001 - acc: 0.7131 - val_loss: 646.0287 - val_acc: 0.6632\n",
            "Epoch 731/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 645.9750 - acc: 0.7172 - val_loss: 645.9975 - val_acc: 0.6632\n",
            "Epoch 732/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 645.9323 - acc: 0.7111 - val_loss: 645.9662 - val_acc: 0.6611\n",
            "Epoch 733/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 645.9093 - acc: 0.6865 - val_loss: 645.9349 - val_acc: 0.6632\n",
            "Epoch 734/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 645.8753 - acc: 0.7090 - val_loss: 645.9034 - val_acc: 0.6632\n",
            "Epoch 735/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 645.8348 - acc: 0.7418 - val_loss: 645.8720 - val_acc: 0.6632\n",
            "Epoch 736/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 645.8153 - acc: 0.6926 - val_loss: 645.8407 - val_acc: 0.6632\n",
            "Epoch 737/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 645.7794 - acc: 0.7049 - val_loss: 645.8094 - val_acc: 0.6632\n",
            "Epoch 738/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 645.7550 - acc: 0.7049 - val_loss: 645.7782 - val_acc: 0.6632\n",
            "Epoch 739/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 645.7144 - acc: 0.7029 - val_loss: 645.7468 - val_acc: 0.6632\n",
            "Epoch 740/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 645.6802 - acc: 0.7070 - val_loss: 645.7154 - val_acc: 0.6632\n",
            "Epoch 741/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 645.6432 - acc: 0.7377 - val_loss: 645.6841 - val_acc: 0.6632\n",
            "Epoch 742/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 645.6159 - acc: 0.7357 - val_loss: 645.6526 - val_acc: 0.6653\n",
            "Epoch 743/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 645.5836 - acc: 0.7275 - val_loss: 645.6212 - val_acc: 0.6653\n",
            "Epoch 744/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 645.5707 - acc: 0.6947 - val_loss: 645.5900 - val_acc: 0.6653\n",
            "Epoch 745/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 645.5243 - acc: 0.7254 - val_loss: 645.5586 - val_acc: 0.6653\n",
            "Epoch 746/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 645.4987 - acc: 0.7193 - val_loss: 645.5273 - val_acc: 0.6653\n",
            "Epoch 747/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 645.4610 - acc: 0.7254 - val_loss: 645.4961 - val_acc: 0.6653\n",
            "Epoch 748/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 645.4299 - acc: 0.7049 - val_loss: 645.4646 - val_acc: 0.6653\n",
            "Epoch 749/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 645.4061 - acc: 0.7070 - val_loss: 645.4333 - val_acc: 0.6653\n",
            "Epoch 750/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 645.3737 - acc: 0.7070 - val_loss: 645.4020 - val_acc: 0.6653\n",
            "Epoch 751/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 645.3375 - acc: 0.7049 - val_loss: 645.3706 - val_acc: 0.6653\n",
            "Epoch 752/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 645.3165 - acc: 0.6906 - val_loss: 645.3392 - val_acc: 0.6653\n",
            "Epoch 753/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 645.2741 - acc: 0.7275 - val_loss: 645.3080 - val_acc: 0.6653\n",
            "Epoch 754/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 645.2417 - acc: 0.7008 - val_loss: 645.2768 - val_acc: 0.6632\n",
            "Epoch 755/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 645.2210 - acc: 0.6803 - val_loss: 645.2454 - val_acc: 0.6653\n",
            "Epoch 756/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 645.1737 - acc: 0.7213 - val_loss: 645.2140 - val_acc: 0.6653\n",
            "Epoch 757/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 645.1508 - acc: 0.7234 - val_loss: 645.1827 - val_acc: 0.6653\n",
            "Epoch 758/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 645.1156 - acc: 0.7008 - val_loss: 645.1513 - val_acc: 0.6632\n",
            "Epoch 759/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 645.0992 - acc: 0.7049 - val_loss: 645.1200 - val_acc: 0.6632\n",
            "Epoch 760/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 645.0620 - acc: 0.7008 - val_loss: 645.0888 - val_acc: 0.6632\n",
            "Epoch 761/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 645.0267 - acc: 0.7254 - val_loss: 645.0574 - val_acc: 0.6632\n",
            "Epoch 762/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 644.9957 - acc: 0.7029 - val_loss: 645.0260 - val_acc: 0.6653\n",
            "Epoch 763/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 644.9673 - acc: 0.6967 - val_loss: 644.9947 - val_acc: 0.6653\n",
            "Epoch 764/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 644.9330 - acc: 0.7275 - val_loss: 644.9633 - val_acc: 0.6653\n",
            "Epoch 765/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 644.9057 - acc: 0.7008 - val_loss: 644.9320 - val_acc: 0.6632\n",
            "Epoch 766/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 644.8695 - acc: 0.7254 - val_loss: 644.9007 - val_acc: 0.6632\n",
            "Epoch 767/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 644.8347 - acc: 0.7152 - val_loss: 644.8693 - val_acc: 0.6632\n",
            "Epoch 768/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 644.8084 - acc: 0.7234 - val_loss: 644.8380 - val_acc: 0.6632\n",
            "Epoch 769/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 644.7645 - acc: 0.7172 - val_loss: 644.8068 - val_acc: 0.6632\n",
            "Epoch 770/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 644.7463 - acc: 0.7090 - val_loss: 644.7756 - val_acc: 0.6632\n",
            "Epoch 771/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 644.7121 - acc: 0.7275 - val_loss: 644.7441 - val_acc: 0.6632\n",
            "Epoch 772/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 644.6930 - acc: 0.7029 - val_loss: 644.7129 - val_acc: 0.6632\n",
            "Epoch 773/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 644.6493 - acc: 0.6947 - val_loss: 644.6817 - val_acc: 0.6632\n",
            "Epoch 774/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 644.6241 - acc: 0.6926 - val_loss: 644.6503 - val_acc: 0.6653\n",
            "Epoch 775/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 644.5847 - acc: 0.7193 - val_loss: 644.6189 - val_acc: 0.6653\n",
            "Epoch 776/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 644.5515 - acc: 0.7234 - val_loss: 644.5877 - val_acc: 0.6653\n",
            "Epoch 777/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 644.5341 - acc: 0.6926 - val_loss: 644.5562 - val_acc: 0.6653\n",
            "Epoch 778/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 644.4906 - acc: 0.7029 - val_loss: 644.5249 - val_acc: 0.6674\n",
            "Epoch 779/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 644.4570 - acc: 0.7213 - val_loss: 644.4936 - val_acc: 0.6674\n",
            "Epoch 780/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 644.4211 - acc: 0.7336 - val_loss: 644.4624 - val_acc: 0.6674\n",
            "Epoch 781/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 644.3912 - acc: 0.7295 - val_loss: 644.4312 - val_acc: 0.6674\n",
            "Epoch 782/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 644.3720 - acc: 0.7131 - val_loss: 644.3998 - val_acc: 0.6674\n",
            "Epoch 783/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 644.3371 - acc: 0.7008 - val_loss: 644.3685 - val_acc: 0.6674\n",
            "Epoch 784/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 644.3012 - acc: 0.7213 - val_loss: 644.3371 - val_acc: 0.6674\n",
            "Epoch 785/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 644.2684 - acc: 0.7193 - val_loss: 644.3058 - val_acc: 0.6674\n",
            "Epoch 786/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 644.2376 - acc: 0.7131 - val_loss: 644.2746 - val_acc: 0.6674\n",
            "Epoch 787/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 644.2161 - acc: 0.7172 - val_loss: 644.2432 - val_acc: 0.6674\n",
            "Epoch 788/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 644.1776 - acc: 0.7152 - val_loss: 644.2119 - val_acc: 0.6674\n",
            "Epoch 789/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 644.1544 - acc: 0.7090 - val_loss: 644.1806 - val_acc: 0.6674\n",
            "Epoch 790/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 644.1134 - acc: 0.7336 - val_loss: 644.1492 - val_acc: 0.6674\n",
            "Epoch 791/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 644.0864 - acc: 0.7111 - val_loss: 644.1178 - val_acc: 0.6674\n",
            "Epoch 792/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 644.0492 - acc: 0.7254 - val_loss: 644.0865 - val_acc: 0.6674\n",
            "Epoch 793/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 644.0222 - acc: 0.7152 - val_loss: 644.0552 - val_acc: 0.6674\n",
            "Epoch 794/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 644.0031 - acc: 0.7008 - val_loss: 644.0239 - val_acc: 0.6674\n",
            "Epoch 795/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 643.9587 - acc: 0.7152 - val_loss: 643.9926 - val_acc: 0.6674\n",
            "Epoch 796/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 643.9298 - acc: 0.6926 - val_loss: 643.9613 - val_acc: 0.6674\n",
            "Epoch 797/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 643.9070 - acc: 0.7193 - val_loss: 643.9300 - val_acc: 0.6674\n",
            "Epoch 798/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 643.8728 - acc: 0.7008 - val_loss: 643.8986 - val_acc: 0.6674\n",
            "Epoch 799/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 643.8358 - acc: 0.7152 - val_loss: 643.8674 - val_acc: 0.6674\n",
            "Epoch 800/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 643.7959 - acc: 0.7131 - val_loss: 643.8361 - val_acc: 0.6674\n",
            "Epoch 801/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 643.7751 - acc: 0.7254 - val_loss: 643.8048 - val_acc: 0.6653\n",
            "Epoch 802/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 643.7370 - acc: 0.7275 - val_loss: 643.7735 - val_acc: 0.6674\n",
            "Epoch 803/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 643.7145 - acc: 0.7152 - val_loss: 643.7422 - val_acc: 0.6674\n",
            "Epoch 804/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 643.6630 - acc: 0.7357 - val_loss: 643.7109 - val_acc: 0.6653\n",
            "Epoch 805/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 643.6535 - acc: 0.7111 - val_loss: 643.6796 - val_acc: 0.6653\n",
            "Epoch 806/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 643.6104 - acc: 0.7213 - val_loss: 643.6483 - val_acc: 0.6653\n",
            "Epoch 807/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 643.5844 - acc: 0.7295 - val_loss: 643.6170 - val_acc: 0.6674\n",
            "Epoch 808/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 643.5524 - acc: 0.7152 - val_loss: 643.5857 - val_acc: 0.6674\n",
            "Epoch 809/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 643.5268 - acc: 0.7111 - val_loss: 643.5544 - val_acc: 0.6653\n",
            "Epoch 810/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 643.4910 - acc: 0.6906 - val_loss: 643.5231 - val_acc: 0.6674\n",
            "Epoch 811/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 643.4592 - acc: 0.7275 - val_loss: 643.4918 - val_acc: 0.6653\n",
            "Epoch 812/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 643.4277 - acc: 0.6865 - val_loss: 643.4606 - val_acc: 0.6653\n",
            "Epoch 813/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 643.3982 - acc: 0.6885 - val_loss: 643.4293 - val_acc: 0.6653\n",
            "Epoch 814/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 643.3574 - acc: 0.7418 - val_loss: 643.3980 - val_acc: 0.6653\n",
            "Epoch 815/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 643.3358 - acc: 0.7152 - val_loss: 643.3667 - val_acc: 0.6653\n",
            "Epoch 816/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 643.2891 - acc: 0.7357 - val_loss: 643.3355 - val_acc: 0.6653\n",
            "Epoch 817/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 643.2756 - acc: 0.7029 - val_loss: 643.3042 - val_acc: 0.6653\n",
            "Epoch 818/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 643.2327 - acc: 0.7295 - val_loss: 643.2730 - val_acc: 0.6653\n",
            "Epoch 819/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 643.2083 - acc: 0.7131 - val_loss: 643.2417 - val_acc: 0.6653\n",
            "Epoch 820/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 643.1675 - acc: 0.7480 - val_loss: 643.2104 - val_acc: 0.6653\n",
            "Epoch 821/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 643.1560 - acc: 0.7049 - val_loss: 643.1792 - val_acc: 0.6653\n",
            "Epoch 822/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 643.1176 - acc: 0.6885 - val_loss: 643.1479 - val_acc: 0.6653\n",
            "Epoch 823/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 643.0821 - acc: 0.7152 - val_loss: 643.1166 - val_acc: 0.6653\n",
            "Epoch 824/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 643.0517 - acc: 0.7316 - val_loss: 643.0853 - val_acc: 0.6653\n",
            "Epoch 825/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 643.0272 - acc: 0.7213 - val_loss: 643.0541 - val_acc: 0.6674\n",
            "Epoch 826/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 642.9909 - acc: 0.7090 - val_loss: 643.0227 - val_acc: 0.6674\n",
            "Epoch 827/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 642.9697 - acc: 0.6926 - val_loss: 642.9915 - val_acc: 0.6674\n",
            "Epoch 828/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 642.9268 - acc: 0.7398 - val_loss: 642.9604 - val_acc: 0.6695\n",
            "Epoch 829/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 642.8880 - acc: 0.7090 - val_loss: 642.9291 - val_acc: 0.6674\n",
            "Epoch 830/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 642.8763 - acc: 0.6865 - val_loss: 642.8979 - val_acc: 0.6674\n",
            "Epoch 831/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 642.8388 - acc: 0.7070 - val_loss: 642.8666 - val_acc: 0.6715\n",
            "Epoch 832/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 642.8007 - acc: 0.7234 - val_loss: 642.8353 - val_acc: 0.6715\n",
            "Epoch 833/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 642.7685 - acc: 0.7295 - val_loss: 642.8042 - val_acc: 0.6715\n",
            "Epoch 834/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 642.7393 - acc: 0.7131 - val_loss: 642.7728 - val_acc: 0.6695\n",
            "Epoch 835/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 642.7015 - acc: 0.7234 - val_loss: 642.7417 - val_acc: 0.6695\n",
            "Epoch 836/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 642.6846 - acc: 0.6926 - val_loss: 642.7103 - val_acc: 0.6695\n",
            "Epoch 837/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 642.6527 - acc: 0.7275 - val_loss: 642.6790 - val_acc: 0.6695\n",
            "Epoch 838/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 642.6110 - acc: 0.7377 - val_loss: 642.6478 - val_acc: 0.6695\n",
            "Epoch 839/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 642.5932 - acc: 0.7213 - val_loss: 642.6166 - val_acc: 0.6695\n",
            "Epoch 840/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 642.5414 - acc: 0.7439 - val_loss: 642.5853 - val_acc: 0.6695\n",
            "Epoch 841/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 642.5194 - acc: 0.7357 - val_loss: 642.5540 - val_acc: 0.6695\n",
            "Epoch 842/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 642.4929 - acc: 0.7254 - val_loss: 642.5229 - val_acc: 0.6695\n",
            "Epoch 843/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 642.4604 - acc: 0.7213 - val_loss: 642.4917 - val_acc: 0.6674\n",
            "Epoch 844/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 642.4227 - acc: 0.7459 - val_loss: 642.4605 - val_acc: 0.6674\n",
            "Epoch 845/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 642.3934 - acc: 0.7193 - val_loss: 642.4290 - val_acc: 0.6674\n",
            "Epoch 846/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 642.3640 - acc: 0.7480 - val_loss: 642.3979 - val_acc: 0.6674\n",
            "Epoch 847/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 642.3297 - acc: 0.7131 - val_loss: 642.3665 - val_acc: 0.6674\n",
            "Epoch 848/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 642.3071 - acc: 0.7131 - val_loss: 642.3353 - val_acc: 0.6674\n",
            "Epoch 849/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 642.2776 - acc: 0.6988 - val_loss: 642.3040 - val_acc: 0.6674\n",
            "Epoch 850/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 642.2312 - acc: 0.7418 - val_loss: 642.2728 - val_acc: 0.6674\n",
            "Epoch 851/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 642.1970 - acc: 0.7336 - val_loss: 642.2416 - val_acc: 0.6674\n",
            "Epoch 852/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 642.1697 - acc: 0.7357 - val_loss: 642.2103 - val_acc: 0.6674\n",
            "Epoch 853/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 642.1365 - acc: 0.7295 - val_loss: 642.1792 - val_acc: 0.6674\n",
            "Epoch 854/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 642.1194 - acc: 0.6947 - val_loss: 642.1480 - val_acc: 0.6674\n",
            "Epoch 855/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 642.0854 - acc: 0.7152 - val_loss: 642.1166 - val_acc: 0.6674\n",
            "Epoch 856/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 642.0504 - acc: 0.7336 - val_loss: 642.0855 - val_acc: 0.6674\n",
            "Epoch 857/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 642.0196 - acc: 0.7275 - val_loss: 642.0541 - val_acc: 0.6674\n",
            "Epoch 858/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 641.9816 - acc: 0.7582 - val_loss: 642.0229 - val_acc: 0.6674\n",
            "Epoch 859/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 641.9560 - acc: 0.7254 - val_loss: 641.9918 - val_acc: 0.6674\n",
            "Epoch 860/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 641.9430 - acc: 0.6967 - val_loss: 641.9607 - val_acc: 0.6674\n",
            "Epoch 861/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 641.8982 - acc: 0.7131 - val_loss: 641.9293 - val_acc: 0.6674\n",
            "Epoch 862/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 641.8752 - acc: 0.7131 - val_loss: 641.8982 - val_acc: 0.6674\n",
            "Epoch 863/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 641.8246 - acc: 0.7213 - val_loss: 641.8669 - val_acc: 0.6674\n",
            "Epoch 864/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 641.8075 - acc: 0.7172 - val_loss: 641.8357 - val_acc: 0.6674\n",
            "Epoch 865/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 641.7627 - acc: 0.7377 - val_loss: 641.8044 - val_acc: 0.6674\n",
            "Epoch 866/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 641.7367 - acc: 0.7193 - val_loss: 641.7732 - val_acc: 0.6674\n",
            "Epoch 867/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 641.7125 - acc: 0.6988 - val_loss: 641.7420 - val_acc: 0.6674\n",
            "Epoch 868/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 641.6688 - acc: 0.7643 - val_loss: 641.7109 - val_acc: 0.6674\n",
            "Epoch 869/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 641.6443 - acc: 0.7193 - val_loss: 641.6797 - val_acc: 0.6653\n",
            "Epoch 870/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 641.6207 - acc: 0.7029 - val_loss: 641.6485 - val_acc: 0.6653\n",
            "Epoch 871/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 641.5904 - acc: 0.6844 - val_loss: 641.6172 - val_acc: 0.6653\n",
            "Epoch 872/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 641.5557 - acc: 0.7295 - val_loss: 641.5860 - val_acc: 0.6653\n",
            "Epoch 873/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 641.5158 - acc: 0.7316 - val_loss: 641.5548 - val_acc: 0.6653\n",
            "Epoch 874/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 641.4913 - acc: 0.7111 - val_loss: 641.5237 - val_acc: 0.6653\n",
            "Epoch 875/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 641.4607 - acc: 0.7336 - val_loss: 641.4924 - val_acc: 0.6653\n",
            "Epoch 876/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 641.4241 - acc: 0.7029 - val_loss: 641.4611 - val_acc: 0.6653\n",
            "Epoch 877/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 641.4001 - acc: 0.7131 - val_loss: 641.4299 - val_acc: 0.6653\n",
            "Epoch 878/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 641.3675 - acc: 0.7049 - val_loss: 641.3987 - val_acc: 0.6653\n",
            "Epoch 879/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 641.3219 - acc: 0.7316 - val_loss: 641.3675 - val_acc: 0.6653\n",
            "Epoch 880/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 641.3051 - acc: 0.7172 - val_loss: 641.3363 - val_acc: 0.6653\n",
            "Epoch 881/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 641.2680 - acc: 0.7295 - val_loss: 641.3052 - val_acc: 0.6653\n",
            "Epoch 882/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 641.2347 - acc: 0.7357 - val_loss: 641.2740 - val_acc: 0.6653\n",
            "Epoch 883/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 641.2103 - acc: 0.7234 - val_loss: 641.2429 - val_acc: 0.6653\n",
            "Epoch 884/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 641.1742 - acc: 0.7111 - val_loss: 641.2116 - val_acc: 0.6653\n",
            "Epoch 885/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 641.1417 - acc: 0.7213 - val_loss: 641.1804 - val_acc: 0.6653\n",
            "Epoch 886/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 641.1119 - acc: 0.7254 - val_loss: 641.1492 - val_acc: 0.6653\n",
            "Epoch 887/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 641.0772 - acc: 0.7111 - val_loss: 641.1180 - val_acc: 0.6653\n",
            "Epoch 888/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 641.0511 - acc: 0.7254 - val_loss: 641.0868 - val_acc: 0.6653\n",
            "Epoch 889/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 641.0272 - acc: 0.7398 - val_loss: 641.0555 - val_acc: 0.6674\n",
            "Epoch 890/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 640.9915 - acc: 0.7357 - val_loss: 641.0244 - val_acc: 0.6632\n",
            "Epoch 891/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 640.9655 - acc: 0.6947 - val_loss: 640.9932 - val_acc: 0.6653\n",
            "Epoch 892/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 640.9213 - acc: 0.7398 - val_loss: 640.9620 - val_acc: 0.6653\n",
            "Epoch 893/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 640.8859 - acc: 0.7295 - val_loss: 640.9307 - val_acc: 0.6653\n",
            "Epoch 894/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 640.8637 - acc: 0.7193 - val_loss: 640.8995 - val_acc: 0.6653\n",
            "Epoch 895/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 640.8397 - acc: 0.6967 - val_loss: 640.8685 - val_acc: 0.6653\n",
            "Epoch 896/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 640.8044 - acc: 0.7131 - val_loss: 640.8373 - val_acc: 0.6653\n",
            "Epoch 897/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 640.7663 - acc: 0.7295 - val_loss: 640.8061 - val_acc: 0.6653\n",
            "Epoch 898/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 640.7378 - acc: 0.7193 - val_loss: 640.7749 - val_acc: 0.6653\n",
            "Epoch 899/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 640.7047 - acc: 0.7336 - val_loss: 640.7437 - val_acc: 0.6653\n",
            "Epoch 900/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 640.6812 - acc: 0.7111 - val_loss: 640.7125 - val_acc: 0.6653\n",
            "Epoch 901/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 640.6388 - acc: 0.7439 - val_loss: 640.6814 - val_acc: 0.6653\n",
            "Epoch 902/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 640.6172 - acc: 0.7213 - val_loss: 640.6501 - val_acc: 0.6653\n",
            "Epoch 903/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 640.5810 - acc: 0.7193 - val_loss: 640.6189 - val_acc: 0.6653\n",
            "Epoch 904/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 640.5541 - acc: 0.7213 - val_loss: 640.5877 - val_acc: 0.6653\n",
            "Epoch 905/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 640.5294 - acc: 0.7070 - val_loss: 640.5566 - val_acc: 0.6653\n",
            "Epoch 906/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 640.4813 - acc: 0.7295 - val_loss: 640.5254 - val_acc: 0.6653\n",
            "Epoch 907/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 640.4541 - acc: 0.7131 - val_loss: 640.4942 - val_acc: 0.6653\n",
            "Epoch 908/3000\n",
            "488/488 [==============================] - 0s 318us/step - loss: 640.4279 - acc: 0.7111 - val_loss: 640.4630 - val_acc: 0.6653\n",
            "Epoch 909/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 640.3942 - acc: 0.7500 - val_loss: 640.4318 - val_acc: 0.6653\n",
            "Epoch 910/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 640.3688 - acc: 0.7316 - val_loss: 640.4006 - val_acc: 0.6653\n",
            "Epoch 911/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 640.3227 - acc: 0.7357 - val_loss: 640.3693 - val_acc: 0.6653\n",
            "Epoch 912/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 640.3019 - acc: 0.7090 - val_loss: 640.3381 - val_acc: 0.6653\n",
            "Epoch 913/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 640.2799 - acc: 0.7070 - val_loss: 640.3070 - val_acc: 0.6653\n",
            "Epoch 914/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 640.2371 - acc: 0.7398 - val_loss: 640.2758 - val_acc: 0.6653\n",
            "Epoch 915/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 640.2018 - acc: 0.7172 - val_loss: 640.2447 - val_acc: 0.6632\n",
            "Epoch 916/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 640.1822 - acc: 0.7193 - val_loss: 640.2136 - val_acc: 0.6632\n",
            "Epoch 917/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 640.1460 - acc: 0.7234 - val_loss: 640.1824 - val_acc: 0.6653\n",
            "Epoch 918/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 640.1024 - acc: 0.7398 - val_loss: 640.1512 - val_acc: 0.6653\n",
            "Epoch 919/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 640.0794 - acc: 0.7254 - val_loss: 640.1200 - val_acc: 0.6632\n",
            "Epoch 920/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 640.0621 - acc: 0.7070 - val_loss: 640.0890 - val_acc: 0.6632\n",
            "Epoch 921/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 640.0186 - acc: 0.7418 - val_loss: 640.0578 - val_acc: 0.6632\n",
            "Epoch 922/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 639.9893 - acc: 0.7234 - val_loss: 640.0266 - val_acc: 0.6632\n",
            "Epoch 923/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 639.9646 - acc: 0.7131 - val_loss: 639.9955 - val_acc: 0.6632\n",
            "Epoch 924/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 639.9268 - acc: 0.7193 - val_loss: 639.9642 - val_acc: 0.6632\n",
            "Epoch 925/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 639.8943 - acc: 0.7377 - val_loss: 639.9330 - val_acc: 0.6611\n",
            "Epoch 926/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 639.8565 - acc: 0.7561 - val_loss: 639.9019 - val_acc: 0.6632\n",
            "Epoch 927/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 639.8360 - acc: 0.7336 - val_loss: 639.8707 - val_acc: 0.6632\n",
            "Epoch 928/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 639.7998 - acc: 0.7295 - val_loss: 639.8396 - val_acc: 0.6632\n",
            "Epoch 929/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 639.7665 - acc: 0.7172 - val_loss: 639.8084 - val_acc: 0.6632\n",
            "Epoch 930/3000\n",
            "488/488 [==============================] - 0s 317us/step - loss: 639.7425 - acc: 0.7459 - val_loss: 639.7773 - val_acc: 0.6632\n",
            "Epoch 931/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 639.7110 - acc: 0.7152 - val_loss: 639.7462 - val_acc: 0.6632\n",
            "Epoch 932/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 639.6687 - acc: 0.7275 - val_loss: 639.7150 - val_acc: 0.6632\n",
            "Epoch 933/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 639.6398 - acc: 0.7213 - val_loss: 639.6838 - val_acc: 0.6632\n",
            "Epoch 934/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 639.6200 - acc: 0.7316 - val_loss: 639.6526 - val_acc: 0.6632\n",
            "Epoch 935/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 639.5832 - acc: 0.7295 - val_loss: 639.6215 - val_acc: 0.6632\n",
            "Epoch 936/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 639.5448 - acc: 0.7398 - val_loss: 639.5903 - val_acc: 0.6632\n",
            "Epoch 937/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 639.5181 - acc: 0.7213 - val_loss: 639.5592 - val_acc: 0.6632\n",
            "Epoch 938/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 639.4869 - acc: 0.7295 - val_loss: 639.5282 - val_acc: 0.6611\n",
            "Epoch 939/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 639.4571 - acc: 0.7439 - val_loss: 639.4970 - val_acc: 0.6611\n",
            "Epoch 940/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 639.4383 - acc: 0.7029 - val_loss: 639.4658 - val_acc: 0.6611\n",
            "Epoch 941/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 639.3938 - acc: 0.7234 - val_loss: 639.4345 - val_acc: 0.6611\n",
            "Epoch 942/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 639.3648 - acc: 0.7172 - val_loss: 639.4036 - val_acc: 0.6611\n",
            "Epoch 943/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 639.3359 - acc: 0.7275 - val_loss: 639.3724 - val_acc: 0.6611\n",
            "Epoch 944/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 639.3050 - acc: 0.7172 - val_loss: 639.3412 - val_acc: 0.6611\n",
            "Epoch 945/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 639.2805 - acc: 0.7090 - val_loss: 639.3101 - val_acc: 0.6611\n",
            "Epoch 946/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 639.2418 - acc: 0.7357 - val_loss: 639.2789 - val_acc: 0.6632\n",
            "Epoch 947/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 639.2024 - acc: 0.7418 - val_loss: 639.2479 - val_acc: 0.6632\n",
            "Epoch 948/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 639.1702 - acc: 0.7602 - val_loss: 639.2166 - val_acc: 0.6632\n",
            "Epoch 949/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 639.1467 - acc: 0.7295 - val_loss: 639.1855 - val_acc: 0.6632\n",
            "Epoch 950/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 639.1193 - acc: 0.7357 - val_loss: 639.1542 - val_acc: 0.6632\n",
            "Epoch 951/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 639.0847 - acc: 0.7561 - val_loss: 639.1231 - val_acc: 0.6632\n",
            "Epoch 952/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 639.0499 - acc: 0.7131 - val_loss: 639.0919 - val_acc: 0.6632\n",
            "Epoch 953/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 639.0282 - acc: 0.7336 - val_loss: 639.0609 - val_acc: 0.6632\n",
            "Epoch 954/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 639.0043 - acc: 0.7275 - val_loss: 639.0296 - val_acc: 0.6632\n",
            "Epoch 955/3000\n",
            "488/488 [==============================] - 0s 318us/step - loss: 638.9496 - acc: 0.7520 - val_loss: 638.9985 - val_acc: 0.6632\n",
            "Epoch 956/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 638.9402 - acc: 0.7111 - val_loss: 638.9674 - val_acc: 0.6632\n",
            "Epoch 957/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 638.9043 - acc: 0.7254 - val_loss: 638.9362 - val_acc: 0.6632\n",
            "Epoch 958/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 638.8678 - acc: 0.7131 - val_loss: 638.9051 - val_acc: 0.6632\n",
            "Epoch 959/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 638.8496 - acc: 0.7049 - val_loss: 638.8739 - val_acc: 0.6632\n",
            "Epoch 960/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 638.8192 - acc: 0.7008 - val_loss: 638.8429 - val_acc: 0.6632\n",
            "Epoch 961/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 638.7659 - acc: 0.7316 - val_loss: 638.8117 - val_acc: 0.6611\n",
            "Epoch 962/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 638.7431 - acc: 0.7275 - val_loss: 638.7806 - val_acc: 0.6611\n",
            "Epoch 963/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 638.7134 - acc: 0.7459 - val_loss: 638.7495 - val_acc: 0.6611\n",
            "Epoch 964/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 638.6814 - acc: 0.7193 - val_loss: 638.7184 - val_acc: 0.6611\n",
            "Epoch 965/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 638.6404 - acc: 0.7439 - val_loss: 638.6872 - val_acc: 0.6611\n",
            "Epoch 966/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 638.6115 - acc: 0.7316 - val_loss: 638.6561 - val_acc: 0.6611\n",
            "Epoch 967/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 638.5873 - acc: 0.7295 - val_loss: 638.6249 - val_acc: 0.6611\n",
            "Epoch 968/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 638.5530 - acc: 0.7234 - val_loss: 638.5937 - val_acc: 0.6611\n",
            "Epoch 969/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 638.5281 - acc: 0.7480 - val_loss: 638.5626 - val_acc: 0.6611\n",
            "Epoch 970/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 638.5016 - acc: 0.7049 - val_loss: 638.5315 - val_acc: 0.6611\n",
            "Epoch 971/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 638.4486 - acc: 0.7439 - val_loss: 638.5004 - val_acc: 0.6611\n",
            "Epoch 972/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 638.4264 - acc: 0.7439 - val_loss: 638.4692 - val_acc: 0.6632\n",
            "Epoch 973/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 638.4002 - acc: 0.7336 - val_loss: 638.4381 - val_acc: 0.6632\n",
            "Epoch 974/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 638.3641 - acc: 0.7357 - val_loss: 638.4070 - val_acc: 0.6632\n",
            "Epoch 975/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 638.3466 - acc: 0.7049 - val_loss: 638.3759 - val_acc: 0.6632\n",
            "Epoch 976/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 638.3042 - acc: 0.7357 - val_loss: 638.3448 - val_acc: 0.6632\n",
            "Epoch 977/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 638.2683 - acc: 0.7418 - val_loss: 638.3137 - val_acc: 0.6632\n",
            "Epoch 978/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 638.2520 - acc: 0.7398 - val_loss: 638.2827 - val_acc: 0.6632\n",
            "Epoch 979/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 638.2147 - acc: 0.7213 - val_loss: 638.2517 - val_acc: 0.6632\n",
            "Epoch 980/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 638.1830 - acc: 0.7193 - val_loss: 638.2205 - val_acc: 0.6632\n",
            "Epoch 981/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 638.1350 - acc: 0.7398 - val_loss: 638.1895 - val_acc: 0.6632\n",
            "Epoch 982/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 638.1190 - acc: 0.7295 - val_loss: 638.1583 - val_acc: 0.6632\n",
            "Epoch 983/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 638.0862 - acc: 0.7275 - val_loss: 638.1273 - val_acc: 0.6632\n",
            "Epoch 984/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 638.0501 - acc: 0.7418 - val_loss: 638.0961 - val_acc: 0.6653\n",
            "Epoch 985/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 638.0274 - acc: 0.7275 - val_loss: 638.0651 - val_acc: 0.6653\n",
            "Epoch 986/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 637.9949 - acc: 0.7398 - val_loss: 638.0340 - val_acc: 0.6653\n",
            "Epoch 987/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 637.9654 - acc: 0.7234 - val_loss: 638.0030 - val_acc: 0.6653\n",
            "Epoch 988/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 637.9321 - acc: 0.7439 - val_loss: 637.9720 - val_acc: 0.6653\n",
            "Epoch 989/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 637.8948 - acc: 0.7541 - val_loss: 637.9409 - val_acc: 0.6653\n",
            "Epoch 990/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 637.8683 - acc: 0.7336 - val_loss: 637.9098 - val_acc: 0.6653\n",
            "Epoch 991/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 637.8309 - acc: 0.7234 - val_loss: 637.8787 - val_acc: 0.6653\n",
            "Epoch 992/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 637.8066 - acc: 0.7213 - val_loss: 637.8477 - val_acc: 0.6674\n",
            "Epoch 993/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 637.7819 - acc: 0.7172 - val_loss: 637.8166 - val_acc: 0.6674\n",
            "Epoch 994/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 637.7396 - acc: 0.7459 - val_loss: 637.7855 - val_acc: 0.6674\n",
            "Epoch 995/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 637.7119 - acc: 0.7316 - val_loss: 637.7543 - val_acc: 0.6674\n",
            "Epoch 996/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 637.6755 - acc: 0.7418 - val_loss: 637.7233 - val_acc: 0.6674\n",
            "Epoch 997/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 637.6540 - acc: 0.7377 - val_loss: 637.6921 - val_acc: 0.6674\n",
            "Epoch 998/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 637.6213 - acc: 0.7213 - val_loss: 637.6610 - val_acc: 0.6653\n",
            "Epoch 999/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 637.5899 - acc: 0.7377 - val_loss: 637.6300 - val_acc: 0.6674\n",
            "Epoch 1000/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 637.5663 - acc: 0.7049 - val_loss: 637.5989 - val_acc: 0.6674\n",
            "Epoch 1001/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 637.5199 - acc: 0.7295 - val_loss: 637.5680 - val_acc: 0.6653\n",
            "Epoch 1002/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 637.4938 - acc: 0.7418 - val_loss: 637.5368 - val_acc: 0.6653\n",
            "Epoch 1003/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 637.4660 - acc: 0.7152 - val_loss: 637.5058 - val_acc: 0.6653\n",
            "Epoch 1004/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 637.4399 - acc: 0.7275 - val_loss: 637.4747 - val_acc: 0.6653\n",
            "Epoch 1005/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 637.3992 - acc: 0.7459 - val_loss: 637.4436 - val_acc: 0.6653\n",
            "Epoch 1006/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 637.3826 - acc: 0.7193 - val_loss: 637.4125 - val_acc: 0.6611\n",
            "Epoch 1007/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 637.3414 - acc: 0.7377 - val_loss: 637.3813 - val_acc: 0.6653\n",
            "Epoch 1008/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 637.3016 - acc: 0.7582 - val_loss: 637.3502 - val_acc: 0.6653\n",
            "Epoch 1009/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 637.2775 - acc: 0.7459 - val_loss: 637.3191 - val_acc: 0.6653\n",
            "Epoch 1010/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 637.2492 - acc: 0.7377 - val_loss: 637.2881 - val_acc: 0.6653\n",
            "Epoch 1011/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 637.2123 - acc: 0.7500 - val_loss: 637.2571 - val_acc: 0.6653\n",
            "Epoch 1012/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 637.1850 - acc: 0.7439 - val_loss: 637.2260 - val_acc: 0.6653\n",
            "Epoch 1013/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 637.1539 - acc: 0.7480 - val_loss: 637.1949 - val_acc: 0.6653\n",
            "Epoch 1014/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 637.1156 - acc: 0.7500 - val_loss: 637.1639 - val_acc: 0.6653\n",
            "Epoch 1015/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 637.0897 - acc: 0.7541 - val_loss: 637.1329 - val_acc: 0.6653\n",
            "Epoch 1016/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 637.0643 - acc: 0.7336 - val_loss: 637.1017 - val_acc: 0.6653\n",
            "Epoch 1017/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 637.0293 - acc: 0.7336 - val_loss: 637.0708 - val_acc: 0.6653\n",
            "Epoch 1018/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 636.9998 - acc: 0.7213 - val_loss: 637.0395 - val_acc: 0.6653\n",
            "Epoch 1019/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 636.9682 - acc: 0.7500 - val_loss: 637.0086 - val_acc: 0.6653\n",
            "Epoch 1020/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 636.9445 - acc: 0.7234 - val_loss: 636.9776 - val_acc: 0.6611\n",
            "Epoch 1021/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 636.9115 - acc: 0.7234 - val_loss: 636.9465 - val_acc: 0.6611\n",
            "Epoch 1022/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 636.8750 - acc: 0.7398 - val_loss: 636.9156 - val_acc: 0.6611\n",
            "Epoch 1023/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 636.8350 - acc: 0.7480 - val_loss: 636.8844 - val_acc: 0.6632\n",
            "Epoch 1024/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 636.8156 - acc: 0.7418 - val_loss: 636.8534 - val_acc: 0.6632\n",
            "Epoch 1025/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 636.7880 - acc: 0.7070 - val_loss: 636.8224 - val_acc: 0.6632\n",
            "Epoch 1026/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 636.7446 - acc: 0.7336 - val_loss: 636.7913 - val_acc: 0.6632\n",
            "Epoch 1027/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 636.7161 - acc: 0.7398 - val_loss: 636.7603 - val_acc: 0.6632\n",
            "Epoch 1028/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 636.6798 - acc: 0.7336 - val_loss: 636.7292 - val_acc: 0.6632\n",
            "Epoch 1029/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 636.6395 - acc: 0.7520 - val_loss: 636.6983 - val_acc: 0.6632\n",
            "Epoch 1030/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 636.6321 - acc: 0.7152 - val_loss: 636.6672 - val_acc: 0.6632\n",
            "Epoch 1031/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 636.5928 - acc: 0.7480 - val_loss: 636.6362 - val_acc: 0.6632\n",
            "Epoch 1032/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 636.5630 - acc: 0.7316 - val_loss: 636.6052 - val_acc: 0.6632\n",
            "Epoch 1033/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 636.5283 - acc: 0.7234 - val_loss: 636.5742 - val_acc: 0.6632\n",
            "Epoch 1034/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 636.5056 - acc: 0.7357 - val_loss: 636.5432 - val_acc: 0.6611\n",
            "Epoch 1035/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 636.4671 - acc: 0.7398 - val_loss: 636.5121 - val_acc: 0.6611\n",
            "Epoch 1036/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 636.4370 - acc: 0.7377 - val_loss: 636.4811 - val_acc: 0.6611\n",
            "Epoch 1037/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 636.4107 - acc: 0.7336 - val_loss: 636.4501 - val_acc: 0.6611\n",
            "Epoch 1038/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 636.3945 - acc: 0.7049 - val_loss: 636.4190 - val_acc: 0.6611\n",
            "Epoch 1039/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 636.3539 - acc: 0.7357 - val_loss: 636.3879 - val_acc: 0.6611\n",
            "Epoch 1040/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 636.3192 - acc: 0.7254 - val_loss: 636.3568 - val_acc: 0.6611\n",
            "Epoch 1041/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 636.2858 - acc: 0.7500 - val_loss: 636.3258 - val_acc: 0.6611\n",
            "Epoch 1042/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 636.2549 - acc: 0.7357 - val_loss: 636.2949 - val_acc: 0.6611\n",
            "Epoch 1043/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 636.2183 - acc: 0.7275 - val_loss: 636.2637 - val_acc: 0.6611\n",
            "Epoch 1044/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 636.1877 - acc: 0.7480 - val_loss: 636.2326 - val_acc: 0.6611\n",
            "Epoch 1045/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 636.1593 - acc: 0.7234 - val_loss: 636.2016 - val_acc: 0.6611\n",
            "Epoch 1046/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 636.1210 - acc: 0.7316 - val_loss: 636.1706 - val_acc: 0.6611\n",
            "Epoch 1047/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 636.0949 - acc: 0.7275 - val_loss: 636.1397 - val_acc: 0.6611\n",
            "Epoch 1048/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 636.0680 - acc: 0.7439 - val_loss: 636.1087 - val_acc: 0.6611\n",
            "Epoch 1049/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 636.0281 - acc: 0.7398 - val_loss: 636.0775 - val_acc: 0.6611\n",
            "Epoch 1050/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 635.9992 - acc: 0.7398 - val_loss: 636.0465 - val_acc: 0.6611\n",
            "Epoch 1051/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 635.9625 - acc: 0.7623 - val_loss: 636.0155 - val_acc: 0.6611\n",
            "Epoch 1052/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 635.9554 - acc: 0.7152 - val_loss: 635.9846 - val_acc: 0.6611\n",
            "Epoch 1053/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 635.9088 - acc: 0.7439 - val_loss: 635.9537 - val_acc: 0.6611\n",
            "Epoch 1054/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 635.8810 - acc: 0.7295 - val_loss: 635.9226 - val_acc: 0.6611\n",
            "Epoch 1055/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 635.8498 - acc: 0.7152 - val_loss: 635.8916 - val_acc: 0.6611\n",
            "Epoch 1056/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 635.8150 - acc: 0.7418 - val_loss: 635.8607 - val_acc: 0.6632\n",
            "Epoch 1057/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 635.7929 - acc: 0.7172 - val_loss: 635.8297 - val_acc: 0.6632\n",
            "Epoch 1058/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 635.7567 - acc: 0.7398 - val_loss: 635.7987 - val_acc: 0.6632\n",
            "Epoch 1059/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 635.7292 - acc: 0.7480 - val_loss: 635.7676 - val_acc: 0.6632\n",
            "Epoch 1060/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 635.6798 - acc: 0.7582 - val_loss: 635.7365 - val_acc: 0.6632\n",
            "Epoch 1061/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 635.6637 - acc: 0.7582 - val_loss: 635.7055 - val_acc: 0.6632\n",
            "Epoch 1062/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 635.6343 - acc: 0.7398 - val_loss: 635.6744 - val_acc: 0.6632\n",
            "Epoch 1063/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 635.6022 - acc: 0.7316 - val_loss: 635.6433 - val_acc: 0.6632\n",
            "Epoch 1064/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 635.5549 - acc: 0.7582 - val_loss: 635.6125 - val_acc: 0.6632\n",
            "Epoch 1065/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 635.5340 - acc: 0.7357 - val_loss: 635.5814 - val_acc: 0.6632\n",
            "Epoch 1066/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 635.5089 - acc: 0.7520 - val_loss: 635.5505 - val_acc: 0.6632\n",
            "Epoch 1067/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 635.4805 - acc: 0.7295 - val_loss: 635.5195 - val_acc: 0.6632\n",
            "Epoch 1068/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 635.4477 - acc: 0.7316 - val_loss: 635.4885 - val_acc: 0.6611\n",
            "Epoch 1069/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 635.4162 - acc: 0.7398 - val_loss: 635.4576 - val_acc: 0.6632\n",
            "Epoch 1070/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 635.3874 - acc: 0.7398 - val_loss: 635.4267 - val_acc: 0.6632\n",
            "Epoch 1071/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 635.3627 - acc: 0.7357 - val_loss: 635.3955 - val_acc: 0.6632\n",
            "Epoch 1072/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 635.3184 - acc: 0.7480 - val_loss: 635.3646 - val_acc: 0.6632\n",
            "Epoch 1073/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 635.2950 - acc: 0.7480 - val_loss: 635.3335 - val_acc: 0.6632\n",
            "Epoch 1074/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 635.2681 - acc: 0.7500 - val_loss: 635.3025 - val_acc: 0.6632\n",
            "Epoch 1075/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 635.2348 - acc: 0.7008 - val_loss: 635.2716 - val_acc: 0.6632\n",
            "Epoch 1076/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 635.1937 - acc: 0.7480 - val_loss: 635.2406 - val_acc: 0.6632\n",
            "Epoch 1077/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 635.1667 - acc: 0.7336 - val_loss: 635.2096 - val_acc: 0.6632\n",
            "Epoch 1078/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 635.1263 - acc: 0.7357 - val_loss: 635.1785 - val_acc: 0.6632\n",
            "Epoch 1079/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 635.1040 - acc: 0.7316 - val_loss: 635.1476 - val_acc: 0.6632\n",
            "Epoch 1080/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 635.0603 - acc: 0.7643 - val_loss: 635.1165 - val_acc: 0.6632\n",
            "Epoch 1081/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 635.0396 - acc: 0.7275 - val_loss: 635.0856 - val_acc: 0.6632\n",
            "Epoch 1082/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 635.0114 - acc: 0.7500 - val_loss: 635.0547 - val_acc: 0.6632\n",
            "Epoch 1083/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 634.9851 - acc: 0.7275 - val_loss: 635.0237 - val_acc: 0.6632\n",
            "Epoch 1084/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 634.9603 - acc: 0.7480 - val_loss: 634.9927 - val_acc: 0.6632\n",
            "Epoch 1085/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 634.9288 - acc: 0.7213 - val_loss: 634.9617 - val_acc: 0.6632\n",
            "Epoch 1086/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 634.8890 - acc: 0.7254 - val_loss: 634.9307 - val_acc: 0.6653\n",
            "Epoch 1087/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 634.8601 - acc: 0.7377 - val_loss: 634.8998 - val_acc: 0.6653\n",
            "Epoch 1088/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 634.8325 - acc: 0.7357 - val_loss: 634.8688 - val_acc: 0.6653\n",
            "Epoch 1089/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 634.8003 - acc: 0.7131 - val_loss: 634.8379 - val_acc: 0.6653\n",
            "Epoch 1090/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 634.7746 - acc: 0.7070 - val_loss: 634.8069 - val_acc: 0.6653\n",
            "Epoch 1091/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 634.7357 - acc: 0.7439 - val_loss: 634.7759 - val_acc: 0.6653\n",
            "Epoch 1092/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 634.7141 - acc: 0.7234 - val_loss: 634.7449 - val_acc: 0.6653\n",
            "Epoch 1093/3000\n",
            "488/488 [==============================] - 0s 318us/step - loss: 634.6799 - acc: 0.7234 - val_loss: 634.7139 - val_acc: 0.6632\n",
            "Epoch 1094/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 634.6337 - acc: 0.7520 - val_loss: 634.6830 - val_acc: 0.6632\n",
            "Epoch 1095/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 634.6195 - acc: 0.7316 - val_loss: 634.6521 - val_acc: 0.6632\n",
            "Epoch 1096/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 634.5737 - acc: 0.7439 - val_loss: 634.6211 - val_acc: 0.6632\n",
            "Epoch 1097/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 634.5384 - acc: 0.7398 - val_loss: 634.5901 - val_acc: 0.6632\n",
            "Epoch 1098/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 634.5210 - acc: 0.7111 - val_loss: 634.5591 - val_acc: 0.6632\n",
            "Epoch 1099/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 634.4775 - acc: 0.7459 - val_loss: 634.5281 - val_acc: 0.6632\n",
            "Epoch 1100/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 634.4556 - acc: 0.7234 - val_loss: 634.4972 - val_acc: 0.6632\n",
            "Epoch 1101/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 634.4178 - acc: 0.7316 - val_loss: 634.4662 - val_acc: 0.6632\n",
            "Epoch 1102/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 634.4004 - acc: 0.7275 - val_loss: 634.4353 - val_acc: 0.6632\n",
            "Epoch 1103/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 634.3443 - acc: 0.7541 - val_loss: 634.4044 - val_acc: 0.6632\n",
            "Epoch 1104/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 634.3264 - acc: 0.7398 - val_loss: 634.3734 - val_acc: 0.6632\n",
            "Epoch 1105/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 634.2972 - acc: 0.7336 - val_loss: 634.3425 - val_acc: 0.6632\n",
            "Epoch 1106/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 634.2731 - acc: 0.7172 - val_loss: 634.3116 - val_acc: 0.6632\n",
            "Epoch 1107/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 634.2405 - acc: 0.7520 - val_loss: 634.2806 - val_acc: 0.6632\n",
            "Epoch 1108/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 634.2109 - acc: 0.7316 - val_loss: 634.2497 - val_acc: 0.6632\n",
            "Epoch 1109/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 634.1873 - acc: 0.7152 - val_loss: 634.2187 - val_acc: 0.6632\n",
            "Epoch 1110/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 634.1420 - acc: 0.7377 - val_loss: 634.1880 - val_acc: 0.6632\n",
            "Epoch 1111/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 634.1093 - acc: 0.7357 - val_loss: 634.1571 - val_acc: 0.6632\n",
            "Epoch 1112/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 634.0780 - acc: 0.7561 - val_loss: 634.1261 - val_acc: 0.6632\n",
            "Epoch 1113/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 634.0489 - acc: 0.7500 - val_loss: 634.0951 - val_acc: 0.6632\n",
            "Epoch 1114/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 634.0212 - acc: 0.7459 - val_loss: 634.0642 - val_acc: 0.6632\n",
            "Epoch 1115/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 633.9855 - acc: 0.7582 - val_loss: 634.0333 - val_acc: 0.6632\n",
            "Epoch 1116/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 633.9583 - acc: 0.7357 - val_loss: 634.0024 - val_acc: 0.6632\n",
            "Epoch 1117/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 633.9293 - acc: 0.7643 - val_loss: 633.9714 - val_acc: 0.6632\n",
            "Epoch 1118/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 633.9085 - acc: 0.7111 - val_loss: 633.9405 - val_acc: 0.6632\n",
            "Epoch 1119/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 633.8543 - acc: 0.7480 - val_loss: 633.9097 - val_acc: 0.6632\n",
            "Epoch 1120/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 633.8443 - acc: 0.7090 - val_loss: 633.8786 - val_acc: 0.6632\n",
            "Epoch 1121/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 633.7974 - acc: 0.7643 - val_loss: 633.8478 - val_acc: 0.6632\n",
            "Epoch 1122/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 633.7821 - acc: 0.7049 - val_loss: 633.8168 - val_acc: 0.6632\n",
            "Epoch 1123/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 633.7394 - acc: 0.7398 - val_loss: 633.7859 - val_acc: 0.6632\n",
            "Epoch 1124/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 633.7128 - acc: 0.7500 - val_loss: 633.7550 - val_acc: 0.6632\n",
            "Epoch 1125/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 633.6839 - acc: 0.7254 - val_loss: 633.7241 - val_acc: 0.6632\n",
            "Epoch 1126/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 633.6556 - acc: 0.7234 - val_loss: 633.6932 - val_acc: 0.6632\n",
            "Epoch 1127/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 633.6131 - acc: 0.7398 - val_loss: 633.6623 - val_acc: 0.6632\n",
            "Epoch 1128/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 633.5787 - acc: 0.7561 - val_loss: 633.6314 - val_acc: 0.6632\n",
            "Epoch 1129/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 633.5577 - acc: 0.7377 - val_loss: 633.6005 - val_acc: 0.6632\n",
            "Epoch 1130/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 633.5260 - acc: 0.7439 - val_loss: 633.5695 - val_acc: 0.6632\n",
            "Epoch 1131/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 633.4953 - acc: 0.7254 - val_loss: 633.5388 - val_acc: 0.6632\n",
            "Epoch 1132/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 633.4596 - acc: 0.7520 - val_loss: 633.5078 - val_acc: 0.6632\n",
            "Epoch 1133/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 633.4356 - acc: 0.7357 - val_loss: 633.4768 - val_acc: 0.6632\n",
            "Epoch 1134/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 633.3987 - acc: 0.7398 - val_loss: 633.4460 - val_acc: 0.6632\n",
            "Epoch 1135/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 633.3634 - acc: 0.7561 - val_loss: 633.4150 - val_acc: 0.6632\n",
            "Epoch 1136/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 633.3354 - acc: 0.7459 - val_loss: 633.3842 - val_acc: 0.6632\n",
            "Epoch 1137/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 633.3010 - acc: 0.7602 - val_loss: 633.3533 - val_acc: 0.6632\n",
            "Epoch 1138/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 633.2866 - acc: 0.7275 - val_loss: 633.3222 - val_acc: 0.6632\n",
            "Epoch 1139/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 633.2555 - acc: 0.7234 - val_loss: 633.2914 - val_acc: 0.6632\n",
            "Epoch 1140/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 633.2128 - acc: 0.7541 - val_loss: 633.2605 - val_acc: 0.6632\n",
            "Epoch 1141/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 633.1769 - acc: 0.7725 - val_loss: 633.2297 - val_acc: 0.6632\n",
            "Epoch 1142/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 633.1556 - acc: 0.7295 - val_loss: 633.1988 - val_acc: 0.6632\n",
            "Epoch 1143/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 633.1356 - acc: 0.7275 - val_loss: 633.1678 - val_acc: 0.6632\n",
            "Epoch 1144/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 633.0984 - acc: 0.7316 - val_loss: 633.1370 - val_acc: 0.6632\n",
            "Epoch 1145/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 633.0572 - acc: 0.7541 - val_loss: 633.1061 - val_acc: 0.6632\n",
            "Epoch 1146/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 633.0259 - acc: 0.7684 - val_loss: 633.0752 - val_acc: 0.6611\n",
            "Epoch 1147/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 633.0034 - acc: 0.7234 - val_loss: 633.0443 - val_acc: 0.6632\n",
            "Epoch 1148/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 632.9817 - acc: 0.7131 - val_loss: 633.0133 - val_acc: 0.6632\n",
            "Epoch 1149/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 632.9412 - acc: 0.7520 - val_loss: 632.9823 - val_acc: 0.6632\n",
            "Epoch 1150/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 632.9058 - acc: 0.7520 - val_loss: 632.9515 - val_acc: 0.6632\n",
            "Epoch 1151/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 632.8683 - acc: 0.7500 - val_loss: 632.9205 - val_acc: 0.6611\n",
            "Epoch 1152/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 632.8396 - acc: 0.7459 - val_loss: 632.8897 - val_acc: 0.6611\n",
            "Epoch 1153/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 632.8127 - acc: 0.7500 - val_loss: 632.8587 - val_acc: 0.6611\n",
            "Epoch 1154/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 632.7854 - acc: 0.7520 - val_loss: 632.8278 - val_acc: 0.6611\n",
            "Epoch 1155/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 632.7503 - acc: 0.7459 - val_loss: 632.7970 - val_acc: 0.6611\n",
            "Epoch 1156/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 632.7193 - acc: 0.7439 - val_loss: 632.7661 - val_acc: 0.6611\n",
            "Epoch 1157/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 632.6903 - acc: 0.7439 - val_loss: 632.7352 - val_acc: 0.6611\n",
            "Epoch 1158/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 632.6549 - acc: 0.7398 - val_loss: 632.7043 - val_acc: 0.6611\n",
            "Epoch 1159/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 632.6320 - acc: 0.7561 - val_loss: 632.6734 - val_acc: 0.6611\n",
            "Epoch 1160/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 632.5985 - acc: 0.7418 - val_loss: 632.6426 - val_acc: 0.6611\n",
            "Epoch 1161/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 632.5647 - acc: 0.7582 - val_loss: 632.6117 - val_acc: 0.6611\n",
            "Epoch 1162/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 632.5225 - acc: 0.7602 - val_loss: 632.5807 - val_acc: 0.6611\n",
            "Epoch 1163/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 632.4999 - acc: 0.7520 - val_loss: 632.5498 - val_acc: 0.6611\n",
            "Epoch 1164/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 632.4734 - acc: 0.7275 - val_loss: 632.5187 - val_acc: 0.6611\n",
            "Epoch 1165/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 632.4389 - acc: 0.7561 - val_loss: 632.4880 - val_acc: 0.6611\n",
            "Epoch 1166/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 632.4119 - acc: 0.7459 - val_loss: 632.4571 - val_acc: 0.6611\n",
            "Epoch 1167/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 632.3731 - acc: 0.7684 - val_loss: 632.4263 - val_acc: 0.6611\n",
            "Epoch 1168/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 632.3634 - acc: 0.7439 - val_loss: 632.3954 - val_acc: 0.6611\n",
            "Epoch 1169/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 632.3198 - acc: 0.7377 - val_loss: 632.3644 - val_acc: 0.6611\n",
            "Epoch 1170/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 632.2805 - acc: 0.7889 - val_loss: 632.3336 - val_acc: 0.6611\n",
            "Epoch 1171/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 632.2545 - acc: 0.7439 - val_loss: 632.3028 - val_acc: 0.6611\n",
            "Epoch 1172/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 632.2321 - acc: 0.7234 - val_loss: 632.2720 - val_acc: 0.6611\n",
            "Epoch 1173/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 632.1957 - acc: 0.7316 - val_loss: 632.2411 - val_acc: 0.6611\n",
            "Epoch 1174/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 632.1704 - acc: 0.7357 - val_loss: 632.2101 - val_acc: 0.6611\n",
            "Epoch 1175/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 632.1344 - acc: 0.7275 - val_loss: 632.1794 - val_acc: 0.6611\n",
            "Epoch 1176/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 632.1044 - acc: 0.7295 - val_loss: 632.1483 - val_acc: 0.6611\n",
            "Epoch 1177/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 632.0684 - acc: 0.7582 - val_loss: 632.1175 - val_acc: 0.6611\n",
            "Epoch 1178/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 632.0423 - acc: 0.7398 - val_loss: 632.0866 - val_acc: 0.6611\n",
            "Epoch 1179/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 632.0082 - acc: 0.7377 - val_loss: 632.0557 - val_acc: 0.6611\n",
            "Epoch 1180/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 631.9787 - acc: 0.7520 - val_loss: 632.0249 - val_acc: 0.6611\n",
            "Epoch 1181/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 631.9544 - acc: 0.7295 - val_loss: 631.9940 - val_acc: 0.6611\n",
            "Epoch 1182/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 631.9084 - acc: 0.7541 - val_loss: 631.9632 - val_acc: 0.6611\n",
            "Epoch 1183/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 631.8727 - acc: 0.7520 - val_loss: 631.9324 - val_acc: 0.6611\n",
            "Epoch 1184/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 631.8660 - acc: 0.7152 - val_loss: 631.9016 - val_acc: 0.6611\n",
            "Epoch 1185/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 631.8245 - acc: 0.7377 - val_loss: 631.8707 - val_acc: 0.6611\n",
            "Epoch 1186/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 631.7881 - acc: 0.7480 - val_loss: 631.8399 - val_acc: 0.6611\n",
            "Epoch 1187/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 631.7595 - acc: 0.7459 - val_loss: 631.8090 - val_acc: 0.6611\n",
            "Epoch 1188/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 631.7293 - acc: 0.7582 - val_loss: 631.7782 - val_acc: 0.6611\n",
            "Epoch 1189/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 631.7061 - acc: 0.7377 - val_loss: 631.7473 - val_acc: 0.6611\n",
            "Epoch 1190/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 631.6699 - acc: 0.7172 - val_loss: 631.7164 - val_acc: 0.6611\n",
            "Epoch 1191/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 631.6482 - acc: 0.7193 - val_loss: 631.6855 - val_acc: 0.6611\n",
            "Epoch 1192/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 631.6203 - acc: 0.7295 - val_loss: 631.6546 - val_acc: 0.6632\n",
            "Epoch 1193/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 631.5832 - acc: 0.7623 - val_loss: 631.6238 - val_acc: 0.6632\n",
            "Epoch 1194/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 631.5587 - acc: 0.7234 - val_loss: 631.5928 - val_acc: 0.6632\n",
            "Epoch 1195/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 631.5154 - acc: 0.7336 - val_loss: 631.5619 - val_acc: 0.6653\n",
            "Epoch 1196/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 631.4755 - acc: 0.7480 - val_loss: 631.5311 - val_acc: 0.6632\n",
            "Epoch 1197/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 631.4406 - acc: 0.7807 - val_loss: 631.5002 - val_acc: 0.6653\n",
            "Epoch 1198/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 631.4245 - acc: 0.7398 - val_loss: 631.4694 - val_acc: 0.6653\n",
            "Epoch 1199/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 631.3878 - acc: 0.7500 - val_loss: 631.4386 - val_acc: 0.6653\n",
            "Epoch 1200/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 631.3661 - acc: 0.7377 - val_loss: 631.4078 - val_acc: 0.6653\n",
            "Epoch 1201/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 631.3211 - acc: 0.7766 - val_loss: 631.3769 - val_acc: 0.6653\n",
            "Epoch 1202/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 631.3036 - acc: 0.7623 - val_loss: 631.3460 - val_acc: 0.6632\n",
            "Epoch 1203/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 631.2744 - acc: 0.7357 - val_loss: 631.3152 - val_acc: 0.6632\n",
            "Epoch 1204/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 631.2302 - acc: 0.7520 - val_loss: 631.2844 - val_acc: 0.6611\n",
            "Epoch 1205/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 631.2007 - acc: 0.7561 - val_loss: 631.2534 - val_acc: 0.6611\n",
            "Epoch 1206/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 631.1818 - acc: 0.7193 - val_loss: 631.2226 - val_acc: 0.6611\n",
            "Epoch 1207/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 631.1428 - acc: 0.7541 - val_loss: 631.1917 - val_acc: 0.6611\n",
            "Epoch 1208/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 631.1172 - acc: 0.7480 - val_loss: 631.1610 - val_acc: 0.6632\n",
            "Epoch 1209/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 631.0821 - acc: 0.7582 - val_loss: 631.1302 - val_acc: 0.6611\n",
            "Epoch 1210/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 631.0503 - acc: 0.7459 - val_loss: 631.0994 - val_acc: 0.6632\n",
            "Epoch 1211/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 631.0219 - acc: 0.7275 - val_loss: 631.0684 - val_acc: 0.6611\n",
            "Epoch 1212/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 630.9926 - acc: 0.7234 - val_loss: 631.0376 - val_acc: 0.6632\n",
            "Epoch 1213/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 630.9664 - acc: 0.7295 - val_loss: 631.0068 - val_acc: 0.6632\n",
            "Epoch 1214/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 630.9322 - acc: 0.7439 - val_loss: 630.9759 - val_acc: 0.6653\n",
            "Epoch 1215/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 630.8955 - acc: 0.7520 - val_loss: 630.9452 - val_acc: 0.6632\n",
            "Epoch 1216/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 630.8745 - acc: 0.7295 - val_loss: 630.9143 - val_acc: 0.6632\n",
            "Epoch 1217/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 630.8364 - acc: 0.7459 - val_loss: 630.8835 - val_acc: 0.6632\n",
            "Epoch 1218/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 630.8036 - acc: 0.7357 - val_loss: 630.8526 - val_acc: 0.6632\n",
            "Epoch 1219/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 630.7711 - acc: 0.7418 - val_loss: 630.8219 - val_acc: 0.6632\n",
            "Epoch 1220/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 630.7486 - acc: 0.7234 - val_loss: 630.7910 - val_acc: 0.6632\n",
            "Epoch 1221/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 630.7087 - acc: 0.7643 - val_loss: 630.7601 - val_acc: 0.6632\n",
            "Epoch 1222/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 630.6764 - acc: 0.7520 - val_loss: 630.7293 - val_acc: 0.6632\n",
            "Epoch 1223/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 630.6560 - acc: 0.7254 - val_loss: 630.6985 - val_acc: 0.6632\n",
            "Epoch 1224/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 630.6294 - acc: 0.7398 - val_loss: 630.6676 - val_acc: 0.6632\n",
            "Epoch 1225/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 630.5820 - acc: 0.7664 - val_loss: 630.6368 - val_acc: 0.6653\n",
            "Epoch 1226/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 630.5587 - acc: 0.7295 - val_loss: 630.6059 - val_acc: 0.6653\n",
            "Epoch 1227/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 630.5227 - acc: 0.7480 - val_loss: 630.5749 - val_acc: 0.6653\n",
            "Epoch 1228/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 630.5048 - acc: 0.7418 - val_loss: 630.5441 - val_acc: 0.6653\n",
            "Epoch 1229/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 630.4580 - acc: 0.7500 - val_loss: 630.5134 - val_acc: 0.6653\n",
            "Epoch 1230/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 630.4432 - acc: 0.7336 - val_loss: 630.4826 - val_acc: 0.6653\n",
            "Epoch 1231/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 630.4016 - acc: 0.7623 - val_loss: 630.4518 - val_acc: 0.6653\n",
            "Epoch 1232/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 630.3647 - acc: 0.7459 - val_loss: 630.4209 - val_acc: 0.6653\n",
            "Epoch 1233/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 630.3462 - acc: 0.7398 - val_loss: 630.3901 - val_acc: 0.6653\n",
            "Epoch 1234/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 630.3162 - acc: 0.7582 - val_loss: 630.3592 - val_acc: 0.6632\n",
            "Epoch 1235/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 630.2620 - acc: 0.7889 - val_loss: 630.3284 - val_acc: 0.6653\n",
            "Epoch 1236/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 630.2551 - acc: 0.7500 - val_loss: 630.2975 - val_acc: 0.6632\n",
            "Epoch 1237/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 630.2273 - acc: 0.7316 - val_loss: 630.2668 - val_acc: 0.6632\n",
            "Epoch 1238/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 630.1913 - acc: 0.7316 - val_loss: 630.2359 - val_acc: 0.6632\n",
            "Epoch 1239/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 630.1556 - acc: 0.7357 - val_loss: 630.2051 - val_acc: 0.6632\n",
            "Epoch 1240/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 630.1194 - acc: 0.7520 - val_loss: 630.1742 - val_acc: 0.6653\n",
            "Epoch 1241/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 630.1079 - acc: 0.6988 - val_loss: 630.1434 - val_acc: 0.6653\n",
            "Epoch 1242/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 630.0690 - acc: 0.7480 - val_loss: 630.1127 - val_acc: 0.6674\n",
            "Epoch 1243/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 630.0320 - acc: 0.7602 - val_loss: 630.0818 - val_acc: 0.6653\n",
            "Epoch 1244/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 629.9952 - acc: 0.7582 - val_loss: 630.0509 - val_acc: 0.6653\n",
            "Epoch 1245/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 629.9697 - acc: 0.7582 - val_loss: 630.0201 - val_acc: 0.6632\n",
            "Epoch 1246/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 629.9464 - acc: 0.7418 - val_loss: 629.9893 - val_acc: 0.6653\n",
            "Epoch 1247/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 629.9115 - acc: 0.7500 - val_loss: 629.9584 - val_acc: 0.6653\n",
            "Epoch 1248/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 629.8855 - acc: 0.7357 - val_loss: 629.9276 - val_acc: 0.6653\n",
            "Epoch 1249/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 629.8474 - acc: 0.7459 - val_loss: 629.8967 - val_acc: 0.6653\n",
            "Epoch 1250/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 629.8222 - acc: 0.7357 - val_loss: 629.8658 - val_acc: 0.6653\n",
            "Epoch 1251/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 629.7991 - acc: 0.7398 - val_loss: 629.8351 - val_acc: 0.6653\n",
            "Epoch 1252/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 629.7647 - acc: 0.7275 - val_loss: 629.8042 - val_acc: 0.6674\n",
            "Epoch 1253/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 629.7286 - acc: 0.7602 - val_loss: 629.7736 - val_acc: 0.6653\n",
            "Epoch 1254/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 629.6882 - acc: 0.7746 - val_loss: 629.7427 - val_acc: 0.6674\n",
            "Epoch 1255/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 629.6620 - acc: 0.7357 - val_loss: 629.7118 - val_acc: 0.6653\n",
            "Epoch 1256/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 629.6260 - acc: 0.7459 - val_loss: 629.6811 - val_acc: 0.6653\n",
            "Epoch 1257/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 629.6104 - acc: 0.7541 - val_loss: 629.6502 - val_acc: 0.6674\n",
            "Epoch 1258/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 629.5722 - acc: 0.7643 - val_loss: 629.6194 - val_acc: 0.6632\n",
            "Epoch 1259/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 629.5442 - acc: 0.7439 - val_loss: 629.5886 - val_acc: 0.6632\n",
            "Epoch 1260/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 629.5122 - acc: 0.7398 - val_loss: 629.5578 - val_acc: 0.6632\n",
            "Epoch 1261/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 629.4854 - acc: 0.7561 - val_loss: 629.5271 - val_acc: 0.6653\n",
            "Epoch 1262/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 629.4435 - acc: 0.7684 - val_loss: 629.4963 - val_acc: 0.6653\n",
            "Epoch 1263/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 629.4225 - acc: 0.7275 - val_loss: 629.4655 - val_acc: 0.6653\n",
            "Epoch 1264/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 629.3902 - acc: 0.7623 - val_loss: 629.4348 - val_acc: 0.6653\n",
            "Epoch 1265/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 629.3591 - acc: 0.7357 - val_loss: 629.4040 - val_acc: 0.6632\n",
            "Epoch 1266/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 629.3334 - acc: 0.7213 - val_loss: 629.3731 - val_acc: 0.6632\n",
            "Epoch 1267/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 629.2985 - acc: 0.7459 - val_loss: 629.3423 - val_acc: 0.6632\n",
            "Epoch 1268/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 629.2553 - acc: 0.7623 - val_loss: 629.3115 - val_acc: 0.6632\n",
            "Epoch 1269/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 629.2457 - acc: 0.7377 - val_loss: 629.2808 - val_acc: 0.6674\n",
            "Epoch 1270/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 629.2088 - acc: 0.7377 - val_loss: 629.2499 - val_acc: 0.6674\n",
            "Epoch 1271/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 629.1681 - acc: 0.7459 - val_loss: 629.2192 - val_acc: 0.6674\n",
            "Epoch 1272/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 629.1447 - acc: 0.7459 - val_loss: 629.1882 - val_acc: 0.6674\n",
            "Epoch 1273/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 629.1048 - acc: 0.7684 - val_loss: 629.1575 - val_acc: 0.6674\n",
            "Epoch 1274/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 629.0805 - acc: 0.7623 - val_loss: 629.1266 - val_acc: 0.6674\n",
            "Epoch 1275/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 629.0495 - acc: 0.7541 - val_loss: 629.0958 - val_acc: 0.6653\n",
            "Epoch 1276/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 629.0271 - acc: 0.7275 - val_loss: 629.0649 - val_acc: 0.6653\n",
            "Epoch 1277/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 628.9870 - acc: 0.7377 - val_loss: 629.0342 - val_acc: 0.6632\n",
            "Epoch 1278/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 628.9597 - acc: 0.7541 - val_loss: 629.0034 - val_acc: 0.6632\n",
            "Epoch 1279/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 628.9353 - acc: 0.7357 - val_loss: 628.9726 - val_acc: 0.6632\n",
            "Epoch 1280/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 628.9049 - acc: 0.7480 - val_loss: 628.9419 - val_acc: 0.6632\n",
            "Epoch 1281/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 628.8594 - acc: 0.7684 - val_loss: 628.9111 - val_acc: 0.6653\n",
            "Epoch 1282/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 628.8306 - acc: 0.7643 - val_loss: 628.8803 - val_acc: 0.6674\n",
            "Epoch 1283/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 628.7886 - acc: 0.7828 - val_loss: 628.8494 - val_acc: 0.6653\n",
            "Epoch 1284/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 628.7756 - acc: 0.7520 - val_loss: 628.8187 - val_acc: 0.6653\n",
            "Epoch 1285/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 628.7447 - acc: 0.7295 - val_loss: 628.7879 - val_acc: 0.6653\n",
            "Epoch 1286/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 628.7147 - acc: 0.7336 - val_loss: 628.7571 - val_acc: 0.6674\n",
            "Epoch 1287/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 628.6880 - acc: 0.7561 - val_loss: 628.7261 - val_acc: 0.6695\n",
            "Epoch 1288/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 628.6446 - acc: 0.7766 - val_loss: 628.6953 - val_acc: 0.6674\n",
            "Epoch 1289/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 628.6194 - acc: 0.7439 - val_loss: 628.6645 - val_acc: 0.6674\n",
            "Epoch 1290/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 628.5894 - acc: 0.7398 - val_loss: 628.6338 - val_acc: 0.6653\n",
            "Epoch 1291/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 628.5613 - acc: 0.7459 - val_loss: 628.6030 - val_acc: 0.6695\n",
            "Epoch 1292/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 628.5165 - acc: 0.7807 - val_loss: 628.5722 - val_acc: 0.6695\n",
            "Epoch 1293/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 628.4989 - acc: 0.7480 - val_loss: 628.5414 - val_acc: 0.6695\n",
            "Epoch 1294/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 628.4602 - acc: 0.7684 - val_loss: 628.5106 - val_acc: 0.6695\n",
            "Epoch 1295/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 628.4424 - acc: 0.7480 - val_loss: 628.4800 - val_acc: 0.6695\n",
            "Epoch 1296/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 628.3986 - acc: 0.7684 - val_loss: 628.4492 - val_acc: 0.6695\n",
            "Epoch 1297/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 628.3750 - acc: 0.7500 - val_loss: 628.4183 - val_acc: 0.6695\n",
            "Epoch 1298/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 628.3374 - acc: 0.7684 - val_loss: 628.3876 - val_acc: 0.6695\n",
            "Epoch 1299/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 628.2955 - acc: 0.7889 - val_loss: 628.3568 - val_acc: 0.6695\n",
            "Epoch 1300/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 628.2757 - acc: 0.7520 - val_loss: 628.3261 - val_acc: 0.6695\n",
            "Epoch 1301/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 628.2505 - acc: 0.7541 - val_loss: 628.2954 - val_acc: 0.6674\n",
            "Epoch 1302/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 628.2152 - acc: 0.7664 - val_loss: 628.2646 - val_acc: 0.6653\n",
            "Epoch 1303/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 628.1804 - acc: 0.7459 - val_loss: 628.2339 - val_acc: 0.6653\n",
            "Epoch 1304/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 628.1597 - acc: 0.7357 - val_loss: 628.2031 - val_acc: 0.6674\n",
            "Epoch 1305/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 628.1190 - acc: 0.7480 - val_loss: 628.1724 - val_acc: 0.6674\n",
            "Epoch 1306/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 628.0898 - acc: 0.7561 - val_loss: 628.1417 - val_acc: 0.6653\n",
            "Epoch 1307/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 628.0634 - acc: 0.7336 - val_loss: 628.1108 - val_acc: 0.6653\n",
            "Epoch 1308/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 628.0246 - acc: 0.7541 - val_loss: 628.0801 - val_acc: 0.6653\n",
            "Epoch 1309/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 628.0147 - acc: 0.7480 - val_loss: 628.0493 - val_acc: 0.6653\n",
            "Epoch 1310/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 627.9704 - acc: 0.7602 - val_loss: 628.0186 - val_acc: 0.6632\n",
            "Epoch 1311/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 627.9494 - acc: 0.7561 - val_loss: 627.9879 - val_acc: 0.6653\n",
            "Epoch 1312/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 627.9138 - acc: 0.7439 - val_loss: 627.9571 - val_acc: 0.6695\n",
            "Epoch 1313/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 627.8730 - acc: 0.7357 - val_loss: 627.9263 - val_acc: 0.6695\n",
            "Epoch 1314/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 627.8324 - acc: 0.7725 - val_loss: 627.8956 - val_acc: 0.6695\n",
            "Epoch 1315/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 627.8198 - acc: 0.7520 - val_loss: 627.8649 - val_acc: 0.6674\n",
            "Epoch 1316/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 627.7863 - acc: 0.7623 - val_loss: 627.8340 - val_acc: 0.6674\n",
            "Epoch 1317/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 627.7596 - acc: 0.7561 - val_loss: 627.8031 - val_acc: 0.6695\n",
            "Epoch 1318/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 627.7238 - acc: 0.7602 - val_loss: 627.7724 - val_acc: 0.6695\n",
            "Epoch 1319/3000\n",
            "488/488 [==============================] - 0s 319us/step - loss: 627.6955 - acc: 0.7480 - val_loss: 627.7417 - val_acc: 0.6695\n",
            "Epoch 1320/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 627.6613 - acc: 0.7684 - val_loss: 627.7110 - val_acc: 0.6674\n",
            "Epoch 1321/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 627.6391 - acc: 0.7500 - val_loss: 627.6803 - val_acc: 0.6653\n",
            "Epoch 1322/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 627.5889 - acc: 0.7664 - val_loss: 627.6494 - val_acc: 0.6653\n",
            "Epoch 1323/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 627.5640 - acc: 0.7643 - val_loss: 627.6187 - val_acc: 0.6674\n",
            "Epoch 1324/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 627.5344 - acc: 0.7643 - val_loss: 627.5879 - val_acc: 0.6695\n",
            "Epoch 1325/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 627.5084 - acc: 0.7643 - val_loss: 627.5572 - val_acc: 0.6695\n",
            "Epoch 1326/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 627.4794 - acc: 0.7459 - val_loss: 627.5264 - val_acc: 0.6695\n",
            "Epoch 1327/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 627.4433 - acc: 0.7541 - val_loss: 627.4957 - val_acc: 0.6695\n",
            "Epoch 1328/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 627.4223 - acc: 0.7439 - val_loss: 627.4649 - val_acc: 0.6695\n",
            "Epoch 1329/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 627.3876 - acc: 0.7541 - val_loss: 627.4342 - val_acc: 0.6674\n",
            "Epoch 1330/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 627.3510 - acc: 0.7541 - val_loss: 627.4035 - val_acc: 0.6674\n",
            "Epoch 1331/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 627.3275 - acc: 0.7193 - val_loss: 627.3727 - val_acc: 0.6674\n",
            "Epoch 1332/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 627.2865 - acc: 0.7418 - val_loss: 627.3419 - val_acc: 0.6674\n",
            "Epoch 1333/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 627.2597 - acc: 0.7520 - val_loss: 627.3112 - val_acc: 0.6674\n",
            "Epoch 1334/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 627.2372 - acc: 0.7541 - val_loss: 627.2805 - val_acc: 0.6674\n",
            "Epoch 1335/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 627.2034 - acc: 0.7336 - val_loss: 627.2498 - val_acc: 0.6674\n",
            "Epoch 1336/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 627.1660 - acc: 0.7480 - val_loss: 627.2191 - val_acc: 0.6674\n",
            "Epoch 1337/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 627.1494 - acc: 0.7377 - val_loss: 627.1883 - val_acc: 0.6674\n",
            "Epoch 1338/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 627.1078 - acc: 0.7480 - val_loss: 627.1576 - val_acc: 0.6695\n",
            "Epoch 1339/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 627.0775 - acc: 0.7480 - val_loss: 627.1269 - val_acc: 0.6695\n",
            "Epoch 1340/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 627.0473 - acc: 0.7684 - val_loss: 627.0961 - val_acc: 0.6695\n",
            "Epoch 1341/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 627.0232 - acc: 0.7336 - val_loss: 627.0653 - val_acc: 0.6695\n",
            "Epoch 1342/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 626.9871 - acc: 0.7520 - val_loss: 627.0347 - val_acc: 0.6674\n",
            "Epoch 1343/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 626.9605 - acc: 0.7275 - val_loss: 627.0038 - val_acc: 0.6653\n",
            "Epoch 1344/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 626.9181 - acc: 0.7623 - val_loss: 626.9731 - val_acc: 0.6674\n",
            "Epoch 1345/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 626.8841 - acc: 0.7664 - val_loss: 626.9423 - val_acc: 0.6674\n",
            "Epoch 1346/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 626.8626 - acc: 0.7684 - val_loss: 626.9116 - val_acc: 0.6695\n",
            "Epoch 1347/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 626.8373 - acc: 0.7643 - val_loss: 626.8809 - val_acc: 0.6695\n",
            "Epoch 1348/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 626.8053 - acc: 0.7336 - val_loss: 626.8502 - val_acc: 0.6695\n",
            "Epoch 1349/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 626.7677 - acc: 0.7746 - val_loss: 626.8195 - val_acc: 0.6695\n",
            "Epoch 1350/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 626.7346 - acc: 0.7889 - val_loss: 626.7888 - val_acc: 0.6674\n",
            "Epoch 1351/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 626.7033 - acc: 0.7582 - val_loss: 626.7580 - val_acc: 0.6674\n",
            "Epoch 1352/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 626.6770 - acc: 0.7561 - val_loss: 626.7272 - val_acc: 0.6674\n",
            "Epoch 1353/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 626.6361 - acc: 0.7869 - val_loss: 626.6966 - val_acc: 0.6695\n",
            "Epoch 1354/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 626.6272 - acc: 0.7418 - val_loss: 626.6659 - val_acc: 0.6674\n",
            "Epoch 1355/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 626.5912 - acc: 0.7500 - val_loss: 626.6351 - val_acc: 0.6674\n",
            "Epoch 1356/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 626.5421 - acc: 0.7889 - val_loss: 626.6044 - val_acc: 0.6674\n",
            "Epoch 1357/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 626.5171 - acc: 0.7746 - val_loss: 626.5735 - val_acc: 0.6674\n",
            "Epoch 1358/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 626.4877 - acc: 0.7602 - val_loss: 626.5427 - val_acc: 0.6674\n",
            "Epoch 1359/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 626.4609 - acc: 0.7664 - val_loss: 626.5121 - val_acc: 0.6674\n",
            "Epoch 1360/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 626.4357 - acc: 0.7561 - val_loss: 626.4814 - val_acc: 0.6674\n",
            "Epoch 1361/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 626.4043 - acc: 0.7480 - val_loss: 626.4506 - val_acc: 0.6674\n",
            "Epoch 1362/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 626.3726 - acc: 0.7541 - val_loss: 626.4199 - val_acc: 0.6674\n",
            "Epoch 1363/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 626.3464 - acc: 0.7459 - val_loss: 626.3892 - val_acc: 0.6674\n",
            "Epoch 1364/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 626.3024 - acc: 0.7951 - val_loss: 626.3585 - val_acc: 0.6674\n",
            "Epoch 1365/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 626.2733 - acc: 0.7746 - val_loss: 626.3279 - val_acc: 0.6674\n",
            "Epoch 1366/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 626.2457 - acc: 0.7500 - val_loss: 626.2972 - val_acc: 0.6674\n",
            "Epoch 1367/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 626.2075 - acc: 0.7643 - val_loss: 626.2665 - val_acc: 0.6674\n",
            "Epoch 1368/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 626.1729 - acc: 0.7848 - val_loss: 626.2358 - val_acc: 0.6674\n",
            "Epoch 1369/3000\n",
            "488/488 [==============================] - 0s 375us/step - loss: 626.1531 - acc: 0.7561 - val_loss: 626.2050 - val_acc: 0.6674\n",
            "Epoch 1370/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 626.1238 - acc: 0.7561 - val_loss: 626.1743 - val_acc: 0.6674\n",
            "Epoch 1371/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 626.0869 - acc: 0.7725 - val_loss: 626.1436 - val_acc: 0.6674\n",
            "Epoch 1372/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 626.0598 - acc: 0.7664 - val_loss: 626.1129 - val_acc: 0.6674\n",
            "Epoch 1373/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 626.0225 - acc: 0.7787 - val_loss: 626.0822 - val_acc: 0.6674\n",
            "Epoch 1374/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 626.0053 - acc: 0.7336 - val_loss: 626.0515 - val_acc: 0.6674\n",
            "Epoch 1375/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 625.9693 - acc: 0.7746 - val_loss: 626.0208 - val_acc: 0.6674\n",
            "Epoch 1376/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 625.9340 - acc: 0.7746 - val_loss: 625.9901 - val_acc: 0.6674\n",
            "Epoch 1377/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 625.9102 - acc: 0.7439 - val_loss: 625.9593 - val_acc: 0.6674\n",
            "Epoch 1378/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 625.8814 - acc: 0.7398 - val_loss: 625.9286 - val_acc: 0.6674\n",
            "Epoch 1379/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 625.8578 - acc: 0.7357 - val_loss: 625.8979 - val_acc: 0.6674\n",
            "Epoch 1380/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 625.8142 - acc: 0.7582 - val_loss: 625.8673 - val_acc: 0.6674\n",
            "Epoch 1381/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 625.7753 - acc: 0.7705 - val_loss: 625.8367 - val_acc: 0.6674\n",
            "Epoch 1382/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 625.7493 - acc: 0.7582 - val_loss: 625.8061 - val_acc: 0.6674\n",
            "Epoch 1383/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 625.7340 - acc: 0.7439 - val_loss: 625.7753 - val_acc: 0.6674\n",
            "Epoch 1384/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 625.6948 - acc: 0.7664 - val_loss: 625.7446 - val_acc: 0.6674\n",
            "Epoch 1385/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 625.6675 - acc: 0.7520 - val_loss: 625.7139 - val_acc: 0.6674\n",
            "Epoch 1386/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 625.6286 - acc: 0.7643 - val_loss: 625.6833 - val_acc: 0.6674\n",
            "Epoch 1387/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 625.6083 - acc: 0.7398 - val_loss: 625.6526 - val_acc: 0.6674\n",
            "Epoch 1388/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 625.5689 - acc: 0.7643 - val_loss: 625.6220 - val_acc: 0.6653\n",
            "Epoch 1389/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 625.5518 - acc: 0.7439 - val_loss: 625.5914 - val_acc: 0.6653\n",
            "Epoch 1390/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 625.5015 - acc: 0.7684 - val_loss: 625.5606 - val_acc: 0.6653\n",
            "Epoch 1391/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 625.4844 - acc: 0.7541 - val_loss: 625.5300 - val_acc: 0.6653\n",
            "Epoch 1392/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 625.4509 - acc: 0.7541 - val_loss: 625.4993 - val_acc: 0.6653\n",
            "Epoch 1393/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 625.4119 - acc: 0.7541 - val_loss: 625.4686 - val_acc: 0.6653\n",
            "Epoch 1394/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 625.3908 - acc: 0.7664 - val_loss: 625.4380 - val_acc: 0.6653\n",
            "Epoch 1395/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 625.3592 - acc: 0.7541 - val_loss: 625.4073 - val_acc: 0.6653\n",
            "Epoch 1396/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 625.3208 - acc: 0.7684 - val_loss: 625.3766 - val_acc: 0.6653\n",
            "Epoch 1397/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 625.3001 - acc: 0.7643 - val_loss: 625.3460 - val_acc: 0.6653\n",
            "Epoch 1398/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 625.2654 - acc: 0.7582 - val_loss: 625.3152 - val_acc: 0.6674\n",
            "Epoch 1399/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 625.2417 - acc: 0.7766 - val_loss: 625.2844 - val_acc: 0.6674\n",
            "Epoch 1400/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 625.2019 - acc: 0.7746 - val_loss: 625.2536 - val_acc: 0.6674\n",
            "Epoch 1401/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 625.1667 - acc: 0.7418 - val_loss: 625.2231 - val_acc: 0.6674\n",
            "Epoch 1402/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 625.1431 - acc: 0.7541 - val_loss: 625.1924 - val_acc: 0.6674\n",
            "Epoch 1403/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 625.1146 - acc: 0.7500 - val_loss: 625.1616 - val_acc: 0.6674\n",
            "Epoch 1404/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 625.0820 - acc: 0.7643 - val_loss: 625.1309 - val_acc: 0.6695\n",
            "Epoch 1405/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 625.0476 - acc: 0.7705 - val_loss: 625.1002 - val_acc: 0.6715\n",
            "Epoch 1406/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 625.0187 - acc: 0.7643 - val_loss: 625.0696 - val_acc: 0.6695\n",
            "Epoch 1407/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 624.9829 - acc: 0.7582 - val_loss: 625.0388 - val_acc: 0.6695\n",
            "Epoch 1408/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 624.9531 - acc: 0.7664 - val_loss: 625.0082 - val_acc: 0.6695\n",
            "Epoch 1409/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 624.9335 - acc: 0.7520 - val_loss: 624.9775 - val_acc: 0.6695\n",
            "Epoch 1410/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 624.8944 - acc: 0.7480 - val_loss: 624.9468 - val_acc: 0.6715\n",
            "Epoch 1411/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 624.8607 - acc: 0.7602 - val_loss: 624.9162 - val_acc: 0.6715\n",
            "Epoch 1412/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 624.8460 - acc: 0.7439 - val_loss: 624.8856 - val_acc: 0.6695\n",
            "Epoch 1413/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 624.8131 - acc: 0.7336 - val_loss: 624.8548 - val_acc: 0.6674\n",
            "Epoch 1414/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 624.7652 - acc: 0.7766 - val_loss: 624.8242 - val_acc: 0.6695\n",
            "Epoch 1415/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 624.7457 - acc: 0.7623 - val_loss: 624.7936 - val_acc: 0.6695\n",
            "Epoch 1416/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 624.7081 - acc: 0.7582 - val_loss: 624.7628 - val_acc: 0.6695\n",
            "Epoch 1417/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 624.6816 - acc: 0.7623 - val_loss: 624.7322 - val_acc: 0.6674\n",
            "Epoch 1418/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 624.6372 - acc: 0.7623 - val_loss: 624.7015 - val_acc: 0.6674\n",
            "Epoch 1419/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 624.6150 - acc: 0.7684 - val_loss: 624.6708 - val_acc: 0.6674\n",
            "Epoch 1420/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 624.5860 - acc: 0.7623 - val_loss: 624.6400 - val_acc: 0.6695\n",
            "Epoch 1421/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 624.5569 - acc: 0.7500 - val_loss: 624.6094 - val_acc: 0.6695\n",
            "Epoch 1422/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 624.5315 - acc: 0.7254 - val_loss: 624.5787 - val_acc: 0.6674\n",
            "Epoch 1423/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 624.5081 - acc: 0.7480 - val_loss: 624.5481 - val_acc: 0.6695\n",
            "Epoch 1424/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 624.4601 - acc: 0.7807 - val_loss: 624.5172 - val_acc: 0.6695\n",
            "Epoch 1425/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 624.4324 - acc: 0.7582 - val_loss: 624.4866 - val_acc: 0.6695\n",
            "Epoch 1426/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 624.4004 - acc: 0.7725 - val_loss: 624.4559 - val_acc: 0.6674\n",
            "Epoch 1427/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 624.3729 - acc: 0.7643 - val_loss: 624.4254 - val_acc: 0.6674\n",
            "Epoch 1428/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 624.3551 - acc: 0.7418 - val_loss: 624.3947 - val_acc: 0.6674\n",
            "Epoch 1429/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 624.3088 - acc: 0.7561 - val_loss: 624.3640 - val_acc: 0.6653\n",
            "Epoch 1430/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 624.2965 - acc: 0.7541 - val_loss: 624.3334 - val_acc: 0.6653\n",
            "Epoch 1431/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 624.2481 - acc: 0.7705 - val_loss: 624.3028 - val_acc: 0.6653\n",
            "Epoch 1432/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 624.2250 - acc: 0.7500 - val_loss: 624.2720 - val_acc: 0.6674\n",
            "Epoch 1433/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 624.1875 - acc: 0.7705 - val_loss: 624.2413 - val_acc: 0.6674\n",
            "Epoch 1434/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 624.1679 - acc: 0.7398 - val_loss: 624.2106 - val_acc: 0.6674\n",
            "Epoch 1435/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 624.1289 - acc: 0.7787 - val_loss: 624.1800 - val_acc: 0.6715\n",
            "Epoch 1436/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 624.1075 - acc: 0.7418 - val_loss: 624.1494 - val_acc: 0.6715\n",
            "Epoch 1437/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 624.0696 - acc: 0.7561 - val_loss: 624.1187 - val_acc: 0.6695\n",
            "Epoch 1438/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 624.0278 - acc: 0.7705 - val_loss: 624.0881 - val_acc: 0.6715\n",
            "Epoch 1439/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 624.0062 - acc: 0.7520 - val_loss: 624.0575 - val_acc: 0.6715\n",
            "Epoch 1440/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 623.9799 - acc: 0.7520 - val_loss: 624.0267 - val_acc: 0.6715\n",
            "Epoch 1441/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 623.9432 - acc: 0.7643 - val_loss: 623.9960 - val_acc: 0.6715\n",
            "Epoch 1442/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 623.9009 - acc: 0.7828 - val_loss: 623.9654 - val_acc: 0.6736\n",
            "Epoch 1443/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 623.8875 - acc: 0.7705 - val_loss: 623.9347 - val_acc: 0.6736\n",
            "Epoch 1444/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 623.8603 - acc: 0.7336 - val_loss: 623.9041 - val_acc: 0.6736\n",
            "Epoch 1445/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 623.8176 - acc: 0.7746 - val_loss: 623.8736 - val_acc: 0.6736\n",
            "Epoch 1446/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 623.7793 - acc: 0.7766 - val_loss: 623.8429 - val_acc: 0.6715\n",
            "Epoch 1447/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 623.7654 - acc: 0.7541 - val_loss: 623.8121 - val_acc: 0.6715\n",
            "Epoch 1448/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 623.7254 - acc: 0.7746 - val_loss: 623.7816 - val_acc: 0.6715\n",
            "Epoch 1449/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 623.6921 - acc: 0.7746 - val_loss: 623.7509 - val_acc: 0.6715\n",
            "Epoch 1450/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 623.6706 - acc: 0.7602 - val_loss: 623.7202 - val_acc: 0.6715\n",
            "Epoch 1451/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 623.6345 - acc: 0.7684 - val_loss: 623.6894 - val_acc: 0.6715\n",
            "Epoch 1452/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 623.6004 - acc: 0.7705 - val_loss: 623.6588 - val_acc: 0.6757\n",
            "Epoch 1453/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 623.5786 - acc: 0.7602 - val_loss: 623.6282 - val_acc: 0.6736\n",
            "Epoch 1454/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 623.5515 - acc: 0.7480 - val_loss: 623.5975 - val_acc: 0.6736\n",
            "Epoch 1455/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 623.5188 - acc: 0.7439 - val_loss: 623.5670 - val_acc: 0.6695\n",
            "Epoch 1456/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 623.4836 - acc: 0.7684 - val_loss: 623.5363 - val_acc: 0.6695\n",
            "Epoch 1457/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 623.4579 - acc: 0.7684 - val_loss: 623.5057 - val_acc: 0.6695\n",
            "Epoch 1458/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 623.4188 - acc: 0.7623 - val_loss: 623.4750 - val_acc: 0.6736\n",
            "Epoch 1459/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 623.3855 - acc: 0.7705 - val_loss: 623.4444 - val_acc: 0.6715\n",
            "Epoch 1460/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 623.3577 - acc: 0.7643 - val_loss: 623.4138 - val_acc: 0.6695\n",
            "Epoch 1461/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 623.3323 - acc: 0.7541 - val_loss: 623.3832 - val_acc: 0.6695\n",
            "Epoch 1462/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 623.3032 - acc: 0.7480 - val_loss: 623.3525 - val_acc: 0.6695\n",
            "Epoch 1463/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 623.2701 - acc: 0.7582 - val_loss: 623.3219 - val_acc: 0.6715\n",
            "Epoch 1464/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 623.2362 - acc: 0.7664 - val_loss: 623.2914 - val_acc: 0.6715\n",
            "Epoch 1465/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 623.2000 - acc: 0.7725 - val_loss: 623.2607 - val_acc: 0.6715\n",
            "Epoch 1466/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 623.1886 - acc: 0.7480 - val_loss: 623.2300 - val_acc: 0.6715\n",
            "Epoch 1467/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 623.1632 - acc: 0.7131 - val_loss: 623.1993 - val_acc: 0.6715\n",
            "Epoch 1468/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 623.1213 - acc: 0.7500 - val_loss: 623.1687 - val_acc: 0.6715\n",
            "Epoch 1469/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 623.0946 - acc: 0.7254 - val_loss: 623.1380 - val_acc: 0.6715\n",
            "Epoch 1470/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 623.0643 - acc: 0.7398 - val_loss: 623.1075 - val_acc: 0.6695\n",
            "Epoch 1471/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 623.0270 - acc: 0.7480 - val_loss: 623.0768 - val_acc: 0.6757\n",
            "Epoch 1472/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 622.9884 - acc: 0.7766 - val_loss: 623.0461 - val_acc: 0.6757\n",
            "Epoch 1473/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 622.9608 - acc: 0.7746 - val_loss: 623.0156 - val_acc: 0.6757\n",
            "Epoch 1474/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 622.9292 - acc: 0.7643 - val_loss: 622.9848 - val_acc: 0.6736\n",
            "Epoch 1475/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 622.8979 - acc: 0.7766 - val_loss: 622.9541 - val_acc: 0.6736\n",
            "Epoch 1476/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 622.8686 - acc: 0.7643 - val_loss: 622.9235 - val_acc: 0.6736\n",
            "Epoch 1477/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 622.8379 - acc: 0.7684 - val_loss: 622.8929 - val_acc: 0.6757\n",
            "Epoch 1478/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 622.8117 - acc: 0.7746 - val_loss: 622.8621 - val_acc: 0.6736\n",
            "Epoch 1479/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 622.7738 - acc: 0.7623 - val_loss: 622.8314 - val_acc: 0.6736\n",
            "Epoch 1480/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 622.7504 - acc: 0.7459 - val_loss: 622.8009 - val_acc: 0.6757\n",
            "Epoch 1481/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 622.7182 - acc: 0.7561 - val_loss: 622.7702 - val_acc: 0.6757\n",
            "Epoch 1482/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 622.6872 - acc: 0.7664 - val_loss: 622.7395 - val_acc: 0.6757\n",
            "Epoch 1483/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 622.6578 - acc: 0.7582 - val_loss: 622.7089 - val_acc: 0.6757\n",
            "Epoch 1484/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 622.6193 - acc: 0.7664 - val_loss: 622.6783 - val_acc: 0.6715\n",
            "Epoch 1485/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 622.6013 - acc: 0.7664 - val_loss: 622.6477 - val_acc: 0.6736\n",
            "Epoch 1486/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 622.5643 - acc: 0.7561 - val_loss: 622.6170 - val_acc: 0.6715\n",
            "Epoch 1487/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 622.5186 - acc: 0.7910 - val_loss: 622.5863 - val_acc: 0.6757\n",
            "Epoch 1488/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 622.5046 - acc: 0.7439 - val_loss: 622.5558 - val_acc: 0.6736\n",
            "Epoch 1489/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 622.4680 - acc: 0.7684 - val_loss: 622.5252 - val_acc: 0.6715\n",
            "Epoch 1490/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 622.4361 - acc: 0.7602 - val_loss: 622.4945 - val_acc: 0.6715\n",
            "Epoch 1491/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 622.4194 - acc: 0.7623 - val_loss: 622.4640 - val_acc: 0.6715\n",
            "Epoch 1492/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 622.3842 - acc: 0.7561 - val_loss: 622.4334 - val_acc: 0.6736\n",
            "Epoch 1493/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 622.3476 - acc: 0.7541 - val_loss: 622.4029 - val_acc: 0.6736\n",
            "Epoch 1494/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 622.3204 - acc: 0.7725 - val_loss: 622.3723 - val_acc: 0.6736\n",
            "Epoch 1495/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 622.2826 - acc: 0.7582 - val_loss: 622.3417 - val_acc: 0.6736\n",
            "Epoch 1496/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 622.2633 - acc: 0.7500 - val_loss: 622.3110 - val_acc: 0.6736\n",
            "Epoch 1497/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 622.2141 - acc: 0.7951 - val_loss: 622.2803 - val_acc: 0.6736\n",
            "Epoch 1498/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 622.1989 - acc: 0.7480 - val_loss: 622.2498 - val_acc: 0.6757\n",
            "Epoch 1499/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 622.1616 - acc: 0.7807 - val_loss: 622.2193 - val_acc: 0.6736\n",
            "Epoch 1500/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 622.1345 - acc: 0.7623 - val_loss: 622.1885 - val_acc: 0.6757\n",
            "Epoch 1501/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 622.0987 - acc: 0.7643 - val_loss: 622.1580 - val_acc: 0.6736\n",
            "Epoch 1502/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 622.0689 - acc: 0.7664 - val_loss: 622.1272 - val_acc: 0.6736\n",
            "Epoch 1503/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 622.0441 - acc: 0.7561 - val_loss: 622.0967 - val_acc: 0.6757\n",
            "Epoch 1504/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 622.0147 - acc: 0.7459 - val_loss: 622.0660 - val_acc: 0.6757\n",
            "Epoch 1505/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 621.9869 - acc: 0.7602 - val_loss: 622.0353 - val_acc: 0.6757\n",
            "Epoch 1506/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 621.9550 - acc: 0.7623 - val_loss: 622.0047 - val_acc: 0.6736\n",
            "Epoch 1507/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 621.9144 - acc: 0.7480 - val_loss: 621.9742 - val_acc: 0.6736\n",
            "Epoch 1508/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 621.8835 - acc: 0.7828 - val_loss: 621.9436 - val_acc: 0.6736\n",
            "Epoch 1509/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 621.8632 - acc: 0.7623 - val_loss: 621.9130 - val_acc: 0.6736\n",
            "Epoch 1510/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 621.8392 - acc: 0.7623 - val_loss: 621.8823 - val_acc: 0.6736\n",
            "Epoch 1511/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 621.8024 - acc: 0.7459 - val_loss: 621.8518 - val_acc: 0.6736\n",
            "Epoch 1512/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 621.7612 - acc: 0.7582 - val_loss: 621.8211 - val_acc: 0.6736\n",
            "Epoch 1513/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 621.7350 - acc: 0.7766 - val_loss: 621.7906 - val_acc: 0.6715\n",
            "Epoch 1514/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 621.7158 - acc: 0.7541 - val_loss: 621.7599 - val_acc: 0.6715\n",
            "Epoch 1515/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 621.6759 - acc: 0.7705 - val_loss: 621.7293 - val_acc: 0.6715\n",
            "Epoch 1516/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 621.6418 - acc: 0.7869 - val_loss: 621.6986 - val_acc: 0.6715\n",
            "Epoch 1517/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 621.6147 - acc: 0.7787 - val_loss: 621.6680 - val_acc: 0.6715\n",
            "Epoch 1518/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 621.5792 - acc: 0.7766 - val_loss: 621.6374 - val_acc: 0.6715\n",
            "Epoch 1519/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 621.5501 - acc: 0.7910 - val_loss: 621.6070 - val_acc: 0.6736\n",
            "Epoch 1520/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 621.5221 - acc: 0.7705 - val_loss: 621.5764 - val_acc: 0.6736\n",
            "Epoch 1521/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 621.4862 - acc: 0.7684 - val_loss: 621.5457 - val_acc: 0.6736\n",
            "Epoch 1522/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 621.4621 - acc: 0.7541 - val_loss: 621.5152 - val_acc: 0.6736\n",
            "Epoch 1523/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 621.4420 - acc: 0.7541 - val_loss: 621.4846 - val_acc: 0.6736\n",
            "Epoch 1524/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 621.3922 - acc: 0.7951 - val_loss: 621.4540 - val_acc: 0.6736\n",
            "Epoch 1525/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 621.3670 - acc: 0.7684 - val_loss: 621.4233 - val_acc: 0.6736\n",
            "Epoch 1526/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 621.3502 - acc: 0.7500 - val_loss: 621.3928 - val_acc: 0.6736\n",
            "Epoch 1527/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 621.2982 - acc: 0.7520 - val_loss: 621.3621 - val_acc: 0.6736\n",
            "Epoch 1528/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 621.2841 - acc: 0.7561 - val_loss: 621.3315 - val_acc: 0.6736\n",
            "Epoch 1529/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 621.2426 - acc: 0.7725 - val_loss: 621.3010 - val_acc: 0.6736\n",
            "Epoch 1530/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 621.2207 - acc: 0.7623 - val_loss: 621.2704 - val_acc: 0.6736\n",
            "Epoch 1531/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 621.1693 - acc: 0.7746 - val_loss: 621.2399 - val_acc: 0.6736\n",
            "Epoch 1532/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 621.1539 - acc: 0.7664 - val_loss: 621.2093 - val_acc: 0.6736\n",
            "Epoch 1533/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 621.1236 - acc: 0.7869 - val_loss: 621.1790 - val_acc: 0.6736\n",
            "Epoch 1534/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 621.0966 - acc: 0.7418 - val_loss: 621.1483 - val_acc: 0.6736\n",
            "Epoch 1535/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 621.0586 - acc: 0.7643 - val_loss: 621.1176 - val_acc: 0.6736\n",
            "Epoch 1536/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 621.0338 - acc: 0.7643 - val_loss: 621.0869 - val_acc: 0.6736\n",
            "Epoch 1537/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 620.9977 - acc: 0.7725 - val_loss: 621.0564 - val_acc: 0.6736\n",
            "Epoch 1538/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 620.9769 - acc: 0.7705 - val_loss: 621.0258 - val_acc: 0.6736\n",
            "Epoch 1539/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 620.9389 - acc: 0.7766 - val_loss: 620.9952 - val_acc: 0.6736\n",
            "Epoch 1540/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 620.8991 - acc: 0.7889 - val_loss: 620.9645 - val_acc: 0.6736\n",
            "Epoch 1541/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 620.8799 - acc: 0.7643 - val_loss: 620.9341 - val_acc: 0.6736\n",
            "Epoch 1542/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 620.8413 - acc: 0.7746 - val_loss: 620.9037 - val_acc: 0.6736\n",
            "Epoch 1543/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 620.8217 - acc: 0.7541 - val_loss: 620.8731 - val_acc: 0.6736\n",
            "Epoch 1544/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 620.7913 - acc: 0.7746 - val_loss: 620.8427 - val_acc: 0.6736\n",
            "Epoch 1545/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 620.7570 - acc: 0.7582 - val_loss: 620.8120 - val_acc: 0.6736\n",
            "Epoch 1546/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 620.7178 - acc: 0.7664 - val_loss: 620.7814 - val_acc: 0.6736\n",
            "Epoch 1547/3000\n",
            "488/488 [==============================] - 0s 372us/step - loss: 620.6916 - acc: 0.7766 - val_loss: 620.7508 - val_acc: 0.6736\n",
            "Epoch 1548/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 620.6685 - acc: 0.7520 - val_loss: 620.7204 - val_acc: 0.6736\n",
            "Epoch 1549/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 620.6354 - acc: 0.7664 - val_loss: 620.6899 - val_acc: 0.6736\n",
            "Epoch 1550/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 620.6005 - acc: 0.7746 - val_loss: 620.6592 - val_acc: 0.6736\n",
            "Epoch 1551/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 620.5743 - acc: 0.7602 - val_loss: 620.6286 - val_acc: 0.6736\n",
            "Epoch 1552/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 620.5467 - acc: 0.7561 - val_loss: 620.5982 - val_acc: 0.6736\n",
            "Epoch 1553/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 620.5097 - acc: 0.7766 - val_loss: 620.5676 - val_acc: 0.6736\n",
            "Epoch 1554/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 620.4758 - acc: 0.7910 - val_loss: 620.5370 - val_acc: 0.6736\n",
            "Epoch 1555/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 620.4514 - acc: 0.7561 - val_loss: 620.5064 - val_acc: 0.6736\n",
            "Epoch 1556/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 620.4089 - acc: 0.7766 - val_loss: 620.4758 - val_acc: 0.6736\n",
            "Epoch 1557/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 620.3872 - acc: 0.7848 - val_loss: 620.4452 - val_acc: 0.6736\n",
            "Epoch 1558/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 620.3592 - acc: 0.7541 - val_loss: 620.4147 - val_acc: 0.6736\n",
            "Epoch 1559/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 620.3295 - acc: 0.7623 - val_loss: 620.3843 - val_acc: 0.6736\n",
            "Epoch 1560/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 620.2902 - acc: 0.7807 - val_loss: 620.3537 - val_acc: 0.6736\n",
            "Epoch 1561/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 620.2661 - acc: 0.7807 - val_loss: 620.3232 - val_acc: 0.6736\n",
            "Epoch 1562/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 620.2308 - acc: 0.7725 - val_loss: 620.2926 - val_acc: 0.6736\n",
            "Epoch 1563/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 620.2070 - acc: 0.7316 - val_loss: 620.2620 - val_acc: 0.6736\n",
            "Epoch 1564/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 620.1802 - acc: 0.7520 - val_loss: 620.2314 - val_acc: 0.6736\n",
            "Epoch 1565/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 620.1354 - acc: 0.7725 - val_loss: 620.2008 - val_acc: 0.6736\n",
            "Epoch 1566/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 620.1128 - acc: 0.7520 - val_loss: 620.1703 - val_acc: 0.6736\n",
            "Epoch 1567/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 620.0800 - acc: 0.7828 - val_loss: 620.1397 - val_acc: 0.6736\n",
            "Epoch 1568/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 620.0628 - acc: 0.7766 - val_loss: 620.1092 - val_acc: 0.6736\n",
            "Epoch 1569/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 620.0191 - acc: 0.7766 - val_loss: 620.0787 - val_acc: 0.6715\n",
            "Epoch 1570/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 619.9934 - acc: 0.7664 - val_loss: 620.0481 - val_acc: 0.6715\n",
            "Epoch 1571/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 619.9526 - acc: 0.7725 - val_loss: 620.0176 - val_acc: 0.6715\n",
            "Epoch 1572/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 619.9300 - acc: 0.7725 - val_loss: 619.9870 - val_acc: 0.6715\n",
            "Epoch 1573/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 619.9030 - acc: 0.7623 - val_loss: 619.9567 - val_acc: 0.6715\n",
            "Epoch 1574/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 619.8699 - acc: 0.7520 - val_loss: 619.9261 - val_acc: 0.6715\n",
            "Epoch 1575/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 619.8358 - acc: 0.7602 - val_loss: 619.8954 - val_acc: 0.6715\n",
            "Epoch 1576/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 619.8016 - acc: 0.7725 - val_loss: 619.8649 - val_acc: 0.6715\n",
            "Epoch 1577/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 619.7771 - acc: 0.7602 - val_loss: 619.8344 - val_acc: 0.6715\n",
            "Epoch 1578/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 619.7353 - acc: 0.7787 - val_loss: 619.8039 - val_acc: 0.6715\n",
            "Epoch 1579/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 619.7221 - acc: 0.7398 - val_loss: 619.7734 - val_acc: 0.6715\n",
            "Epoch 1580/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 619.6896 - acc: 0.7582 - val_loss: 619.7428 - val_acc: 0.6715\n",
            "Epoch 1581/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 619.6594 - acc: 0.7828 - val_loss: 619.7122 - val_acc: 0.6715\n",
            "Epoch 1582/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 619.6370 - acc: 0.7602 - val_loss: 619.6816 - val_acc: 0.6715\n",
            "Epoch 1583/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 619.6045 - acc: 0.7664 - val_loss: 619.6511 - val_acc: 0.6715\n",
            "Epoch 1584/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 619.5694 - acc: 0.7316 - val_loss: 619.6205 - val_acc: 0.6715\n",
            "Epoch 1585/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 619.5320 - acc: 0.7582 - val_loss: 619.5899 - val_acc: 0.6715\n",
            "Epoch 1586/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 619.5023 - acc: 0.7766 - val_loss: 619.5594 - val_acc: 0.6715\n",
            "Epoch 1587/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 619.4692 - acc: 0.7705 - val_loss: 619.5290 - val_acc: 0.6715\n",
            "Epoch 1588/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 619.4429 - acc: 0.7746 - val_loss: 619.4986 - val_acc: 0.6715\n",
            "Epoch 1589/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 619.4064 - acc: 0.7807 - val_loss: 619.4679 - val_acc: 0.6715\n",
            "Epoch 1590/3000\n",
            "488/488 [==============================] - 0s 373us/step - loss: 619.3955 - acc: 0.7480 - val_loss: 619.4373 - val_acc: 0.6715\n",
            "Epoch 1591/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 619.3471 - acc: 0.7746 - val_loss: 619.4068 - val_acc: 0.6715\n",
            "Epoch 1592/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 619.3325 - acc: 0.7500 - val_loss: 619.3764 - val_acc: 0.6715\n",
            "Epoch 1593/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 619.2905 - acc: 0.7643 - val_loss: 619.3458 - val_acc: 0.6715\n",
            "Epoch 1594/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 619.2469 - acc: 0.8053 - val_loss: 619.3154 - val_acc: 0.6715\n",
            "Epoch 1595/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 619.2232 - acc: 0.7684 - val_loss: 619.2848 - val_acc: 0.6715\n",
            "Epoch 1596/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 619.2036 - acc: 0.7602 - val_loss: 619.2543 - val_acc: 0.6715\n",
            "Epoch 1597/3000\n",
            "488/488 [==============================] - 0s 319us/step - loss: 619.1789 - acc: 0.7295 - val_loss: 619.2237 - val_acc: 0.6715\n",
            "Epoch 1598/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 619.1391 - acc: 0.7828 - val_loss: 619.1932 - val_acc: 0.6715\n",
            "Epoch 1599/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 619.1207 - acc: 0.7377 - val_loss: 619.1627 - val_acc: 0.6715\n",
            "Epoch 1600/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 619.0722 - acc: 0.7992 - val_loss: 619.1322 - val_acc: 0.6715\n",
            "Epoch 1601/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 619.0440 - acc: 0.7725 - val_loss: 619.1018 - val_acc: 0.6715\n",
            "Epoch 1602/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 619.0305 - acc: 0.7398 - val_loss: 619.0713 - val_acc: 0.6715\n",
            "Epoch 1603/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 618.9858 - acc: 0.7602 - val_loss: 619.0409 - val_acc: 0.6715\n",
            "Epoch 1604/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 618.9589 - acc: 0.7623 - val_loss: 619.0104 - val_acc: 0.6715\n",
            "Epoch 1605/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 618.9197 - acc: 0.7561 - val_loss: 618.9797 - val_acc: 0.6715\n",
            "Epoch 1606/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 618.8910 - acc: 0.7848 - val_loss: 618.9492 - val_acc: 0.6715\n",
            "Epoch 1607/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 618.8591 - acc: 0.7643 - val_loss: 618.9188 - val_acc: 0.6695\n",
            "Epoch 1608/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 618.8295 - acc: 0.7787 - val_loss: 618.8882 - val_acc: 0.6715\n",
            "Epoch 1609/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 618.8051 - acc: 0.7848 - val_loss: 618.8576 - val_acc: 0.6695\n",
            "Epoch 1610/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 618.7691 - acc: 0.7664 - val_loss: 618.8271 - val_acc: 0.6695\n",
            "Epoch 1611/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 618.7310 - acc: 0.7725 - val_loss: 618.7967 - val_acc: 0.6695\n",
            "Epoch 1612/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 618.7067 - acc: 0.7602 - val_loss: 618.7663 - val_acc: 0.6695\n",
            "Epoch 1613/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 618.6819 - acc: 0.7561 - val_loss: 618.7358 - val_acc: 0.6695\n",
            "Epoch 1614/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 618.6532 - acc: 0.7623 - val_loss: 618.7053 - val_acc: 0.6695\n",
            "Epoch 1615/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 618.6116 - acc: 0.7869 - val_loss: 618.6747 - val_acc: 0.6715\n",
            "Epoch 1616/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 618.5798 - acc: 0.7848 - val_loss: 618.6442 - val_acc: 0.6715\n",
            "Epoch 1617/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 618.5664 - acc: 0.7582 - val_loss: 618.6137 - val_acc: 0.6715\n",
            "Epoch 1618/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 618.5249 - acc: 0.7623 - val_loss: 618.5832 - val_acc: 0.6715\n",
            "Epoch 1619/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 618.4980 - acc: 0.7582 - val_loss: 618.5526 - val_acc: 0.6715\n",
            "Epoch 1620/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 618.4643 - acc: 0.7766 - val_loss: 618.5222 - val_acc: 0.6715\n",
            "Epoch 1621/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 618.4305 - acc: 0.7725 - val_loss: 618.4917 - val_acc: 0.6715\n",
            "Epoch 1622/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 618.4104 - acc: 0.7561 - val_loss: 618.4612 - val_acc: 0.6715\n",
            "Epoch 1623/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 618.3679 - acc: 0.7848 - val_loss: 618.4308 - val_acc: 0.6715\n",
            "Epoch 1624/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 618.3394 - acc: 0.7684 - val_loss: 618.4003 - val_acc: 0.6695\n",
            "Epoch 1625/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 618.3121 - acc: 0.7807 - val_loss: 618.3697 - val_acc: 0.6695\n",
            "Epoch 1626/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 618.2721 - acc: 0.8012 - val_loss: 618.3394 - val_acc: 0.6695\n",
            "Epoch 1627/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 618.2415 - acc: 0.7848 - val_loss: 618.3089 - val_acc: 0.6695\n",
            "Epoch 1628/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 618.2306 - acc: 0.7684 - val_loss: 618.2785 - val_acc: 0.6715\n",
            "Epoch 1629/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 618.1853 - acc: 0.8053 - val_loss: 618.2481 - val_acc: 0.6715\n",
            "Epoch 1630/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 618.1452 - acc: 0.7951 - val_loss: 618.2176 - val_acc: 0.6715\n",
            "Epoch 1631/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 618.1375 - acc: 0.7439 - val_loss: 618.1871 - val_acc: 0.6715\n",
            "Epoch 1632/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 618.0970 - acc: 0.7807 - val_loss: 618.1565 - val_acc: 0.6695\n",
            "Epoch 1633/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 618.0622 - acc: 0.7500 - val_loss: 618.1259 - val_acc: 0.6695\n",
            "Epoch 1634/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 618.0265 - acc: 0.7848 - val_loss: 618.0954 - val_acc: 0.6695\n",
            "Epoch 1635/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 618.0118 - acc: 0.7684 - val_loss: 618.0649 - val_acc: 0.6695\n",
            "Epoch 1636/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 617.9678 - acc: 0.7889 - val_loss: 618.0344 - val_acc: 0.6695\n",
            "Epoch 1637/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 617.9309 - acc: 0.8074 - val_loss: 618.0041 - val_acc: 0.6695\n",
            "Epoch 1638/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 617.9130 - acc: 0.7664 - val_loss: 617.9736 - val_acc: 0.6695\n",
            "Epoch 1639/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 617.8929 - acc: 0.7705 - val_loss: 617.9431 - val_acc: 0.6695\n",
            "Epoch 1640/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 617.8578 - acc: 0.7787 - val_loss: 617.9126 - val_acc: 0.6695\n",
            "Epoch 1641/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 617.8159 - acc: 0.7766 - val_loss: 617.8822 - val_acc: 0.6695\n",
            "Epoch 1642/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 617.7961 - acc: 0.7582 - val_loss: 617.8518 - val_acc: 0.6695\n",
            "Epoch 1643/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 617.7744 - acc: 0.7643 - val_loss: 617.8214 - val_acc: 0.6695\n",
            "Epoch 1644/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 617.7328 - acc: 0.7889 - val_loss: 617.7909 - val_acc: 0.6695\n",
            "Epoch 1645/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 617.7047 - acc: 0.7787 - val_loss: 617.7604 - val_acc: 0.6695\n",
            "Epoch 1646/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 617.6757 - acc: 0.7705 - val_loss: 617.7298 - val_acc: 0.6695\n",
            "Epoch 1647/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 617.6401 - acc: 0.7787 - val_loss: 617.6994 - val_acc: 0.6695\n",
            "Epoch 1648/3000\n",
            "488/488 [==============================] - 0s 318us/step - loss: 617.6218 - acc: 0.7725 - val_loss: 617.6689 - val_acc: 0.6695\n",
            "Epoch 1649/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 617.5818 - acc: 0.7602 - val_loss: 617.6385 - val_acc: 0.6695\n",
            "Epoch 1650/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 617.5353 - acc: 0.8012 - val_loss: 617.6079 - val_acc: 0.6695\n",
            "Epoch 1651/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 617.5226 - acc: 0.7725 - val_loss: 617.5776 - val_acc: 0.6695\n",
            "Epoch 1652/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 617.4890 - acc: 0.7705 - val_loss: 617.5472 - val_acc: 0.6715\n",
            "Epoch 1653/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 617.4622 - acc: 0.7746 - val_loss: 617.5168 - val_acc: 0.6715\n",
            "Epoch 1654/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 617.4411 - acc: 0.7623 - val_loss: 617.4863 - val_acc: 0.6715\n",
            "Epoch 1655/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 617.3961 - acc: 0.7766 - val_loss: 617.4557 - val_acc: 0.6715\n",
            "Epoch 1656/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 617.3705 - acc: 0.7623 - val_loss: 617.4254 - val_acc: 0.6695\n",
            "Epoch 1657/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 617.3426 - acc: 0.7500 - val_loss: 617.3949 - val_acc: 0.6715\n",
            "Epoch 1658/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 617.2939 - acc: 0.7951 - val_loss: 617.3644 - val_acc: 0.6715\n",
            "Epoch 1659/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 617.2821 - acc: 0.7459 - val_loss: 617.3339 - val_acc: 0.6715\n",
            "Epoch 1660/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 617.2525 - acc: 0.7480 - val_loss: 617.3035 - val_acc: 0.6695\n",
            "Epoch 1661/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 617.2195 - acc: 0.7746 - val_loss: 617.2732 - val_acc: 0.6695\n",
            "Epoch 1662/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 617.1839 - acc: 0.7541 - val_loss: 617.2427 - val_acc: 0.6695\n",
            "Epoch 1663/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 617.1451 - acc: 0.7971 - val_loss: 617.2122 - val_acc: 0.6695\n",
            "Epoch 1664/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 617.1194 - acc: 0.7889 - val_loss: 617.1817 - val_acc: 0.6695\n",
            "Epoch 1665/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 617.1004 - acc: 0.7623 - val_loss: 617.1512 - val_acc: 0.6695\n",
            "Epoch 1666/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 617.0597 - acc: 0.7684 - val_loss: 617.1208 - val_acc: 0.6695\n",
            "Epoch 1667/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 617.0324 - acc: 0.7643 - val_loss: 617.0903 - val_acc: 0.6695\n",
            "Epoch 1668/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 616.9986 - acc: 0.7643 - val_loss: 617.0600 - val_acc: 0.6695\n",
            "Epoch 1669/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 616.9741 - acc: 0.7828 - val_loss: 617.0296 - val_acc: 0.6695\n",
            "Epoch 1670/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 616.9390 - acc: 0.7541 - val_loss: 616.9989 - val_acc: 0.6695\n",
            "Epoch 1671/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 616.9067 - acc: 0.7787 - val_loss: 616.9687 - val_acc: 0.6695\n",
            "Epoch 1672/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 616.8726 - acc: 0.7951 - val_loss: 616.9382 - val_acc: 0.6695\n",
            "Epoch 1673/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 616.8502 - acc: 0.7807 - val_loss: 616.9076 - val_acc: 0.6695\n",
            "Epoch 1674/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 616.8273 - acc: 0.7705 - val_loss: 616.8771 - val_acc: 0.6715\n",
            "Epoch 1675/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 616.7866 - acc: 0.7725 - val_loss: 616.8468 - val_acc: 0.6695\n",
            "Epoch 1676/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 616.7521 - acc: 0.7828 - val_loss: 616.8163 - val_acc: 0.6695\n",
            "Epoch 1677/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 616.7324 - acc: 0.7807 - val_loss: 616.7858 - val_acc: 0.6695\n",
            "Epoch 1678/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 616.6953 - acc: 0.7787 - val_loss: 616.7553 - val_acc: 0.6695\n",
            "Epoch 1679/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 616.6609 - acc: 0.7807 - val_loss: 616.7249 - val_acc: 0.6715\n",
            "Epoch 1680/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 616.6376 - acc: 0.7643 - val_loss: 616.6944 - val_acc: 0.6715\n",
            "Epoch 1681/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 616.6046 - acc: 0.7787 - val_loss: 616.6641 - val_acc: 0.6715\n",
            "Epoch 1682/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 616.5711 - acc: 0.7848 - val_loss: 616.6336 - val_acc: 0.6715\n",
            "Epoch 1683/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 616.5589 - acc: 0.7602 - val_loss: 616.6032 - val_acc: 0.6715\n",
            "Epoch 1684/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 616.5021 - acc: 0.7971 - val_loss: 616.5727 - val_acc: 0.6736\n",
            "Epoch 1685/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 616.4846 - acc: 0.7582 - val_loss: 616.5424 - val_acc: 0.6736\n",
            "Epoch 1686/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 616.4543 - acc: 0.7848 - val_loss: 616.5119 - val_acc: 0.6736\n",
            "Epoch 1687/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 616.4041 - acc: 0.7869 - val_loss: 616.4816 - val_acc: 0.6715\n",
            "Epoch 1688/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 616.3916 - acc: 0.7766 - val_loss: 616.4511 - val_acc: 0.6715\n",
            "Epoch 1689/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 616.3594 - acc: 0.7787 - val_loss: 616.4206 - val_acc: 0.6715\n",
            "Epoch 1690/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 616.3250 - acc: 0.7869 - val_loss: 616.3902 - val_acc: 0.6715\n",
            "Epoch 1691/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 616.2963 - acc: 0.7910 - val_loss: 616.3597 - val_acc: 0.6715\n",
            "Epoch 1692/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 616.2812 - acc: 0.7275 - val_loss: 616.3294 - val_acc: 0.6715\n",
            "Epoch 1693/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 616.2338 - acc: 0.7930 - val_loss: 616.2990 - val_acc: 0.6715\n",
            "Epoch 1694/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 616.2018 - acc: 0.7787 - val_loss: 616.2686 - val_acc: 0.6736\n",
            "Epoch 1695/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 616.1863 - acc: 0.7705 - val_loss: 616.2381 - val_acc: 0.6736\n",
            "Epoch 1696/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 616.1373 - acc: 0.7848 - val_loss: 616.2078 - val_acc: 0.6715\n",
            "Epoch 1697/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 616.1189 - acc: 0.7889 - val_loss: 616.1772 - val_acc: 0.6715\n",
            "Epoch 1698/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 616.0859 - acc: 0.7664 - val_loss: 616.1468 - val_acc: 0.6715\n",
            "Epoch 1699/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 616.0633 - acc: 0.7684 - val_loss: 616.1164 - val_acc: 0.6715\n",
            "Epoch 1700/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 616.0224 - acc: 0.7684 - val_loss: 616.0858 - val_acc: 0.6715\n",
            "Epoch 1701/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 615.9947 - acc: 0.7930 - val_loss: 616.0554 - val_acc: 0.6715\n",
            "Epoch 1702/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 615.9576 - acc: 0.7910 - val_loss: 616.0251 - val_acc: 0.6715\n",
            "Epoch 1703/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 615.9476 - acc: 0.7582 - val_loss: 615.9947 - val_acc: 0.6715\n",
            "Epoch 1704/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 615.9123 - acc: 0.7725 - val_loss: 615.9643 - val_acc: 0.6715\n",
            "Epoch 1705/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 615.8790 - acc: 0.7439 - val_loss: 615.9339 - val_acc: 0.6715\n",
            "Epoch 1706/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 615.8371 - acc: 0.7951 - val_loss: 615.9035 - val_acc: 0.6715\n",
            "Epoch 1707/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 615.8154 - acc: 0.7828 - val_loss: 615.8731 - val_acc: 0.6715\n",
            "Epoch 1708/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 615.7954 - acc: 0.7582 - val_loss: 615.8427 - val_acc: 0.6715\n",
            "Epoch 1709/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 615.7456 - acc: 0.7971 - val_loss: 615.8123 - val_acc: 0.6715\n",
            "Epoch 1710/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 615.7350 - acc: 0.7459 - val_loss: 615.7820 - val_acc: 0.6715\n",
            "Epoch 1711/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 615.6885 - acc: 0.7828 - val_loss: 615.7515 - val_acc: 0.6715\n",
            "Epoch 1712/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 615.6660 - acc: 0.7746 - val_loss: 615.7211 - val_acc: 0.6715\n",
            "Epoch 1713/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 615.6312 - acc: 0.7807 - val_loss: 615.6905 - val_acc: 0.6715\n",
            "Epoch 1714/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 615.6069 - acc: 0.7930 - val_loss: 615.6602 - val_acc: 0.6715\n",
            "Epoch 1715/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 615.5768 - acc: 0.7951 - val_loss: 615.6298 - val_acc: 0.6715\n",
            "Epoch 1716/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 615.5407 - acc: 0.7664 - val_loss: 615.5995 - val_acc: 0.6715\n",
            "Epoch 1717/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 615.5080 - acc: 0.7807 - val_loss: 615.5691 - val_acc: 0.6715\n",
            "Epoch 1718/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 615.4672 - acc: 0.7910 - val_loss: 615.5387 - val_acc: 0.6715\n",
            "Epoch 1719/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 615.4346 - acc: 0.7828 - val_loss: 615.5083 - val_acc: 0.6715\n",
            "Epoch 1720/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 615.4168 - acc: 0.7828 - val_loss: 615.4780 - val_acc: 0.6715\n",
            "Epoch 1721/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 615.3771 - acc: 0.8012 - val_loss: 615.4475 - val_acc: 0.6715\n",
            "Epoch 1722/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 615.3655 - acc: 0.7602 - val_loss: 615.4172 - val_acc: 0.6715\n",
            "Epoch 1723/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 615.3322 - acc: 0.7664 - val_loss: 615.3867 - val_acc: 0.6715\n",
            "Epoch 1724/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 615.2999 - acc: 0.7500 - val_loss: 615.3562 - val_acc: 0.6715\n",
            "Epoch 1725/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 615.2724 - acc: 0.7746 - val_loss: 615.3258 - val_acc: 0.6715\n",
            "Epoch 1726/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 615.2356 - acc: 0.7623 - val_loss: 615.2955 - val_acc: 0.6715\n",
            "Epoch 1727/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 615.2007 - acc: 0.7869 - val_loss: 615.2650 - val_acc: 0.6715\n",
            "Epoch 1728/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 615.1874 - acc: 0.7459 - val_loss: 615.2345 - val_acc: 0.6715\n",
            "Epoch 1729/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 615.1472 - acc: 0.7725 - val_loss: 615.2043 - val_acc: 0.6715\n",
            "Epoch 1730/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 615.1056 - acc: 0.7869 - val_loss: 615.1738 - val_acc: 0.6715\n",
            "Epoch 1731/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 615.0876 - acc: 0.7623 - val_loss: 615.1435 - val_acc: 0.6715\n",
            "Epoch 1732/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 615.0559 - acc: 0.7910 - val_loss: 615.1131 - val_acc: 0.6736\n",
            "Epoch 1733/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 615.0178 - acc: 0.7930 - val_loss: 615.0826 - val_acc: 0.6736\n",
            "Epoch 1734/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 614.9946 - acc: 0.7684 - val_loss: 615.0523 - val_acc: 0.6736\n",
            "Epoch 1735/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 614.9622 - acc: 0.7869 - val_loss: 615.0219 - val_acc: 0.6736\n",
            "Epoch 1736/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 614.9421 - acc: 0.7766 - val_loss: 614.9915 - val_acc: 0.6757\n",
            "Epoch 1737/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 614.9030 - acc: 0.7807 - val_loss: 614.9611 - val_acc: 0.6757\n",
            "Epoch 1738/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 614.8635 - acc: 0.7807 - val_loss: 614.9306 - val_acc: 0.6757\n",
            "Epoch 1739/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 614.8469 - acc: 0.7623 - val_loss: 614.9004 - val_acc: 0.6757\n",
            "Epoch 1740/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 614.8081 - acc: 0.7807 - val_loss: 614.8700 - val_acc: 0.6757\n",
            "Epoch 1741/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 614.7768 - acc: 0.7787 - val_loss: 614.8397 - val_acc: 0.6757\n",
            "Epoch 1742/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 614.7518 - acc: 0.7602 - val_loss: 614.8093 - val_acc: 0.6757\n",
            "Epoch 1743/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 614.7115 - acc: 0.7828 - val_loss: 614.7790 - val_acc: 0.6757\n",
            "Epoch 1744/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 614.6847 - acc: 0.7951 - val_loss: 614.7486 - val_acc: 0.6757\n",
            "Epoch 1745/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 614.6485 - acc: 0.7787 - val_loss: 614.7183 - val_acc: 0.6757\n",
            "Epoch 1746/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 614.6338 - acc: 0.7623 - val_loss: 614.6879 - val_acc: 0.6757\n",
            "Epoch 1747/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 614.5927 - acc: 0.7889 - val_loss: 614.6575 - val_acc: 0.6757\n",
            "Epoch 1748/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 614.5711 - acc: 0.7561 - val_loss: 614.6272 - val_acc: 0.6757\n",
            "Epoch 1749/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 614.5277 - acc: 0.7889 - val_loss: 614.5967 - val_acc: 0.6757\n",
            "Epoch 1750/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 614.5067 - acc: 0.7766 - val_loss: 614.5664 - val_acc: 0.6778\n",
            "Epoch 1751/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 614.4772 - acc: 0.7684 - val_loss: 614.5360 - val_acc: 0.6757\n",
            "Epoch 1752/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 614.4391 - acc: 0.7889 - val_loss: 614.5057 - val_acc: 0.6757\n",
            "Epoch 1753/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 614.4176 - acc: 0.7828 - val_loss: 614.4752 - val_acc: 0.6757\n",
            "Epoch 1754/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 614.3817 - acc: 0.7766 - val_loss: 614.4450 - val_acc: 0.6757\n",
            "Epoch 1755/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 614.3570 - acc: 0.7848 - val_loss: 614.4146 - val_acc: 0.6757\n",
            "Epoch 1756/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 614.3200 - acc: 0.7725 - val_loss: 614.3842 - val_acc: 0.6757\n",
            "Epoch 1757/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 614.2847 - acc: 0.7766 - val_loss: 614.3540 - val_acc: 0.6757\n",
            "Epoch 1758/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 614.2582 - acc: 0.7910 - val_loss: 614.3237 - val_acc: 0.6757\n",
            "Epoch 1759/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 614.2327 - acc: 0.7643 - val_loss: 614.2933 - val_acc: 0.6757\n",
            "Epoch 1760/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 614.2064 - acc: 0.7766 - val_loss: 614.2629 - val_acc: 0.6757\n",
            "Epoch 1761/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 614.1752 - acc: 0.7725 - val_loss: 614.2325 - val_acc: 0.6757\n",
            "Epoch 1762/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 614.1505 - acc: 0.7561 - val_loss: 614.2023 - val_acc: 0.6757\n",
            "Epoch 1763/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 614.1045 - acc: 0.8094 - val_loss: 614.1720 - val_acc: 0.6757\n",
            "Epoch 1764/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 614.0822 - acc: 0.7746 - val_loss: 614.1417 - val_acc: 0.6757\n",
            "Epoch 1765/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 614.0459 - acc: 0.7643 - val_loss: 614.1114 - val_acc: 0.6757\n",
            "Epoch 1766/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 614.0132 - acc: 0.7623 - val_loss: 614.0810 - val_acc: 0.6757\n",
            "Epoch 1767/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 613.9722 - acc: 0.7889 - val_loss: 614.0505 - val_acc: 0.6757\n",
            "Epoch 1768/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 613.9578 - acc: 0.7766 - val_loss: 614.0202 - val_acc: 0.6757\n",
            "Epoch 1769/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 613.9367 - acc: 0.7541 - val_loss: 613.9899 - val_acc: 0.6757\n",
            "Epoch 1770/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 613.8948 - acc: 0.7787 - val_loss: 613.9595 - val_acc: 0.6757\n",
            "Epoch 1771/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 613.8796 - acc: 0.7520 - val_loss: 613.9293 - val_acc: 0.6757\n",
            "Epoch 1772/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 613.8385 - acc: 0.7664 - val_loss: 613.8989 - val_acc: 0.6757\n",
            "Epoch 1773/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 613.8019 - acc: 0.7910 - val_loss: 613.8686 - val_acc: 0.6757\n",
            "Epoch 1774/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 613.7864 - acc: 0.7459 - val_loss: 613.8384 - val_acc: 0.6757\n",
            "Epoch 1775/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 613.7553 - acc: 0.7602 - val_loss: 613.8079 - val_acc: 0.6757\n",
            "Epoch 1776/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 613.7140 - acc: 0.7664 - val_loss: 613.7775 - val_acc: 0.6757\n",
            "Epoch 1777/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 613.6870 - acc: 0.7602 - val_loss: 613.7473 - val_acc: 0.6757\n",
            "Epoch 1778/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 613.6659 - acc: 0.7541 - val_loss: 613.7170 - val_acc: 0.6757\n",
            "Epoch 1779/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 613.6321 - acc: 0.7561 - val_loss: 613.6866 - val_acc: 0.6757\n",
            "Epoch 1780/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 613.6080 - acc: 0.7643 - val_loss: 613.6563 - val_acc: 0.6757\n",
            "Epoch 1781/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 613.5571 - acc: 0.7848 - val_loss: 613.6259 - val_acc: 0.6757\n",
            "Epoch 1782/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 613.5427 - acc: 0.7664 - val_loss: 613.5955 - val_acc: 0.6757\n",
            "Epoch 1783/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 613.5115 - acc: 0.7725 - val_loss: 613.5653 - val_acc: 0.6757\n",
            "Epoch 1784/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 613.4783 - acc: 0.7705 - val_loss: 613.5350 - val_acc: 0.6757\n",
            "Epoch 1785/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 613.4457 - acc: 0.7725 - val_loss: 613.5046 - val_acc: 0.6757\n",
            "Epoch 1786/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 613.4103 - acc: 0.7807 - val_loss: 613.4743 - val_acc: 0.6757\n",
            "Epoch 1787/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 613.3812 - acc: 0.7930 - val_loss: 613.4440 - val_acc: 0.6757\n",
            "Epoch 1788/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 613.3560 - acc: 0.7766 - val_loss: 613.4137 - val_acc: 0.6757\n",
            "Epoch 1789/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 613.3216 - acc: 0.7828 - val_loss: 613.3836 - val_acc: 0.6757\n",
            "Epoch 1790/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 613.2914 - acc: 0.7746 - val_loss: 613.3531 - val_acc: 0.6757\n",
            "Epoch 1791/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 613.2603 - acc: 0.7705 - val_loss: 613.3228 - val_acc: 0.6757\n",
            "Epoch 1792/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 613.2305 - acc: 0.7910 - val_loss: 613.2925 - val_acc: 0.6757\n",
            "Epoch 1793/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 613.1875 - acc: 0.7889 - val_loss: 613.2622 - val_acc: 0.6757\n",
            "Epoch 1794/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 613.1715 - acc: 0.7725 - val_loss: 613.2319 - val_acc: 0.6757\n",
            "Epoch 1795/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 613.1425 - acc: 0.7643 - val_loss: 613.2016 - val_acc: 0.6757\n",
            "Epoch 1796/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 613.1052 - acc: 0.7807 - val_loss: 613.1712 - val_acc: 0.6757\n",
            "Epoch 1797/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 613.0851 - acc: 0.7889 - val_loss: 613.1409 - val_acc: 0.6757\n",
            "Epoch 1798/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 613.0488 - acc: 0.7848 - val_loss: 613.1107 - val_acc: 0.6757\n",
            "Epoch 1799/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 613.0194 - acc: 0.7848 - val_loss: 613.0803 - val_acc: 0.6757\n",
            "Epoch 1800/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 612.9910 - acc: 0.7869 - val_loss: 613.0499 - val_acc: 0.6757\n",
            "Epoch 1801/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 612.9594 - acc: 0.7869 - val_loss: 613.0196 - val_acc: 0.6757\n",
            "Epoch 1802/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 612.9282 - acc: 0.7869 - val_loss: 612.9893 - val_acc: 0.6757\n",
            "Epoch 1803/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 612.8864 - acc: 0.8012 - val_loss: 612.9590 - val_acc: 0.6757\n",
            "Epoch 1804/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 612.8629 - acc: 0.7930 - val_loss: 612.9287 - val_acc: 0.6757\n",
            "Epoch 1805/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 612.8415 - acc: 0.7664 - val_loss: 612.8983 - val_acc: 0.6757\n",
            "Epoch 1806/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 612.7993 - acc: 0.7828 - val_loss: 612.8681 - val_acc: 0.6757\n",
            "Epoch 1807/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 612.7745 - acc: 0.7705 - val_loss: 612.8379 - val_acc: 0.6757\n",
            "Epoch 1808/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 612.7393 - acc: 0.8033 - val_loss: 612.8076 - val_acc: 0.6757\n",
            "Epoch 1809/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 612.7100 - acc: 0.7910 - val_loss: 612.7773 - val_acc: 0.6757\n",
            "Epoch 1810/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 612.6899 - acc: 0.7623 - val_loss: 612.7469 - val_acc: 0.6757\n",
            "Epoch 1811/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 612.6553 - acc: 0.7643 - val_loss: 612.7167 - val_acc: 0.6757\n",
            "Epoch 1812/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 612.6215 - acc: 0.7889 - val_loss: 612.6864 - val_acc: 0.6757\n",
            "Epoch 1813/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 612.5991 - acc: 0.7561 - val_loss: 612.6562 - val_acc: 0.6757\n",
            "Epoch 1814/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 612.5613 - acc: 0.7807 - val_loss: 612.6259 - val_acc: 0.6757\n",
            "Epoch 1815/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 612.5308 - acc: 0.7869 - val_loss: 612.5956 - val_acc: 0.6757\n",
            "Epoch 1816/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 612.4981 - acc: 0.7951 - val_loss: 612.5653 - val_acc: 0.6757\n",
            "Epoch 1817/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 612.4758 - acc: 0.7746 - val_loss: 612.5350 - val_acc: 0.6757\n",
            "Epoch 1818/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 612.4514 - acc: 0.7500 - val_loss: 612.5047 - val_acc: 0.6757\n",
            "Epoch 1819/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 612.4140 - acc: 0.7684 - val_loss: 612.4744 - val_acc: 0.6757\n",
            "Epoch 1820/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 612.3878 - acc: 0.7602 - val_loss: 612.4441 - val_acc: 0.6757\n",
            "Epoch 1821/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 612.3575 - acc: 0.7705 - val_loss: 612.4139 - val_acc: 0.6757\n",
            "Epoch 1822/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 612.3154 - acc: 0.7725 - val_loss: 612.3836 - val_acc: 0.6757\n",
            "Epoch 1823/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 612.2957 - acc: 0.7828 - val_loss: 612.3533 - val_acc: 0.6757\n",
            "Epoch 1824/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 612.2585 - acc: 0.7746 - val_loss: 612.3230 - val_acc: 0.6757\n",
            "Epoch 1825/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 612.2385 - acc: 0.7561 - val_loss: 612.2926 - val_acc: 0.6757\n",
            "Epoch 1826/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 612.2025 - acc: 0.7807 - val_loss: 612.2625 - val_acc: 0.6757\n",
            "Epoch 1827/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 612.1654 - acc: 0.7807 - val_loss: 612.2321 - val_acc: 0.6757\n",
            "Epoch 1828/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 612.1452 - acc: 0.7766 - val_loss: 612.2018 - val_acc: 0.6757\n",
            "Epoch 1829/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 612.1029 - acc: 0.7848 - val_loss: 612.1716 - val_acc: 0.6757\n",
            "Epoch 1830/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 612.0846 - acc: 0.7787 - val_loss: 612.1413 - val_acc: 0.6757\n",
            "Epoch 1831/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 612.0524 - acc: 0.7684 - val_loss: 612.1112 - val_acc: 0.6757\n",
            "Epoch 1832/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 612.0253 - acc: 0.7623 - val_loss: 612.0809 - val_acc: 0.6757\n",
            "Epoch 1833/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 611.9937 - acc: 0.7787 - val_loss: 612.0506 - val_acc: 0.6757\n",
            "Epoch 1834/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 611.9597 - acc: 0.7623 - val_loss: 612.0204 - val_acc: 0.6757\n",
            "Epoch 1835/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 611.9364 - acc: 0.7725 - val_loss: 611.9900 - val_acc: 0.6757\n",
            "Epoch 1836/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 611.8982 - acc: 0.7684 - val_loss: 611.9599 - val_acc: 0.6757\n",
            "Epoch 1837/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 611.8685 - acc: 0.7766 - val_loss: 611.9296 - val_acc: 0.6757\n",
            "Epoch 1838/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 611.8367 - acc: 0.7705 - val_loss: 611.8993 - val_acc: 0.6757\n",
            "Epoch 1839/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 611.8089 - acc: 0.7787 - val_loss: 611.8692 - val_acc: 0.6757\n",
            "Epoch 1840/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 611.7733 - acc: 0.7848 - val_loss: 611.8389 - val_acc: 0.6757\n",
            "Epoch 1841/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 611.7509 - acc: 0.7705 - val_loss: 611.8086 - val_acc: 0.6757\n",
            "Epoch 1842/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 611.7099 - acc: 0.7787 - val_loss: 611.7784 - val_acc: 0.6757\n",
            "Epoch 1843/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 611.6950 - acc: 0.7828 - val_loss: 611.7481 - val_acc: 0.6757\n",
            "Epoch 1844/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 611.6505 - acc: 0.7910 - val_loss: 611.7179 - val_acc: 0.6757\n",
            "Epoch 1845/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 611.6324 - acc: 0.7746 - val_loss: 611.6877 - val_acc: 0.6757\n",
            "Epoch 1846/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 611.5940 - acc: 0.7705 - val_loss: 611.6575 - val_acc: 0.6757\n",
            "Epoch 1847/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 611.5563 - acc: 0.7684 - val_loss: 611.6272 - val_acc: 0.6757\n",
            "Epoch 1848/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 611.5320 - acc: 0.7848 - val_loss: 611.5968 - val_acc: 0.6757\n",
            "Epoch 1849/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 611.4996 - acc: 0.7992 - val_loss: 611.5666 - val_acc: 0.6757\n",
            "Epoch 1850/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 611.4689 - acc: 0.7971 - val_loss: 611.5363 - val_acc: 0.6757\n",
            "Epoch 1851/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 611.4446 - acc: 0.7869 - val_loss: 611.5061 - val_acc: 0.6757\n",
            "Epoch 1852/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 611.4123 - acc: 0.7848 - val_loss: 611.4757 - val_acc: 0.6757\n",
            "Epoch 1853/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 611.3773 - acc: 0.7766 - val_loss: 611.4457 - val_acc: 0.6757\n",
            "Epoch 1854/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 611.3544 - acc: 0.7561 - val_loss: 611.4154 - val_acc: 0.6757\n",
            "Epoch 1855/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 611.3209 - acc: 0.7992 - val_loss: 611.3852 - val_acc: 0.6757\n",
            "Epoch 1856/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 611.2985 - acc: 0.7725 - val_loss: 611.3550 - val_acc: 0.6757\n",
            "Epoch 1857/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 611.2566 - acc: 0.8053 - val_loss: 611.3247 - val_acc: 0.6757\n",
            "Epoch 1858/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 611.2255 - acc: 0.7828 - val_loss: 611.2945 - val_acc: 0.6757\n",
            "Epoch 1859/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 611.1951 - acc: 0.7848 - val_loss: 611.2643 - val_acc: 0.6757\n",
            "Epoch 1860/3000\n",
            "488/488 [==============================] - 0s 370us/step - loss: 611.1749 - acc: 0.7684 - val_loss: 611.2340 - val_acc: 0.6757\n",
            "Epoch 1861/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 611.1433 - acc: 0.7807 - val_loss: 611.2038 - val_acc: 0.6757\n",
            "Epoch 1862/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 611.1118 - acc: 0.7705 - val_loss: 611.1737 - val_acc: 0.6757\n",
            "Epoch 1863/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 611.0889 - acc: 0.7664 - val_loss: 611.1435 - val_acc: 0.6757\n",
            "Epoch 1864/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 611.0438 - acc: 0.7828 - val_loss: 611.1133 - val_acc: 0.6757\n",
            "Epoch 1865/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 611.0162 - acc: 0.8094 - val_loss: 611.0831 - val_acc: 0.6757\n",
            "Epoch 1866/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 610.9875 - acc: 0.7910 - val_loss: 611.0529 - val_acc: 0.6757\n",
            "Epoch 1867/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 610.9633 - acc: 0.7766 - val_loss: 611.0227 - val_acc: 0.6778\n",
            "Epoch 1868/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 610.9315 - acc: 0.7848 - val_loss: 610.9927 - val_acc: 0.6778\n",
            "Epoch 1869/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 610.9031 - acc: 0.7684 - val_loss: 610.9625 - val_acc: 0.6778\n",
            "Epoch 1870/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 610.8689 - acc: 0.7766 - val_loss: 610.9323 - val_acc: 0.6778\n",
            "Epoch 1871/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 610.8313 - acc: 0.7889 - val_loss: 610.9021 - val_acc: 0.6778\n",
            "Epoch 1872/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 610.7974 - acc: 0.7971 - val_loss: 610.8719 - val_acc: 0.6757\n",
            "Epoch 1873/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 610.7695 - acc: 0.7992 - val_loss: 610.8416 - val_acc: 0.6736\n",
            "Epoch 1874/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 610.7539 - acc: 0.7541 - val_loss: 610.8114 - val_acc: 0.6736\n",
            "Epoch 1875/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 610.7301 - acc: 0.7623 - val_loss: 610.7813 - val_acc: 0.6757\n",
            "Epoch 1876/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 610.6846 - acc: 0.7725 - val_loss: 610.7511 - val_acc: 0.6757\n",
            "Epoch 1877/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 610.6557 - acc: 0.7582 - val_loss: 610.7208 - val_acc: 0.6757\n",
            "Epoch 1878/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 610.6273 - acc: 0.7869 - val_loss: 610.6907 - val_acc: 0.6757\n",
            "Epoch 1879/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 610.6005 - acc: 0.7623 - val_loss: 610.6604 - val_acc: 0.6757\n",
            "Epoch 1880/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 610.5690 - acc: 0.7664 - val_loss: 610.6302 - val_acc: 0.6757\n",
            "Epoch 1881/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 610.5350 - acc: 0.7971 - val_loss: 610.6001 - val_acc: 0.6757\n",
            "Epoch 1882/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 610.5015 - acc: 0.7787 - val_loss: 610.5699 - val_acc: 0.6757\n",
            "Epoch 1883/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 610.4740 - acc: 0.7725 - val_loss: 610.5399 - val_acc: 0.6757\n",
            "Epoch 1884/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 610.4461 - acc: 0.7807 - val_loss: 610.5096 - val_acc: 0.6757\n",
            "Epoch 1885/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 610.4215 - acc: 0.7828 - val_loss: 610.4795 - val_acc: 0.6757\n",
            "Epoch 1886/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 610.3871 - acc: 0.7910 - val_loss: 610.4492 - val_acc: 0.6757\n",
            "Epoch 1887/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 610.3576 - acc: 0.7520 - val_loss: 610.4190 - val_acc: 0.6757\n",
            "Epoch 1888/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 610.3258 - acc: 0.7910 - val_loss: 610.3888 - val_acc: 0.6757\n",
            "Epoch 1889/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 610.2937 - acc: 0.8033 - val_loss: 610.3585 - val_acc: 0.6757\n",
            "Epoch 1890/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 610.2700 - acc: 0.7766 - val_loss: 610.3284 - val_acc: 0.6757\n",
            "Epoch 1891/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 610.2332 - acc: 0.7766 - val_loss: 610.2982 - val_acc: 0.6778\n",
            "Epoch 1892/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 610.2065 - acc: 0.7664 - val_loss: 610.2682 - val_acc: 0.6757\n",
            "Epoch 1893/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 610.1768 - acc: 0.7725 - val_loss: 610.2380 - val_acc: 0.6757\n",
            "Epoch 1894/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 610.1440 - acc: 0.7930 - val_loss: 610.2077 - val_acc: 0.6778\n",
            "Epoch 1895/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 610.1077 - acc: 0.8074 - val_loss: 610.1775 - val_acc: 0.6778\n",
            "Epoch 1896/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 610.0825 - acc: 0.7889 - val_loss: 610.1473 - val_acc: 0.6778\n",
            "Epoch 1897/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 610.0447 - acc: 0.7951 - val_loss: 610.1172 - val_acc: 0.6778\n",
            "Epoch 1898/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 610.0223 - acc: 0.7889 - val_loss: 610.0869 - val_acc: 0.6757\n",
            "Epoch 1899/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 610.0044 - acc: 0.7664 - val_loss: 610.0569 - val_acc: 0.6757\n",
            "Epoch 1900/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 609.9607 - acc: 0.7807 - val_loss: 610.0267 - val_acc: 0.6757\n",
            "Epoch 1901/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 609.9326 - acc: 0.7746 - val_loss: 609.9964 - val_acc: 0.6757\n",
            "Epoch 1902/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 609.8913 - acc: 0.7992 - val_loss: 609.9663 - val_acc: 0.6736\n",
            "Epoch 1903/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 609.8659 - acc: 0.8012 - val_loss: 609.9361 - val_acc: 0.6715\n",
            "Epoch 1904/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 609.8408 - acc: 0.7828 - val_loss: 609.9060 - val_acc: 0.6757\n",
            "Epoch 1905/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 609.8149 - acc: 0.7910 - val_loss: 609.8758 - val_acc: 0.6736\n",
            "Epoch 1906/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 609.7849 - acc: 0.7992 - val_loss: 609.8457 - val_acc: 0.6736\n",
            "Epoch 1907/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 609.7603 - acc: 0.7357 - val_loss: 609.8155 - val_acc: 0.6736\n",
            "Epoch 1908/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 609.7176 - acc: 0.8012 - val_loss: 609.7853 - val_acc: 0.6757\n",
            "Epoch 1909/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 609.6884 - acc: 0.8074 - val_loss: 609.7551 - val_acc: 0.6778\n",
            "Epoch 1910/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 609.6624 - acc: 0.7746 - val_loss: 609.7248 - val_acc: 0.6778\n",
            "Epoch 1911/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 609.6389 - acc: 0.7746 - val_loss: 609.6947 - val_acc: 0.6778\n",
            "Epoch 1912/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 609.6053 - acc: 0.7746 - val_loss: 609.6646 - val_acc: 0.6778\n",
            "Epoch 1913/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 609.5558 - acc: 0.8053 - val_loss: 609.6344 - val_acc: 0.6778\n",
            "Epoch 1914/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 609.5464 - acc: 0.7746 - val_loss: 609.6043 - val_acc: 0.6757\n",
            "Epoch 1915/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 609.5083 - acc: 0.7951 - val_loss: 609.5742 - val_acc: 0.6757\n",
            "Epoch 1916/3000\n",
            "488/488 [==============================] - 0s 370us/step - loss: 609.4815 - acc: 0.7828 - val_loss: 609.5440 - val_acc: 0.6757\n",
            "Epoch 1917/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 609.4536 - acc: 0.7705 - val_loss: 609.5139 - val_acc: 0.6757\n",
            "Epoch 1918/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 609.4149 - acc: 0.7869 - val_loss: 609.4838 - val_acc: 0.6757\n",
            "Epoch 1919/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 609.3938 - acc: 0.7828 - val_loss: 609.4536 - val_acc: 0.6757\n",
            "Epoch 1920/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 609.3708 - acc: 0.7766 - val_loss: 609.4235 - val_acc: 0.6757\n",
            "Epoch 1921/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 609.3269 - acc: 0.7807 - val_loss: 609.3933 - val_acc: 0.6757\n",
            "Epoch 1922/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 609.2948 - acc: 0.7725 - val_loss: 609.3633 - val_acc: 0.6757\n",
            "Epoch 1923/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 609.2785 - acc: 0.7828 - val_loss: 609.3331 - val_acc: 0.6757\n",
            "Epoch 1924/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 609.2436 - acc: 0.7787 - val_loss: 609.3029 - val_acc: 0.6757\n",
            "Epoch 1925/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 609.1977 - acc: 0.8012 - val_loss: 609.2728 - val_acc: 0.6757\n",
            "Epoch 1926/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 609.1762 - acc: 0.7787 - val_loss: 609.2425 - val_acc: 0.6757\n",
            "Epoch 1927/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 609.1398 - acc: 0.8012 - val_loss: 609.2124 - val_acc: 0.6778\n",
            "Epoch 1928/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 609.1170 - acc: 0.7971 - val_loss: 609.1823 - val_acc: 0.6778\n",
            "Epoch 1929/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 609.0940 - acc: 0.7500 - val_loss: 609.1522 - val_acc: 0.6778\n",
            "Epoch 1930/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 609.0508 - acc: 0.7848 - val_loss: 609.1221 - val_acc: 0.6757\n",
            "Epoch 1931/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 609.0265 - acc: 0.7848 - val_loss: 609.0921 - val_acc: 0.6757\n",
            "Epoch 1932/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 608.9980 - acc: 0.7992 - val_loss: 609.0618 - val_acc: 0.6757\n",
            "Epoch 1933/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 608.9692 - acc: 0.7848 - val_loss: 609.0318 - val_acc: 0.6778\n",
            "Epoch 1934/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 608.9392 - acc: 0.7582 - val_loss: 609.0016 - val_acc: 0.6757\n",
            "Epoch 1935/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 608.8991 - acc: 0.8074 - val_loss: 608.9714 - val_acc: 0.6757\n",
            "Epoch 1936/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 608.8717 - acc: 0.7828 - val_loss: 608.9414 - val_acc: 0.6757\n",
            "Epoch 1937/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 608.8555 - acc: 0.7787 - val_loss: 608.9113 - val_acc: 0.6757\n",
            "Epoch 1938/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 608.8151 - acc: 0.8033 - val_loss: 608.8812 - val_acc: 0.6757\n",
            "Epoch 1939/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 608.7845 - acc: 0.7705 - val_loss: 608.8511 - val_acc: 0.6757\n",
            "Epoch 1940/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 608.7567 - acc: 0.7951 - val_loss: 608.8211 - val_acc: 0.6757\n",
            "Epoch 1941/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 608.7317 - acc: 0.7869 - val_loss: 608.7908 - val_acc: 0.6757\n",
            "Epoch 1942/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 608.6846 - acc: 0.7930 - val_loss: 608.7608 - val_acc: 0.6757\n",
            "Epoch 1943/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 608.6590 - acc: 0.7889 - val_loss: 608.7306 - val_acc: 0.6757\n",
            "Epoch 1944/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 608.6344 - acc: 0.7828 - val_loss: 608.7005 - val_acc: 0.6757\n",
            "Epoch 1945/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 608.6088 - acc: 0.7807 - val_loss: 608.6704 - val_acc: 0.6757\n",
            "Epoch 1946/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 608.5793 - acc: 0.8053 - val_loss: 608.6403 - val_acc: 0.6757\n",
            "Epoch 1947/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 608.5407 - acc: 0.7766 - val_loss: 608.6102 - val_acc: 0.6757\n",
            "Epoch 1948/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 608.5185 - acc: 0.7807 - val_loss: 608.5799 - val_acc: 0.6757\n",
            "Epoch 1949/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 608.4836 - acc: 0.7930 - val_loss: 608.5499 - val_acc: 0.6757\n",
            "Epoch 1950/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 608.4442 - acc: 0.7930 - val_loss: 608.5198 - val_acc: 0.6757\n",
            "Epoch 1951/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 608.4250 - acc: 0.7828 - val_loss: 608.4897 - val_acc: 0.6757\n",
            "Epoch 1952/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 608.3951 - acc: 0.7971 - val_loss: 608.4597 - val_acc: 0.6757\n",
            "Epoch 1953/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 608.3636 - acc: 0.7746 - val_loss: 608.4296 - val_acc: 0.6778\n",
            "Epoch 1954/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 608.3295 - acc: 0.7971 - val_loss: 608.3994 - val_acc: 0.6757\n",
            "Epoch 1955/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 608.3070 - acc: 0.7930 - val_loss: 608.3693 - val_acc: 0.6757\n",
            "Epoch 1956/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 608.2689 - acc: 0.7971 - val_loss: 608.3391 - val_acc: 0.6757\n",
            "Epoch 1957/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 608.2456 - acc: 0.7869 - val_loss: 608.3091 - val_acc: 0.6757\n",
            "Epoch 1958/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 608.2102 - acc: 0.7910 - val_loss: 608.2791 - val_acc: 0.6757\n",
            "Epoch 1959/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 608.1893 - acc: 0.7910 - val_loss: 608.2489 - val_acc: 0.6778\n",
            "Epoch 1960/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 608.1606 - acc: 0.7848 - val_loss: 608.2189 - val_acc: 0.6757\n",
            "Epoch 1961/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 608.1264 - acc: 0.7766 - val_loss: 608.1888 - val_acc: 0.6757\n",
            "Epoch 1962/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 608.0951 - acc: 0.7889 - val_loss: 608.1587 - val_acc: 0.6778\n",
            "Epoch 1963/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 608.0597 - acc: 0.7889 - val_loss: 608.1287 - val_acc: 0.6778\n",
            "Epoch 1964/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 608.0284 - acc: 0.7951 - val_loss: 608.0987 - val_acc: 0.6820\n",
            "Epoch 1965/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 608.0093 - acc: 0.7766 - val_loss: 608.0687 - val_acc: 0.6820\n",
            "Epoch 1966/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 607.9725 - acc: 0.7725 - val_loss: 608.0385 - val_acc: 0.6820\n",
            "Epoch 1967/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 607.9328 - acc: 0.8012 - val_loss: 608.0084 - val_acc: 0.6820\n",
            "Epoch 1968/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 607.9112 - acc: 0.7992 - val_loss: 607.9783 - val_acc: 0.6820\n",
            "Epoch 1969/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 607.8846 - acc: 0.7725 - val_loss: 607.9483 - val_acc: 0.6820\n",
            "Epoch 1970/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 607.8521 - acc: 0.7766 - val_loss: 607.9181 - val_acc: 0.6799\n",
            "Epoch 1971/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 607.8128 - acc: 0.7971 - val_loss: 607.8881 - val_acc: 0.6799\n",
            "Epoch 1972/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 607.7933 - acc: 0.7910 - val_loss: 607.8581 - val_acc: 0.6799\n",
            "Epoch 1973/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 607.7598 - acc: 0.7930 - val_loss: 607.8281 - val_acc: 0.6799\n",
            "Epoch 1974/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 607.7367 - acc: 0.7910 - val_loss: 607.7979 - val_acc: 0.6778\n",
            "Epoch 1975/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 607.6995 - acc: 0.7848 - val_loss: 607.7679 - val_acc: 0.6778\n",
            "Epoch 1976/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 607.6632 - acc: 0.7930 - val_loss: 607.7379 - val_acc: 0.6778\n",
            "Epoch 1977/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 607.6504 - acc: 0.7705 - val_loss: 607.7079 - val_acc: 0.6778\n",
            "Epoch 1978/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 607.6030 - acc: 0.8033 - val_loss: 607.6778 - val_acc: 0.6778\n",
            "Epoch 1979/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 607.5765 - acc: 0.8094 - val_loss: 607.6478 - val_acc: 0.6799\n",
            "Epoch 1980/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 607.5460 - acc: 0.7951 - val_loss: 607.6178 - val_acc: 0.6799\n",
            "Epoch 1981/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 607.5220 - acc: 0.7910 - val_loss: 607.5876 - val_acc: 0.6799\n",
            "Epoch 1982/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 607.4938 - acc: 0.7869 - val_loss: 607.5576 - val_acc: 0.6799\n",
            "Epoch 1983/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 607.4573 - acc: 0.8094 - val_loss: 607.5275 - val_acc: 0.6799\n",
            "Epoch 1984/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 607.4346 - acc: 0.7684 - val_loss: 607.4974 - val_acc: 0.6799\n",
            "Epoch 1985/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 607.3889 - acc: 0.8033 - val_loss: 607.4673 - val_acc: 0.6799\n",
            "Epoch 1986/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 607.3694 - acc: 0.7889 - val_loss: 607.4373 - val_acc: 0.6799\n",
            "Epoch 1987/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 607.3381 - acc: 0.7766 - val_loss: 607.4072 - val_acc: 0.6799\n",
            "Epoch 1988/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 607.3002 - acc: 0.8217 - val_loss: 607.3773 - val_acc: 0.6799\n",
            "Epoch 1989/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 607.2796 - acc: 0.7992 - val_loss: 607.3473 - val_acc: 0.6799\n",
            "Epoch 1990/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 607.2495 - acc: 0.7828 - val_loss: 607.3173 - val_acc: 0.6799\n",
            "Epoch 1991/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 607.2203 - acc: 0.7828 - val_loss: 607.2872 - val_acc: 0.6799\n",
            "Epoch 1992/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 607.1809 - acc: 0.7869 - val_loss: 607.2572 - val_acc: 0.6799\n",
            "Epoch 1993/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 607.1709 - acc: 0.7951 - val_loss: 607.2272 - val_acc: 0.6799\n",
            "Epoch 1994/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 607.1263 - acc: 0.7807 - val_loss: 607.1972 - val_acc: 0.6799\n",
            "Epoch 1995/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 607.0938 - acc: 0.8033 - val_loss: 607.1670 - val_acc: 0.6799\n",
            "Epoch 1996/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 607.0659 - acc: 0.7889 - val_loss: 607.1370 - val_acc: 0.6799\n",
            "Epoch 1997/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 607.0461 - acc: 0.7807 - val_loss: 607.1070 - val_acc: 0.6799\n",
            "Epoch 1998/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 606.9976 - acc: 0.8033 - val_loss: 607.0770 - val_acc: 0.6799\n",
            "Epoch 1999/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 606.9760 - acc: 0.7971 - val_loss: 607.0470 - val_acc: 0.6799\n",
            "Epoch 2000/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 606.9458 - acc: 0.7930 - val_loss: 607.0171 - val_acc: 0.6799\n",
            "Epoch 2001/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 606.9189 - acc: 0.7807 - val_loss: 606.9869 - val_acc: 0.6799\n",
            "Epoch 2002/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 606.9046 - acc: 0.7582 - val_loss: 606.9567 - val_acc: 0.6799\n",
            "Epoch 2003/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 606.8612 - acc: 0.7848 - val_loss: 606.9267 - val_acc: 0.6799\n",
            "Epoch 2004/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 606.8327 - acc: 0.7807 - val_loss: 606.8967 - val_acc: 0.6799\n",
            "Epoch 2005/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 606.7988 - acc: 0.8033 - val_loss: 606.8667 - val_acc: 0.6799\n",
            "Epoch 2006/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 606.7662 - acc: 0.7992 - val_loss: 606.8367 - val_acc: 0.6799\n",
            "Epoch 2007/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 606.7433 - acc: 0.7766 - val_loss: 606.8067 - val_acc: 0.6799\n",
            "Epoch 2008/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 606.7065 - acc: 0.7869 - val_loss: 606.7767 - val_acc: 0.6799\n",
            "Epoch 2009/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 606.6858 - acc: 0.7766 - val_loss: 606.7467 - val_acc: 0.6799\n",
            "Epoch 2010/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 606.6597 - acc: 0.7602 - val_loss: 606.7166 - val_acc: 0.6799\n",
            "Epoch 2011/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 606.6021 - acc: 0.8135 - val_loss: 606.6866 - val_acc: 0.6799\n",
            "Epoch 2012/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 606.5836 - acc: 0.8115 - val_loss: 606.6566 - val_acc: 0.6778\n",
            "Epoch 2013/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 606.5523 - acc: 0.7951 - val_loss: 606.6266 - val_acc: 0.6799\n",
            "Epoch 2014/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 606.5247 - acc: 0.7889 - val_loss: 606.5967 - val_acc: 0.6799\n",
            "Epoch 2015/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 606.4901 - acc: 0.8074 - val_loss: 606.5667 - val_acc: 0.6778\n",
            "Epoch 2016/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 606.4697 - acc: 0.7869 - val_loss: 606.5366 - val_acc: 0.6778\n",
            "Epoch 2017/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 606.4299 - acc: 0.8053 - val_loss: 606.5066 - val_acc: 0.6778\n",
            "Epoch 2018/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 606.4242 - acc: 0.7643 - val_loss: 606.4767 - val_acc: 0.6799\n",
            "Epoch 2019/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 606.3803 - acc: 0.7910 - val_loss: 606.4467 - val_acc: 0.6799\n",
            "Epoch 2020/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 606.3618 - acc: 0.7951 - val_loss: 606.4166 - val_acc: 0.6799\n",
            "Epoch 2021/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 606.3274 - acc: 0.7643 - val_loss: 606.3867 - val_acc: 0.6799\n",
            "Epoch 2022/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 606.2882 - acc: 0.7930 - val_loss: 606.3566 - val_acc: 0.6799\n",
            "Epoch 2023/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 606.2708 - acc: 0.7643 - val_loss: 606.3266 - val_acc: 0.6799\n",
            "Epoch 2024/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 606.2304 - acc: 0.7951 - val_loss: 606.2966 - val_acc: 0.6778\n",
            "Epoch 2025/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 606.1901 - acc: 0.7828 - val_loss: 606.2666 - val_acc: 0.6778\n",
            "Epoch 2026/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 606.1720 - acc: 0.7725 - val_loss: 606.2367 - val_acc: 0.6778\n",
            "Epoch 2027/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 606.1397 - acc: 0.7787 - val_loss: 606.2069 - val_acc: 0.6799\n",
            "Epoch 2028/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 606.1075 - acc: 0.7869 - val_loss: 606.1768 - val_acc: 0.6799\n",
            "Epoch 2029/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 606.0709 - acc: 0.7951 - val_loss: 606.1468 - val_acc: 0.6799\n",
            "Epoch 2030/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 606.0588 - acc: 0.7500 - val_loss: 606.1168 - val_acc: 0.6799\n",
            "Epoch 2031/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 606.0300 - acc: 0.7725 - val_loss: 606.0869 - val_acc: 0.6799\n",
            "Epoch 2032/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 605.9921 - acc: 0.7746 - val_loss: 606.0568 - val_acc: 0.6799\n",
            "Epoch 2033/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 605.9562 - acc: 0.8053 - val_loss: 606.0268 - val_acc: 0.6799\n",
            "Epoch 2034/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 605.9375 - acc: 0.7807 - val_loss: 605.9969 - val_acc: 0.6799\n",
            "Epoch 2035/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 605.9013 - acc: 0.7787 - val_loss: 605.9669 - val_acc: 0.6799\n",
            "Epoch 2036/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 605.8650 - acc: 0.7910 - val_loss: 605.9369 - val_acc: 0.6799\n",
            "Epoch 2037/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 605.8444 - acc: 0.7828 - val_loss: 605.9069 - val_acc: 0.6799\n",
            "Epoch 2038/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 605.8121 - acc: 0.7807 - val_loss: 605.8770 - val_acc: 0.6799\n",
            "Epoch 2039/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 605.7826 - acc: 0.7869 - val_loss: 605.8470 - val_acc: 0.6799\n",
            "Epoch 2040/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 605.7554 - acc: 0.7848 - val_loss: 605.8170 - val_acc: 0.6799\n",
            "Epoch 2041/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 605.7085 - acc: 0.7910 - val_loss: 605.7871 - val_acc: 0.6778\n",
            "Epoch 2042/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 605.6911 - acc: 0.7889 - val_loss: 605.7571 - val_acc: 0.6778\n",
            "Epoch 2043/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 605.6528 - acc: 0.7971 - val_loss: 605.7271 - val_acc: 0.6778\n",
            "Epoch 2044/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 605.6162 - acc: 0.8053 - val_loss: 605.6971 - val_acc: 0.6778\n",
            "Epoch 2045/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 605.5933 - acc: 0.7889 - val_loss: 605.6673 - val_acc: 0.6799\n",
            "Epoch 2046/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 605.5653 - acc: 0.7807 - val_loss: 605.6373 - val_acc: 0.6799\n",
            "Epoch 2047/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 605.5406 - acc: 0.7869 - val_loss: 605.6074 - val_acc: 0.6820\n",
            "Epoch 2048/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 605.5114 - acc: 0.7766 - val_loss: 605.5774 - val_acc: 0.6820\n",
            "Epoch 2049/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 605.4714 - acc: 0.8053 - val_loss: 605.5474 - val_acc: 0.6820\n",
            "Epoch 2050/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 605.4539 - acc: 0.7725 - val_loss: 605.5173 - val_acc: 0.6820\n",
            "Epoch 2051/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 605.4190 - acc: 0.7971 - val_loss: 605.4876 - val_acc: 0.6820\n",
            "Epoch 2052/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 605.3911 - acc: 0.7869 - val_loss: 605.4575 - val_acc: 0.6820\n",
            "Epoch 2053/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 605.3566 - acc: 0.8094 - val_loss: 605.4276 - val_acc: 0.6820\n",
            "Epoch 2054/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 605.3241 - acc: 0.7869 - val_loss: 605.3975 - val_acc: 0.6820\n",
            "Epoch 2055/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 605.2981 - acc: 0.8033 - val_loss: 605.3676 - val_acc: 0.6820\n",
            "Epoch 2056/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 605.2628 - acc: 0.7971 - val_loss: 605.3377 - val_acc: 0.6820\n",
            "Epoch 2057/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 605.2457 - acc: 0.7910 - val_loss: 605.3078 - val_acc: 0.6820\n",
            "Epoch 2058/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 605.1955 - acc: 0.8094 - val_loss: 605.2777 - val_acc: 0.6799\n",
            "Epoch 2059/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 605.1790 - acc: 0.8012 - val_loss: 605.2479 - val_acc: 0.6799\n",
            "Epoch 2060/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 605.1412 - acc: 0.8320 - val_loss: 605.2179 - val_acc: 0.6820\n",
            "Epoch 2061/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 605.1135 - acc: 0.7951 - val_loss: 605.1881 - val_acc: 0.6820\n",
            "Epoch 2062/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 605.0933 - acc: 0.7889 - val_loss: 605.1581 - val_acc: 0.6820\n",
            "Epoch 2063/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 605.0598 - acc: 0.7951 - val_loss: 605.1280 - val_acc: 0.6820\n",
            "Epoch 2064/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 605.0229 - acc: 0.7910 - val_loss: 605.0980 - val_acc: 0.6799\n",
            "Epoch 2065/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 604.9961 - acc: 0.7971 - val_loss: 605.0682 - val_acc: 0.6799\n",
            "Epoch 2066/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 604.9627 - acc: 0.8033 - val_loss: 605.0382 - val_acc: 0.6799\n",
            "Epoch 2067/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 604.9351 - acc: 0.8176 - val_loss: 605.0083 - val_acc: 0.6799\n",
            "Epoch 2068/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 604.9105 - acc: 0.7787 - val_loss: 604.9784 - val_acc: 0.6799\n",
            "Epoch 2069/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 604.8800 - acc: 0.7766 - val_loss: 604.9484 - val_acc: 0.6799\n",
            "Epoch 2070/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 604.8431 - acc: 0.7787 - val_loss: 604.9185 - val_acc: 0.6799\n",
            "Epoch 2071/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 604.8239 - acc: 0.7869 - val_loss: 604.8886 - val_acc: 0.6799\n",
            "Epoch 2072/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 604.7851 - acc: 0.7869 - val_loss: 604.8586 - val_acc: 0.6799\n",
            "Epoch 2073/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 604.7656 - acc: 0.7561 - val_loss: 604.8287 - val_acc: 0.6778\n",
            "Epoch 2074/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 604.7192 - acc: 0.8033 - val_loss: 604.7989 - val_acc: 0.6799\n",
            "Epoch 2075/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 604.7041 - acc: 0.7705 - val_loss: 604.7690 - val_acc: 0.6799\n",
            "Epoch 2076/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 604.6644 - acc: 0.7869 - val_loss: 604.7391 - val_acc: 0.6799\n",
            "Epoch 2077/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 604.6305 - acc: 0.8135 - val_loss: 604.7092 - val_acc: 0.6820\n",
            "Epoch 2078/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 604.5984 - acc: 0.8033 - val_loss: 604.6793 - val_acc: 0.6820\n",
            "Epoch 2079/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 604.5783 - acc: 0.7951 - val_loss: 604.6494 - val_acc: 0.6799\n",
            "Epoch 2080/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 604.5400 - acc: 0.8135 - val_loss: 604.6195 - val_acc: 0.6799\n",
            "Epoch 2081/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 604.5252 - acc: 0.7910 - val_loss: 604.5895 - val_acc: 0.6820\n",
            "Epoch 2082/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 604.4903 - acc: 0.7971 - val_loss: 604.5597 - val_acc: 0.6778\n",
            "Epoch 2083/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 604.4561 - acc: 0.7869 - val_loss: 604.5297 - val_acc: 0.6820\n",
            "Epoch 2084/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 604.4284 - acc: 0.7910 - val_loss: 604.4998 - val_acc: 0.6820\n",
            "Epoch 2085/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 604.4072 - acc: 0.7971 - val_loss: 604.4700 - val_acc: 0.6820\n",
            "Epoch 2086/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 604.3684 - acc: 0.7869 - val_loss: 604.4401 - val_acc: 0.6778\n",
            "Epoch 2087/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 604.3279 - acc: 0.8361 - val_loss: 604.4103 - val_acc: 0.6778\n",
            "Epoch 2088/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 604.3112 - acc: 0.7951 - val_loss: 604.3803 - val_acc: 0.6778\n",
            "Epoch 2089/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 604.2889 - acc: 0.7623 - val_loss: 604.3504 - val_acc: 0.6778\n",
            "Epoch 2090/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 604.2475 - acc: 0.7889 - val_loss: 604.3205 - val_acc: 0.6799\n",
            "Epoch 2091/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 604.2229 - acc: 0.7930 - val_loss: 604.2907 - val_acc: 0.6799\n",
            "Epoch 2092/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 604.1974 - acc: 0.7787 - val_loss: 604.2608 - val_acc: 0.6799\n",
            "Epoch 2093/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 604.1666 - acc: 0.7602 - val_loss: 604.2309 - val_acc: 0.6799\n",
            "Epoch 2094/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 604.1336 - acc: 0.7828 - val_loss: 604.2011 - val_acc: 0.6799\n",
            "Epoch 2095/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 604.0953 - acc: 0.8115 - val_loss: 604.1712 - val_acc: 0.6799\n",
            "Epoch 2096/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 604.0636 - acc: 0.8033 - val_loss: 604.1413 - val_acc: 0.6799\n",
            "Epoch 2097/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 604.0346 - acc: 0.8033 - val_loss: 604.1114 - val_acc: 0.6799\n",
            "Epoch 2098/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 604.0101 - acc: 0.7828 - val_loss: 604.0816 - val_acc: 0.6799\n",
            "Epoch 2099/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 603.9850 - acc: 0.7705 - val_loss: 604.0517 - val_acc: 0.6820\n",
            "Epoch 2100/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 603.9516 - acc: 0.7807 - val_loss: 604.0219 - val_acc: 0.6820\n",
            "Epoch 2101/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 603.9066 - acc: 0.8217 - val_loss: 603.9919 - val_acc: 0.6820\n",
            "Epoch 2102/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 603.8854 - acc: 0.7930 - val_loss: 603.9621 - val_acc: 0.6820\n",
            "Epoch 2103/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 603.8609 - acc: 0.7971 - val_loss: 603.9321 - val_acc: 0.6820\n",
            "Epoch 2104/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 603.8417 - acc: 0.7869 - val_loss: 603.9023 - val_acc: 0.6820\n",
            "Epoch 2105/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 603.8029 - acc: 0.7869 - val_loss: 603.8725 - val_acc: 0.6820\n",
            "Epoch 2106/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 603.7701 - acc: 0.7971 - val_loss: 603.8427 - val_acc: 0.6799\n",
            "Epoch 2107/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 603.7266 - acc: 0.8258 - val_loss: 603.8127 - val_acc: 0.6820\n",
            "Epoch 2108/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 603.7210 - acc: 0.7725 - val_loss: 603.7829 - val_acc: 0.6820\n",
            "Epoch 2109/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 603.6725 - acc: 0.7848 - val_loss: 603.7530 - val_acc: 0.6820\n",
            "Epoch 2110/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 603.6583 - acc: 0.7971 - val_loss: 603.7231 - val_acc: 0.6799\n",
            "Epoch 2111/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 603.6281 - acc: 0.8012 - val_loss: 603.6934 - val_acc: 0.6799\n",
            "Epoch 2112/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 603.5907 - acc: 0.7951 - val_loss: 603.6634 - val_acc: 0.6820\n",
            "Epoch 2113/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 603.5551 - acc: 0.7992 - val_loss: 603.6337 - val_acc: 0.6820\n",
            "Epoch 2114/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 603.5295 - acc: 0.7705 - val_loss: 603.6038 - val_acc: 0.6820\n",
            "Epoch 2115/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 603.4897 - acc: 0.8094 - val_loss: 603.5740 - val_acc: 0.6820\n",
            "Epoch 2116/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 603.4763 - acc: 0.8012 - val_loss: 603.5440 - val_acc: 0.6799\n",
            "Epoch 2117/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 603.4441 - acc: 0.7705 - val_loss: 603.5141 - val_acc: 0.6778\n",
            "Epoch 2118/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 603.4271 - acc: 0.7787 - val_loss: 603.4843 - val_acc: 0.6778\n",
            "Epoch 2119/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 603.3768 - acc: 0.8176 - val_loss: 603.4546 - val_acc: 0.6799\n",
            "Epoch 2120/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 603.3419 - acc: 0.8135 - val_loss: 603.4247 - val_acc: 0.6799\n",
            "Epoch 2121/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 603.3258 - acc: 0.7869 - val_loss: 603.3951 - val_acc: 0.6799\n",
            "Epoch 2122/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 603.2963 - acc: 0.7992 - val_loss: 603.3652 - val_acc: 0.6799\n",
            "Epoch 2123/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 603.2600 - acc: 0.8094 - val_loss: 603.3353 - val_acc: 0.6799\n",
            "Epoch 2124/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 603.2338 - acc: 0.7951 - val_loss: 603.3054 - val_acc: 0.6799\n",
            "Epoch 2125/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 603.2162 - acc: 0.7746 - val_loss: 603.2756 - val_acc: 0.6799\n",
            "Epoch 2126/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 603.1717 - acc: 0.7807 - val_loss: 603.2457 - val_acc: 0.6799\n",
            "Epoch 2127/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 603.1443 - acc: 0.7766 - val_loss: 603.2158 - val_acc: 0.6799\n",
            "Epoch 2128/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 603.1302 - acc: 0.7746 - val_loss: 603.1861 - val_acc: 0.6799\n",
            "Epoch 2129/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 603.0846 - acc: 0.7848 - val_loss: 603.1561 - val_acc: 0.6778\n",
            "Epoch 2130/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 603.0601 - acc: 0.7869 - val_loss: 603.1264 - val_acc: 0.6799\n",
            "Epoch 2131/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 603.0302 - acc: 0.7992 - val_loss: 603.0966 - val_acc: 0.6799\n",
            "Epoch 2132/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 602.9913 - acc: 0.8012 - val_loss: 603.0669 - val_acc: 0.6799\n",
            "Epoch 2133/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 602.9680 - acc: 0.7848 - val_loss: 603.0370 - val_acc: 0.6799\n",
            "Epoch 2134/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 602.9240 - acc: 0.8012 - val_loss: 603.0072 - val_acc: 0.6799\n",
            "Epoch 2135/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 602.8939 - acc: 0.8094 - val_loss: 602.9775 - val_acc: 0.6799\n",
            "Epoch 2136/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 602.8702 - acc: 0.8094 - val_loss: 602.9476 - val_acc: 0.6799\n",
            "Epoch 2137/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 602.8471 - acc: 0.7807 - val_loss: 602.9178 - val_acc: 0.6799\n",
            "Epoch 2138/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 602.8159 - acc: 0.7807 - val_loss: 602.8881 - val_acc: 0.6799\n",
            "Epoch 2139/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 602.7827 - acc: 0.7951 - val_loss: 602.8584 - val_acc: 0.6799\n",
            "Epoch 2140/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 602.7539 - acc: 0.7787 - val_loss: 602.8285 - val_acc: 0.6799\n",
            "Epoch 2141/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 602.7255 - acc: 0.7889 - val_loss: 602.7986 - val_acc: 0.6799\n",
            "Epoch 2142/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 602.6880 - acc: 0.8053 - val_loss: 602.7688 - val_acc: 0.6799\n",
            "Epoch 2143/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 602.6679 - acc: 0.7930 - val_loss: 602.7390 - val_acc: 0.6799\n",
            "Epoch 2144/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 602.6393 - acc: 0.7910 - val_loss: 602.7092 - val_acc: 0.6799\n",
            "Epoch 2145/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 602.6104 - acc: 0.7889 - val_loss: 602.6793 - val_acc: 0.6799\n",
            "Epoch 2146/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 602.5705 - acc: 0.8033 - val_loss: 602.6495 - val_acc: 0.6799\n",
            "Epoch 2147/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 602.5380 - acc: 0.8053 - val_loss: 602.6196 - val_acc: 0.6799\n",
            "Epoch 2148/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 602.5219 - acc: 0.7848 - val_loss: 602.5898 - val_acc: 0.6799\n",
            "Epoch 2149/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 602.4899 - acc: 0.7889 - val_loss: 602.5601 - val_acc: 0.6799\n",
            "Epoch 2150/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 602.4639 - acc: 0.8012 - val_loss: 602.5302 - val_acc: 0.6799\n",
            "Epoch 2151/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 602.4208 - acc: 0.7930 - val_loss: 602.5006 - val_acc: 0.6799\n",
            "Epoch 2152/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 602.3979 - acc: 0.7971 - val_loss: 602.4707 - val_acc: 0.6820\n",
            "Epoch 2153/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 602.3651 - acc: 0.8012 - val_loss: 602.4409 - val_acc: 0.6820\n",
            "Epoch 2154/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 602.3377 - acc: 0.7725 - val_loss: 602.4111 - val_acc: 0.6820\n",
            "Epoch 2155/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 602.3036 - acc: 0.7787 - val_loss: 602.3813 - val_acc: 0.6820\n",
            "Epoch 2156/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 602.2699 - acc: 0.8217 - val_loss: 602.3515 - val_acc: 0.6820\n",
            "Epoch 2157/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 602.2395 - acc: 0.8053 - val_loss: 602.3216 - val_acc: 0.6820\n",
            "Epoch 2158/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 602.2188 - acc: 0.7971 - val_loss: 602.2919 - val_acc: 0.6820\n",
            "Epoch 2159/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 602.1905 - acc: 0.7746 - val_loss: 602.2619 - val_acc: 0.6820\n",
            "Epoch 2160/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 602.1615 - acc: 0.7664 - val_loss: 602.2321 - val_acc: 0.6820\n",
            "Epoch 2161/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 602.1157 - acc: 0.8074 - val_loss: 602.2024 - val_acc: 0.6820\n",
            "Epoch 2162/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 602.1100 - acc: 0.7971 - val_loss: 602.1727 - val_acc: 0.6820\n",
            "Epoch 2163/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 602.0708 - acc: 0.7910 - val_loss: 602.1429 - val_acc: 0.6820\n",
            "Epoch 2164/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 602.0505 - acc: 0.7705 - val_loss: 602.1131 - val_acc: 0.6820\n",
            "Epoch 2165/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 602.0054 - acc: 0.8074 - val_loss: 602.0833 - val_acc: 0.6820\n",
            "Epoch 2166/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 601.9949 - acc: 0.7623 - val_loss: 602.0536 - val_acc: 0.6820\n",
            "Epoch 2167/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 601.9504 - acc: 0.7787 - val_loss: 602.0238 - val_acc: 0.6820\n",
            "Epoch 2168/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 601.9279 - acc: 0.7848 - val_loss: 601.9941 - val_acc: 0.6820\n",
            "Epoch 2169/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 601.8941 - acc: 0.7848 - val_loss: 601.9643 - val_acc: 0.6820\n",
            "Epoch 2170/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 601.8654 - acc: 0.7828 - val_loss: 601.9345 - val_acc: 0.6820\n",
            "Epoch 2171/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 601.8222 - acc: 0.7971 - val_loss: 601.9047 - val_acc: 0.6820\n",
            "Epoch 2172/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 601.7997 - acc: 0.8012 - val_loss: 601.8750 - val_acc: 0.6820\n",
            "Epoch 2173/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 601.7689 - acc: 0.7930 - val_loss: 601.8453 - val_acc: 0.6820\n",
            "Epoch 2174/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 601.7314 - acc: 0.8156 - val_loss: 601.8154 - val_acc: 0.6820\n",
            "Epoch 2175/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 601.7260 - acc: 0.7889 - val_loss: 601.7857 - val_acc: 0.6820\n",
            "Epoch 2176/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 601.6829 - acc: 0.7910 - val_loss: 601.7558 - val_acc: 0.6820\n",
            "Epoch 2177/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 601.6577 - acc: 0.7889 - val_loss: 601.7260 - val_acc: 0.6820\n",
            "Epoch 2178/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 601.6251 - acc: 0.7746 - val_loss: 601.6963 - val_acc: 0.6820\n",
            "Epoch 2179/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 601.5937 - acc: 0.7930 - val_loss: 601.6666 - val_acc: 0.6820\n",
            "Epoch 2180/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 601.5553 - acc: 0.8156 - val_loss: 601.6369 - val_acc: 0.6820\n",
            "Epoch 2181/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 601.5412 - acc: 0.7848 - val_loss: 601.6071 - val_acc: 0.6820\n",
            "Epoch 2182/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 601.4962 - acc: 0.8094 - val_loss: 601.5773 - val_acc: 0.6820\n",
            "Epoch 2183/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 601.4733 - acc: 0.8053 - val_loss: 601.5477 - val_acc: 0.6820\n",
            "Epoch 2184/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 601.4571 - acc: 0.7746 - val_loss: 601.5179 - val_acc: 0.6820\n",
            "Epoch 2185/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 601.4143 - acc: 0.7971 - val_loss: 601.4880 - val_acc: 0.6820\n",
            "Epoch 2186/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 601.3839 - acc: 0.8012 - val_loss: 601.4581 - val_acc: 0.6820\n",
            "Epoch 2187/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 601.3583 - acc: 0.7869 - val_loss: 601.4284 - val_acc: 0.6820\n",
            "Epoch 2188/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 601.3135 - acc: 0.7971 - val_loss: 601.3987 - val_acc: 0.6820\n",
            "Epoch 2189/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 601.2959 - acc: 0.8033 - val_loss: 601.3690 - val_acc: 0.6820\n",
            "Epoch 2190/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 601.2589 - acc: 0.8033 - val_loss: 601.3394 - val_acc: 0.6820\n",
            "Epoch 2191/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 601.2339 - acc: 0.7869 - val_loss: 601.3096 - val_acc: 0.6820\n",
            "Epoch 2192/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 601.2094 - acc: 0.7725 - val_loss: 601.2799 - val_acc: 0.6820\n",
            "Epoch 2193/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 601.1804 - acc: 0.7951 - val_loss: 601.2501 - val_acc: 0.6820\n",
            "Epoch 2194/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 601.1507 - acc: 0.7971 - val_loss: 601.2204 - val_acc: 0.6820\n",
            "Epoch 2195/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 601.1233 - acc: 0.7869 - val_loss: 601.1907 - val_acc: 0.6820\n",
            "Epoch 2196/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 601.0942 - acc: 0.7992 - val_loss: 601.1610 - val_acc: 0.6820\n",
            "Epoch 2197/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 601.0567 - acc: 0.7951 - val_loss: 601.1313 - val_acc: 0.6820\n",
            "Epoch 2198/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 601.0270 - acc: 0.8053 - val_loss: 601.1016 - val_acc: 0.6820\n",
            "Epoch 2199/3000\n",
            "488/488 [==============================] - 0s 373us/step - loss: 601.0013 - acc: 0.7766 - val_loss: 601.0719 - val_acc: 0.6820\n",
            "Epoch 2200/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 600.9626 - acc: 0.8197 - val_loss: 601.0421 - val_acc: 0.6820\n",
            "Epoch 2201/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 600.9400 - acc: 0.7992 - val_loss: 601.0123 - val_acc: 0.6820\n",
            "Epoch 2202/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 600.9174 - acc: 0.7787 - val_loss: 600.9826 - val_acc: 0.6820\n",
            "Epoch 2203/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 600.8797 - acc: 0.7992 - val_loss: 600.9529 - val_acc: 0.6820\n",
            "Epoch 2204/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 600.8413 - acc: 0.7930 - val_loss: 600.9232 - val_acc: 0.6820\n",
            "Epoch 2205/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 600.8167 - acc: 0.7971 - val_loss: 600.8935 - val_acc: 0.6820\n",
            "Epoch 2206/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 600.7877 - acc: 0.7889 - val_loss: 600.8638 - val_acc: 0.6820\n",
            "Epoch 2207/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 600.7611 - acc: 0.7971 - val_loss: 600.8340 - val_acc: 0.6820\n",
            "Epoch 2208/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 600.7287 - acc: 0.8135 - val_loss: 600.8042 - val_acc: 0.6820\n",
            "Epoch 2209/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 600.6978 - acc: 0.8012 - val_loss: 600.7746 - val_acc: 0.6820\n",
            "Epoch 2210/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 600.6633 - acc: 0.7992 - val_loss: 600.7449 - val_acc: 0.6820\n",
            "Epoch 2211/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 600.6370 - acc: 0.8094 - val_loss: 600.7151 - val_acc: 0.6799\n",
            "Epoch 2212/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 600.6047 - acc: 0.8033 - val_loss: 600.6855 - val_acc: 0.6799\n",
            "Epoch 2213/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 600.5883 - acc: 0.7828 - val_loss: 600.6557 - val_acc: 0.6799\n",
            "Epoch 2214/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 600.5402 - acc: 0.8258 - val_loss: 600.6261 - val_acc: 0.6799\n",
            "Epoch 2215/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 600.5232 - acc: 0.7889 - val_loss: 600.5964 - val_acc: 0.6799\n",
            "Epoch 2216/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 600.4932 - acc: 0.7746 - val_loss: 600.5667 - val_acc: 0.6799\n",
            "Epoch 2217/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 600.4662 - acc: 0.7787 - val_loss: 600.5370 - val_acc: 0.6820\n",
            "Epoch 2218/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 600.4371 - acc: 0.7992 - val_loss: 600.5072 - val_acc: 0.6820\n",
            "Epoch 2219/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 600.4077 - acc: 0.7971 - val_loss: 600.4776 - val_acc: 0.6841\n",
            "Epoch 2220/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 600.3765 - acc: 0.8053 - val_loss: 600.4478 - val_acc: 0.6820\n",
            "Epoch 2221/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 600.3418 - acc: 0.7971 - val_loss: 600.4180 - val_acc: 0.6820\n",
            "Epoch 2222/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 600.3113 - acc: 0.7951 - val_loss: 600.3884 - val_acc: 0.6820\n",
            "Epoch 2223/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 600.2835 - acc: 0.8033 - val_loss: 600.3587 - val_acc: 0.6820\n",
            "Epoch 2224/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 600.2570 - acc: 0.7807 - val_loss: 600.3289 - val_acc: 0.6820\n",
            "Epoch 2225/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 600.2171 - acc: 0.7889 - val_loss: 600.2993 - val_acc: 0.6820\n",
            "Epoch 2226/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 600.1978 - acc: 0.7971 - val_loss: 600.2696 - val_acc: 0.6820\n",
            "Epoch 2227/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 600.1704 - acc: 0.7725 - val_loss: 600.2399 - val_acc: 0.6820\n",
            "Epoch 2228/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 600.1382 - acc: 0.7869 - val_loss: 600.2103 - val_acc: 0.6820\n",
            "Epoch 2229/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 600.1116 - acc: 0.7746 - val_loss: 600.1805 - val_acc: 0.6820\n",
            "Epoch 2230/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 600.0832 - acc: 0.7910 - val_loss: 600.1509 - val_acc: 0.6820\n",
            "Epoch 2231/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 600.0450 - acc: 0.7930 - val_loss: 600.1210 - val_acc: 0.6820\n",
            "Epoch 2232/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 600.0097 - acc: 0.8033 - val_loss: 600.0914 - val_acc: 0.6799\n",
            "Epoch 2233/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 600.0031 - acc: 0.7766 - val_loss: 600.0618 - val_acc: 0.6799\n",
            "Epoch 2234/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 599.9483 - acc: 0.7910 - val_loss: 600.0321 - val_acc: 0.6799\n",
            "Epoch 2235/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 599.9343 - acc: 0.7951 - val_loss: 600.0025 - val_acc: 0.6799\n",
            "Epoch 2236/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 599.8915 - acc: 0.8033 - val_loss: 599.9727 - val_acc: 0.6799\n",
            "Epoch 2237/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 599.8579 - acc: 0.7951 - val_loss: 599.9429 - val_acc: 0.6799\n",
            "Epoch 2238/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 599.8278 - acc: 0.8135 - val_loss: 599.9133 - val_acc: 0.6799\n",
            "Epoch 2239/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 599.7989 - acc: 0.8074 - val_loss: 599.8835 - val_acc: 0.6799\n",
            "Epoch 2240/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 599.7841 - acc: 0.7848 - val_loss: 599.8538 - val_acc: 0.6799\n",
            "Epoch 2241/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 599.7402 - acc: 0.8135 - val_loss: 599.8240 - val_acc: 0.6799\n",
            "Epoch 2242/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 599.7233 - acc: 0.7869 - val_loss: 599.7943 - val_acc: 0.6799\n",
            "Epoch 2243/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 599.6893 - acc: 0.7951 - val_loss: 599.7648 - val_acc: 0.6799\n",
            "Epoch 2244/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 599.6584 - acc: 0.7910 - val_loss: 599.7350 - val_acc: 0.6799\n",
            "Epoch 2245/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 599.6350 - acc: 0.8074 - val_loss: 599.7053 - val_acc: 0.6799\n",
            "Epoch 2246/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 599.6073 - acc: 0.7766 - val_loss: 599.6758 - val_acc: 0.6799\n",
            "Epoch 2247/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 599.5750 - acc: 0.7951 - val_loss: 599.6461 - val_acc: 0.6799\n",
            "Epoch 2248/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 599.5480 - acc: 0.7848 - val_loss: 599.6164 - val_acc: 0.6799\n",
            "Epoch 2249/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 599.5120 - acc: 0.7828 - val_loss: 599.5868 - val_acc: 0.6799\n",
            "Epoch 2250/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 599.4895 - acc: 0.7766 - val_loss: 599.5571 - val_acc: 0.6799\n",
            "Epoch 2251/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 599.4616 - acc: 0.7971 - val_loss: 599.5275 - val_acc: 0.6799\n",
            "Epoch 2252/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 599.4153 - acc: 0.7971 - val_loss: 599.4977 - val_acc: 0.6799\n",
            "Epoch 2253/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 599.3924 - acc: 0.7930 - val_loss: 599.4682 - val_acc: 0.6799\n",
            "Epoch 2254/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 599.3646 - acc: 0.7766 - val_loss: 599.4385 - val_acc: 0.6799\n",
            "Epoch 2255/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 599.3403 - acc: 0.7889 - val_loss: 599.4088 - val_acc: 0.6778\n",
            "Epoch 2256/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 599.3035 - acc: 0.8033 - val_loss: 599.3792 - val_acc: 0.6778\n",
            "Epoch 2257/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 599.2772 - acc: 0.7848 - val_loss: 599.3494 - val_acc: 0.6778\n",
            "Epoch 2258/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 599.2471 - acc: 0.7705 - val_loss: 599.3198 - val_acc: 0.6778\n",
            "Epoch 2259/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 599.2241 - acc: 0.7828 - val_loss: 599.2902 - val_acc: 0.6778\n",
            "Epoch 2260/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 599.1824 - acc: 0.7971 - val_loss: 599.2605 - val_acc: 0.6778\n",
            "Epoch 2261/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 599.1493 - acc: 0.8115 - val_loss: 599.2308 - val_acc: 0.6778\n",
            "Epoch 2262/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 599.1317 - acc: 0.7869 - val_loss: 599.2012 - val_acc: 0.6778\n",
            "Epoch 2263/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 599.1007 - acc: 0.7930 - val_loss: 599.1715 - val_acc: 0.6778\n",
            "Epoch 2264/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 599.0662 - acc: 0.8012 - val_loss: 599.1420 - val_acc: 0.6778\n",
            "Epoch 2265/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 599.0333 - acc: 0.7951 - val_loss: 599.1122 - val_acc: 0.6778\n",
            "Epoch 2266/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 599.0138 - acc: 0.7910 - val_loss: 599.0826 - val_acc: 0.6799\n",
            "Epoch 2267/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 598.9787 - acc: 0.8074 - val_loss: 599.0530 - val_acc: 0.6799\n",
            "Epoch 2268/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 598.9513 - acc: 0.8012 - val_loss: 599.0232 - val_acc: 0.6778\n",
            "Epoch 2269/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 598.9177 - acc: 0.7930 - val_loss: 598.9936 - val_acc: 0.6778\n",
            "Epoch 2270/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 598.8890 - acc: 0.8053 - val_loss: 598.9639 - val_acc: 0.6778\n",
            "Epoch 2271/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 598.8609 - acc: 0.7992 - val_loss: 598.9343 - val_acc: 0.6778\n",
            "Epoch 2272/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 598.8232 - acc: 0.8053 - val_loss: 598.9046 - val_acc: 0.6778\n",
            "Epoch 2273/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 598.8076 - acc: 0.7848 - val_loss: 598.8749 - val_acc: 0.6778\n",
            "Epoch 2274/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 598.7601 - acc: 0.8074 - val_loss: 598.8453 - val_acc: 0.6778\n",
            "Epoch 2275/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 598.7419 - acc: 0.7889 - val_loss: 598.8155 - val_acc: 0.6778\n",
            "Epoch 2276/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 598.7113 - acc: 0.7910 - val_loss: 598.7858 - val_acc: 0.6778\n",
            "Epoch 2277/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 598.6826 - acc: 0.8033 - val_loss: 598.7562 - val_acc: 0.6778\n",
            "Epoch 2278/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 598.6607 - acc: 0.7766 - val_loss: 598.7266 - val_acc: 0.6778\n",
            "Epoch 2279/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 598.6256 - acc: 0.7828 - val_loss: 598.6971 - val_acc: 0.6778\n",
            "Epoch 2280/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 598.5962 - acc: 0.7930 - val_loss: 598.6674 - val_acc: 0.6778\n",
            "Epoch 2281/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 598.5654 - acc: 0.7910 - val_loss: 598.6376 - val_acc: 0.6778\n",
            "Epoch 2282/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 598.5387 - acc: 0.7951 - val_loss: 598.6080 - val_acc: 0.6778\n",
            "Epoch 2283/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 598.5079 - acc: 0.7930 - val_loss: 598.5785 - val_acc: 0.6778\n",
            "Epoch 2284/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 598.4787 - acc: 0.7951 - val_loss: 598.5489 - val_acc: 0.6778\n",
            "Epoch 2285/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 598.4404 - acc: 0.7971 - val_loss: 598.5194 - val_acc: 0.6778\n",
            "Epoch 2286/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 598.4140 - acc: 0.8217 - val_loss: 598.4896 - val_acc: 0.6778\n",
            "Epoch 2287/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 598.3984 - acc: 0.7766 - val_loss: 598.4601 - val_acc: 0.6778\n",
            "Epoch 2288/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 598.3501 - acc: 0.8115 - val_loss: 598.4304 - val_acc: 0.6778\n",
            "Epoch 2289/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 598.3232 - acc: 0.7889 - val_loss: 598.4008 - val_acc: 0.6778\n",
            "Epoch 2290/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 598.2786 - acc: 0.8176 - val_loss: 598.3712 - val_acc: 0.6778\n",
            "Epoch 2291/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 598.2614 - acc: 0.7930 - val_loss: 598.3414 - val_acc: 0.6778\n",
            "Epoch 2292/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 598.2408 - acc: 0.7910 - val_loss: 598.3119 - val_acc: 0.6778\n",
            "Epoch 2293/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 598.2142 - acc: 0.7889 - val_loss: 598.2823 - val_acc: 0.6778\n",
            "Epoch 2294/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 598.1791 - acc: 0.7992 - val_loss: 598.2526 - val_acc: 0.6778\n",
            "Epoch 2295/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 598.1506 - acc: 0.7951 - val_loss: 598.2229 - val_acc: 0.6778\n",
            "Epoch 2296/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 598.1182 - acc: 0.7869 - val_loss: 598.1934 - val_acc: 0.6778\n",
            "Epoch 2297/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 598.1010 - acc: 0.7910 - val_loss: 598.1637 - val_acc: 0.6778\n",
            "Epoch 2298/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 598.0516 - acc: 0.8197 - val_loss: 598.1341 - val_acc: 0.6778\n",
            "Epoch 2299/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 598.0377 - acc: 0.7828 - val_loss: 598.1044 - val_acc: 0.6778\n",
            "Epoch 2300/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 598.0104 - acc: 0.7930 - val_loss: 598.0748 - val_acc: 0.6778\n",
            "Epoch 2301/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 597.9862 - acc: 0.7725 - val_loss: 598.0453 - val_acc: 0.6778\n",
            "Epoch 2302/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 597.9424 - acc: 0.8176 - val_loss: 598.0156 - val_acc: 0.6778\n",
            "Epoch 2303/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 597.9241 - acc: 0.7889 - val_loss: 597.9860 - val_acc: 0.6778\n",
            "Epoch 2304/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 597.8821 - acc: 0.7807 - val_loss: 597.9564 - val_acc: 0.6778\n",
            "Epoch 2305/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 597.8517 - acc: 0.8012 - val_loss: 597.9266 - val_acc: 0.6778\n",
            "Epoch 2306/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 597.8202 - acc: 0.8033 - val_loss: 597.8971 - val_acc: 0.6778\n",
            "Epoch 2307/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 597.7859 - acc: 0.7971 - val_loss: 597.8674 - val_acc: 0.6778\n",
            "Epoch 2308/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 597.7615 - acc: 0.8012 - val_loss: 597.8378 - val_acc: 0.6778\n",
            "Epoch 2309/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 597.7196 - acc: 0.8094 - val_loss: 597.8081 - val_acc: 0.6778\n",
            "Epoch 2310/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 597.6977 - acc: 0.8053 - val_loss: 597.7786 - val_acc: 0.6778\n",
            "Epoch 2311/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 597.6698 - acc: 0.8012 - val_loss: 597.7489 - val_acc: 0.6778\n",
            "Epoch 2312/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 597.6437 - acc: 0.7971 - val_loss: 597.7194 - val_acc: 0.6778\n",
            "Epoch 2313/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 597.6045 - acc: 0.7971 - val_loss: 597.6899 - val_acc: 0.6778\n",
            "Epoch 2314/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 597.5818 - acc: 0.7992 - val_loss: 597.6601 - val_acc: 0.6778\n",
            "Epoch 2315/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 597.5514 - acc: 0.8135 - val_loss: 597.6305 - val_acc: 0.6778\n",
            "Epoch 2316/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 597.5188 - acc: 0.8156 - val_loss: 597.6008 - val_acc: 0.6778\n",
            "Epoch 2317/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 597.5029 - acc: 0.7746 - val_loss: 597.5712 - val_acc: 0.6778\n",
            "Epoch 2318/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 597.4715 - acc: 0.7930 - val_loss: 597.5416 - val_acc: 0.6778\n",
            "Epoch 2319/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 597.4262 - acc: 0.7951 - val_loss: 597.5120 - val_acc: 0.6778\n",
            "Epoch 2320/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 597.4041 - acc: 0.8033 - val_loss: 597.4824 - val_acc: 0.6778\n",
            "Epoch 2321/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 597.3669 - acc: 0.8094 - val_loss: 597.4528 - val_acc: 0.6778\n",
            "Epoch 2322/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 597.3533 - acc: 0.8135 - val_loss: 597.4232 - val_acc: 0.6778\n",
            "Epoch 2323/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 597.3171 - acc: 0.8156 - val_loss: 597.3935 - val_acc: 0.6778\n",
            "Epoch 2324/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 597.2849 - acc: 0.8033 - val_loss: 597.3640 - val_acc: 0.6778\n",
            "Epoch 2325/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 597.2608 - acc: 0.8012 - val_loss: 597.3345 - val_acc: 0.6778\n",
            "Epoch 2326/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 597.2262 - acc: 0.8176 - val_loss: 597.3049 - val_acc: 0.6778\n",
            "Epoch 2327/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 597.1933 - acc: 0.8053 - val_loss: 597.2753 - val_acc: 0.6778\n",
            "Epoch 2328/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 597.1716 - acc: 0.8012 - val_loss: 597.2456 - val_acc: 0.6778\n",
            "Epoch 2329/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 597.1313 - acc: 0.8176 - val_loss: 597.2160 - val_acc: 0.6778\n",
            "Epoch 2330/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 597.1033 - acc: 0.8033 - val_loss: 597.1864 - val_acc: 0.6778\n",
            "Epoch 2331/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 597.0718 - acc: 0.8053 - val_loss: 597.1568 - val_acc: 0.6778\n",
            "Epoch 2332/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 597.0383 - acc: 0.8176 - val_loss: 597.1273 - val_acc: 0.6778\n",
            "Epoch 2333/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 597.0112 - acc: 0.8258 - val_loss: 597.0976 - val_acc: 0.6778\n",
            "Epoch 2334/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 596.9900 - acc: 0.8012 - val_loss: 597.0680 - val_acc: 0.6778\n",
            "Epoch 2335/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 596.9643 - acc: 0.7992 - val_loss: 597.0384 - val_acc: 0.6778\n",
            "Epoch 2336/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 596.9383 - acc: 0.7951 - val_loss: 597.0088 - val_acc: 0.6778\n",
            "Epoch 2337/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 596.8930 - acc: 0.8238 - val_loss: 596.9792 - val_acc: 0.6778\n",
            "Epoch 2338/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 596.8679 - acc: 0.8135 - val_loss: 596.9496 - val_acc: 0.6778\n",
            "Epoch 2339/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 596.8369 - acc: 0.8115 - val_loss: 596.9200 - val_acc: 0.6778\n",
            "Epoch 2340/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 596.8101 - acc: 0.7910 - val_loss: 596.8904 - val_acc: 0.6778\n",
            "Epoch 2341/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 596.7775 - acc: 0.8094 - val_loss: 596.8609 - val_acc: 0.6778\n",
            "Epoch 2342/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 596.7533 - acc: 0.8115 - val_loss: 596.8312 - val_acc: 0.6778\n",
            "Epoch 2343/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 596.7308 - acc: 0.7848 - val_loss: 596.8017 - val_acc: 0.6778\n",
            "Epoch 2344/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 596.6990 - acc: 0.7971 - val_loss: 596.7720 - val_acc: 0.6778\n",
            "Epoch 2345/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 596.6601 - acc: 0.8012 - val_loss: 596.7426 - val_acc: 0.6778\n",
            "Epoch 2346/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 596.6276 - acc: 0.8135 - val_loss: 596.7131 - val_acc: 0.6778\n",
            "Epoch 2347/3000\n",
            "488/488 [==============================] - 0s 385us/step - loss: 596.6029 - acc: 0.8115 - val_loss: 596.6833 - val_acc: 0.6778\n",
            "Epoch 2348/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 596.5796 - acc: 0.8053 - val_loss: 596.6537 - val_acc: 0.6778\n",
            "Epoch 2349/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 596.5479 - acc: 0.7807 - val_loss: 596.6241 - val_acc: 0.6778\n",
            "Epoch 2350/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 596.5166 - acc: 0.7971 - val_loss: 596.5945 - val_acc: 0.6778\n",
            "Epoch 2351/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 596.4863 - acc: 0.7971 - val_loss: 596.5649 - val_acc: 0.6778\n",
            "Epoch 2352/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 596.4627 - acc: 0.7664 - val_loss: 596.5354 - val_acc: 0.6778\n",
            "Epoch 2353/3000\n",
            "488/488 [==============================] - 0s 381us/step - loss: 596.4223 - acc: 0.8074 - val_loss: 596.5057 - val_acc: 0.6778\n",
            "Epoch 2354/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 596.3946 - acc: 0.8074 - val_loss: 596.4762 - val_acc: 0.6778\n",
            "Epoch 2355/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 596.3647 - acc: 0.8012 - val_loss: 596.4466 - val_acc: 0.6778\n",
            "Epoch 2356/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 596.3355 - acc: 0.8053 - val_loss: 596.4170 - val_acc: 0.6778\n",
            "Epoch 2357/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 596.3095 - acc: 0.8176 - val_loss: 596.3875 - val_acc: 0.6778\n",
            "Epoch 2358/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 596.2795 - acc: 0.7869 - val_loss: 596.3579 - val_acc: 0.6778\n",
            "Epoch 2359/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 596.2409 - acc: 0.8197 - val_loss: 596.3284 - val_acc: 0.6778\n",
            "Epoch 2360/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 596.2216 - acc: 0.8033 - val_loss: 596.2989 - val_acc: 0.6778\n",
            "Epoch 2361/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 596.1899 - acc: 0.7910 - val_loss: 596.2693 - val_acc: 0.6778\n",
            "Epoch 2362/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 596.1708 - acc: 0.7807 - val_loss: 596.2397 - val_acc: 0.6799\n",
            "Epoch 2363/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 596.1366 - acc: 0.7725 - val_loss: 596.2101 - val_acc: 0.6799\n",
            "Epoch 2364/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 596.0917 - acc: 0.8279 - val_loss: 596.1804 - val_acc: 0.6799\n",
            "Epoch 2365/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 596.0714 - acc: 0.8074 - val_loss: 596.1509 - val_acc: 0.6799\n",
            "Epoch 2366/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 596.0399 - acc: 0.8135 - val_loss: 596.1213 - val_acc: 0.6799\n",
            "Epoch 2367/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 596.0132 - acc: 0.8135 - val_loss: 596.0917 - val_acc: 0.6799\n",
            "Epoch 2368/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 595.9845 - acc: 0.7930 - val_loss: 596.0620 - val_acc: 0.6799\n",
            "Epoch 2369/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 595.9472 - acc: 0.7889 - val_loss: 596.0325 - val_acc: 0.6799\n",
            "Epoch 2370/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 595.9268 - acc: 0.8012 - val_loss: 596.0028 - val_acc: 0.6799\n",
            "Epoch 2371/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 595.8876 - acc: 0.7992 - val_loss: 595.9734 - val_acc: 0.6799\n",
            "Epoch 2372/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 595.8680 - acc: 0.7828 - val_loss: 595.9438 - val_acc: 0.6799\n",
            "Epoch 2373/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 595.8453 - acc: 0.7971 - val_loss: 595.9142 - val_acc: 0.6799\n",
            "Epoch 2374/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 595.8086 - acc: 0.7807 - val_loss: 595.8847 - val_acc: 0.6799\n",
            "Epoch 2375/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 595.7724 - acc: 0.8094 - val_loss: 595.8550 - val_acc: 0.6799\n",
            "Epoch 2376/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 595.7559 - acc: 0.7951 - val_loss: 595.8255 - val_acc: 0.6799\n",
            "Epoch 2377/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 595.7184 - acc: 0.8115 - val_loss: 595.7958 - val_acc: 0.6799\n",
            "Epoch 2378/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 595.6755 - acc: 0.8279 - val_loss: 595.7662 - val_acc: 0.6799\n",
            "Epoch 2379/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 595.6584 - acc: 0.8197 - val_loss: 595.7368 - val_acc: 0.6799\n",
            "Epoch 2380/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 595.6210 - acc: 0.8115 - val_loss: 595.7073 - val_acc: 0.6799\n",
            "Epoch 2381/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 595.6031 - acc: 0.7951 - val_loss: 595.6777 - val_acc: 0.6799\n",
            "Epoch 2382/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 595.5626 - acc: 0.7951 - val_loss: 595.6481 - val_acc: 0.6799\n",
            "Epoch 2383/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 595.5476 - acc: 0.8053 - val_loss: 595.6187 - val_acc: 0.6799\n",
            "Epoch 2384/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 595.5031 - acc: 0.8053 - val_loss: 595.5891 - val_acc: 0.6799\n",
            "Epoch 2385/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 595.4890 - acc: 0.7828 - val_loss: 595.5595 - val_acc: 0.6799\n",
            "Epoch 2386/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 595.4583 - acc: 0.7992 - val_loss: 595.5298 - val_acc: 0.6799\n",
            "Epoch 2387/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 595.4193 - acc: 0.7992 - val_loss: 595.5004 - val_acc: 0.6820\n",
            "Epoch 2388/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 595.3987 - acc: 0.7930 - val_loss: 595.4708 - val_acc: 0.6799\n",
            "Epoch 2389/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 595.3642 - acc: 0.7971 - val_loss: 595.4413 - val_acc: 0.6820\n",
            "Epoch 2390/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 595.3313 - acc: 0.8053 - val_loss: 595.4116 - val_acc: 0.6820\n",
            "Epoch 2391/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 595.2993 - acc: 0.8094 - val_loss: 595.3820 - val_acc: 0.6820\n",
            "Epoch 2392/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 595.2723 - acc: 0.8033 - val_loss: 595.3524 - val_acc: 0.6799\n",
            "Epoch 2393/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 595.2440 - acc: 0.8074 - val_loss: 595.3230 - val_acc: 0.6799\n",
            "Epoch 2394/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 595.2184 - acc: 0.8238 - val_loss: 595.2935 - val_acc: 0.6799\n",
            "Epoch 2395/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 595.1772 - acc: 0.7971 - val_loss: 595.2640 - val_acc: 0.6799\n",
            "Epoch 2396/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 595.1508 - acc: 0.8053 - val_loss: 595.2346 - val_acc: 0.6799\n",
            "Epoch 2397/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 595.1242 - acc: 0.7992 - val_loss: 595.2050 - val_acc: 0.6799\n",
            "Epoch 2398/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 595.0983 - acc: 0.7725 - val_loss: 595.1755 - val_acc: 0.6799\n",
            "Epoch 2399/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 595.0649 - acc: 0.8094 - val_loss: 595.1460 - val_acc: 0.6799\n",
            "Epoch 2400/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 595.0261 - acc: 0.8238 - val_loss: 595.1164 - val_acc: 0.6799\n",
            "Epoch 2401/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 595.0083 - acc: 0.7910 - val_loss: 595.0868 - val_acc: 0.6799\n",
            "Epoch 2402/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 594.9828 - acc: 0.7951 - val_loss: 595.0574 - val_acc: 0.6799\n",
            "Epoch 2403/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 594.9473 - acc: 0.8053 - val_loss: 595.0278 - val_acc: 0.6799\n",
            "Epoch 2404/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 594.9286 - acc: 0.7971 - val_loss: 594.9984 - val_acc: 0.6778\n",
            "Epoch 2405/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 594.8851 - acc: 0.8238 - val_loss: 594.9687 - val_acc: 0.6778\n",
            "Epoch 2406/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 594.8592 - acc: 0.7992 - val_loss: 594.9393 - val_acc: 0.6778\n",
            "Epoch 2407/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 594.8405 - acc: 0.7992 - val_loss: 594.9096 - val_acc: 0.6778\n",
            "Epoch 2408/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 594.8013 - acc: 0.8176 - val_loss: 594.8801 - val_acc: 0.6778\n",
            "Epoch 2409/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 594.7704 - acc: 0.8074 - val_loss: 594.8506 - val_acc: 0.6778\n",
            "Epoch 2410/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 594.7414 - acc: 0.8094 - val_loss: 594.8209 - val_acc: 0.6778\n",
            "Epoch 2411/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 594.7135 - acc: 0.7889 - val_loss: 594.7914 - val_acc: 0.6778\n",
            "Epoch 2412/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 594.6780 - acc: 0.8361 - val_loss: 594.7618 - val_acc: 0.6778\n",
            "Epoch 2413/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 594.6580 - acc: 0.8012 - val_loss: 594.7323 - val_acc: 0.6799\n",
            "Epoch 2414/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 594.6173 - acc: 0.8381 - val_loss: 594.7029 - val_acc: 0.6799\n",
            "Epoch 2415/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 594.5982 - acc: 0.7889 - val_loss: 594.6733 - val_acc: 0.6799\n",
            "Epoch 2416/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 594.5697 - acc: 0.7951 - val_loss: 594.6439 - val_acc: 0.6778\n",
            "Epoch 2417/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 594.5353 - acc: 0.7807 - val_loss: 594.6144 - val_acc: 0.6799\n",
            "Epoch 2418/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 594.5140 - acc: 0.7992 - val_loss: 594.5849 - val_acc: 0.6799\n",
            "Epoch 2419/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 594.4605 - acc: 0.8258 - val_loss: 594.5553 - val_acc: 0.6799\n",
            "Epoch 2420/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 594.4511 - acc: 0.7910 - val_loss: 594.5257 - val_acc: 0.6799\n",
            "Epoch 2421/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 594.4064 - acc: 0.8012 - val_loss: 594.4961 - val_acc: 0.6799\n",
            "Epoch 2422/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 594.3898 - acc: 0.7848 - val_loss: 594.4667 - val_acc: 0.6799\n",
            "Epoch 2423/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 594.3636 - acc: 0.8053 - val_loss: 594.4373 - val_acc: 0.6778\n",
            "Epoch 2424/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 594.3267 - acc: 0.8033 - val_loss: 594.4078 - val_acc: 0.6799\n",
            "Epoch 2425/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 594.3017 - acc: 0.7910 - val_loss: 594.3784 - val_acc: 0.6799\n",
            "Epoch 2426/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 594.2678 - acc: 0.7889 - val_loss: 594.3487 - val_acc: 0.6799\n",
            "Epoch 2427/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 594.2367 - acc: 0.8053 - val_loss: 594.3193 - val_acc: 0.6799\n",
            "Epoch 2428/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 594.2212 - acc: 0.7807 - val_loss: 594.2897 - val_acc: 0.6799\n",
            "Epoch 2429/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 594.1908 - acc: 0.7828 - val_loss: 594.2602 - val_acc: 0.6799\n",
            "Epoch 2430/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 594.1486 - acc: 0.7889 - val_loss: 594.2308 - val_acc: 0.6799\n",
            "Epoch 2431/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 594.1147 - acc: 0.8217 - val_loss: 594.2013 - val_acc: 0.6799\n",
            "Epoch 2432/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 594.0904 - acc: 0.8074 - val_loss: 594.1717 - val_acc: 0.6799\n",
            "Epoch 2433/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 594.0687 - acc: 0.8012 - val_loss: 594.1422 - val_acc: 0.6799\n",
            "Epoch 2434/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 594.0318 - acc: 0.7992 - val_loss: 594.1127 - val_acc: 0.6820\n",
            "Epoch 2435/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 594.0044 - acc: 0.7971 - val_loss: 594.0832 - val_acc: 0.6799\n",
            "Epoch 2436/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 593.9688 - acc: 0.8053 - val_loss: 594.0536 - val_acc: 0.6799\n",
            "Epoch 2437/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 593.9400 - acc: 0.8053 - val_loss: 594.0241 - val_acc: 0.6820\n",
            "Epoch 2438/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 593.9183 - acc: 0.8033 - val_loss: 593.9946 - val_acc: 0.6820\n",
            "Epoch 2439/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 593.8922 - acc: 0.8156 - val_loss: 593.9651 - val_acc: 0.6820\n",
            "Epoch 2440/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 593.8440 - acc: 0.8115 - val_loss: 593.9355 - val_acc: 0.6820\n",
            "Epoch 2441/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 593.8242 - acc: 0.8074 - val_loss: 593.9061 - val_acc: 0.6820\n",
            "Epoch 2442/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 593.7934 - acc: 0.8094 - val_loss: 593.8767 - val_acc: 0.6799\n",
            "Epoch 2443/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 593.7664 - acc: 0.8279 - val_loss: 593.8472 - val_acc: 0.6799\n",
            "Epoch 2444/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 593.7284 - acc: 0.8176 - val_loss: 593.8178 - val_acc: 0.6799\n",
            "Epoch 2445/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 593.7102 - acc: 0.7869 - val_loss: 593.7883 - val_acc: 0.6799\n",
            "Epoch 2446/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 593.6786 - acc: 0.8012 - val_loss: 593.7587 - val_acc: 0.6799\n",
            "Epoch 2447/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 593.6501 - acc: 0.8156 - val_loss: 593.7292 - val_acc: 0.6799\n",
            "Epoch 2448/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 593.6194 - acc: 0.8094 - val_loss: 593.6997 - val_acc: 0.6799\n",
            "Epoch 2449/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 593.5845 - acc: 0.7930 - val_loss: 593.6702 - val_acc: 0.6799\n",
            "Epoch 2450/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 593.5591 - acc: 0.8033 - val_loss: 593.6407 - val_acc: 0.6799\n",
            "Epoch 2451/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 593.5265 - acc: 0.8033 - val_loss: 593.6113 - val_acc: 0.6799\n",
            "Epoch 2452/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 593.5032 - acc: 0.8053 - val_loss: 593.5817 - val_acc: 0.6799\n",
            "Epoch 2453/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 593.4719 - acc: 0.8033 - val_loss: 593.5523 - val_acc: 0.6799\n",
            "Epoch 2454/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 593.4423 - acc: 0.7992 - val_loss: 593.5228 - val_acc: 0.6799\n",
            "Epoch 2455/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 593.4135 - acc: 0.8217 - val_loss: 593.4932 - val_acc: 0.6799\n",
            "Epoch 2456/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 593.3817 - acc: 0.8115 - val_loss: 593.4638 - val_acc: 0.6799\n",
            "Epoch 2457/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 593.3528 - acc: 0.8012 - val_loss: 593.4344 - val_acc: 0.6799\n",
            "Epoch 2458/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 593.3349 - acc: 0.7807 - val_loss: 593.4051 - val_acc: 0.6799\n",
            "Epoch 2459/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 593.2937 - acc: 0.8135 - val_loss: 593.3756 - val_acc: 0.6799\n",
            "Epoch 2460/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 593.2661 - acc: 0.7951 - val_loss: 593.3458 - val_acc: 0.6799\n",
            "Epoch 2461/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 593.2387 - acc: 0.7951 - val_loss: 593.3165 - val_acc: 0.6799\n",
            "Epoch 2462/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 593.2114 - acc: 0.7930 - val_loss: 593.2870 - val_acc: 0.6799\n",
            "Epoch 2463/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 593.1671 - acc: 0.8135 - val_loss: 593.2575 - val_acc: 0.6799\n",
            "Epoch 2464/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 593.1574 - acc: 0.7848 - val_loss: 593.2282 - val_acc: 0.6799\n",
            "Epoch 2465/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 593.1235 - acc: 0.7807 - val_loss: 593.1985 - val_acc: 0.6799\n",
            "Epoch 2466/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 593.0896 - acc: 0.7930 - val_loss: 593.1691 - val_acc: 0.6799\n",
            "Epoch 2467/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 593.0625 - acc: 0.7807 - val_loss: 593.1397 - val_acc: 0.6820\n",
            "Epoch 2468/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 593.0326 - acc: 0.8115 - val_loss: 593.1102 - val_acc: 0.6799\n",
            "Epoch 2469/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 593.0027 - acc: 0.7930 - val_loss: 593.0807 - val_acc: 0.6820\n",
            "Epoch 2470/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 592.9707 - acc: 0.8053 - val_loss: 593.0512 - val_acc: 0.6820\n",
            "Epoch 2471/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 592.9460 - acc: 0.8012 - val_loss: 593.0219 - val_acc: 0.6841\n",
            "Epoch 2472/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 592.9172 - acc: 0.7848 - val_loss: 592.9923 - val_acc: 0.6820\n",
            "Epoch 2473/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 592.8834 - acc: 0.8012 - val_loss: 592.9629 - val_acc: 0.6799\n",
            "Epoch 2474/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 592.8515 - acc: 0.8074 - val_loss: 592.9333 - val_acc: 0.6820\n",
            "Epoch 2475/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 592.8219 - acc: 0.8033 - val_loss: 592.9038 - val_acc: 0.6799\n",
            "Epoch 2476/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 592.7900 - acc: 0.8012 - val_loss: 592.8743 - val_acc: 0.6820\n",
            "Epoch 2477/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 592.7616 - acc: 0.7930 - val_loss: 592.8449 - val_acc: 0.6820\n",
            "Epoch 2478/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 592.7355 - acc: 0.7705 - val_loss: 592.8155 - val_acc: 0.6820\n",
            "Epoch 2479/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 592.6959 - acc: 0.8094 - val_loss: 592.7859 - val_acc: 0.6820\n",
            "Epoch 2480/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 592.6814 - acc: 0.7910 - val_loss: 592.7565 - val_acc: 0.6820\n",
            "Epoch 2481/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 592.6525 - acc: 0.8115 - val_loss: 592.7271 - val_acc: 0.6799\n",
            "Epoch 2482/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 592.6177 - acc: 0.7930 - val_loss: 592.6974 - val_acc: 0.6799\n",
            "Epoch 2483/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 592.5757 - acc: 0.8279 - val_loss: 592.6680 - val_acc: 0.6799\n",
            "Epoch 2484/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 592.5603 - acc: 0.7971 - val_loss: 592.6385 - val_acc: 0.6799\n",
            "Epoch 2485/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 592.5187 - acc: 0.8156 - val_loss: 592.6091 - val_acc: 0.6799\n",
            "Epoch 2486/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 592.5077 - acc: 0.7951 - val_loss: 592.5796 - val_acc: 0.6799\n",
            "Epoch 2487/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 592.4663 - acc: 0.8176 - val_loss: 592.5501 - val_acc: 0.6820\n",
            "Epoch 2488/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 592.4440 - acc: 0.7951 - val_loss: 592.5207 - val_acc: 0.6820\n",
            "Epoch 2489/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 592.4110 - acc: 0.8094 - val_loss: 592.4911 - val_acc: 0.6820\n",
            "Epoch 2490/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 592.3770 - acc: 0.7992 - val_loss: 592.4617 - val_acc: 0.6820\n",
            "Epoch 2491/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 592.3511 - acc: 0.8258 - val_loss: 592.4323 - val_acc: 0.6841\n",
            "Epoch 2492/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 592.3222 - acc: 0.8279 - val_loss: 592.4030 - val_acc: 0.6820\n",
            "Epoch 2493/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 592.2929 - acc: 0.7992 - val_loss: 592.3734 - val_acc: 0.6820\n",
            "Epoch 2494/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 592.2675 - acc: 0.7951 - val_loss: 592.3440 - val_acc: 0.6820\n",
            "Epoch 2495/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 592.2339 - acc: 0.8238 - val_loss: 592.3145 - val_acc: 0.6820\n",
            "Epoch 2496/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 592.2091 - acc: 0.7848 - val_loss: 592.2851 - val_acc: 0.6820\n",
            "Epoch 2497/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 592.1701 - acc: 0.7869 - val_loss: 592.2556 - val_acc: 0.6820\n",
            "Epoch 2498/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 592.1461 - acc: 0.8135 - val_loss: 592.2262 - val_acc: 0.6820\n",
            "Epoch 2499/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 592.1062 - acc: 0.8258 - val_loss: 592.1967 - val_acc: 0.6820\n",
            "Epoch 2500/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 592.0909 - acc: 0.8053 - val_loss: 592.1673 - val_acc: 0.6820\n",
            "Epoch 2501/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 592.0547 - acc: 0.8176 - val_loss: 592.1377 - val_acc: 0.6820\n",
            "Epoch 2502/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 592.0237 - acc: 0.7910 - val_loss: 592.1084 - val_acc: 0.6820\n",
            "Epoch 2503/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 591.9984 - acc: 0.7910 - val_loss: 592.0789 - val_acc: 0.6820\n",
            "Epoch 2504/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 591.9739 - acc: 0.7869 - val_loss: 592.0495 - val_acc: 0.6820\n",
            "Epoch 2505/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 591.9369 - acc: 0.8053 - val_loss: 592.0201 - val_acc: 0.6820\n",
            "Epoch 2506/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 591.8974 - acc: 0.8320 - val_loss: 591.9907 - val_acc: 0.6820\n",
            "Epoch 2507/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 591.8799 - acc: 0.8012 - val_loss: 591.9613 - val_acc: 0.6820\n",
            "Epoch 2508/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 591.8411 - acc: 0.8053 - val_loss: 591.9318 - val_acc: 0.6820\n",
            "Epoch 2509/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 591.8204 - acc: 0.8033 - val_loss: 591.9023 - val_acc: 0.6820\n",
            "Epoch 2510/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 591.7912 - acc: 0.8074 - val_loss: 591.8730 - val_acc: 0.6820\n",
            "Epoch 2511/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 591.7616 - acc: 0.7951 - val_loss: 591.8435 - val_acc: 0.6820\n",
            "Epoch 2512/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 591.7312 - acc: 0.8074 - val_loss: 591.8141 - val_acc: 0.6820\n",
            "Epoch 2513/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 591.7030 - acc: 0.8217 - val_loss: 591.7846 - val_acc: 0.6820\n",
            "Epoch 2514/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 591.6726 - acc: 0.7951 - val_loss: 591.7552 - val_acc: 0.6820\n",
            "Epoch 2515/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 591.6325 - acc: 0.8115 - val_loss: 591.7257 - val_acc: 0.6820\n",
            "Epoch 2516/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 591.6160 - acc: 0.7971 - val_loss: 591.6962 - val_acc: 0.6820\n",
            "Epoch 2517/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 591.5793 - acc: 0.8135 - val_loss: 591.6667 - val_acc: 0.6841\n",
            "Epoch 2518/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 591.5591 - acc: 0.8053 - val_loss: 591.6374 - val_acc: 0.6820\n",
            "Epoch 2519/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 591.5219 - acc: 0.8197 - val_loss: 591.6081 - val_acc: 0.6841\n",
            "Epoch 2520/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 591.4933 - acc: 0.8361 - val_loss: 591.5786 - val_acc: 0.6841\n",
            "Epoch 2521/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 591.4691 - acc: 0.8033 - val_loss: 591.5492 - val_acc: 0.6820\n",
            "Epoch 2522/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 591.4366 - acc: 0.7910 - val_loss: 591.5197 - val_acc: 0.6841\n",
            "Epoch 2523/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 591.4003 - acc: 0.8217 - val_loss: 591.4904 - val_acc: 0.6841\n",
            "Epoch 2524/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 591.3677 - acc: 0.8279 - val_loss: 591.4609 - val_acc: 0.6841\n",
            "Epoch 2525/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 591.3423 - acc: 0.8279 - val_loss: 591.4315 - val_acc: 0.6841\n",
            "Epoch 2526/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 591.3185 - acc: 0.7889 - val_loss: 591.4021 - val_acc: 0.6820\n",
            "Epoch 2527/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 591.3012 - acc: 0.8053 - val_loss: 591.3726 - val_acc: 0.6820\n",
            "Epoch 2528/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 591.2580 - acc: 0.8012 - val_loss: 591.3431 - val_acc: 0.6820\n",
            "Epoch 2529/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 591.2231 - acc: 0.8135 - val_loss: 591.3138 - val_acc: 0.6820\n",
            "Epoch 2530/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 591.1968 - acc: 0.8115 - val_loss: 591.2843 - val_acc: 0.6820\n",
            "Epoch 2531/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 591.1719 - acc: 0.7971 - val_loss: 591.2548 - val_acc: 0.6841\n",
            "Epoch 2532/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 591.1547 - acc: 0.7787 - val_loss: 591.2254 - val_acc: 0.6820\n",
            "Epoch 2533/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 591.1169 - acc: 0.7992 - val_loss: 591.1960 - val_acc: 0.6841\n",
            "Epoch 2534/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 591.0849 - acc: 0.8074 - val_loss: 591.1665 - val_acc: 0.6820\n",
            "Epoch 2535/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 591.0494 - acc: 0.8094 - val_loss: 591.1371 - val_acc: 0.6820\n",
            "Epoch 2536/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 591.0358 - acc: 0.7725 - val_loss: 591.1077 - val_acc: 0.6820\n",
            "Epoch 2537/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 591.0026 - acc: 0.7992 - val_loss: 591.0782 - val_acc: 0.6820\n",
            "Epoch 2538/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 590.9661 - acc: 0.8115 - val_loss: 591.0488 - val_acc: 0.6841\n",
            "Epoch 2539/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 590.9383 - acc: 0.8156 - val_loss: 591.0195 - val_acc: 0.6841\n",
            "Epoch 2540/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 590.9179 - acc: 0.7971 - val_loss: 590.9901 - val_acc: 0.6841\n",
            "Epoch 2541/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 590.8878 - acc: 0.7951 - val_loss: 590.9607 - val_acc: 0.6841\n",
            "Epoch 2542/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 590.8436 - acc: 0.8094 - val_loss: 590.9311 - val_acc: 0.6841\n",
            "Epoch 2543/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 590.8064 - acc: 0.8258 - val_loss: 590.9018 - val_acc: 0.6841\n",
            "Epoch 2544/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 590.7868 - acc: 0.8176 - val_loss: 590.8724 - val_acc: 0.6820\n",
            "Epoch 2545/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 590.7590 - acc: 0.7971 - val_loss: 590.8432 - val_acc: 0.6820\n",
            "Epoch 2546/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 590.7342 - acc: 0.7869 - val_loss: 590.8137 - val_acc: 0.6841\n",
            "Epoch 2547/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 590.7042 - acc: 0.8053 - val_loss: 590.7844 - val_acc: 0.6841\n",
            "Epoch 2548/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 590.6778 - acc: 0.8033 - val_loss: 590.7551 - val_acc: 0.6841\n",
            "Epoch 2549/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 590.6393 - acc: 0.7951 - val_loss: 590.7258 - val_acc: 0.6862\n",
            "Epoch 2550/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 590.6088 - acc: 0.8197 - val_loss: 590.6964 - val_acc: 0.6862\n",
            "Epoch 2551/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 590.5839 - acc: 0.8156 - val_loss: 590.6670 - val_acc: 0.6862\n",
            "Epoch 2552/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 590.5510 - acc: 0.8033 - val_loss: 590.6375 - val_acc: 0.6862\n",
            "Epoch 2553/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 590.5177 - acc: 0.8197 - val_loss: 590.6082 - val_acc: 0.6862\n",
            "Epoch 2554/3000\n",
            "488/488 [==============================] - 0s 359us/step - loss: 590.4940 - acc: 0.7951 - val_loss: 590.5788 - val_acc: 0.6862\n",
            "Epoch 2555/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 590.4619 - acc: 0.8094 - val_loss: 590.5492 - val_acc: 0.6862\n",
            "Epoch 2556/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 590.4292 - acc: 0.8238 - val_loss: 590.5198 - val_acc: 0.6862\n",
            "Epoch 2557/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 590.3992 - acc: 0.8012 - val_loss: 590.4905 - val_acc: 0.6862\n",
            "Epoch 2558/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 590.3820 - acc: 0.8012 - val_loss: 590.4612 - val_acc: 0.6862\n",
            "Epoch 2559/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 590.3497 - acc: 0.8033 - val_loss: 590.4316 - val_acc: 0.6862\n",
            "Epoch 2560/3000\n",
            "488/488 [==============================] - 0s 376us/step - loss: 590.3188 - acc: 0.8094 - val_loss: 590.4023 - val_acc: 0.6862\n",
            "Epoch 2561/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 590.2906 - acc: 0.8258 - val_loss: 590.3729 - val_acc: 0.6862\n",
            "Epoch 2562/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 590.2603 - acc: 0.8299 - val_loss: 590.3437 - val_acc: 0.6862\n",
            "Epoch 2563/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 590.2291 - acc: 0.8176 - val_loss: 590.3142 - val_acc: 0.6862\n",
            "Epoch 2564/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 590.1953 - acc: 0.8135 - val_loss: 590.2848 - val_acc: 0.6883\n",
            "Epoch 2565/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 590.1708 - acc: 0.8094 - val_loss: 590.2555 - val_acc: 0.6862\n",
            "Epoch 2566/3000\n",
            "488/488 [==============================] - 0s 368us/step - loss: 590.1472 - acc: 0.7869 - val_loss: 590.2262 - val_acc: 0.6862\n",
            "Epoch 2567/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 590.1115 - acc: 0.7971 - val_loss: 590.1967 - val_acc: 0.6862\n",
            "Epoch 2568/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 590.0787 - acc: 0.8074 - val_loss: 590.1671 - val_acc: 0.6883\n",
            "Epoch 2569/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 590.0576 - acc: 0.8033 - val_loss: 590.1378 - val_acc: 0.6883\n",
            "Epoch 2570/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 590.0256 - acc: 0.7971 - val_loss: 590.1083 - val_acc: 0.6883\n",
            "Epoch 2571/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 589.9881 - acc: 0.8156 - val_loss: 590.0790 - val_acc: 0.6883\n",
            "Epoch 2572/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 589.9520 - acc: 0.8156 - val_loss: 590.0497 - val_acc: 0.6883\n",
            "Epoch 2573/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 589.9356 - acc: 0.8074 - val_loss: 590.0204 - val_acc: 0.6883\n",
            "Epoch 2574/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 589.9033 - acc: 0.8197 - val_loss: 589.9910 - val_acc: 0.6862\n",
            "Epoch 2575/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 589.8819 - acc: 0.8012 - val_loss: 589.9616 - val_acc: 0.6862\n",
            "Epoch 2576/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 589.8475 - acc: 0.8115 - val_loss: 589.9323 - val_acc: 0.6862\n",
            "Epoch 2577/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 589.8098 - acc: 0.8074 - val_loss: 589.9030 - val_acc: 0.6883\n",
            "Epoch 2578/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 589.7954 - acc: 0.8094 - val_loss: 589.8736 - val_acc: 0.6883\n",
            "Epoch 2579/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 589.7565 - acc: 0.8197 - val_loss: 589.8442 - val_acc: 0.6862\n",
            "Epoch 2580/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 589.7292 - acc: 0.8176 - val_loss: 589.8149 - val_acc: 0.6862\n",
            "Epoch 2581/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 589.6970 - acc: 0.8115 - val_loss: 589.7855 - val_acc: 0.6862\n",
            "Epoch 2582/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 589.6589 - acc: 0.8299 - val_loss: 589.7560 - val_acc: 0.6862\n",
            "Epoch 2583/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 589.6377 - acc: 0.8238 - val_loss: 589.7266 - val_acc: 0.6883\n",
            "Epoch 2584/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 589.6093 - acc: 0.8012 - val_loss: 589.6972 - val_acc: 0.6862\n",
            "Epoch 2585/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 589.5921 - acc: 0.8033 - val_loss: 589.6678 - val_acc: 0.6862\n",
            "Epoch 2586/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 589.5561 - acc: 0.7951 - val_loss: 589.6386 - val_acc: 0.6883\n",
            "Epoch 2587/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 589.5211 - acc: 0.8197 - val_loss: 589.6093 - val_acc: 0.6862\n",
            "Epoch 2588/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 589.4922 - acc: 0.8053 - val_loss: 589.5800 - val_acc: 0.6883\n",
            "Epoch 2589/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 589.4692 - acc: 0.8156 - val_loss: 589.5506 - val_acc: 0.6883\n",
            "Epoch 2590/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 589.4407 - acc: 0.7889 - val_loss: 589.5214 - val_acc: 0.6883\n",
            "Epoch 2591/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 589.3988 - acc: 0.8217 - val_loss: 589.4919 - val_acc: 0.6883\n",
            "Epoch 2592/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 589.3697 - acc: 0.8402 - val_loss: 589.4627 - val_acc: 0.6883\n",
            "Epoch 2593/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 589.3558 - acc: 0.7889 - val_loss: 589.4332 - val_acc: 0.6883\n",
            "Epoch 2594/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 589.3224 - acc: 0.7971 - val_loss: 589.4040 - val_acc: 0.6883\n",
            "Epoch 2595/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 589.2913 - acc: 0.8033 - val_loss: 589.3746 - val_acc: 0.6883\n",
            "Epoch 2596/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 589.2521 - acc: 0.8094 - val_loss: 589.3453 - val_acc: 0.6883\n",
            "Epoch 2597/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 589.2256 - acc: 0.8135 - val_loss: 589.3157 - val_acc: 0.6883\n",
            "Epoch 2598/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 589.2023 - acc: 0.8012 - val_loss: 589.2864 - val_acc: 0.6883\n",
            "Epoch 2599/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 589.1717 - acc: 0.8258 - val_loss: 589.2570 - val_acc: 0.6904\n",
            "Epoch 2600/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 589.1482 - acc: 0.7992 - val_loss: 589.2277 - val_acc: 0.6904\n",
            "Epoch 2601/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 589.1072 - acc: 0.8217 - val_loss: 589.1983 - val_acc: 0.6904\n",
            "Epoch 2602/3000\n",
            "488/488 [==============================] - 0s 372us/step - loss: 589.0865 - acc: 0.8135 - val_loss: 589.1690 - val_acc: 0.6904\n",
            "Epoch 2603/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 589.0520 - acc: 0.8279 - val_loss: 589.1397 - val_acc: 0.6904\n",
            "Epoch 2604/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 589.0225 - acc: 0.8115 - val_loss: 589.1105 - val_acc: 0.6904\n",
            "Epoch 2605/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 588.9901 - acc: 0.8033 - val_loss: 589.0811 - val_acc: 0.6904\n",
            "Epoch 2606/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 588.9687 - acc: 0.8074 - val_loss: 589.0518 - val_acc: 0.6904\n",
            "Epoch 2607/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 588.9344 - acc: 0.8258 - val_loss: 589.0224 - val_acc: 0.6904\n",
            "Epoch 2608/3000\n",
            "488/488 [==============================] - 0s 364us/step - loss: 588.9093 - acc: 0.8074 - val_loss: 588.9930 - val_acc: 0.6904\n",
            "Epoch 2609/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 588.8807 - acc: 0.8074 - val_loss: 588.9637 - val_acc: 0.6904\n",
            "Epoch 2610/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 588.8455 - acc: 0.8197 - val_loss: 588.9344 - val_acc: 0.6883\n",
            "Epoch 2611/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 588.8230 - acc: 0.8074 - val_loss: 588.9052 - val_acc: 0.6883\n",
            "Epoch 2612/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 588.7867 - acc: 0.8115 - val_loss: 588.8759 - val_acc: 0.6883\n",
            "Epoch 2613/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 588.7642 - acc: 0.7889 - val_loss: 588.8464 - val_acc: 0.6883\n",
            "Epoch 2614/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 588.7253 - acc: 0.8197 - val_loss: 588.8172 - val_acc: 0.6883\n",
            "Epoch 2615/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 588.6962 - acc: 0.8156 - val_loss: 588.7878 - val_acc: 0.6883\n",
            "Epoch 2616/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 588.6766 - acc: 0.8074 - val_loss: 588.7585 - val_acc: 0.6883\n",
            "Epoch 2617/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 588.6403 - acc: 0.7992 - val_loss: 588.7292 - val_acc: 0.6883\n",
            "Epoch 2618/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 588.6248 - acc: 0.8053 - val_loss: 588.6999 - val_acc: 0.6883\n",
            "Epoch 2619/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 588.5943 - acc: 0.7971 - val_loss: 588.6706 - val_acc: 0.6883\n",
            "Epoch 2620/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 588.5556 - acc: 0.8074 - val_loss: 588.6413 - val_acc: 0.6904\n",
            "Epoch 2621/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 588.5177 - acc: 0.8320 - val_loss: 588.6119 - val_acc: 0.6904\n",
            "Epoch 2622/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 588.4950 - acc: 0.8299 - val_loss: 588.5827 - val_acc: 0.6904\n",
            "Epoch 2623/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 588.4733 - acc: 0.8176 - val_loss: 588.5534 - val_acc: 0.6904\n",
            "Epoch 2624/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 588.4427 - acc: 0.8012 - val_loss: 588.5241 - val_acc: 0.6904\n",
            "Epoch 2625/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 588.4100 - acc: 0.8074 - val_loss: 588.4947 - val_acc: 0.6904\n",
            "Epoch 2626/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 588.3800 - acc: 0.8074 - val_loss: 588.4653 - val_acc: 0.6904\n",
            "Epoch 2627/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 588.3569 - acc: 0.8074 - val_loss: 588.4361 - val_acc: 0.6904\n",
            "Epoch 2628/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 588.3144 - acc: 0.8094 - val_loss: 588.4069 - val_acc: 0.6904\n",
            "Epoch 2629/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 588.2937 - acc: 0.7992 - val_loss: 588.3776 - val_acc: 0.6904\n",
            "Epoch 2630/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 588.2651 - acc: 0.7910 - val_loss: 588.3480 - val_acc: 0.6904\n",
            "Epoch 2631/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 588.2320 - acc: 0.8033 - val_loss: 588.3188 - val_acc: 0.6904\n",
            "Epoch 2632/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 588.1981 - acc: 0.8258 - val_loss: 588.2895 - val_acc: 0.6904\n",
            "Epoch 2633/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 588.1759 - acc: 0.8135 - val_loss: 588.2603 - val_acc: 0.6904\n",
            "Epoch 2634/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 588.1511 - acc: 0.7930 - val_loss: 588.2310 - val_acc: 0.6904\n",
            "Epoch 2635/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 588.1233 - acc: 0.7787 - val_loss: 588.2017 - val_acc: 0.6904\n",
            "Epoch 2636/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 588.0824 - acc: 0.8320 - val_loss: 588.1723 - val_acc: 0.6904\n",
            "Epoch 2637/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 588.0589 - acc: 0.8053 - val_loss: 588.1431 - val_acc: 0.6904\n",
            "Epoch 2638/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 588.0239 - acc: 0.8217 - val_loss: 588.1138 - val_acc: 0.6904\n",
            "Epoch 2639/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 588.0005 - acc: 0.8053 - val_loss: 588.0845 - val_acc: 0.6904\n",
            "Epoch 2640/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 587.9691 - acc: 0.8115 - val_loss: 588.0552 - val_acc: 0.6925\n",
            "Epoch 2641/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 587.9435 - acc: 0.8074 - val_loss: 588.0259 - val_acc: 0.6925\n",
            "Epoch 2642/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 587.9116 - acc: 0.8135 - val_loss: 587.9966 - val_acc: 0.6925\n",
            "Epoch 2643/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 587.8872 - acc: 0.8012 - val_loss: 587.9673 - val_acc: 0.6904\n",
            "Epoch 2644/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 587.8580 - acc: 0.8033 - val_loss: 587.9380 - val_acc: 0.6883\n",
            "Epoch 2645/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 587.8267 - acc: 0.8135 - val_loss: 587.9087 - val_acc: 0.6883\n",
            "Epoch 2646/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 587.7833 - acc: 0.8156 - val_loss: 587.8794 - val_acc: 0.6883\n",
            "Epoch 2647/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 587.7637 - acc: 0.8361 - val_loss: 587.8503 - val_acc: 0.6883\n",
            "Epoch 2648/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 587.7457 - acc: 0.8033 - val_loss: 587.8210 - val_acc: 0.6904\n",
            "Epoch 2649/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 587.6987 - acc: 0.8217 - val_loss: 587.7916 - val_acc: 0.6904\n",
            "Epoch 2650/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 587.6825 - acc: 0.8156 - val_loss: 587.7623 - val_acc: 0.6883\n",
            "Epoch 2651/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 587.6464 - acc: 0.8074 - val_loss: 587.7331 - val_acc: 0.6883\n",
            "Epoch 2652/3000\n",
            "488/488 [==============================] - 0s 363us/step - loss: 587.6278 - acc: 0.8012 - val_loss: 587.7039 - val_acc: 0.6883\n",
            "Epoch 2653/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 587.5937 - acc: 0.7971 - val_loss: 587.6746 - val_acc: 0.6883\n",
            "Epoch 2654/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 587.5616 - acc: 0.7992 - val_loss: 587.6453 - val_acc: 0.6904\n",
            "Epoch 2655/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 587.5228 - acc: 0.8176 - val_loss: 587.6160 - val_acc: 0.6883\n",
            "Epoch 2656/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 587.5024 - acc: 0.8074 - val_loss: 587.5868 - val_acc: 0.6904\n",
            "Epoch 2657/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 587.4755 - acc: 0.7951 - val_loss: 587.5575 - val_acc: 0.6904\n",
            "Epoch 2658/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 587.4589 - acc: 0.7787 - val_loss: 587.5282 - val_acc: 0.6904\n",
            "Epoch 2659/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 587.4135 - acc: 0.8156 - val_loss: 587.4989 - val_acc: 0.6904\n",
            "Epoch 2660/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 587.3885 - acc: 0.8053 - val_loss: 587.4696 - val_acc: 0.6904\n",
            "Epoch 2661/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 587.3535 - acc: 0.7971 - val_loss: 587.4403 - val_acc: 0.6904\n",
            "Epoch 2662/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 587.3291 - acc: 0.8094 - val_loss: 587.4112 - val_acc: 0.6904\n",
            "Epoch 2663/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 587.3025 - acc: 0.7992 - val_loss: 587.3817 - val_acc: 0.6904\n",
            "Epoch 2664/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 587.2644 - acc: 0.8217 - val_loss: 587.3524 - val_acc: 0.6925\n",
            "Epoch 2665/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 587.2317 - acc: 0.8217 - val_loss: 587.3231 - val_acc: 0.6925\n",
            "Epoch 2666/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 587.1957 - acc: 0.8381 - val_loss: 587.2939 - val_acc: 0.6925\n",
            "Epoch 2667/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 587.1846 - acc: 0.8012 - val_loss: 587.2648 - val_acc: 0.6925\n",
            "Epoch 2668/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 587.1610 - acc: 0.8053 - val_loss: 587.2354 - val_acc: 0.6925\n",
            "Epoch 2669/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 587.1208 - acc: 0.8238 - val_loss: 587.2061 - val_acc: 0.6904\n",
            "Epoch 2670/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 587.0821 - acc: 0.8279 - val_loss: 587.1768 - val_acc: 0.6904\n",
            "Epoch 2671/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 587.0651 - acc: 0.8238 - val_loss: 587.1476 - val_acc: 0.6904\n",
            "Epoch 2672/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 587.0337 - acc: 0.8176 - val_loss: 587.1183 - val_acc: 0.6925\n",
            "Epoch 2673/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 586.9922 - acc: 0.8279 - val_loss: 587.0890 - val_acc: 0.6925\n",
            "Epoch 2674/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 586.9764 - acc: 0.8320 - val_loss: 587.0598 - val_acc: 0.6925\n",
            "Epoch 2675/3000\n",
            "488/488 [==============================] - 0s 405us/step - loss: 586.9375 - acc: 0.8279 - val_loss: 587.0307 - val_acc: 0.6925\n",
            "Epoch 2676/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 586.9173 - acc: 0.8156 - val_loss: 587.0015 - val_acc: 0.6925\n",
            "Epoch 2677/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 586.8971 - acc: 0.8135 - val_loss: 586.9721 - val_acc: 0.6925\n",
            "Epoch 2678/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 586.8616 - acc: 0.8217 - val_loss: 586.9428 - val_acc: 0.6925\n",
            "Epoch 2679/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 586.8301 - acc: 0.8135 - val_loss: 586.9136 - val_acc: 0.6925\n",
            "Epoch 2680/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 586.7959 - acc: 0.8094 - val_loss: 586.8843 - val_acc: 0.6925\n",
            "Epoch 2681/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 586.7658 - acc: 0.8135 - val_loss: 586.8551 - val_acc: 0.6925\n",
            "Epoch 2682/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 586.7305 - acc: 0.8443 - val_loss: 586.8258 - val_acc: 0.6925\n",
            "Epoch 2683/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 586.7163 - acc: 0.7992 - val_loss: 586.7967 - val_acc: 0.6925\n",
            "Epoch 2684/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 586.6827 - acc: 0.7930 - val_loss: 586.7675 - val_acc: 0.6925\n",
            "Epoch 2685/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 586.6567 - acc: 0.8033 - val_loss: 586.7382 - val_acc: 0.6925\n",
            "Epoch 2686/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 586.6245 - acc: 0.8033 - val_loss: 586.7089 - val_acc: 0.6925\n",
            "Epoch 2687/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 586.5908 - acc: 0.8299 - val_loss: 586.6797 - val_acc: 0.6925\n",
            "Epoch 2688/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 586.5701 - acc: 0.8176 - val_loss: 586.6504 - val_acc: 0.6925\n",
            "Epoch 2689/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 586.5317 - acc: 0.8074 - val_loss: 586.6211 - val_acc: 0.6925\n",
            "Epoch 2690/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 586.5147 - acc: 0.8012 - val_loss: 586.5919 - val_acc: 0.6925\n",
            "Epoch 2691/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 586.4854 - acc: 0.8176 - val_loss: 586.5626 - val_acc: 0.6925\n",
            "Epoch 2692/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 586.4490 - acc: 0.8197 - val_loss: 586.5333 - val_acc: 0.6925\n",
            "Epoch 2693/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 586.4169 - acc: 0.7992 - val_loss: 586.5041 - val_acc: 0.6925\n",
            "Epoch 2694/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 586.3916 - acc: 0.8033 - val_loss: 586.4748 - val_acc: 0.6925\n",
            "Epoch 2695/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 586.3557 - acc: 0.8258 - val_loss: 586.4456 - val_acc: 0.6925\n",
            "Epoch 2696/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 586.3270 - acc: 0.8053 - val_loss: 586.4162 - val_acc: 0.6925\n",
            "Epoch 2697/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 586.2986 - acc: 0.8361 - val_loss: 586.3871 - val_acc: 0.6925\n",
            "Epoch 2698/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 586.2700 - acc: 0.8197 - val_loss: 586.3580 - val_acc: 0.6925\n",
            "Epoch 2699/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 586.2364 - acc: 0.8238 - val_loss: 586.3288 - val_acc: 0.6925\n",
            "Epoch 2700/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 586.2179 - acc: 0.8074 - val_loss: 586.2995 - val_acc: 0.6925\n",
            "Epoch 2701/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 586.1892 - acc: 0.8197 - val_loss: 586.2703 - val_acc: 0.6925\n",
            "Epoch 2702/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 586.1556 - acc: 0.8094 - val_loss: 586.2411 - val_acc: 0.6904\n",
            "Epoch 2703/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 586.1275 - acc: 0.8074 - val_loss: 586.2118 - val_acc: 0.6904\n",
            "Epoch 2704/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 586.0895 - acc: 0.8135 - val_loss: 586.1826 - val_acc: 0.6904\n",
            "Epoch 2705/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 586.0695 - acc: 0.8279 - val_loss: 586.1534 - val_acc: 0.6904\n",
            "Epoch 2706/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 586.0448 - acc: 0.8012 - val_loss: 586.1241 - val_acc: 0.6904\n",
            "Epoch 2707/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 585.9973 - acc: 0.8238 - val_loss: 586.0949 - val_acc: 0.6904\n",
            "Epoch 2708/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 585.9777 - acc: 0.8074 - val_loss: 586.0656 - val_acc: 0.6904\n",
            "Epoch 2709/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 585.9503 - acc: 0.8176 - val_loss: 586.0365 - val_acc: 0.6904\n",
            "Epoch 2710/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 585.9220 - acc: 0.8156 - val_loss: 586.0070 - val_acc: 0.6925\n",
            "Epoch 2711/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 585.8968 - acc: 0.8197 - val_loss: 585.9779 - val_acc: 0.6925\n",
            "Epoch 2712/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 585.8622 - acc: 0.7992 - val_loss: 585.9488 - val_acc: 0.6925\n",
            "Epoch 2713/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 585.8301 - acc: 0.8197 - val_loss: 585.9196 - val_acc: 0.6904\n",
            "Epoch 2714/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 585.8029 - acc: 0.8012 - val_loss: 585.8903 - val_acc: 0.6904\n",
            "Epoch 2715/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 585.7834 - acc: 0.7910 - val_loss: 585.8610 - val_acc: 0.6925\n",
            "Epoch 2716/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 585.7342 - acc: 0.8258 - val_loss: 585.8320 - val_acc: 0.6925\n",
            "Epoch 2717/3000\n",
            "488/488 [==============================] - 0s 319us/step - loss: 585.7151 - acc: 0.8197 - val_loss: 585.8026 - val_acc: 0.6904\n",
            "Epoch 2718/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 585.6858 - acc: 0.8217 - val_loss: 585.7735 - val_acc: 0.6925\n",
            "Epoch 2719/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 585.6528 - acc: 0.8279 - val_loss: 585.7442 - val_acc: 0.6925\n",
            "Epoch 2720/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 585.6241 - acc: 0.8258 - val_loss: 585.7150 - val_acc: 0.6925\n",
            "Epoch 2721/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 585.5978 - acc: 0.8156 - val_loss: 585.6858 - val_acc: 0.6925\n",
            "Epoch 2722/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 585.5737 - acc: 0.8074 - val_loss: 585.6566 - val_acc: 0.6925\n",
            "Epoch 2723/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 585.5425 - acc: 0.8115 - val_loss: 585.6275 - val_acc: 0.6925\n",
            "Epoch 2724/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 585.5200 - acc: 0.8074 - val_loss: 585.5982 - val_acc: 0.6925\n",
            "Epoch 2725/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 585.4780 - acc: 0.8074 - val_loss: 585.5690 - val_acc: 0.6925\n",
            "Epoch 2726/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 585.4558 - acc: 0.8156 - val_loss: 585.5398 - val_acc: 0.6925\n",
            "Epoch 2727/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 585.4289 - acc: 0.8074 - val_loss: 585.5107 - val_acc: 0.6925\n",
            "Epoch 2728/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 585.3984 - acc: 0.8094 - val_loss: 585.4815 - val_acc: 0.6925\n",
            "Epoch 2729/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 585.3546 - acc: 0.8217 - val_loss: 585.4522 - val_acc: 0.6925\n",
            "Epoch 2730/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 585.3347 - acc: 0.8156 - val_loss: 585.4230 - val_acc: 0.6925\n",
            "Epoch 2731/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 585.3078 - acc: 0.8094 - val_loss: 585.3938 - val_acc: 0.6925\n",
            "Epoch 2732/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 585.2704 - acc: 0.8258 - val_loss: 585.3646 - val_acc: 0.6904\n",
            "Epoch 2733/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 585.2486 - acc: 0.8217 - val_loss: 585.3354 - val_acc: 0.6904\n",
            "Epoch 2734/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 585.2130 - acc: 0.8299 - val_loss: 585.3061 - val_acc: 0.6904\n",
            "Epoch 2735/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 585.1897 - acc: 0.8258 - val_loss: 585.2771 - val_acc: 0.6904\n",
            "Epoch 2736/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 585.1600 - acc: 0.8320 - val_loss: 585.2479 - val_acc: 0.6925\n",
            "Epoch 2737/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 585.1311 - acc: 0.8156 - val_loss: 585.2187 - val_acc: 0.6904\n",
            "Epoch 2738/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 585.1045 - acc: 0.8135 - val_loss: 585.1895 - val_acc: 0.6904\n",
            "Epoch 2739/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 585.0675 - acc: 0.8156 - val_loss: 585.1604 - val_acc: 0.6925\n",
            "Epoch 2740/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 585.0436 - acc: 0.8135 - val_loss: 585.1312 - val_acc: 0.6925\n",
            "Epoch 2741/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 585.0097 - acc: 0.8279 - val_loss: 585.1021 - val_acc: 0.6904\n",
            "Epoch 2742/3000\n",
            "488/488 [==============================] - 0s 318us/step - loss: 584.9858 - acc: 0.8197 - val_loss: 585.0729 - val_acc: 0.6904\n",
            "Epoch 2743/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 584.9527 - acc: 0.8197 - val_loss: 585.0437 - val_acc: 0.6925\n",
            "Epoch 2744/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 584.9259 - acc: 0.8217 - val_loss: 585.0144 - val_acc: 0.6925\n",
            "Epoch 2745/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 584.8943 - acc: 0.8176 - val_loss: 584.9852 - val_acc: 0.6925\n",
            "Epoch 2746/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 584.8671 - acc: 0.8135 - val_loss: 584.9561 - val_acc: 0.6904\n",
            "Epoch 2747/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 584.8445 - acc: 0.8094 - val_loss: 584.9270 - val_acc: 0.6904\n",
            "Epoch 2748/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 584.8156 - acc: 0.8156 - val_loss: 584.8978 - val_acc: 0.6904\n",
            "Epoch 2749/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 584.7809 - acc: 0.8074 - val_loss: 584.8685 - val_acc: 0.6925\n",
            "Epoch 2750/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 584.7606 - acc: 0.7930 - val_loss: 584.8394 - val_acc: 0.6925\n",
            "Epoch 2751/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 584.7234 - acc: 0.8053 - val_loss: 584.8101 - val_acc: 0.6904\n",
            "Epoch 2752/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 584.6810 - acc: 0.8197 - val_loss: 584.7810 - val_acc: 0.6904\n",
            "Epoch 2753/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 584.6700 - acc: 0.7992 - val_loss: 584.7519 - val_acc: 0.6904\n",
            "Epoch 2754/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 584.6315 - acc: 0.8320 - val_loss: 584.7226 - val_acc: 0.6904\n",
            "Epoch 2755/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 584.6051 - acc: 0.8197 - val_loss: 584.6935 - val_acc: 0.6904\n",
            "Epoch 2756/3000\n",
            "488/488 [==============================] - 0s 374us/step - loss: 584.5733 - acc: 0.8279 - val_loss: 584.6644 - val_acc: 0.6904\n",
            "Epoch 2757/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 584.5446 - acc: 0.8197 - val_loss: 584.6352 - val_acc: 0.6904\n",
            "Epoch 2758/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 584.5176 - acc: 0.8197 - val_loss: 584.6060 - val_acc: 0.6904\n",
            "Epoch 2759/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 584.4966 - acc: 0.8033 - val_loss: 584.5769 - val_acc: 0.6904\n",
            "Epoch 2760/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 584.4504 - acc: 0.8176 - val_loss: 584.5476 - val_acc: 0.6904\n",
            "Epoch 2761/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 584.4388 - acc: 0.7910 - val_loss: 584.5183 - val_acc: 0.6904\n",
            "Epoch 2762/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 584.3921 - acc: 0.8258 - val_loss: 584.4892 - val_acc: 0.6904\n",
            "Epoch 2763/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 584.3680 - acc: 0.8053 - val_loss: 584.4600 - val_acc: 0.6904\n",
            "Epoch 2764/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 584.3365 - acc: 0.8156 - val_loss: 584.4308 - val_acc: 0.6904\n",
            "Epoch 2765/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 584.3117 - acc: 0.7992 - val_loss: 584.4018 - val_acc: 0.6904\n",
            "Epoch 2766/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 584.2876 - acc: 0.8115 - val_loss: 584.3725 - val_acc: 0.6904\n",
            "Epoch 2767/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 584.2562 - acc: 0.8197 - val_loss: 584.3435 - val_acc: 0.6904\n",
            "Epoch 2768/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 584.2357 - acc: 0.7930 - val_loss: 584.3143 - val_acc: 0.6904\n",
            "Epoch 2769/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 584.2113 - acc: 0.7951 - val_loss: 584.2852 - val_acc: 0.6904\n",
            "Epoch 2770/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 584.1656 - acc: 0.8238 - val_loss: 584.2560 - val_acc: 0.6904\n",
            "Epoch 2771/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 584.1306 - acc: 0.8074 - val_loss: 584.2267 - val_acc: 0.6904\n",
            "Epoch 2772/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 584.1111 - acc: 0.8053 - val_loss: 584.1977 - val_acc: 0.6904\n",
            "Epoch 2773/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 584.0825 - acc: 0.8135 - val_loss: 584.1684 - val_acc: 0.6904\n",
            "Epoch 2774/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 584.0471 - acc: 0.8299 - val_loss: 584.1392 - val_acc: 0.6904\n",
            "Epoch 2775/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 584.0265 - acc: 0.8094 - val_loss: 584.1100 - val_acc: 0.6904\n",
            "Epoch 2776/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 583.9997 - acc: 0.8176 - val_loss: 584.0808 - val_acc: 0.6904\n",
            "Epoch 2777/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 583.9629 - acc: 0.8115 - val_loss: 584.0518 - val_acc: 0.6904\n",
            "Epoch 2778/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 583.9370 - acc: 0.8176 - val_loss: 584.0227 - val_acc: 0.6904\n",
            "Epoch 2779/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 583.9064 - acc: 0.8074 - val_loss: 583.9935 - val_acc: 0.6904\n",
            "Epoch 2780/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 583.8745 - acc: 0.8053 - val_loss: 583.9644 - val_acc: 0.6904\n",
            "Epoch 2781/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 583.8426 - acc: 0.8197 - val_loss: 583.9352 - val_acc: 0.6904\n",
            "Epoch 2782/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 583.8146 - acc: 0.8156 - val_loss: 583.9059 - val_acc: 0.6904\n",
            "Epoch 2783/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 583.7914 - acc: 0.8053 - val_loss: 583.8768 - val_acc: 0.6904\n",
            "Epoch 2784/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 583.7564 - acc: 0.8074 - val_loss: 583.8475 - val_acc: 0.6904\n",
            "Epoch 2785/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 583.7300 - acc: 0.8361 - val_loss: 583.8184 - val_acc: 0.6925\n",
            "Epoch 2786/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 583.7039 - acc: 0.8053 - val_loss: 583.7891 - val_acc: 0.6925\n",
            "Epoch 2787/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 583.6723 - acc: 0.7971 - val_loss: 583.7600 - val_acc: 0.6904\n",
            "Epoch 2788/3000\n",
            "488/488 [==============================] - 0s 315us/step - loss: 583.6359 - acc: 0.8340 - val_loss: 583.7308 - val_acc: 0.6904\n",
            "Epoch 2789/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 583.6088 - acc: 0.8258 - val_loss: 583.7016 - val_acc: 0.6904\n",
            "Epoch 2790/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 583.5957 - acc: 0.8012 - val_loss: 583.6725 - val_acc: 0.6904\n",
            "Epoch 2791/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 583.5537 - acc: 0.8320 - val_loss: 583.6434 - val_acc: 0.6904\n",
            "Epoch 2792/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 583.5219 - acc: 0.8299 - val_loss: 583.6142 - val_acc: 0.6925\n",
            "Epoch 2793/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 583.4983 - acc: 0.8135 - val_loss: 583.5852 - val_acc: 0.6904\n",
            "Epoch 2794/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 583.4762 - acc: 0.7992 - val_loss: 583.5560 - val_acc: 0.6904\n",
            "Epoch 2795/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 583.4286 - acc: 0.8402 - val_loss: 583.5270 - val_acc: 0.6904\n",
            "Epoch 2796/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 583.4106 - acc: 0.8238 - val_loss: 583.4978 - val_acc: 0.6904\n",
            "Epoch 2797/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 583.3852 - acc: 0.7869 - val_loss: 583.4688 - val_acc: 0.6904\n",
            "Epoch 2798/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 583.3487 - acc: 0.8176 - val_loss: 583.4395 - val_acc: 0.6904\n",
            "Epoch 2799/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 583.3303 - acc: 0.7930 - val_loss: 583.4104 - val_acc: 0.6904\n",
            "Epoch 2800/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 583.2948 - acc: 0.8033 - val_loss: 583.3812 - val_acc: 0.6904\n",
            "Epoch 2801/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 583.2533 - acc: 0.8422 - val_loss: 583.3522 - val_acc: 0.6904\n",
            "Epoch 2802/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 583.2284 - acc: 0.8258 - val_loss: 583.3231 - val_acc: 0.6904\n",
            "Epoch 2803/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 583.1899 - acc: 0.8320 - val_loss: 583.2939 - val_acc: 0.6904\n",
            "Epoch 2804/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 583.1727 - acc: 0.8156 - val_loss: 583.2648 - val_acc: 0.6904\n",
            "Epoch 2805/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 583.1475 - acc: 0.8217 - val_loss: 583.2358 - val_acc: 0.6904\n",
            "Epoch 2806/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 583.1129 - acc: 0.8340 - val_loss: 583.2066 - val_acc: 0.6904\n",
            "Epoch 2807/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 583.0925 - acc: 0.8176 - val_loss: 583.1776 - val_acc: 0.6904\n",
            "Epoch 2808/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 583.0584 - acc: 0.7951 - val_loss: 583.1484 - val_acc: 0.6904\n",
            "Epoch 2809/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 583.0349 - acc: 0.8176 - val_loss: 583.1193 - val_acc: 0.6904\n",
            "Epoch 2810/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 582.9986 - acc: 0.8197 - val_loss: 583.0902 - val_acc: 0.6904\n",
            "Epoch 2811/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 582.9653 - acc: 0.8258 - val_loss: 583.0610 - val_acc: 0.6904\n",
            "Epoch 2812/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 582.9420 - acc: 0.8094 - val_loss: 583.0319 - val_acc: 0.6904\n",
            "Epoch 2813/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 582.9038 - acc: 0.8361 - val_loss: 583.0028 - val_acc: 0.6904\n",
            "Epoch 2814/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 582.8890 - acc: 0.8115 - val_loss: 582.9738 - val_acc: 0.6904\n",
            "Epoch 2815/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 582.8524 - acc: 0.8238 - val_loss: 582.9446 - val_acc: 0.6904\n",
            "Epoch 2816/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 582.8160 - acc: 0.8443 - val_loss: 582.9155 - val_acc: 0.6904\n",
            "Epoch 2817/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 582.8003 - acc: 0.8094 - val_loss: 582.8864 - val_acc: 0.6904\n",
            "Epoch 2818/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 582.7653 - acc: 0.8279 - val_loss: 582.8572 - val_acc: 0.6904\n",
            "Epoch 2819/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 582.7358 - acc: 0.8197 - val_loss: 582.8283 - val_acc: 0.6904\n",
            "Epoch 2820/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 582.7039 - acc: 0.8279 - val_loss: 582.7993 - val_acc: 0.6904\n",
            "Epoch 2821/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 582.6781 - acc: 0.8197 - val_loss: 582.7701 - val_acc: 0.6904\n",
            "Epoch 2822/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 582.6524 - acc: 0.8279 - val_loss: 582.7410 - val_acc: 0.6904\n",
            "Epoch 2823/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 582.6187 - acc: 0.8238 - val_loss: 582.7119 - val_acc: 0.6904\n",
            "Epoch 2824/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 582.6035 - acc: 0.7992 - val_loss: 582.6829 - val_acc: 0.6904\n",
            "Epoch 2825/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 582.5636 - acc: 0.8176 - val_loss: 582.6538 - val_acc: 0.6904\n",
            "Epoch 2826/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 582.5333 - acc: 0.8094 - val_loss: 582.6246 - val_acc: 0.6904\n",
            "Epoch 2827/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 582.4957 - acc: 0.8422 - val_loss: 582.5954 - val_acc: 0.6904\n",
            "Epoch 2828/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 582.4717 - acc: 0.8299 - val_loss: 582.5664 - val_acc: 0.6904\n",
            "Epoch 2829/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 582.4488 - acc: 0.8033 - val_loss: 582.5373 - val_acc: 0.6904\n",
            "Epoch 2830/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 582.4156 - acc: 0.8217 - val_loss: 582.5083 - val_acc: 0.6904\n",
            "Epoch 2831/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 582.3848 - acc: 0.8238 - val_loss: 582.4792 - val_acc: 0.6904\n",
            "Epoch 2832/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 582.3621 - acc: 0.8115 - val_loss: 582.4501 - val_acc: 0.6904\n",
            "Epoch 2833/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 582.3303 - acc: 0.7910 - val_loss: 582.4211 - val_acc: 0.6904\n",
            "Epoch 2834/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 582.2932 - acc: 0.8340 - val_loss: 582.3919 - val_acc: 0.6904\n",
            "Epoch 2835/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 582.2694 - acc: 0.8197 - val_loss: 582.3628 - val_acc: 0.6904\n",
            "Epoch 2836/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 582.2404 - acc: 0.8238 - val_loss: 582.3336 - val_acc: 0.6904\n",
            "Epoch 2837/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 582.2133 - acc: 0.8238 - val_loss: 582.3046 - val_acc: 0.6904\n",
            "Epoch 2838/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 582.1841 - acc: 0.8422 - val_loss: 582.2755 - val_acc: 0.6904\n",
            "Epoch 2839/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 582.1582 - acc: 0.8115 - val_loss: 582.2463 - val_acc: 0.6904\n",
            "Epoch 2840/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 582.1265 - acc: 0.8197 - val_loss: 582.2171 - val_acc: 0.6904\n",
            "Epoch 2841/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 582.0915 - acc: 0.8525 - val_loss: 582.1882 - val_acc: 0.6904\n",
            "Epoch 2842/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 582.0738 - acc: 0.8258 - val_loss: 582.1590 - val_acc: 0.6904\n",
            "Epoch 2843/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 582.0389 - acc: 0.8422 - val_loss: 582.1300 - val_acc: 0.6904\n",
            "Epoch 2844/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 582.0195 - acc: 0.7971 - val_loss: 582.1010 - val_acc: 0.6904\n",
            "Epoch 2845/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 581.9864 - acc: 0.8176 - val_loss: 582.0718 - val_acc: 0.6904\n",
            "Epoch 2846/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 581.9394 - acc: 0.8094 - val_loss: 582.0428 - val_acc: 0.6925\n",
            "Epoch 2847/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 581.9245 - acc: 0.7992 - val_loss: 582.0137 - val_acc: 0.6925\n",
            "Epoch 2848/3000\n",
            "488/488 [==============================] - 0s 320us/step - loss: 581.8860 - acc: 0.8156 - val_loss: 581.9848 - val_acc: 0.6925\n",
            "Epoch 2849/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 581.8719 - acc: 0.7971 - val_loss: 581.9558 - val_acc: 0.6904\n",
            "Epoch 2850/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 581.8356 - acc: 0.8115 - val_loss: 581.9267 - val_acc: 0.6904\n",
            "Epoch 2851/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 581.8018 - acc: 0.8156 - val_loss: 581.8976 - val_acc: 0.6904\n",
            "Epoch 2852/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 581.7890 - acc: 0.7889 - val_loss: 581.8685 - val_acc: 0.6904\n",
            "Epoch 2853/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 581.7534 - acc: 0.8156 - val_loss: 581.8394 - val_acc: 0.6904\n",
            "Epoch 2854/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 581.7185 - acc: 0.8156 - val_loss: 581.8103 - val_acc: 0.6904\n",
            "Epoch 2855/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 581.6886 - acc: 0.8176 - val_loss: 581.7813 - val_acc: 0.6904\n",
            "Epoch 2856/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 581.6659 - acc: 0.8094 - val_loss: 581.7522 - val_acc: 0.6904\n",
            "Epoch 2857/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 581.6250 - acc: 0.8033 - val_loss: 581.7232 - val_acc: 0.6904\n",
            "Epoch 2858/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 581.6062 - acc: 0.8217 - val_loss: 581.6941 - val_acc: 0.6904\n",
            "Epoch 2859/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 581.5773 - acc: 0.8115 - val_loss: 581.6649 - val_acc: 0.6904\n",
            "Epoch 2860/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 581.5577 - acc: 0.8033 - val_loss: 581.6358 - val_acc: 0.6904\n",
            "Epoch 2861/3000\n",
            "488/488 [==============================] - 0s 356us/step - loss: 581.5217 - acc: 0.8074 - val_loss: 581.6068 - val_acc: 0.6904\n",
            "Epoch 2862/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 581.4883 - acc: 0.7992 - val_loss: 581.5779 - val_acc: 0.6904\n",
            "Epoch 2863/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 581.4534 - acc: 0.8217 - val_loss: 581.5487 - val_acc: 0.6904\n",
            "Epoch 2864/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 581.4256 - acc: 0.8176 - val_loss: 581.5198 - val_acc: 0.6904\n",
            "Epoch 2865/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 581.3972 - acc: 0.8197 - val_loss: 581.4905 - val_acc: 0.6904\n",
            "Epoch 2866/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 581.3766 - acc: 0.8299 - val_loss: 581.4615 - val_acc: 0.6904\n",
            "Epoch 2867/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 581.3327 - acc: 0.8176 - val_loss: 581.4325 - val_acc: 0.6904\n",
            "Epoch 2868/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 581.3155 - acc: 0.8176 - val_loss: 581.4035 - val_acc: 0.6883\n",
            "Epoch 2869/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 581.2842 - acc: 0.8279 - val_loss: 581.3744 - val_acc: 0.6883\n",
            "Epoch 2870/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 581.2472 - acc: 0.8299 - val_loss: 581.3454 - val_acc: 0.6883\n",
            "Epoch 2871/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 581.2325 - acc: 0.8074 - val_loss: 581.3164 - val_acc: 0.6883\n",
            "Epoch 2872/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 581.1824 - acc: 0.8258 - val_loss: 581.2873 - val_acc: 0.6883\n",
            "Epoch 2873/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 581.1705 - acc: 0.8156 - val_loss: 581.2581 - val_acc: 0.6883\n",
            "Epoch 2874/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 581.1355 - acc: 0.8094 - val_loss: 581.2292 - val_acc: 0.6883\n",
            "Epoch 2875/3000\n",
            "488/488 [==============================] - 0s 325us/step - loss: 581.1216 - acc: 0.7848 - val_loss: 581.2001 - val_acc: 0.6883\n",
            "Epoch 2876/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 581.0771 - acc: 0.8156 - val_loss: 581.1711 - val_acc: 0.6883\n",
            "Epoch 2877/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 581.0502 - acc: 0.8279 - val_loss: 581.1420 - val_acc: 0.6883\n",
            "Epoch 2878/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 581.0303 - acc: 0.8197 - val_loss: 581.1129 - val_acc: 0.6883\n",
            "Epoch 2879/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 580.9988 - acc: 0.8176 - val_loss: 581.0837 - val_acc: 0.6883\n",
            "Epoch 2880/3000\n",
            "488/488 [==============================] - 0s 350us/step - loss: 580.9692 - acc: 0.8135 - val_loss: 581.0547 - val_acc: 0.6883\n",
            "Epoch 2881/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 580.9377 - acc: 0.8074 - val_loss: 581.0256 - val_acc: 0.6883\n",
            "Epoch 2882/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 580.8999 - acc: 0.8156 - val_loss: 580.9966 - val_acc: 0.6883\n",
            "Epoch 2883/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 580.8796 - acc: 0.8238 - val_loss: 580.9675 - val_acc: 0.6883\n",
            "Epoch 2884/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 580.8475 - acc: 0.8258 - val_loss: 580.9385 - val_acc: 0.6883\n",
            "Epoch 2885/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 580.8100 - acc: 0.8279 - val_loss: 580.9096 - val_acc: 0.6883\n",
            "Epoch 2886/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 580.7836 - acc: 0.8197 - val_loss: 580.8807 - val_acc: 0.6883\n",
            "Epoch 2887/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 580.7525 - acc: 0.8484 - val_loss: 580.8516 - val_acc: 0.6883\n",
            "Epoch 2888/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 580.7265 - acc: 0.8299 - val_loss: 580.8226 - val_acc: 0.6883\n",
            "Epoch 2889/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 580.7015 - acc: 0.8299 - val_loss: 580.7936 - val_acc: 0.6883\n",
            "Epoch 2890/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 580.6738 - acc: 0.8176 - val_loss: 580.7646 - val_acc: 0.6883\n",
            "Epoch 2891/3000\n",
            "488/488 [==============================] - 0s 335us/step - loss: 580.6374 - acc: 0.8197 - val_loss: 580.7355 - val_acc: 0.6883\n",
            "Epoch 2892/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 580.6240 - acc: 0.8156 - val_loss: 580.7062 - val_acc: 0.6904\n",
            "Epoch 2893/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 580.5897 - acc: 0.8012 - val_loss: 580.6773 - val_acc: 0.6904\n",
            "Epoch 2894/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 580.5604 - acc: 0.8074 - val_loss: 580.6482 - val_acc: 0.6883\n",
            "Epoch 2895/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 580.5292 - acc: 0.8176 - val_loss: 580.6192 - val_acc: 0.6883\n",
            "Epoch 2896/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 580.5022 - acc: 0.8135 - val_loss: 580.5902 - val_acc: 0.6904\n",
            "Epoch 2897/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 580.4752 - acc: 0.8074 - val_loss: 580.5612 - val_acc: 0.6904\n",
            "Epoch 2898/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 580.4430 - acc: 0.8156 - val_loss: 580.5323 - val_acc: 0.6904\n",
            "Epoch 2899/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 580.4078 - acc: 0.8258 - val_loss: 580.5032 - val_acc: 0.6883\n",
            "Epoch 2900/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 580.3774 - acc: 0.8238 - val_loss: 580.4742 - val_acc: 0.6883\n",
            "Epoch 2901/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 580.3427 - acc: 0.8361 - val_loss: 580.4452 - val_acc: 0.6883\n",
            "Epoch 2902/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 580.3197 - acc: 0.8361 - val_loss: 580.4161 - val_acc: 0.6883\n",
            "Epoch 2903/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 580.2932 - acc: 0.8361 - val_loss: 580.3872 - val_acc: 0.6883\n",
            "Epoch 2904/3000\n",
            "488/488 [==============================] - 0s 361us/step - loss: 580.2686 - acc: 0.8012 - val_loss: 580.3582 - val_acc: 0.6883\n",
            "Epoch 2905/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 580.2379 - acc: 0.8156 - val_loss: 580.3291 - val_acc: 0.6883\n",
            "Epoch 2906/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 580.2116 - acc: 0.8238 - val_loss: 580.3001 - val_acc: 0.6883\n",
            "Epoch 2907/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 580.1846 - acc: 0.7971 - val_loss: 580.2710 - val_acc: 0.6883\n",
            "Epoch 2908/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 580.1531 - acc: 0.8135 - val_loss: 580.2421 - val_acc: 0.6883\n",
            "Epoch 2909/3000\n",
            "488/488 [==============================] - 0s 321us/step - loss: 580.1206 - acc: 0.8279 - val_loss: 580.2131 - val_acc: 0.6883\n",
            "Epoch 2910/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 580.0966 - acc: 0.8238 - val_loss: 580.1840 - val_acc: 0.6883\n",
            "Epoch 2911/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 580.0582 - acc: 0.8320 - val_loss: 580.1550 - val_acc: 0.6883\n",
            "Epoch 2912/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 580.0369 - acc: 0.7930 - val_loss: 580.1259 - val_acc: 0.6883\n",
            "Epoch 2913/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 580.0044 - acc: 0.8340 - val_loss: 580.0970 - val_acc: 0.6904\n",
            "Epoch 2914/3000\n",
            "488/488 [==============================] - 0s 323us/step - loss: 579.9742 - acc: 0.8217 - val_loss: 580.0678 - val_acc: 0.6904\n",
            "Epoch 2915/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 579.9500 - acc: 0.8074 - val_loss: 580.0388 - val_acc: 0.6904\n",
            "Epoch 2916/3000\n",
            "488/488 [==============================] - 0s 322us/step - loss: 579.9110 - acc: 0.8176 - val_loss: 580.0099 - val_acc: 0.6904\n",
            "Epoch 2917/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 579.8951 - acc: 0.8258 - val_loss: 579.9808 - val_acc: 0.6904\n",
            "Epoch 2918/3000\n",
            "488/488 [==============================] - 0s 353us/step - loss: 579.8509 - acc: 0.8361 - val_loss: 579.9519 - val_acc: 0.6904\n",
            "Epoch 2919/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 579.8304 - acc: 0.7951 - val_loss: 579.9227 - val_acc: 0.6904\n",
            "Epoch 2920/3000\n",
            "488/488 [==============================] - 0s 319us/step - loss: 579.8040 - acc: 0.8279 - val_loss: 579.8937 - val_acc: 0.6883\n",
            "Epoch 2921/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 579.7774 - acc: 0.8156 - val_loss: 579.8648 - val_acc: 0.6883\n",
            "Epoch 2922/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 579.7351 - acc: 0.8340 - val_loss: 579.8358 - val_acc: 0.6883\n",
            "Epoch 2923/3000\n",
            "488/488 [==============================] - 0s 365us/step - loss: 579.7121 - acc: 0.8361 - val_loss: 579.8068 - val_acc: 0.6883\n",
            "Epoch 2924/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 579.6809 - acc: 0.8381 - val_loss: 579.7777 - val_acc: 0.6904\n",
            "Epoch 2925/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 579.6486 - acc: 0.8258 - val_loss: 579.7487 - val_acc: 0.6904\n",
            "Epoch 2926/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 579.6344 - acc: 0.8156 - val_loss: 579.7198 - val_acc: 0.6904\n",
            "Epoch 2927/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 579.5993 - acc: 0.8033 - val_loss: 579.6908 - val_acc: 0.6904\n",
            "Epoch 2928/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 579.5583 - acc: 0.8402 - val_loss: 579.6617 - val_acc: 0.6904\n",
            "Epoch 2929/3000\n",
            "488/488 [==============================] - 0s 367us/step - loss: 579.5396 - acc: 0.8299 - val_loss: 579.6328 - val_acc: 0.6904\n",
            "Epoch 2930/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 579.5133 - acc: 0.8156 - val_loss: 579.6038 - val_acc: 0.6883\n",
            "Epoch 2931/3000\n",
            "488/488 [==============================] - 0s 347us/step - loss: 579.4891 - acc: 0.8053 - val_loss: 579.5748 - val_acc: 0.6904\n",
            "Epoch 2932/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 579.4627 - acc: 0.8012 - val_loss: 579.5457 - val_acc: 0.6904\n",
            "Epoch 2933/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 579.4211 - acc: 0.8156 - val_loss: 579.5166 - val_acc: 0.6883\n",
            "Epoch 2934/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 579.3986 - acc: 0.8258 - val_loss: 579.4876 - val_acc: 0.6904\n",
            "Epoch 2935/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 579.3530 - acc: 0.8299 - val_loss: 579.4587 - val_acc: 0.6904\n",
            "Epoch 2936/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 579.3431 - acc: 0.8012 - val_loss: 579.4297 - val_acc: 0.6904\n",
            "Epoch 2937/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 579.3035 - acc: 0.8238 - val_loss: 579.4010 - val_acc: 0.6904\n",
            "Epoch 2938/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 579.2671 - acc: 0.8381 - val_loss: 579.3719 - val_acc: 0.6883\n",
            "Epoch 2939/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 579.2583 - acc: 0.8197 - val_loss: 579.3431 - val_acc: 0.6883\n",
            "Epoch 2940/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 579.2180 - acc: 0.8279 - val_loss: 579.3141 - val_acc: 0.6883\n",
            "Epoch 2941/3000\n",
            "488/488 [==============================] - 0s 362us/step - loss: 579.1945 - acc: 0.8279 - val_loss: 579.2851 - val_acc: 0.6883\n",
            "Epoch 2942/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 579.1692 - acc: 0.8156 - val_loss: 579.2560 - val_acc: 0.6883\n",
            "Epoch 2943/3000\n",
            "488/488 [==============================] - 0s 349us/step - loss: 579.1352 - acc: 0.8279 - val_loss: 579.2271 - val_acc: 0.6883\n",
            "Epoch 2944/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 579.1011 - acc: 0.8299 - val_loss: 579.1981 - val_acc: 0.6883\n",
            "Epoch 2945/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 579.0677 - acc: 0.8217 - val_loss: 579.1691 - val_acc: 0.6883\n",
            "Epoch 2946/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 579.0478 - acc: 0.8115 - val_loss: 579.1402 - val_acc: 0.6883\n",
            "Epoch 2947/3000\n",
            "488/488 [==============================] - 0s 351us/step - loss: 579.0158 - acc: 0.8238 - val_loss: 579.1113 - val_acc: 0.6883\n",
            "Epoch 2948/3000\n",
            "488/488 [==============================] - 0s 339us/step - loss: 578.9841 - acc: 0.8238 - val_loss: 579.0824 - val_acc: 0.6883\n",
            "Epoch 2949/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 578.9491 - acc: 0.8381 - val_loss: 579.0535 - val_acc: 0.6883\n",
            "Epoch 2950/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 578.9187 - acc: 0.8422 - val_loss: 579.0246 - val_acc: 0.6883\n",
            "Epoch 2951/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 578.8937 - acc: 0.8238 - val_loss: 578.9956 - val_acc: 0.6883\n",
            "Epoch 2952/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 578.8780 - acc: 0.8238 - val_loss: 578.9664 - val_acc: 0.6904\n",
            "Epoch 2953/3000\n",
            "488/488 [==============================] - 0s 348us/step - loss: 578.8508 - acc: 0.8197 - val_loss: 578.9376 - val_acc: 0.6904\n",
            "Epoch 2954/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 578.8108 - acc: 0.8156 - val_loss: 578.9087 - val_acc: 0.6904\n",
            "Epoch 2955/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 578.7867 - acc: 0.8238 - val_loss: 578.8798 - val_acc: 0.6904\n",
            "Epoch 2956/3000\n",
            "488/488 [==============================] - 0s 354us/step - loss: 578.7558 - acc: 0.8176 - val_loss: 578.8506 - val_acc: 0.6904\n",
            "Epoch 2957/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 578.7167 - acc: 0.8361 - val_loss: 578.8217 - val_acc: 0.6904\n",
            "Epoch 2958/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 578.7007 - acc: 0.8340 - val_loss: 578.7927 - val_acc: 0.6904\n",
            "Epoch 2959/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 578.6695 - acc: 0.8320 - val_loss: 578.7638 - val_acc: 0.6904\n",
            "Epoch 2960/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 578.6445 - acc: 0.8156 - val_loss: 578.7347 - val_acc: 0.6904\n",
            "Epoch 2961/3000\n",
            "488/488 [==============================] - 0s 352us/step - loss: 578.6207 - acc: 0.8074 - val_loss: 578.7058 - val_acc: 0.6904\n",
            "Epoch 2962/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 578.5892 - acc: 0.8258 - val_loss: 578.6769 - val_acc: 0.6904\n",
            "Epoch 2963/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 578.5631 - acc: 0.7992 - val_loss: 578.6480 - val_acc: 0.6904\n",
            "Epoch 2964/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 578.5295 - acc: 0.8156 - val_loss: 578.6191 - val_acc: 0.6904\n",
            "Epoch 2965/3000\n",
            "488/488 [==============================] - 0s 329us/step - loss: 578.4926 - acc: 0.8299 - val_loss: 578.5901 - val_acc: 0.6904\n",
            "Epoch 2966/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 578.4671 - acc: 0.8217 - val_loss: 578.5611 - val_acc: 0.6904\n",
            "Epoch 2967/3000\n",
            "488/488 [==============================] - 0s 341us/step - loss: 578.4377 - acc: 0.8279 - val_loss: 578.5322 - val_acc: 0.6904\n",
            "Epoch 2968/3000\n",
            "488/488 [==============================] - 0s 332us/step - loss: 578.4067 - acc: 0.8217 - val_loss: 578.5034 - val_acc: 0.6904\n",
            "Epoch 2969/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 578.3862 - acc: 0.8197 - val_loss: 578.4744 - val_acc: 0.6904\n",
            "Epoch 2970/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 578.3632 - acc: 0.8074 - val_loss: 578.4454 - val_acc: 0.6904\n",
            "Epoch 2971/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 578.3200 - acc: 0.8135 - val_loss: 578.4164 - val_acc: 0.6904\n",
            "Epoch 2972/3000\n",
            "488/488 [==============================] - 0s 357us/step - loss: 578.3010 - acc: 0.8197 - val_loss: 578.3874 - val_acc: 0.6904\n",
            "Epoch 2973/3000\n",
            "488/488 [==============================] - 0s 333us/step - loss: 578.2713 - acc: 0.8074 - val_loss: 578.3584 - val_acc: 0.6904\n",
            "Epoch 2974/3000\n",
            "488/488 [==============================] - 0s 340us/step - loss: 578.2328 - acc: 0.8217 - val_loss: 578.3296 - val_acc: 0.6904\n",
            "Epoch 2975/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 578.2011 - acc: 0.8340 - val_loss: 578.3006 - val_acc: 0.6904\n",
            "Epoch 2976/3000\n",
            "488/488 [==============================] - 0s 324us/step - loss: 578.1662 - acc: 0.8545 - val_loss: 578.2716 - val_acc: 0.6904\n",
            "Epoch 2977/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 578.1486 - acc: 0.8176 - val_loss: 578.2429 - val_acc: 0.6904\n",
            "Epoch 2978/3000\n",
            "488/488 [==============================] - 0s 358us/step - loss: 578.1232 - acc: 0.8197 - val_loss: 578.2138 - val_acc: 0.6904\n",
            "Epoch 2979/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 578.0847 - acc: 0.8299 - val_loss: 578.1851 - val_acc: 0.6904\n",
            "Epoch 2980/3000\n",
            "488/488 [==============================] - 0s 326us/step - loss: 578.0600 - acc: 0.8279 - val_loss: 578.1560 - val_acc: 0.6904\n",
            "Epoch 2981/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 578.0302 - acc: 0.8340 - val_loss: 578.1272 - val_acc: 0.6904\n",
            "Epoch 2982/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 578.0082 - acc: 0.8197 - val_loss: 578.0983 - val_acc: 0.6904\n",
            "Epoch 2983/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 577.9855 - acc: 0.8074 - val_loss: 578.0694 - val_acc: 0.6904\n",
            "Epoch 2984/3000\n",
            "488/488 [==============================] - 0s 355us/step - loss: 577.9417 - acc: 0.8279 - val_loss: 578.0404 - val_acc: 0.6904\n",
            "Epoch 2985/3000\n",
            "488/488 [==============================] - 0s 343us/step - loss: 577.9168 - acc: 0.8074 - val_loss: 578.0115 - val_acc: 0.6904\n",
            "Epoch 2986/3000\n",
            "488/488 [==============================] - 0s 336us/step - loss: 577.8881 - acc: 0.8115 - val_loss: 577.9825 - val_acc: 0.6904\n",
            "Epoch 2987/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 577.8576 - acc: 0.8176 - val_loss: 577.9537 - val_acc: 0.6904\n",
            "Epoch 2988/3000\n",
            "488/488 [==============================] - 0s 327us/step - loss: 577.8358 - acc: 0.8012 - val_loss: 577.9247 - val_acc: 0.6904\n",
            "Epoch 2989/3000\n",
            "488/488 [==============================] - 0s 342us/step - loss: 577.8045 - acc: 0.8197 - val_loss: 577.8958 - val_acc: 0.6904\n",
            "Epoch 2990/3000\n",
            "488/488 [==============================] - 0s 369us/step - loss: 577.7677 - acc: 0.8156 - val_loss: 577.8669 - val_acc: 0.6904\n",
            "Epoch 2991/3000\n",
            "488/488 [==============================] - 0s 331us/step - loss: 577.7371 - acc: 0.8217 - val_loss: 577.8379 - val_acc: 0.6904\n",
            "Epoch 2992/3000\n",
            "488/488 [==============================] - 0s 334us/step - loss: 577.7142 - acc: 0.8053 - val_loss: 577.8090 - val_acc: 0.6904\n",
            "Epoch 2993/3000\n",
            "488/488 [==============================] - 0s 337us/step - loss: 577.6988 - acc: 0.8197 - val_loss: 577.7800 - val_acc: 0.6904\n",
            "Epoch 2994/3000\n",
            "488/488 [==============================] - 0s 330us/step - loss: 577.6538 - acc: 0.8422 - val_loss: 577.7511 - val_acc: 0.6904\n",
            "Epoch 2995/3000\n",
            "488/488 [==============================] - 0s 338us/step - loss: 577.6188 - acc: 0.8402 - val_loss: 577.7222 - val_acc: 0.6904\n",
            "Epoch 2996/3000\n",
            "488/488 [==============================] - 0s 360us/step - loss: 577.5961 - acc: 0.8320 - val_loss: 577.6934 - val_acc: 0.6904\n",
            "Epoch 2997/3000\n",
            "488/488 [==============================] - 0s 345us/step - loss: 577.5723 - acc: 0.8279 - val_loss: 577.6645 - val_acc: 0.6904\n",
            "Epoch 2998/3000\n",
            "488/488 [==============================] - 0s 344us/step - loss: 577.5455 - acc: 0.8258 - val_loss: 577.6355 - val_acc: 0.6904\n",
            "Epoch 2999/3000\n",
            "488/488 [==============================] - 0s 346us/step - loss: 577.5105 - acc: 0.8176 - val_loss: 577.6067 - val_acc: 0.6904\n",
            "Epoch 3000/3000\n",
            "488/488 [==============================] - 0s 328us/step - loss: 577.4780 - acc: 0.8484 - val_loss: 577.5778 - val_acc: 0.6904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jI-GwpxKZtZu",
        "outputId": "aecfdb0a-e507-40d9-d322-a03b924dafcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "model.evaluate(x=test_x, y=test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "530/530 [==============================] - 0s 89us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[577.7016981592718, 0.5433962267524791]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "72JxjgAAZtZm",
        "outputId": "2383ba21-3fcb-4727-e816-b9e7af266385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.legend(loc='upper right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd4e8365358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T7KP3P98ZtZq",
        "outputId": "066a37be-4194-4bbd-dc8f-04ba316477b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "plt.plot(hist.epoch, hist.history[\"acc\"], label=\"train accuracy\")\n",
        "plt.plot(hist.epoch, hist.history[\"val_acc\"], label=\"Validation accuracy\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd4e82c5be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFYj/B0E7xALP3msVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19VXlRpYyV8f",
        "colab_type": "text"
      },
      "source": [
        "# Observation\n",
        "The best model trained in this notebook is the first model which has an accuracy of ~72%. This model was trained with 50 examples of each class. "
      ]
    }
  ]
}